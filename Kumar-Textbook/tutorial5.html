<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Module 5 - Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="tutorial5_files/libs/clipboard/clipboard.min.js"></script>
<script src="tutorial5_files/libs/quarto-html/quarto.js"></script>
<script src="tutorial5_files/libs/quarto-html/popper.min.js"></script>
<script src="tutorial5_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="tutorial5_files/libs/quarto-html/anchor.min.js"></script>
<link href="tutorial5_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="tutorial5_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="tutorial5_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="tutorial5_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="tutorial5_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#synthetic-data-generation" id="toc-synthetic-data-generation" class="nav-link active" data-scroll-target="#synthetic-data-generation">5.1 Synthetic Data Generation</a></li>
  <li><a href="#multiple-linear-regression" id="toc-multiple-linear-regression" class="nav-link" data-scroll-target="#multiple-linear-regression">5.2 Multiple Linear Regression</a></li>
  <li><a href="#effect-of-correlated-attributes" id="toc-effect-of-correlated-attributes" class="nav-link" data-scroll-target="#effect-of-correlated-attributes">5.3 Effect of Correlated Attributes</a></li>
  <li><a href="#ridge-regression" id="toc-ridge-regression" class="nav-link" data-scroll-target="#ridge-regression">5.4 Ridge Regression</a></li>
  <li><a href="#lasso-regression" id="toc-lasso-regression" class="nav-link" data-scroll-target="#lasso-regression">5.5 Lasso Regression</a></li>
  <li><a href="#hyperparameter-selection-via-cross-validation" id="toc-hyperparameter-selection-via-cross-validation" class="nav-link" data-scroll-target="#hyperparameter-selection-via-cross-validation">5.6 Hyperparameter Selection via Cross-Validation</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">5.7 Summary</a></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Module 5 - Regression</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>The following tutorial contains Python examples for solving regression problems. You should refer to the Appendix chapter on regression of the “Introduction to Data Mining” book to understand some of the concepts introduced in this tutorial. The notebook can be downloaded from http://www.cse.msu.edu/~ptan/dmbook/tutorials/tutorial5/tutorial5.ipynb.</p>
<p>Regression is a modeling technique for predicting quantitative-valued target attributes. The goals for this tutorial are as follows: 1. To provide examples of using different regression methods from the scikit-learn library package. 2. To demonstrate the problem of model overfitting due to correlated attributes in the data. 3. To illustrate how regularization can be used to avoid model overfitting.</p>
<p>Read the step-by-step instructions below carefully. To execute the code, click on the corresponding cell and press the SHIFT-ENTER keys simultaneously.</p>
<section id="synthetic-data-generation" class="level2">
<h2 class="anchored" data-anchor-id="synthetic-data-generation">5.1 Synthetic Data Generation</h2>
<p>To illustrate how linear regression works, we first generate a random 1-dimensional vector of predictor variables, x, from a uniform distribution. The response variable y has a linear relationship with x according to the following equation: y = -3x + 1 + epsilon, where epsilon corresponds to random noise sampled from a Gaussian distribution with mean 0 and standard deviation of 1.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>seed <span class="op">=</span> <span class="dv">1</span>            <span class="co"># seed for random number generation </span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>numInstances <span class="op">=</span> <span class="dv">200</span>  <span class="co"># number of data instances</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>np.random.seed(seed)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.rand(numInstances,<span class="dv">1</span>).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>y_true <span class="op">=</span> <span class="op">-</span><span class="dv">3</span><span class="op">*</span>X <span class="op">+</span> <span class="dv">1</span> </span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> y_true <span class="op">+</span> np.random.normal(size<span class="op">=</span>numInstances).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>plt.scatter(X, y,  color<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>plt.plot(X, y_true, color<span class="op">=</span><span class="st">'blue'</span>, linewidth<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'True function: y = -3X + 1'</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'X'</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'y'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>Text(0, 0.5, 'y')</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="tutorial5_files/figure-html/cell-2-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="multiple-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="multiple-linear-regression">5.2 Multiple Linear Regression</h2>
<p>In this example, we illustrate how to use Python scikit-learn package to fit a multiple linear regression (MLR) model. Given a training set {X,y}, MLR is designed to learn the regression function <span class="math inline">\(f(X,w) = X^T w + w_0\)</span> by minimizing the following loss function given a training set <span class="math inline">\(\{X_i,y_i\}_{i=1}^N\)</span>: <span class="math display">\[L(y,f(X,w)) = \sum_{i=1}^N \|y_i - X_i w - w_0\|^2,\]</span> where <span class="math inline">\(w\)</span> (slope) and <span class="math inline">\(w_0\)</span> (intercept) are the regression coefficients.</p>
<p>Given the input dataset, the following steps are performed: 1. Split the input data into their respective training and test sets. 2. Fit multiple linear regression to the training data. 3. Apply the model to the test data. 4. Evaluate the performance of the model. 5. Postprocessing: Visualizing the fitted model.</p>
<section id="step-1-split-input-data-into-training-and-test-sets" class="level4">
<h4 class="anchored" data-anchor-id="step-1-split-input-data-into-training-and-test-sets">Step 1: Split Input Data into Training and Test Sets</h4>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>numTrain <span class="op">=</span> <span class="dv">20</span>   <span class="co"># number of training instances</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>numTest <span class="op">=</span> numInstances <span class="op">-</span> numTrain</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> X[:<span class="op">-</span>numTest]</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> X[<span class="op">-</span>numTest:]</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> y[:<span class="op">-</span>numTest]</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> y[<span class="op">-</span>numTest:]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="step-2-fit-regression-model-to-training-set" class="level4">
<h4 class="anchored" data-anchor-id="step-2-fit-regression-model-to-training-set">Step 2: Fit Regression Model to Training Set</h4>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> linear_model</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error, r2_score</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create linear regression object</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>regr <span class="op">=</span> linear_model.LinearRegression()</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit regression model to the training set</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>regr.fit(X_train, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked=""><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">LinearRegression</label><div class="sk-toggleable__content"><pre>LinearRegression()</pre></div></div></div></div></div>
</div>
</div>
</section>
<section id="step-3-apply-model-to-the-test-set" class="level4">
<h4 class="anchored" data-anchor-id="step-3-apply-model-to-the-test-set">Step 3: Apply Model to the Test Set</h4>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply model to the test set</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>y_pred_test <span class="op">=</span> regr.predict(X_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="step-4-evaluate-model-performance-on-test-set" class="level4">
<h4 class="anchored" data-anchor-id="step-4-evaluate-model-performance-on-test-set">Step 4: Evaluate Model Performance on Test Set</h4>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Comparing true versus predicted values</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>plt.scatter(y_test, y_pred_test, color<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Comparing true and predicted values for test set'</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'True values for y'</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Predicted values for y'</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Model evaluation</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Root mean squared error = </span><span class="sc">%.4f</span><span class="st">"</span> <span class="op">%</span> np.sqrt(mean_squared_error(y_test, y_pred_test)))</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'R-squared = </span><span class="sc">%.4f</span><span class="st">'</span> <span class="op">%</span> r2_score(y_test, y_pred_test))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Root mean squared error = 1.0476
R-squared = 0.4443</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="tutorial5_files/figure-html/cell-6-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="step-5-postprocessing" class="level4">
<h4 class="anchored" data-anchor-id="step-5-postprocessing">Step 5: Postprocessing</h4>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Display model parameters</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Slope = '</span>, regr.coef_[<span class="dv">0</span>][<span class="dv">0</span>])</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Intercept = '</span>, regr.intercept_[<span class="dv">0</span>])<span class="co">### Step 4: Postprocessing</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot outputs</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_test, y_test,  color<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>plt.plot(X_test, y_pred_test, color<span class="op">=</span><span class="st">'blue'</span>, linewidth<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>titlestr <span class="op">=</span> <span class="st">'Predicted Function: y = </span><span class="sc">%.2f</span><span class="st">X + </span><span class="sc">%.2f</span><span class="st">'</span> <span class="op">%</span> (regr.coef_[<span class="dv">0</span>], regr.intercept_[<span class="dv">0</span>])</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>plt.title(titlestr)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'X'</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'y'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Slope =  -3.242354544656501
Intercept =  1.0805993038584834</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>Text(0, 0.5, 'y')</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="tutorial5_files/figure-html/cell-7-output-3.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="effect-of-correlated-attributes" class="level2">
<h2 class="anchored" data-anchor-id="effect-of-correlated-attributes">5.3 Effect of Correlated Attributes</h2>
<p>In this example, we illustrate how the presence of correlated attributes can affect the performance of regression models. Specifically, we create 4 additional variables, X2, X3, X4, and X5 that are strongly correlated with the previous variable X created in Section 5.1. The relationship between X and y remains the same as before. We then fit y against the predictor variables and compare their training and test set errors.</p>
<p>First, we create the correlated attributes below.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>seed <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(seed)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>X2 <span class="op">=</span> <span class="fl">0.5</span><span class="op">*</span>X <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.04</span>, size<span class="op">=</span>numInstances).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>X3 <span class="op">=</span> <span class="fl">0.5</span><span class="op">*</span>X2 <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.01</span>, size<span class="op">=</span>numInstances).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>X4 <span class="op">=</span> <span class="fl">0.5</span><span class="op">*</span>X3 <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.01</span>, size<span class="op">=</span>numInstances).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>X5 <span class="op">=</span> <span class="fl">0.5</span><span class="op">*</span>X4 <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.01</span>, size<span class="op">=</span>numInstances).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>fig, ((ax1, ax2),(ax3,ax4)) <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>,<span class="dv">9</span>))</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>ax1.scatter(X, X2, color<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'X'</span>)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'X2'</span>)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> np.corrcoef(np.column_stack((X[:<span class="op">-</span>numTest],X2[:<span class="op">-</span>numTest])).T)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>titlestr <span class="op">=</span> <span class="st">'Correlation between X and X2 = </span><span class="sc">%.4f</span><span class="st">'</span> <span class="op">%</span> (c[<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>ax1.set_title(titlestr)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>ax2.scatter(X2, X3, color<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'X2'</span>)</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'X3'</span>)</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> np.corrcoef(np.column_stack((X2[:<span class="op">-</span>numTest],X3[:<span class="op">-</span>numTest])).T)</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>titlestr <span class="op">=</span> <span class="st">'Correlation between X2 and X3 = </span><span class="sc">%.4f</span><span class="st">'</span> <span class="op">%</span> (c[<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>ax2.set_title(titlestr)</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>ax3.scatter(X3, X4, color<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>ax3.set_xlabel(<span class="st">'X3'</span>)</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>ax3.set_ylabel(<span class="st">'X4'</span>)</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> np.corrcoef(np.column_stack((X3[:<span class="op">-</span>numTest],X4[:<span class="op">-</span>numTest])).T)</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>titlestr <span class="op">=</span> <span class="st">'Correlation between X3 and X4 = </span><span class="sc">%.4f</span><span class="st">'</span> <span class="op">%</span> (c[<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>ax3.set_title(titlestr)</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>ax4.scatter(X4, X5, color<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>ax4.set_xlabel(<span class="st">'X4'</span>)</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>ax4.set_ylabel(<span class="st">'X5'</span>)</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> np.corrcoef(np.column_stack((X4[:<span class="op">-</span>numTest],X5[:<span class="op">-</span>numTest])).T)</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>titlestr <span class="op">=</span> <span class="st">'Correlation between X4 and X5 = </span><span class="sc">%.4f</span><span class="st">'</span> <span class="op">%</span> (c[<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>ax4.set_title(titlestr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>Text(0.5, 1.0, 'Correlation between X4 and X5 = 0.9190')</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="tutorial5_files/figure-html/cell-8-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>Next, we create 4 additional versions of the training and test sets. The first version, X_train2 and X_test2 have 2 correlated predictor variables, X and X2. The second version, X_train3 and X_test3 have 3 correlated predictor variables, X, X2, and X3. The third version have 4 correlated variables, X, X2, X3, and X4 whereas the last version have 5 correlated variables, X, X2, X3, X4, and X5.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>X_train2 <span class="op">=</span> np.column_stack((X[:<span class="op">-</span>numTest],X2[:<span class="op">-</span>numTest]))</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>X_test2 <span class="op">=</span> np.column_stack((X[<span class="op">-</span>numTest:],X2[<span class="op">-</span>numTest:]))</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>X_train3 <span class="op">=</span> np.column_stack((X[:<span class="op">-</span>numTest],X2[:<span class="op">-</span>numTest],X3[:<span class="op">-</span>numTest]))</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>X_test3 <span class="op">=</span> np.column_stack((X[<span class="op">-</span>numTest:],X2[<span class="op">-</span>numTest:],X3[<span class="op">-</span>numTest:]))</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>X_train4 <span class="op">=</span> np.column_stack((X[:<span class="op">-</span>numTest],X2[:<span class="op">-</span>numTest],X3[:<span class="op">-</span>numTest],X4[:<span class="op">-</span>numTest]))</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>X_test4 <span class="op">=</span> np.column_stack((X[<span class="op">-</span>numTest:],X2[<span class="op">-</span>numTest:],X3[<span class="op">-</span>numTest:],X4[<span class="op">-</span>numTest:]))</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>X_train5 <span class="op">=</span> np.column_stack((X[:<span class="op">-</span>numTest],X2[:<span class="op">-</span>numTest],X3[:<span class="op">-</span>numTest],X4[:<span class="op">-</span>numTest],X5[:<span class="op">-</span>numTest]))</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>X_test5 <span class="op">=</span> np.column_stack((X[<span class="op">-</span>numTest:],X2[<span class="op">-</span>numTest:],X3[<span class="op">-</span>numTest:],X4[<span class="op">-</span>numTest:],X5[<span class="op">-</span>numTest:]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Below, we train 4 new regression models based on the 4 versions of training and test data created in the previous step.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>regr2 <span class="op">=</span> linear_model.LinearRegression()</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>regr2.fit(X_train2, y_train)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>regr3 <span class="op">=</span> linear_model.LinearRegression()</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>regr3.fit(X_train3, y_train)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>regr4 <span class="op">=</span> linear_model.LinearRegression()</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>regr4.fit(X_train4, y_train)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>regr5 <span class="op">=</span> linear_model.LinearRegression()</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>regr5.fit(X_train5, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-2" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox" checked=""><label for="sk-estimator-id-2" class="sk-toggleable__label sk-toggleable__label-arrow">LinearRegression</label><div class="sk-toggleable__content"><pre>LinearRegression()</pre></div></div></div></div></div>
</div>
</div>
<p>All 4 versions of the regression models are then applied to the training and test sets.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>y_pred_train <span class="op">=</span> regr.predict(X_train)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>y_pred_test <span class="op">=</span> regr.predict(X_test)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>y_pred_train2 <span class="op">=</span> regr2.predict(X_train2)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>y_pred_test2 <span class="op">=</span> regr2.predict(X_test2)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>y_pred_train3 <span class="op">=</span> regr3.predict(X_train3)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>y_pred_test3 <span class="op">=</span> regr3.predict(X_test3)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>y_pred_train4 <span class="op">=</span> regr4.predict(X_train4)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>y_pred_test4 <span class="op">=</span> regr4.predict(X_test4)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>y_pred_train5 <span class="op">=</span> regr5.predict(X_train5)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>y_pred_test5 <span class="op">=</span> regr5.predict(X_test5)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For postprocessing, we compute both the training and test errors of the models. We can also show the resulting model and the sum of the absolute weights of the regression coefficients, i.e., <span class="math inline">\(\sum_{j=0}^d |w_j|\)</span>, where <span class="math inline">\(d\)</span> is the number of predictor attributes.</p>
<div class="cell" data-scrolled="true" data-execution_count="11">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>columns <span class="op">=</span> [<span class="st">'Model'</span>, <span class="st">'Train error'</span>, <span class="st">'Test error'</span>, <span class="st">'Sum of Absolute Weights'</span>]</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>model1 <span class="op">=</span> <span class="st">"</span><span class="sc">%.2f</span><span class="st"> X + </span><span class="sc">%.2f</span><span class="st">"</span> <span class="op">%</span> (regr.coef_[<span class="dv">0</span>][<span class="dv">0</span>], regr.intercept_[<span class="dv">0</span>])</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>values1 <span class="op">=</span> [ model1, np.sqrt(mean_squared_error(y_train, y_pred_train)), </span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>           np.sqrt(mean_squared_error(y_test, y_pred_test)),</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>           np.absolute(regr.coef_[<span class="dv">0</span>]).<span class="bu">sum</span>() <span class="op">+</span> np.absolute(regr.intercept_[<span class="dv">0</span>])]</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>model2 <span class="op">=</span> <span class="st">"</span><span class="sc">%.2f</span><span class="st"> X + </span><span class="sc">%.2f</span><span class="st"> X2 + </span><span class="sc">%.2f</span><span class="st">"</span> <span class="op">%</span> (regr2.coef_[<span class="dv">0</span>][<span class="dv">0</span>], regr2.coef_[<span class="dv">0</span>][<span class="dv">1</span>], regr2.intercept_[<span class="dv">0</span>])</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>values2 <span class="op">=</span> [ model2, np.sqrt(mean_squared_error(y_train, y_pred_train2)), </span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>           np.sqrt(mean_squared_error(y_test, y_pred_test2)),</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>           np.absolute(regr2.coef_[<span class="dv">0</span>]).<span class="bu">sum</span>() <span class="op">+</span> np.absolute(regr2.intercept_[<span class="dv">0</span>])]</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>model3 <span class="op">=</span> <span class="st">"</span><span class="sc">%.2f</span><span class="st"> X + </span><span class="sc">%.2f</span><span class="st"> X2 + </span><span class="sc">%.2f</span><span class="st"> X3 + </span><span class="sc">%.2f</span><span class="st">"</span> <span class="op">%</span> (regr3.coef_[<span class="dv">0</span>][<span class="dv">0</span>], regr3.coef_[<span class="dv">0</span>][<span class="dv">1</span>], </span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>                                                regr3.coef_[<span class="dv">0</span>][<span class="dv">2</span>], regr3.intercept_[<span class="dv">0</span>])</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>values3 <span class="op">=</span> [ model3, np.sqrt(mean_squared_error(y_train, y_pred_train3)), </span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>           np.sqrt(mean_squared_error(y_test, y_pred_test3)),</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>           np.absolute(regr3.coef_[<span class="dv">0</span>]).<span class="bu">sum</span>() <span class="op">+</span> np.absolute(regr3.intercept_[<span class="dv">0</span>])]</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>model4 <span class="op">=</span> <span class="st">"</span><span class="sc">%.2f</span><span class="st"> X + </span><span class="sc">%.2f</span><span class="st"> X2 + </span><span class="sc">%.2f</span><span class="st"> X3 + </span><span class="sc">%.2f</span><span class="st"> X4 + </span><span class="sc">%.2f</span><span class="st">"</span> <span class="op">%</span> (regr4.coef_[<span class="dv">0</span>][<span class="dv">0</span>], regr4.coef_[<span class="dv">0</span>][<span class="dv">1</span>], </span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>                                        regr4.coef_[<span class="dv">0</span>][<span class="dv">2</span>], regr4.coef_[<span class="dv">0</span>][<span class="dv">3</span>], regr4.intercept_[<span class="dv">0</span>])</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>values4 <span class="op">=</span> [ model4, np.sqrt(mean_squared_error(y_train, y_pred_train4)), </span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>           np.sqrt(mean_squared_error(y_test, y_pred_test4)),</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>           np.absolute(regr4.coef_[<span class="dv">0</span>]).<span class="bu">sum</span>() <span class="op">+</span> np.absolute(regr4.intercept_[<span class="dv">0</span>])]</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>model5 <span class="op">=</span> <span class="st">"</span><span class="sc">%.2f</span><span class="st"> X + </span><span class="sc">%.2f</span><span class="st"> X2 + </span><span class="sc">%.2f</span><span class="st"> X3 + </span><span class="sc">%.2f</span><span class="st"> X4 + </span><span class="sc">%.2f</span><span class="st"> X5 + </span><span class="sc">%.2f</span><span class="st">"</span> <span class="op">%</span> (regr5.coef_[<span class="dv">0</span>][<span class="dv">0</span>], </span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>                                        regr5.coef_[<span class="dv">0</span>][<span class="dv">1</span>], regr5.coef_[<span class="dv">0</span>][<span class="dv">2</span>], </span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>                                        regr5.coef_[<span class="dv">0</span>][<span class="dv">3</span>], regr5.coef_[<span class="dv">0</span>][<span class="dv">4</span>], regr5.intercept_[<span class="dv">0</span>])</span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>values5 <span class="op">=</span> [ model5, np.sqrt(mean_squared_error(y_train, y_pred_train5)), </span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a>           np.sqrt(mean_squared_error(y_test, y_pred_test5)),</span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>           np.absolute(regr5.coef_[<span class="dv">0</span>]).<span class="bu">sum</span>() <span class="op">+</span> np.absolute(regr5.intercept_[<span class="dv">0</span>])]</span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> pd.DataFrame([values1, values2, values3, values4, values5], columns<span class="op">=</span>columns)</span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>plt.plot(results[<span class="st">'Sum of Absolute Weights'</span>], results[<span class="st">'Train error'</span>], <span class="st">'ro-'</span>)</span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a>plt.plot(results[<span class="st">'Sum of Absolute Weights'</span>], results[<span class="st">'Test error'</span>], <span class="st">'k*--'</span>)</span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a>plt.legend([<span class="st">'Train error'</span>, <span class="st">'Test error'</span>])</span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Sum of Absolute Weights'</span>)</span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Error rate'</span>)</span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a>results</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>Model</th>
      <th>Train error</th>
      <th>Test error</th>
      <th>Sum of Absolute Weights</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-3.24 X + 1.08</td>
      <td>0.891873</td>
      <td>1.047626</td>
      <td>4.322954</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-5.90 X + 5.92 X2 + 1.00</td>
      <td>0.856157</td>
      <td>1.087601</td>
      <td>12.817040</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-6.22 X + -2.30 X2 + 17.14 X3 + 1.08</td>
      <td>0.834238</td>
      <td>1.094661</td>
      <td>26.744867</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-7.16 X + 0.93 X2 + 8.39 X3 + 11.85 X4 + 1.12</td>
      <td>0.825722</td>
      <td>1.128861</td>
      <td>29.453660</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-7.16 X + 4.50 X2 + 3.52 X3 + -6.55 X4 + 25.68...</td>
      <td>0.799399</td>
      <td>1.146546</td>
      <td>48.614927</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<div class="cell-output cell-output-display">
<p><img src="tutorial5_files/figure-html/cell-12-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>The results above show that the first model, which fits y against X only, has the largest training error, but smallest test error, whereas the fifth model, which fits y against X and other correlated attributes, has the smallest training error but largest test error. This is due to a phenomenon known as model overfitting, in which the low training error of the model does not reflect how well the model will perform on previously unseen test instances. From the plot shown above, observe that the disparity between the training and test errors becomes wider as the sum of absolute weights of the model (which represents the model complexity) increases. Thus, one should control the complexity of the regression model to avoid the model overfitting problem.</p>
</section>
<section id="ridge-regression" class="level2">
<h2 class="anchored" data-anchor-id="ridge-regression">5.4 Ridge Regression</h2>
<p>Ridge regression is a variant of MLR designed to fit a linear model to the dataset by minimizing the following regularized least-square loss function: <span class="math display">\[L_{\textrm{ridge}}(y,f(X,w)) = \sum_{i=1}^N \|y_i - X_iw - w_0\|^2 + \alpha \bigg[\|w\|^2 + w_0^2 \bigg],\]</span> where <span class="math inline">\(\alpha\)</span> is the hyperparameter for ridge regression. Note that the ridge regression model reduces to MLR when <span class="math inline">\(\alpha = 0\)</span>. By increasing the value of <span class="math inline">\(\alpha\)</span>, we can control the complexity of the model as will be shown in the example below.</p>
<p>In the example shown below, we fit a ridge regression model to the previously created training set with correlated attributes. We compare the results of the ridge regression model against those obtained using MLR.</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> linear_model</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>ridge <span class="op">=</span> linear_model.Ridge(alpha<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>ridge.fit(X_train5, y_train)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>y_pred_train_ridge <span class="op">=</span> ridge.predict(X_train5)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>y_pred_test_ridge <span class="op">=</span> ridge.predict(X_test5)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>model6 <span class="op">=</span> <span class="st">"</span><span class="sc">%.2f</span><span class="st"> X + </span><span class="sc">%.2f</span><span class="st"> X2 + </span><span class="sc">%.2f</span><span class="st"> X3 + </span><span class="sc">%.2f</span><span class="st"> X4 + </span><span class="sc">%.2f</span><span class="st"> X5 + </span><span class="sc">%.2f</span><span class="st">"</span> <span class="op">%</span> (ridge.coef_[<span class="dv">0</span>][<span class="dv">0</span>], </span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>                                        ridge.coef_[<span class="dv">0</span>][<span class="dv">1</span>], ridge.coef_[<span class="dv">0</span>][<span class="dv">2</span>], </span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>                                        ridge.coef_[<span class="dv">0</span>][<span class="dv">3</span>], ridge.coef_[<span class="dv">0</span>][<span class="dv">4</span>], ridge.intercept_[<span class="dv">0</span>])</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>values6 <span class="op">=</span> [ model6, np.sqrt(mean_squared_error(y_train, y_pred_train_ridge)), </span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>           np.sqrt(mean_squared_error(y_test, y_pred_test_ridge)),</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>           np.absolute(ridge.coef_[<span class="dv">0</span>]).<span class="bu">sum</span>() <span class="op">+</span> np.absolute(ridge.intercept_[<span class="dv">0</span>])]</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>ridge_results <span class="op">=</span> pd.DataFrame([values6], columns<span class="op">=</span>columns, index<span class="op">=</span>[<span class="st">'Ridge'</span>])</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>pd.concat([results, ridge_results])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>Model</th>
      <th>Train error</th>
      <th>Test error</th>
      <th>Sum of Absolute Weights</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-3.24 X + 1.08</td>
      <td>0.891873</td>
      <td>1.047626</td>
      <td>4.322954</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-5.90 X + 5.92 X2 + 1.00</td>
      <td>0.856157</td>
      <td>1.087601</td>
      <td>12.817040</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-6.22 X + -2.30 X2 + 17.14 X3 + 1.08</td>
      <td>0.834238</td>
      <td>1.094661</td>
      <td>26.744867</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-7.16 X + 0.93 X2 + 8.39 X3 + 11.85 X4 + 1.12</td>
      <td>0.825722</td>
      <td>1.128861</td>
      <td>29.453660</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-7.16 X + 4.50 X2 + 3.52 X3 + -6.55 X4 + 25.68...</td>
      <td>0.799399</td>
      <td>1.146546</td>
      <td>48.614927</td>
    </tr>
    <tr>
      <th>Ridge</th>
      <td>-2.24 X + -0.43 X2 + -0.14 X3 + -0.10 X4 + 0.0...</td>
      <td>0.917456</td>
      <td>1.052388</td>
      <td>3.765759</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>By setting an appropriate value for the hyperparameter, <span class="math inline">\(\alpha\)</span>, we can control the sum of absolute weights, thus producing a test error that is quite comparable to that of MLR without the correlated attributes.</p>
</section>
<section id="lasso-regression" class="level2">
<h2 class="anchored" data-anchor-id="lasso-regression">5.5 Lasso Regression</h2>
<p>One of the limitations of ridge regression is that, although it was able to reduce the regression coefficients associated with the correlated attributes and reduce the effect of model overfitting, the resulting model is still not sparse. Another variation of MLR, called lasso regression, is designed to produce sparser models by imposing an <span class="math inline">\(\ell_1\)</span> regularization on the regression coefficients as shown below: <span class="math display">\[L_{\textrm{lasso}}(y,f(X,w)) = \sum_{i=1}^N \|y_i - X_iw - w_0\|^2 + \alpha \bigg[ \|w\|_1 + |w_0|\bigg]\]</span></p>
<p>The example code below shows the results of applying lasso regression to the previously used correlated dataset.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> linear_model</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>lasso <span class="op">=</span> linear_model.Lasso(alpha<span class="op">=</span><span class="fl">0.02</span>)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>lasso.fit(X_train5, y_train)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>y_pred_train_lasso <span class="op">=</span> lasso.predict(X_train5)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>y_pred_test_lasso <span class="op">=</span> lasso.predict(X_test5)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>model7 <span class="op">=</span> <span class="st">"</span><span class="sc">%.2f</span><span class="st"> X + </span><span class="sc">%.2f</span><span class="st"> X2 + </span><span class="sc">%.2f</span><span class="st"> X3 + </span><span class="sc">%.2f</span><span class="st"> X4 + </span><span class="sc">%.2f</span><span class="st"> X5 + </span><span class="sc">%.2f</span><span class="st">"</span> <span class="op">%</span> (lasso.coef_[<span class="dv">0</span>], </span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>                                        lasso.coef_[<span class="dv">1</span>], lasso.coef_[<span class="dv">2</span>], </span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>                                        lasso.coef_[<span class="dv">3</span>], lasso.coef_[<span class="dv">4</span>], lasso.intercept_[<span class="dv">0</span>])</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>values7 <span class="op">=</span> [ model7, np.sqrt(mean_squared_error(y_train, y_pred_train_lasso)), </span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>           np.sqrt(mean_squared_error(y_test, y_pred_test_lasso)),</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>           np.absolute(lasso.coef_[<span class="dv">0</span>]).<span class="bu">sum</span>() <span class="op">+</span> np.absolute(lasso.intercept_[<span class="dv">0</span>])]</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>lasso_results <span class="op">=</span> pd.DataFrame([values7], columns<span class="op">=</span>columns, index<span class="op">=</span>[<span class="st">'Lasso'</span>])</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>pd.concat([results, ridge_results, lasso_results])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>Model</th>
      <th>Train error</th>
      <th>Test error</th>
      <th>Sum of Absolute Weights</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-3.24 X + 1.08</td>
      <td>0.891873</td>
      <td>1.047626</td>
      <td>4.322954</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-5.90 X + 5.92 X2 + 1.00</td>
      <td>0.856157</td>
      <td>1.087601</td>
      <td>12.817040</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-6.22 X + -2.30 X2 + 17.14 X3 + 1.08</td>
      <td>0.834238</td>
      <td>1.094661</td>
      <td>26.744867</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-7.16 X + 0.93 X2 + 8.39 X3 + 11.85 X4 + 1.12</td>
      <td>0.825722</td>
      <td>1.128861</td>
      <td>29.453660</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-7.16 X + 4.50 X2 + 3.52 X3 + -6.55 X4 + 25.68...</td>
      <td>0.799399</td>
      <td>1.146546</td>
      <td>48.614927</td>
    </tr>
    <tr>
      <th>Ridge</th>
      <td>-2.24 X + -0.43 X2 + -0.14 X3 + -0.10 X4 + 0.0...</td>
      <td>0.917456</td>
      <td>1.052388</td>
      <td>3.765759</td>
    </tr>
    <tr>
      <th>Lasso</th>
      <td>-2.90 X + 0.00 X2 + 0.00 X3 + 0.00 X4 + 0.00 X...</td>
      <td>0.895692</td>
      <td>1.043334</td>
      <td>3.856242</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>Observe that the lasso regression model sets the coefficients for the correlated attributes, X2, X3, X4, and X5 to exactly zero unlike the ridge regression model. As a result, its test error is significantly better than that for ridge regression.</p>
</section>
<section id="hyperparameter-selection-via-cross-validation" class="level2">
<h2 class="anchored" data-anchor-id="hyperparameter-selection-via-cross-validation">5.6 Hyperparameter Selection via Cross-Validation</h2>
<p>While both ridge and lasso regression methods can potentially alleviate the model overfitting problem, one of the challenges is how to select the appropriate hyperparameter value, <span class="math inline">\(\alpha\)</span>. In the examples shown below, we demonstrate examples of using a 5-fold cross-validation method to select the best hyperparameter of the model. More details about the model selection problem and cross-validation method are described in Chapter 3 of the book.</p>
<p>In the first sample code below, we vary the hyperparameter <span class="math inline">\(\alpha\)</span> for ridge regression to a range between 0.2 and 1.0. Using the RidgeCV() function, we can train a model with 5-fold cross-validation and select the best hyperparameter value.</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> linear_model</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>ridge <span class="op">=</span> linear_model.RidgeCV(cv<span class="op">=</span><span class="dv">5</span>,alphas<span class="op">=</span>[<span class="fl">0.2</span>, <span class="fl">0.4</span>, <span class="fl">0.6</span>, <span class="fl">0.8</span>, <span class="fl">1.0</span>])</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>ridge.fit(X_train5, y_train)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>y_pred_train_ridge <span class="op">=</span> ridge.predict(X_train5)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>y_pred_test_ridge <span class="op">=</span> ridge.predict(X_test5)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>model6 <span class="op">=</span> <span class="st">"</span><span class="sc">%.2f</span><span class="st"> X + </span><span class="sc">%.2f</span><span class="st"> X2 + </span><span class="sc">%.2f</span><span class="st"> X3 + </span><span class="sc">%.2f</span><span class="st"> X4 + </span><span class="sc">%.2f</span><span class="st"> X5 + </span><span class="sc">%.2f</span><span class="st">"</span> <span class="op">%</span> (ridge.coef_[<span class="dv">0</span>][<span class="dv">0</span>], </span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>                                        ridge.coef_[<span class="dv">0</span>][<span class="dv">1</span>], ridge.coef_[<span class="dv">0</span>][<span class="dv">2</span>], </span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>                                        ridge.coef_[<span class="dv">0</span>][<span class="dv">3</span>], ridge.coef_[<span class="dv">0</span>][<span class="dv">4</span>], ridge.intercept_[<span class="dv">0</span>])</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>values6 <span class="op">=</span> [ model6, np.sqrt(mean_squared_error(y_train, y_pred_train_ridge)), </span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>           np.sqrt(mean_squared_error(y_test, y_pred_test_ridge)),</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>           np.absolute(ridge.coef_[<span class="dv">0</span>]).<span class="bu">sum</span>() <span class="op">+</span> np.absolute(ridge.intercept_[<span class="dv">0</span>])]</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Selected alpha = </span><span class="sc">%.2f</span><span class="st">"</span> <span class="op">%</span> ridge.alpha_)</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>ridge_results <span class="op">=</span> pd.DataFrame([values6], columns<span class="op">=</span>columns, index<span class="op">=</span>[<span class="st">'RidgeCV'</span>])</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>pd.concat([results, ridge_results])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Selected alpha = 0.20</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="14">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>Model</th>
      <th>Train error</th>
      <th>Test error</th>
      <th>Sum of Absolute Weights</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-3.24 X + 1.08</td>
      <td>0.891873</td>
      <td>1.047626</td>
      <td>4.322954</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-5.90 X + 5.92 X2 + 1.00</td>
      <td>0.856157</td>
      <td>1.087601</td>
      <td>12.817040</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-6.22 X + -2.30 X2 + 17.14 X3 + 1.08</td>
      <td>0.834238</td>
      <td>1.094661</td>
      <td>26.744867</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-7.16 X + 0.93 X2 + 8.39 X3 + 11.85 X4 + 1.12</td>
      <td>0.825722</td>
      <td>1.128861</td>
      <td>29.453660</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-7.16 X + 4.50 X2 + 3.52 X3 + -6.55 X4 + 25.68...</td>
      <td>0.799399</td>
      <td>1.146546</td>
      <td>48.614927</td>
    </tr>
    <tr>
      <th>RidgeCV</th>
      <td>-2.74 X + -0.16 X2 + 0.09 X3 + 0.01 X4 + 0.21 ...</td>
      <td>0.899190</td>
      <td>1.044401</td>
      <td>4.112120</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>In this next example, we illustrate how to apply cross-validation to select the best hyperparameter value for fitting a lasso regression model.</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> linear_model</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>lasso <span class="op">=</span> linear_model.LassoCV(cv<span class="op">=</span><span class="dv">5</span>, alphas<span class="op">=</span>[<span class="fl">0.01</span>, <span class="fl">0.02</span>, <span class="fl">0.05</span>, <span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">1.0</span>])</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>lasso.fit(X_train5, y_train.reshape(y_train.shape[<span class="dv">0</span>]))</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>y_pred_train_lasso <span class="op">=</span> lasso.predict(X_train5)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>y_pred_test_lasso <span class="op">=</span> lasso.predict(X_test5)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>model7 <span class="op">=</span> <span class="st">"</span><span class="sc">%.2f</span><span class="st"> X + </span><span class="sc">%.2f</span><span class="st"> X2 + </span><span class="sc">%.2f</span><span class="st"> X3 + </span><span class="sc">%.2f</span><span class="st"> X4 + </span><span class="sc">%.2f</span><span class="st"> X5 + </span><span class="sc">%.2f</span><span class="st">"</span> <span class="op">%</span> (lasso.coef_[<span class="dv">0</span>], </span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>                                        lasso.coef_[<span class="dv">1</span>], lasso.coef_[<span class="dv">2</span>], </span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>                                        lasso.coef_[<span class="dv">3</span>], lasso.coef_[<span class="dv">4</span>], lasso.intercept_)</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>values7 <span class="op">=</span> [ model7, np.sqrt(mean_squared_error(y_train, y_pred_train_lasso)), </span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>           np.sqrt(mean_squared_error(y_test, y_pred_test_lasso)),</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>           np.absolute(lasso.coef_[<span class="dv">0</span>]).<span class="bu">sum</span>() <span class="op">+</span> np.absolute(lasso.intercept_)]</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Selected alpha = </span><span class="sc">%.2f</span><span class="st">"</span> <span class="op">%</span> lasso.alpha_)</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>lasso_results <span class="op">=</span> pd.DataFrame([values7], columns<span class="op">=</span>columns, index<span class="op">=</span>[<span class="st">'LassoCV'</span>])</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>pd.concat([results, ridge_results, lasso_results])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Selected alpha = 0.01</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="15">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>Model</th>
      <th>Train error</th>
      <th>Test error</th>
      <th>Sum of Absolute Weights</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-3.24 X + 1.08</td>
      <td>0.891873</td>
      <td>1.047626</td>
      <td>4.322954</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-5.90 X + 5.92 X2 + 1.00</td>
      <td>0.856157</td>
      <td>1.087601</td>
      <td>12.817040</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-6.22 X + -2.30 X2 + 17.14 X3 + 1.08</td>
      <td>0.834238</td>
      <td>1.094661</td>
      <td>26.744867</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-7.16 X + 0.93 X2 + 8.39 X3 + 11.85 X4 + 1.12</td>
      <td>0.825722</td>
      <td>1.128861</td>
      <td>29.453660</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-7.16 X + 4.50 X2 + 3.52 X3 + -6.55 X4 + 25.68...</td>
      <td>0.799399</td>
      <td>1.146546</td>
      <td>48.614927</td>
    </tr>
    <tr>
      <th>RidgeCV</th>
      <td>-2.74 X + -0.16 X2 + 0.09 X3 + 0.01 X4 + 0.21 ...</td>
      <td>0.899190</td>
      <td>1.044401</td>
      <td>4.112120</td>
    </tr>
    <tr>
      <th>LassoCV</th>
      <td>-3.07 X + 0.00 X2 + 0.00 X3 + 0.00 X4 + 0.00 X...</td>
      <td>0.892829</td>
      <td>1.043911</td>
      <td>4.089598</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">5.7 Summary</h2>
<p>This section presents example Python code for fitting linear regression models to a dataset. We also illustrate the problem of model overfitting and shows two alternative methods, called ridge and lasso regression, that can help alleviate such problem. While the model overfitting problem shown here is illustrated in the context of correlated attributes, the problem is more general and may arise due to other factors such as noise and other exceptional values in the data.</p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>