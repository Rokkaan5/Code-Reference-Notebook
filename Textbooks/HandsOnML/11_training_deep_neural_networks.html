<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.542">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Chapter 11 – Training Deep Neural Networks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="11_training_deep_neural_networks_files/libs/clipboard/clipboard.min.js"></script>
<script src="11_training_deep_neural_networks_files/libs/quarto-html/quarto.js"></script>
<script src="11_training_deep_neural_networks_files/libs/quarto-html/popper.min.js"></script>
<script src="11_training_deep_neural_networks_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="11_training_deep_neural_networks_files/libs/quarto-html/anchor.min.js"></script>
<link href="11_training_deep_neural_networks_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="11_training_deep_neural_networks_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="11_training_deep_neural_networks_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="11_training_deep_neural_networks_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="11_training_deep_neural_networks_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#setup" id="toc-setup" class="nav-link active" data-scroll-target="#setup">Setup</a></li>
  <li><a href="#vanishingexploding-gradients-problem" id="toc-vanishingexploding-gradients-problem" class="nav-link" data-scroll-target="#vanishingexploding-gradients-problem">Vanishing/Exploding Gradients Problem</a>
  <ul class="collapse">
  <li><a href="#xavier-and-he-initialization" id="toc-xavier-and-he-initialization" class="nav-link" data-scroll-target="#xavier-and-he-initialization">Xavier and He Initialization</a></li>
  <li><a href="#nonsaturating-activation-functions" id="toc-nonsaturating-activation-functions" class="nav-link" data-scroll-target="#nonsaturating-activation-functions">Nonsaturating Activation Functions</a>
  <ul class="collapse">
  <li><a href="#leaky-relu" id="toc-leaky-relu" class="nav-link" data-scroll-target="#leaky-relu">Leaky ReLU</a></li>
  <li><a href="#elu" id="toc-elu" class="nav-link" data-scroll-target="#elu">ELU</a></li>
  <li><a href="#selu" id="toc-selu" class="nav-link" data-scroll-target="#selu">SELU</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#batch-normalization" id="toc-batch-normalization" class="nav-link" data-scroll-target="#batch-normalization">Batch Normalization</a>
  <ul class="collapse">
  <li><a href="#gradient-clipping" id="toc-gradient-clipping" class="nav-link" data-scroll-target="#gradient-clipping">Gradient Clipping</a></li>
  <li><a href="#reusing-pretrained-layers" id="toc-reusing-pretrained-layers" class="nav-link" data-scroll-target="#reusing-pretrained-layers">Reusing Pretrained Layers</a>
  <ul class="collapse">
  <li><a href="#reusing-a-keras-model" id="toc-reusing-a-keras-model" class="nav-link" data-scroll-target="#reusing-a-keras-model">Reusing a Keras model</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#faster-optimizers" id="toc-faster-optimizers" class="nav-link" data-scroll-target="#faster-optimizers">Faster Optimizers</a>
  <ul class="collapse">
  <li><a href="#momentum-optimization" id="toc-momentum-optimization" class="nav-link" data-scroll-target="#momentum-optimization">Momentum optimization</a></li>
  <li><a href="#nesterov-accelerated-gradient" id="toc-nesterov-accelerated-gradient" class="nav-link" data-scroll-target="#nesterov-accelerated-gradient">Nesterov Accelerated Gradient</a></li>
  <li><a href="#adagrad" id="toc-adagrad" class="nav-link" data-scroll-target="#adagrad">AdaGrad</a></li>
  <li><a href="#rmsprop" id="toc-rmsprop" class="nav-link" data-scroll-target="#rmsprop">RMSProp</a></li>
  <li><a href="#adam-optimization" id="toc-adam-optimization" class="nav-link" data-scroll-target="#adam-optimization">Adam Optimization</a></li>
  <li><a href="#adamax-optimization" id="toc-adamax-optimization" class="nav-link" data-scroll-target="#adamax-optimization">Adamax Optimization</a></li>
  <li><a href="#nadam-optimization" id="toc-nadam-optimization" class="nav-link" data-scroll-target="#nadam-optimization">Nadam Optimization</a></li>
  <li><a href="#learning-rate-scheduling" id="toc-learning-rate-scheduling" class="nav-link" data-scroll-target="#learning-rate-scheduling">Learning Rate Scheduling</a>
  <ul class="collapse">
  <li><a href="#power-scheduling" id="toc-power-scheduling" class="nav-link" data-scroll-target="#power-scheduling">Power Scheduling</a></li>
  <li><a href="#exponential-scheduling" id="toc-exponential-scheduling" class="nav-link" data-scroll-target="#exponential-scheduling">Exponential Scheduling</a></li>
  <li><a href="#piecewise-constant-scheduling" id="toc-piecewise-constant-scheduling" class="nav-link" data-scroll-target="#piecewise-constant-scheduling">Piecewise Constant Scheduling</a></li>
  <li><a href="#performance-scheduling" id="toc-performance-scheduling" class="nav-link" data-scroll-target="#performance-scheduling">Performance Scheduling</a></li>
  <li><a href="#tf.keras-schedulers" id="toc-tf.keras-schedulers" class="nav-link" data-scroll-target="#tf.keras-schedulers">tf.keras schedulers</a></li>
  <li><a href="#cycle-scheduling" id="toc-cycle-scheduling" class="nav-link" data-scroll-target="#cycle-scheduling">1Cycle scheduling</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#avoiding-overfitting-through-regularization" id="toc-avoiding-overfitting-through-regularization" class="nav-link" data-scroll-target="#avoiding-overfitting-through-regularization">Avoiding Overfitting Through Regularization</a>
  <ul class="collapse">
  <li><a href="#ell_1-and-ell_2-regularization" id="toc-ell_1-and-ell_2-regularization" class="nav-link" data-scroll-target="#ell_1-and-ell_2-regularization"><span class="math inline">\(\ell_1\)</span> and <span class="math inline">\(\ell_2\)</span> regularization</a></li>
  <li><a href="#dropout" id="toc-dropout" class="nav-link" data-scroll-target="#dropout">Dropout</a></li>
  <li><a href="#alpha-dropout" id="toc-alpha-dropout" class="nav-link" data-scroll-target="#alpha-dropout">Alpha Dropout</a></li>
  <li><a href="#mc-dropout" id="toc-mc-dropout" class="nav-link" data-scroll-target="#mc-dropout">MC Dropout</a></li>
  <li><a href="#max-norm" id="toc-max-norm" class="nav-link" data-scroll-target="#max-norm">Max norm</a></li>
  </ul></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises">Exercises</a>
  <ul class="collapse">
  <li><a href="#to-7." id="toc-to-7." class="nav-link" data-scroll-target="#to-7.">1. to 7.</a></li>
  <li><a href="#deep-learning-on-cifar10" id="toc-deep-learning-on-cifar10" class="nav-link" data-scroll-target="#deep-learning-on-cifar10">8. Deep Learning on CIFAR10</a>
  <ul class="collapse">
  <li><a href="#a." id="toc-a." class="nav-link" data-scroll-target="#a.">a.</a></li>
  <li><a href="#b." id="toc-b." class="nav-link" data-scroll-target="#b.">b.</a></li>
  <li><a href="#c." id="toc-c." class="nav-link" data-scroll-target="#c.">c.</a></li>
  <li><a href="#d." id="toc-d." class="nav-link" data-scroll-target="#d.">d.</a></li>
  <li><a href="#e." id="toc-e." class="nav-link" data-scroll-target="#e.">e.</a></li>
  <li><a href="#f." id="toc-f." class="nav-link" data-scroll-target="#f.">f.</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Chapter 11 – Training Deep Neural Networks</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><em>This notebook contains all the sample code and solutions to the exercises in chapter 11.</em></p>
<section id="setup" class="level1">
<h1>Setup</h1>
<p>First, let’s import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0.</p>
<div id="cell-4" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Python ≥3.5 is required</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> sys.version_info <span class="op">&gt;=</span> (<span class="dv">3</span>, <span class="dv">5</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Scikit-Learn ≥0.20 is required</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> sklearn.__version__ <span class="op">&gt;=</span> <span class="st">"0.20"</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># %tensorflow_version only exists in Colab.</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="op">%</span>tensorflow_version <span class="fl">2.</span><span class="er">x</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">Exception</span>:</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">pass</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># TensorFlow ≥2.0 is required</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> keras</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> tf.__version__ <span class="op">&gt;=</span> <span class="st">"2.0"</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext tensorboard</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Common imports</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="co"># to make this notebook's output stable across runs</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="co"># To plot pretty figures</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>mpl.rc(<span class="st">'axes'</span>, labelsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>mpl.rc(<span class="st">'xtick'</span>, labelsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>mpl.rc(<span class="st">'ytick'</span>, labelsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Where to save the figures</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>PROJECT_ROOT_DIR <span class="op">=</span> <span class="st">"."</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>CHAPTER_ID <span class="op">=</span> <span class="st">"deep"</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>IMAGES_PATH <span class="op">=</span> os.path.join(PROJECT_ROOT_DIR, <span class="st">"images"</span>, CHAPTER_ID)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>os.makedirs(IMAGES_PATH, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> save_fig(fig_id, tight_layout<span class="op">=</span><span class="va">True</span>, fig_extension<span class="op">=</span><span class="st">"png"</span>, resolution<span class="op">=</span><span class="dv">300</span>):</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>    path <span class="op">=</span> os.path.join(IMAGES_PATH, fig_id <span class="op">+</span> <span class="st">"."</span> <span class="op">+</span> fig_extension)</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Saving figure"</span>, fig_id)</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> tight_layout:</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>        plt.tight_layout()</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>    plt.savefig(path, <span class="bu">format</span><span class="op">=</span>fig_extension, dpi<span class="op">=</span>resolution)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="vanishingexploding-gradients-problem" class="level1">
<h1>Vanishing/Exploding Gradients Problem</h1>
<div id="cell-6" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> logit(z):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>z))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-7" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">200</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>], [<span class="dv">0</span>, <span class="dv">0</span>], <span class="st">'k-'</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>], [<span class="dv">1</span>, <span class="dv">1</span>], <span class="st">'k--'</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="dv">0</span>, <span class="dv">0</span>], [<span class="op">-</span><span class="fl">0.2</span>, <span class="fl">1.2</span>], <span class="st">'k-'</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>], [<span class="op">-</span><span class="dv">3</span><span class="op">/</span><span class="dv">4</span>, <span class="dv">7</span><span class="op">/</span><span class="dv">4</span>], <span class="st">'g--'</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>plt.plot(z, logit(z), <span class="st">"b-"</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>props <span class="op">=</span> <span class="bu">dict</span>(facecolor<span class="op">=</span><span class="st">'black'</span>, shrink<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>plt.annotate(<span class="st">'Saturating'</span>, xytext<span class="op">=</span>(<span class="fl">3.5</span>, <span class="fl">0.7</span>), xy<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">1</span>), arrowprops<span class="op">=</span>props, fontsize<span class="op">=</span><span class="dv">14</span>, ha<span class="op">=</span><span class="st">"center"</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>plt.annotate(<span class="st">'Saturating'</span>, xytext<span class="op">=</span>(<span class="op">-</span><span class="fl">3.5</span>, <span class="fl">0.3</span>), xy<span class="op">=</span>(<span class="op">-</span><span class="dv">5</span>, <span class="dv">0</span>), arrowprops<span class="op">=</span>props, fontsize<span class="op">=</span><span class="dv">14</span>, ha<span class="op">=</span><span class="st">"center"</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>plt.annotate(<span class="st">'Linear'</span>, xytext<span class="op">=</span>(<span class="dv">2</span>, <span class="fl">0.2</span>), xy<span class="op">=</span>(<span class="dv">0</span>, <span class="fl">0.5</span>), arrowprops<span class="op">=</span>props, fontsize<span class="op">=</span><span class="dv">14</span>, ha<span class="op">=</span><span class="st">"center"</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Sigmoid activation function"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>plt.axis([<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">1.2</span>])</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>save_fig(<span class="st">"sigmoid_saturation_plot"</span>)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Saving figure sigmoid_saturation_plot</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="11_training_deep_neural_networks_files/figure-html/cell-4-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="xavier-and-he-initialization" class="level2">
<h2 class="anchored" data-anchor-id="xavier-and-he-initialization">Xavier and He Initialization</h2>
<div id="cell-9" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>[name <span class="cf">for</span> name <span class="kw">in</span> <span class="bu">dir</span>(keras.initializers) <span class="cf">if</span> <span class="kw">not</span> name.startswith(<span class="st">"_"</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>['Constant',
 'GlorotNormal',
 'GlorotUniform',
 'HeNormal',
 'HeUniform',
 'Identity',
 'Initializer',
 'LecunNormal',
 'LecunUniform',
 'Ones',
 'Orthogonal',
 'RandomNormal',
 'RandomUniform',
 'TruncatedNormal',
 'VarianceScaling',
 'Zeros',
 'constant',
 'deserialize',
 'get',
 'glorot_normal',
 'glorot_uniform',
 'he_normal',
 'he_uniform',
 'identity',
 'lecun_normal',
 'lecun_uniform',
 'ones',
 'orthogonal',
 'random_normal',
 'random_uniform',
 'serialize',
 'truncated_normal',
 'variance_scaling',
 'zeros']</code></pre>
</div>
</div>
<div id="cell-10" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">"relu"</span>, kernel_initializer<span class="op">=</span><span class="st">"he_normal"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>&lt;tensorflow.python.keras.layers.core.Dense at 0x7ff1b2321690&gt;</code></pre>
</div>
</div>
<div id="cell-11" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>init <span class="op">=</span> keras.initializers.VarianceScaling(scale<span class="op">=</span><span class="fl">2.</span>, mode<span class="op">=</span><span class="st">'fan_avg'</span>,</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>                                          distribution<span class="op">=</span><span class="st">'uniform'</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">"relu"</span>, kernel_initializer<span class="op">=</span>init)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>&lt;tensorflow.python.keras.layers.core.Dense at 0x7ff180a32410&gt;</code></pre>
</div>
</div>
</section>
<section id="nonsaturating-activation-functions" class="level2">
<h2 class="anchored" data-anchor-id="nonsaturating-activation-functions">Nonsaturating Activation Functions</h2>
<section id="leaky-relu" class="level3">
<h3 class="anchored" data-anchor-id="leaky-relu">Leaky ReLU</h3>
<div id="cell-14" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> leaky_relu(z, alpha<span class="op">=</span><span class="fl">0.01</span>):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.maximum(alpha<span class="op">*</span>z, z)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-15" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>plt.plot(z, leaky_relu(z, <span class="fl">0.05</span>), <span class="st">"b-"</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>], [<span class="dv">0</span>, <span class="dv">0</span>], <span class="st">'k-'</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="dv">0</span>, <span class="dv">0</span>], [<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">4.2</span>], <span class="st">'k-'</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>props <span class="op">=</span> <span class="bu">dict</span>(facecolor<span class="op">=</span><span class="st">'black'</span>, shrink<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>plt.annotate(<span class="st">'Leak'</span>, xytext<span class="op">=</span>(<span class="op">-</span><span class="fl">3.5</span>, <span class="fl">0.5</span>), xy<span class="op">=</span>(<span class="op">-</span><span class="dv">5</span>, <span class="op">-</span><span class="fl">0.2</span>), arrowprops<span class="op">=</span>props, fontsize<span class="op">=</span><span class="dv">14</span>, ha<span class="op">=</span><span class="st">"center"</span>)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Leaky ReLU activation function"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>plt.axis([<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="op">-</span><span class="fl">0.5</span>, <span class="fl">4.2</span>])</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>save_fig(<span class="st">"leaky_relu_plot"</span>)</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Saving figure leaky_relu_plot</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="11_training_deep_neural_networks_files/figure-html/cell-9-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-16" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>[m <span class="cf">for</span> m <span class="kw">in</span> <span class="bu">dir</span>(keras.activations) <span class="cf">if</span> <span class="kw">not</span> m.startswith(<span class="st">"_"</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>['deserialize',
 'elu',
 'exponential',
 'gelu',
 'get',
 'hard_sigmoid',
 'linear',
 'relu',
 'selu',
 'serialize',
 'sigmoid',
 'softmax',
 'softplus',
 'softsign',
 'swish',
 'tanh']</code></pre>
</div>
</div>
<div id="cell-17" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>[m <span class="cf">for</span> m <span class="kw">in</span> <span class="bu">dir</span>(keras.layers) <span class="cf">if</span> <span class="st">"relu"</span> <span class="kw">in</span> m.lower()]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>['LeakyReLU', 'PReLU', 'ReLU', 'ThresholdedReLU']</code></pre>
</div>
</div>
<p>Let’s train a neural network on Fashion MNIST using the Leaky ReLU:</p>
<div id="cell-19" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>(X_train_full, y_train_full), (X_test, y_test) <span class="op">=</span> keras.datasets.fashion_mnist.load_data()</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>X_train_full <span class="op">=</span> X_train_full <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> X_test <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>X_valid, X_train <span class="op">=</span> X_train_full[:<span class="dv">5000</span>], X_train_full[<span class="dv">5000</span>:]</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>y_valid, y_train <span class="op">=</span> y_train_full[:<span class="dv">5000</span>], y_train_full[<span class="dv">5000</span>:]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-20" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>tf.random.set_seed(<span class="dv">42</span>)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.models.Sequential([</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    keras.layers.Flatten(input_shape<span class="op">=</span>[<span class="dv">28</span>, <span class="dv">28</span>]),</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">300</span>, kernel_initializer<span class="op">=</span><span class="st">"he_normal"</span>),</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>    keras.layers.LeakyReLU(),</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">100</span>, kernel_initializer<span class="op">=</span><span class="st">"he_normal"</span>),</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    keras.layers.LeakyReLU(),</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">"softmax"</span>)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-21" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>,</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>              optimizer<span class="op">=</span>keras.optimizers.SGD(learning_rate<span class="op">=</span><span class="fl">1e-3</span>),</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-22" class="cell" data-scrolled="true" data-execution_count="14">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(X_train, y_train, epochs<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>                    validation_data<span class="op">=</span>(X_valid, y_valid))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/10
1719/1719 [==============================] - 2s 1ms/step - loss: 1.6314 - accuracy: 0.5054 - val_loss: 0.8886 - val_accuracy: 0.7160
Epoch 2/10
1719/1719 [==============================] - 2s 892us/step - loss: 0.8416 - accuracy: 0.7247 - val_loss: 0.7130 - val_accuracy: 0.7656
Epoch 3/10
1719/1719 [==============================] - 2s 879us/step - loss: 0.7053 - accuracy: 0.7637 - val_loss: 0.6427 - val_accuracy: 0.7898
Epoch 4/10
1719/1719 [==============================] - 2s 883us/step - loss: 0.6325 - accuracy: 0.7908 - val_loss: 0.5900 - val_accuracy: 0.8066
Epoch 5/10
1719/1719 [==============================] - 2s 887us/step - loss: 0.5992 - accuracy: 0.8021 - val_loss: 0.5582 - val_accuracy: 0.8200
Epoch 6/10
1719/1719 [==============================] - 2s 881us/step - loss: 0.5624 - accuracy: 0.8142 - val_loss: 0.5350 - val_accuracy: 0.8238
Epoch 7/10
1719/1719 [==============================] - 2s 892us/step - loss: 0.5379 - accuracy: 0.8217 - val_loss: 0.5157 - val_accuracy: 0.8304
Epoch 8/10
1719/1719 [==============================] - 2s 895us/step - loss: 0.5152 - accuracy: 0.8295 - val_loss: 0.5078 - val_accuracy: 0.8284
Epoch 9/10
1719/1719 [==============================] - 2s 911us/step - loss: 0.5100 - accuracy: 0.8268 - val_loss: 0.4895 - val_accuracy: 0.8390
Epoch 10/10
1719/1719 [==============================] - 2s 897us/step - loss: 0.4918 - accuracy: 0.8340 - val_loss: 0.4817 - val_accuracy: 0.8396</code></pre>
</div>
</div>
<p>Now let’s try PReLU:</p>
<div id="cell-24" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>tf.random.set_seed(<span class="dv">42</span>)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.models.Sequential([</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>    keras.layers.Flatten(input_shape<span class="op">=</span>[<span class="dv">28</span>, <span class="dv">28</span>]),</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">300</span>, kernel_initializer<span class="op">=</span><span class="st">"he_normal"</span>),</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>    keras.layers.PReLU(),</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">100</span>, kernel_initializer<span class="op">=</span><span class="st">"he_normal"</span>),</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>    keras.layers.PReLU(),</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">"softmax"</span>)</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-25" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>,</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>              optimizer<span class="op">=</span>keras.optimizers.SGD(learning_rate<span class="op">=</span><span class="fl">1e-3</span>),</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-26" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(X_train, y_train, epochs<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>                    validation_data<span class="op">=</span>(X_valid, y_valid))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/10
1719/1719 [==============================] - 2s 1ms/step - loss: 1.6969 - accuracy: 0.4974 - val_loss: 0.9255 - val_accuracy: 0.7186
Epoch 2/10
1719/1719 [==============================] - 2s 990us/step - loss: 0.8706 - accuracy: 0.7247 - val_loss: 0.7305 - val_accuracy: 0.7630
Epoch 3/10
1719/1719 [==============================] - 2s 980us/step - loss: 0.7211 - accuracy: 0.7621 - val_loss: 0.6564 - val_accuracy: 0.7882
Epoch 4/10
1719/1719 [==============================] - 2s 985us/step - loss: 0.6447 - accuracy: 0.7879 - val_loss: 0.6003 - val_accuracy: 0.8048
Epoch 5/10
1719/1719 [==============================] - 2s 967us/step - loss: 0.6077 - accuracy: 0.8004 - val_loss: 0.5656 - val_accuracy: 0.8182
Epoch 6/10
1719/1719 [==============================] - 2s 984us/step - loss: 0.5692 - accuracy: 0.8118 - val_loss: 0.5406 - val_accuracy: 0.8236
Epoch 7/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.5428 - accuracy: 0.8194 - val_loss: 0.5196 - val_accuracy: 0.8314
Epoch 8/10
1719/1719 [==============================] - 2s 983us/step - loss: 0.5193 - accuracy: 0.8284 - val_loss: 0.5113 - val_accuracy: 0.8316
Epoch 9/10
1719/1719 [==============================] - 2s 992us/step - loss: 0.5128 - accuracy: 0.8272 - val_loss: 0.4916 - val_accuracy: 0.8378
Epoch 10/10
1719/1719 [==============================] - 2s 988us/step - loss: 0.4941 - accuracy: 0.8314 - val_loss: 0.4826 - val_accuracy: 0.8398</code></pre>
</div>
</div>
</section>
<section id="elu" class="level3">
<h3 class="anchored" data-anchor-id="elu">ELU</h3>
<div id="cell-28" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> elu(z, alpha<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.where(z <span class="op">&lt;</span> <span class="dv">0</span>, alpha <span class="op">*</span> (np.exp(z) <span class="op">-</span> <span class="dv">1</span>), z)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-29" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>plt.plot(z, elu(z), <span class="st">"b-"</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>], [<span class="dv">0</span>, <span class="dv">0</span>], <span class="st">'k-'</span>)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>], [<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>], <span class="st">'k--'</span>)</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="dv">0</span>, <span class="dv">0</span>], [<span class="op">-</span><span class="fl">2.2</span>, <span class="fl">3.2</span>], <span class="st">'k-'</span>)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="vs">r"ELU activation function ($\alpha=1$)"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>plt.axis([<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="op">-</span><span class="fl">2.2</span>, <span class="fl">3.2</span>])</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>save_fig(<span class="st">"elu_plot"</span>)</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Saving figure elu_plot</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="11_training_deep_neural_networks_files/figure-html/cell-20-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Implementing ELU in TensorFlow is trivial, just specify the activation function when building each layer:</p>
<div id="cell-31" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">"elu"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>&lt;tensorflow.python.keras.layers.core.Dense at 0x7ff1b25afad0&gt;</code></pre>
</div>
</div>
</section>
<section id="selu" class="level3">
<h3 class="anchored" data-anchor-id="selu">SELU</h3>
<p>This activation function was proposed in this <a href="https://arxiv.org/pdf/1706.02515.pdf">great paper</a> by Günter Klambauer, Thomas Unterthiner and Andreas Mayr, published in June 2017. During training, a neural network composed exclusively of a stack of dense layers using the SELU activation function and LeCun initialization will self-normalize: the output of each layer will tend to preserve the same mean and variance during training, which solves the vanishing/exploding gradients problem. As a result, this activation function outperforms the other activation functions very significantly for such neural nets, so you should really try it out. Unfortunately, the self-normalizing property of the SELU activation function is easily broken: you cannot use ℓ<sub>1</sub> or ℓ<sub>2</sub> regularization, regular dropout, max-norm, skip connections or other non-sequential topologies (so recurrent neural networks won’t self-normalize). However, in practice it works quite well with sequential CNNs. If you break self-normalization, SELU will not necessarily outperform other activation functions.</p>
<div id="cell-34" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.special <span class="im">import</span> erfc</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="co"># alpha and scale to self normalize with mean 0 and standard deviation 1</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="co"># (see equation 14 in the paper):</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>alpha_0_1 <span class="op">=</span> <span class="op">-</span>np.sqrt(<span class="dv">2</span> <span class="op">/</span> np.pi) <span class="op">/</span> (erfc(<span class="dv">1</span><span class="op">/</span>np.sqrt(<span class="dv">2</span>)) <span class="op">*</span> np.exp(<span class="dv">1</span><span class="op">/</span><span class="dv">2</span>) <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>scale_0_1 <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> erfc(<span class="dv">1</span> <span class="op">/</span> np.sqrt(<span class="dv">2</span>)) <span class="op">*</span> np.sqrt(np.e)) <span class="op">*</span> np.sqrt(<span class="dv">2</span> <span class="op">*</span> np.pi) <span class="op">*</span> (<span class="dv">2</span> <span class="op">*</span> erfc(np.sqrt(<span class="dv">2</span>))<span class="op">*</span>np.e<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> np.pi<span class="op">*</span>erfc(<span class="dv">1</span><span class="op">/</span>np.sqrt(<span class="dv">2</span>))<span class="op">**</span><span class="dv">2</span><span class="op">*</span>np.e <span class="op">-</span> <span class="dv">2</span><span class="op">*</span>(<span class="dv">2</span><span class="op">+</span>np.pi)<span class="op">*</span>erfc(<span class="dv">1</span><span class="op">/</span>np.sqrt(<span class="dv">2</span>))<span class="op">*</span>np.sqrt(np.e)<span class="op">+</span>np.pi<span class="op">+</span><span class="dv">2</span>)<span class="op">**</span>(<span class="op">-</span><span class="dv">1</span><span class="op">/</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-35" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> selu(z, scale<span class="op">=</span>scale_0_1, alpha<span class="op">=</span>alpha_0_1):</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> scale <span class="op">*</span> elu(z, alpha)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-36" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>plt.plot(z, selu(z), <span class="st">"b-"</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>], [<span class="dv">0</span>, <span class="dv">0</span>], <span class="st">'k-'</span>)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>], [<span class="op">-</span><span class="fl">1.758</span>, <span class="op">-</span><span class="fl">1.758</span>], <span class="st">'k--'</span>)</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="dv">0</span>, <span class="dv">0</span>], [<span class="op">-</span><span class="fl">2.2</span>, <span class="fl">3.2</span>], <span class="st">'k-'</span>)</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"SELU activation function"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>plt.axis([<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="op">-</span><span class="fl">2.2</span>, <span class="fl">3.2</span>])</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>save_fig(<span class="st">"selu_plot"</span>)</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Saving figure selu_plot</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="11_training_deep_neural_networks_files/figure-html/cell-24-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>By default, the SELU hyperparameters (<code>scale</code> and <code>alpha</code>) are tuned in such a way that the mean output of each neuron remains close to 0, and the standard deviation remains close to 1 (assuming the inputs are standardized with mean 0 and standard deviation 1 too). Using this activation function, even a 1,000 layer deep neural network preserves roughly mean 0 and standard deviation 1 across all layers, avoiding the exploding/vanishing gradients problem:</p>
<div id="cell-38" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> np.random.normal(size<span class="op">=</span>(<span class="dv">500</span>, <span class="dv">100</span>)) <span class="co"># standardized inputs</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>    W <span class="op">=</span> np.random.normal(size<span class="op">=</span>(<span class="dv">100</span>, <span class="dv">100</span>), scale<span class="op">=</span>np.sqrt(<span class="dv">1</span> <span class="op">/</span> <span class="dv">100</span>)) <span class="co"># LeCun initialization</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> selu(np.dot(Z, W))</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>    means <span class="op">=</span> np.mean(Z, axis<span class="op">=</span><span class="dv">0</span>).mean()</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>    stds <span class="op">=</span> np.std(Z, axis<span class="op">=</span><span class="dv">0</span>).mean()</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> layer <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Layer </span><span class="sc">{}</span><span class="st">: mean </span><span class="sc">{:.2f}</span><span class="st">, std deviation </span><span class="sc">{:.2f}</span><span class="st">"</span>.<span class="bu">format</span>(layer, means, stds))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Layer 0: mean -0.00, std deviation 1.00
Layer 100: mean 0.02, std deviation 0.96
Layer 200: mean 0.01, std deviation 0.90
Layer 300: mean -0.02, std deviation 0.92
Layer 400: mean 0.05, std deviation 0.89
Layer 500: mean 0.01, std deviation 0.93
Layer 600: mean 0.02, std deviation 0.92
Layer 700: mean -0.02, std deviation 0.90
Layer 800: mean 0.05, std deviation 0.83
Layer 900: mean 0.02, std deviation 1.00</code></pre>
</div>
</div>
<p>Using SELU is easy:</p>
<div id="cell-40" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">"selu"</span>,</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>                   kernel_initializer<span class="op">=</span><span class="st">"lecun_normal"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="25">
<pre><code>&lt;tensorflow.python.keras.layers.core.Dense at 0x7ff190832b10&gt;</code></pre>
</div>
</div>
<p>Let’s create a neural net for Fashion MNIST with 100 hidden layers, using the SELU activation function:</p>
<div id="cell-42" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>tf.random.set_seed(<span class="dv">42</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-43" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.models.Sequential()</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>model.add(keras.layers.Flatten(input_shape<span class="op">=</span>[<span class="dv">28</span>, <span class="dv">28</span>]))</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>model.add(keras.layers.Dense(<span class="dv">300</span>, activation<span class="op">=</span><span class="st">"selu"</span>,</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>                             kernel_initializer<span class="op">=</span><span class="st">"lecun_normal"</span>))</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">99</span>):</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>    model.add(keras.layers.Dense(<span class="dv">100</span>, activation<span class="op">=</span><span class="st">"selu"</span>,</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>                                 kernel_initializer<span class="op">=</span><span class="st">"lecun_normal"</span>))</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>model.add(keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">"softmax"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-44" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>,</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>              optimizer<span class="op">=</span>keras.optimizers.SGD(learning_rate<span class="op">=</span><span class="fl">1e-3</span>),</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now let’s train it. Do not forget to scale the inputs to mean 0 and standard deviation 1:</p>
<div id="cell-46" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>pixel_means <span class="op">=</span> X_train.mean(axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>pixel_stds <span class="op">=</span> X_train.std(axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>X_train_scaled <span class="op">=</span> (X_train <span class="op">-</span> pixel_means) <span class="op">/</span> pixel_stds</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>X_valid_scaled <span class="op">=</span> (X_valid <span class="op">-</span> pixel_means) <span class="op">/</span> pixel_stds</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>X_test_scaled <span class="op">=</span> (X_test <span class="op">-</span> pixel_means) <span class="op">/</span> pixel_stds</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-47" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(X_train_scaled, y_train, epochs<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>                    validation_data<span class="op">=</span>(X_valid_scaled, y_valid))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/5
1719/1719 [==============================] - 12s 6ms/step - loss: 1.3556 - accuracy: 0.4808 - val_loss: 0.7711 - val_accuracy: 0.6858
Epoch 2/5
1719/1719 [==============================] - 9s 5ms/step - loss: 0.7537 - accuracy: 0.7235 - val_loss: 0.7534 - val_accuracy: 0.7384
Epoch 3/5
1719/1719 [==============================] - 9s 5ms/step - loss: 0.7451 - accuracy: 0.7357 - val_loss: 0.5943 - val_accuracy: 0.7834
Epoch 4/5
1719/1719 [==============================] - 9s 5ms/step - loss: 0.5699 - accuracy: 0.7906 - val_loss: 0.5434 - val_accuracy: 0.8066
Epoch 5/5
1719/1719 [==============================] - 9s 5ms/step - loss: 0.5569 - accuracy: 0.8051 - val_loss: 0.4907 - val_accuracy: 0.8218</code></pre>
</div>
</div>
<p>Now look at what happens if we try to use the ReLU activation function instead:</p>
<div id="cell-49" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>tf.random.set_seed(<span class="dv">42</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-50" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.models.Sequential()</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>model.add(keras.layers.Flatten(input_shape<span class="op">=</span>[<span class="dv">28</span>, <span class="dv">28</span>]))</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>model.add(keras.layers.Dense(<span class="dv">300</span>, activation<span class="op">=</span><span class="st">"relu"</span>, kernel_initializer<span class="op">=</span><span class="st">"he_normal"</span>))</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">99</span>):</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>    model.add(keras.layers.Dense(<span class="dv">100</span>, activation<span class="op">=</span><span class="st">"relu"</span>, kernel_initializer<span class="op">=</span><span class="st">"he_normal"</span>))</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>model.add(keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">"softmax"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-51" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>,</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>              optimizer<span class="op">=</span>keras.optimizers.SGD(learning_rate<span class="op">=</span><span class="fl">1e-3</span>),</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-52" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(X_train_scaled, y_train, epochs<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>                    validation_data<span class="op">=</span>(X_valid_scaled, y_valid))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/5
1719/1719 [==============================] - 11s 5ms/step - loss: 2.0460 - accuracy: 0.1919 - val_loss: 1.5971 - val_accuracy: 0.3048
Epoch 2/5
1719/1719 [==============================] - 8s 5ms/step - loss: 1.2654 - accuracy: 0.4591 - val_loss: 0.9156 - val_accuracy: 0.6372
Epoch 3/5
1719/1719 [==============================] - 8s 5ms/step - loss: 0.9312 - accuracy: 0.6169 - val_loss: 0.8928 - val_accuracy: 0.6246
Epoch 4/5
1719/1719 [==============================] - 8s 5ms/step - loss: 0.8188 - accuracy: 0.6710 - val_loss: 0.6914 - val_accuracy: 0.7396
Epoch 5/5
1719/1719 [==============================] - 8s 5ms/step - loss: 0.7288 - accuracy: 0.7152 - val_loss: 0.6638 - val_accuracy: 0.7380</code></pre>
</div>
</div>
<p>Not great at all, we suffered from the vanishing/exploding gradients problem.</p>
</section>
</section>
</section>
<section id="batch-normalization" class="level1">
<h1>Batch Normalization</h1>
<div id="cell-55" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.models.Sequential([</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>    keras.layers.Flatten(input_shape<span class="op">=</span>[<span class="dv">28</span>, <span class="dv">28</span>]),</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>    keras.layers.BatchNormalization(),</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">300</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>    keras.layers.BatchNormalization(),</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">100</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>    keras.layers.BatchNormalization(),</span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">"softmax"</span>)</span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-56" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>model.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "sequential_4"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_4 (Flatten)          (None, 784)               0         
_________________________________________________________________
batch_normalization (BatchNo (None, 784)               3136      
_________________________________________________________________
dense_212 (Dense)            (None, 300)               235500    
_________________________________________________________________
batch_normalization_1 (Batch (None, 300)               1200      
_________________________________________________________________
dense_213 (Dense)            (None, 100)               30100     
_________________________________________________________________
batch_normalization_2 (Batch (None, 100)               400       
_________________________________________________________________
dense_214 (Dense)            (None, 10)                1010      
=================================================================
Total params: 271,346
Trainable params: 268,978
Non-trainable params: 2,368
_________________________________________________________________</code></pre>
</div>
</div>
<div id="cell-57" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>bn1 <span class="op">=</span> model.layers[<span class="dv">1</span>]</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>[(var.name, var.trainable) <span class="cf">for</span> var <span class="kw">in</span> bn1.variables]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="37">
<pre><code>[('batch_normalization/gamma:0', True),
 ('batch_normalization/beta:0', True),
 ('batch_normalization/moving_mean:0', False),
 ('batch_normalization/moving_variance:0', False)]</code></pre>
</div>
</div>
<div id="cell-58" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co">#bn1.updates #deprecated</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-59" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>,</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>              optimizer<span class="op">=</span>keras.optimizers.SGD(learning_rate<span class="op">=</span><span class="fl">1e-3</span>),</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-60" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(X_train, y_train, epochs<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>                    validation_data<span class="op">=</span>(X_valid, y_valid))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/10
1719/1719 [==============================] - 3s 1ms/step - loss: 1.2287 - accuracy: 0.5993 - val_loss: 0.5526 - val_accuracy: 0.8230
Epoch 2/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.5996 - accuracy: 0.7959 - val_loss: 0.4725 - val_accuracy: 0.8468
Epoch 3/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.5312 - accuracy: 0.8168 - val_loss: 0.4375 - val_accuracy: 0.8558
Epoch 4/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.4884 - accuracy: 0.8294 - val_loss: 0.4153 - val_accuracy: 0.8596
Epoch 5/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.4717 - accuracy: 0.8343 - val_loss: 0.3997 - val_accuracy: 0.8640
Epoch 6/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.4420 - accuracy: 0.8461 - val_loss: 0.3867 - val_accuracy: 0.8694
Epoch 7/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.4285 - accuracy: 0.8496 - val_loss: 0.3763 - val_accuracy: 0.8710
Epoch 8/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.4086 - accuracy: 0.8552 - val_loss: 0.3711 - val_accuracy: 0.8740
Epoch 9/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.4079 - accuracy: 0.8566 - val_loss: 0.3631 - val_accuracy: 0.8752
Epoch 10/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.3903 - accuracy: 0.8617 - val_loss: 0.3573 - val_accuracy: 0.8750</code></pre>
</div>
</div>
<p>Sometimes applying BN before the activation function works better (there’s a debate on this topic). Moreover, the layer before a <code>BatchNormalization</code> layer does not need to have bias terms, since the <code>BatchNormalization</code> layer has some as well, it would be a waste of parameters, so you can set <code>use_bias=False</code> when creating those layers:</p>
<div id="cell-62" class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.models.Sequential([</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>    keras.layers.Flatten(input_shape<span class="op">=</span>[<span class="dv">28</span>, <span class="dv">28</span>]),</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>    keras.layers.BatchNormalization(),</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">300</span>, use_bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>    keras.layers.BatchNormalization(),</span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>    keras.layers.Activation(<span class="st">"relu"</span>),</span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">100</span>, use_bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>    keras.layers.BatchNormalization(),</span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a>    keras.layers.Activation(<span class="st">"relu"</span>),</span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">"softmax"</span>)</span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-63" class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>,</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>              optimizer<span class="op">=</span>keras.optimizers.SGD(learning_rate<span class="op">=</span><span class="fl">1e-3</span>),</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-64" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(X_train, y_train, epochs<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>                    validation_data<span class="op">=</span>(X_valid, y_valid))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/10
1719/1719 [==============================] - 3s 1ms/step - loss: 1.3677 - accuracy: 0.5604 - val_loss: 0.6767 - val_accuracy: 0.7812
Epoch 2/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.7136 - accuracy: 0.7702 - val_loss: 0.5566 - val_accuracy: 0.8184
Epoch 3/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.6123 - accuracy: 0.7990 - val_loss: 0.5007 - val_accuracy: 0.8360
Epoch 4/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.5547 - accuracy: 0.8148 - val_loss: 0.4666 - val_accuracy: 0.8448
Epoch 5/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.5255 - accuracy: 0.8230 - val_loss: 0.4434 - val_accuracy: 0.8534
Epoch 6/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.4947 - accuracy: 0.8328 - val_loss: 0.4263 - val_accuracy: 0.8550
Epoch 7/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.4736 - accuracy: 0.8385 - val_loss: 0.4130 - val_accuracy: 0.8566
Epoch 8/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.4550 - accuracy: 0.8446 - val_loss: 0.4035 - val_accuracy: 0.8612
Epoch 9/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.4495 - accuracy: 0.8440 - val_loss: 0.3943 - val_accuracy: 0.8638
Epoch 10/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.4333 - accuracy: 0.8494 - val_loss: 0.3875 - val_accuracy: 0.8660</code></pre>
</div>
</div>
<section id="gradient-clipping" class="level2">
<h2 class="anchored" data-anchor-id="gradient-clipping">Gradient Clipping</h2>
<p>All Keras optimizers accept <code>clipnorm</code> or <code>clipvalue</code> arguments:</p>
<div id="cell-67" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> keras.optimizers.SGD(clipvalue<span class="op">=</span><span class="fl">1.0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-68" class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> keras.optimizers.SGD(clipnorm<span class="op">=</span><span class="fl">1.0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="reusing-pretrained-layers" class="level2">
<h2 class="anchored" data-anchor-id="reusing-pretrained-layers">Reusing Pretrained Layers</h2>
<section id="reusing-a-keras-model" class="level3">
<h3 class="anchored" data-anchor-id="reusing-a-keras-model">Reusing a Keras model</h3>
<p>Let’s split the fashion MNIST training set in two: * <code>X_train_A</code>: all images of all items except for sandals and shirts (classes 5 and 6). * <code>X_train_B</code>: a much smaller training set of just the first 200 images of sandals or shirts.</p>
<p>The validation set and the test set are also split this way, but without restricting the number of images.</p>
<p>We will train a model on set A (classification task with 8 classes), and try to reuse it to tackle set B (binary classification). We hope to transfer a little bit of knowledge from task A to task B, since classes in set A (sneakers, ankle boots, coats, t-shirts, etc.) are somewhat similar to classes in set B (sandals and shirts). However, since we are using <code>Dense</code> layers, only patterns that occur at the same location can be reused (in contrast, convolutional layers will transfer much better, since learned patterns can be detected anywhere on the image, as we will see in the CNN chapter).</p>
<div id="cell-72" class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> split_dataset(X, y):</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>    y_5_or_6 <span class="op">=</span> (y <span class="op">==</span> <span class="dv">5</span>) <span class="op">|</span> (y <span class="op">==</span> <span class="dv">6</span>) <span class="co"># sandals or shirts</span></span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>    y_A <span class="op">=</span> y[<span class="op">~</span>y_5_or_6]</span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a>    y_A[y_A <span class="op">&gt;</span> <span class="dv">6</span>] <span class="op">-=</span> <span class="dv">2</span> <span class="co"># class indices 7, 8, 9 should be moved to 5, 6, 7</span></span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>    y_B <span class="op">=</span> (y[y_5_or_6] <span class="op">==</span> <span class="dv">6</span>).astype(np.float32) <span class="co"># binary classification task: is it a shirt (class 6)?</span></span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ((X[<span class="op">~</span>y_5_or_6], y_A),</span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a>            (X[y_5_or_6], y_B))</span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-9"><a href="#cb66-9" aria-hidden="true" tabindex="-1"></a>(X_train_A, y_train_A), (X_train_B, y_train_B) <span class="op">=</span> split_dataset(X_train, y_train)</span>
<span id="cb66-10"><a href="#cb66-10" aria-hidden="true" tabindex="-1"></a>(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) <span class="op">=</span> split_dataset(X_valid, y_valid)</span>
<span id="cb66-11"><a href="#cb66-11" aria-hidden="true" tabindex="-1"></a>(X_test_A, y_test_A), (X_test_B, y_test_B) <span class="op">=</span> split_dataset(X_test, y_test)</span>
<span id="cb66-12"><a href="#cb66-12" aria-hidden="true" tabindex="-1"></a>X_train_B <span class="op">=</span> X_train_B[:<span class="dv">200</span>]</span>
<span id="cb66-13"><a href="#cb66-13" aria-hidden="true" tabindex="-1"></a>y_train_B <span class="op">=</span> y_train_B[:<span class="dv">200</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-73" class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>X_train_A.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="47">
<pre><code>(43986, 28, 28)</code></pre>
</div>
</div>
<div id="cell-74" class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>X_train_B.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="48">
<pre><code>(200, 28, 28)</code></pre>
</div>
</div>
<div id="cell-75" class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>y_train_A[:<span class="dv">30</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="49">
<pre><code>array([4, 0, 5, 7, 7, 7, 4, 4, 3, 4, 0, 1, 6, 3, 4, 3, 2, 6, 5, 3, 4, 5,
       1, 3, 4, 2, 0, 6, 7, 1], dtype=uint8)</code></pre>
</div>
</div>
<div id="cell-76" class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>y_train_B[:<span class="dv">30</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="50">
<pre><code>array([1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.,
       0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1.], dtype=float32)</code></pre>
</div>
</div>
<div id="cell-77" class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>tf.random.set_seed(<span class="dv">42</span>)</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-78" class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>model_A <span class="op">=</span> keras.models.Sequential()</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>model_A.add(keras.layers.Flatten(input_shape<span class="op">=</span>[<span class="dv">28</span>, <span class="dv">28</span>]))</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n_hidden <span class="kw">in</span> (<span class="dv">300</span>, <span class="dv">100</span>, <span class="dv">50</span>, <span class="dv">50</span>, <span class="dv">50</span>):</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>    model_A.add(keras.layers.Dense(n_hidden, activation<span class="op">=</span><span class="st">"selu"</span>))</span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>model_A.add(keras.layers.Dense(<span class="dv">8</span>, activation<span class="op">=</span><span class="st">"softmax"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-79" class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>model_A.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>,</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>                optimizer<span class="op">=</span>keras.optimizers.SGD(learning_rate<span class="op">=</span><span class="fl">1e-3</span>),</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>                metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-80" class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model_A.fit(X_train_A, y_train_A, epochs<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>                    validation_data<span class="op">=</span>(X_valid_A, y_valid_A))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/20
1375/1375 [==============================] - 2s 1ms/step - loss: 0.9249 - accuracy: 0.6994 - val_loss: 0.3896 - val_accuracy: 0.8662
Epoch 2/20
1375/1375 [==============================] - 1s 1ms/step - loss: 0.3651 - accuracy: 0.8745 - val_loss: 0.3288 - val_accuracy: 0.8827
Epoch 3/20
1375/1375 [==============================] - 1s 989us/step - loss: 0.3182 - accuracy: 0.8897 - val_loss: 0.3013 - val_accuracy: 0.8991
Epoch 4/20
1375/1375 [==============================] - 1s 1ms/step - loss: 0.3048 - accuracy: 0.8954 - val_loss: 0.2896 - val_accuracy: 0.9021
Epoch 5/20
1375/1375 [==============================] - 1s 1ms/step - loss: 0.2804 - accuracy: 0.9029 - val_loss: 0.2773 - val_accuracy: 0.9061
Epoch 6/20
1375/1375 [==============================] - 1s 1ms/step - loss: 0.2701 - accuracy: 0.9075 - val_loss: 0.2735 - val_accuracy: 0.9066
Epoch 7/20
1375/1375 [==============================] - 1s 1ms/step - loss: 0.2627 - accuracy: 0.9093 - val_loss: 0.2721 - val_accuracy: 0.9081
Epoch 8/20
1375/1375 [==============================] - 1s 997us/step - loss: 0.2609 - accuracy: 0.9122 - val_loss: 0.2589 - val_accuracy: 0.9141
Epoch 9/20
1375/1375 [==============================] - 1s 1ms/step - loss: 0.2558 - accuracy: 0.9110 - val_loss: 0.2562 - val_accuracy: 0.9136
Epoch 10/20
1375/1375 [==============================] - 1s 997us/step - loss: 0.2512 - accuracy: 0.9138 - val_loss: 0.2544 - val_accuracy: 0.9160
Epoch 11/20
1375/1375 [==============================] - 1s 1000us/step - loss: 0.2431 - accuracy: 0.9170 - val_loss: 0.2495 - val_accuracy: 0.9153
Epoch 12/20
1375/1375 [==============================] - 1s 995us/step - loss: 0.2422 - accuracy: 0.9168 - val_loss: 0.2515 - val_accuracy: 0.9126
Epoch 13/20
1375/1375 [==============================] - 1s 1ms/step - loss: 0.2360 - accuracy: 0.9181 - val_loss: 0.2446 - val_accuracy: 0.9160
Epoch 14/20
1375/1375 [==============================] - 1s 1ms/step - loss: 0.2266 - accuracy: 0.9232 - val_loss: 0.2415 - val_accuracy: 0.9178
Epoch 15/20
1375/1375 [==============================] - 1s 988us/step - loss: 0.2225 - accuracy: 0.9239 - val_loss: 0.2447 - val_accuracy: 0.9195
Epoch 16/20
1375/1375 [==============================] - 1s 995us/step - loss: 0.2261 - accuracy: 0.9216 - val_loss: 0.2384 - val_accuracy: 0.9198
Epoch 17/20
1375/1375 [==============================] - 1s 1ms/step - loss: 0.2191 - accuracy: 0.9251 - val_loss: 0.2412 - val_accuracy: 0.9175
Epoch 18/20
1375/1375 [==============================] - 1s 991us/step - loss: 0.2171 - accuracy: 0.9254 - val_loss: 0.2429 - val_accuracy: 0.9158
Epoch 19/20
1375/1375 [==============================] - 1s 992us/step - loss: 0.2180 - accuracy: 0.9252 - val_loss: 0.2330 - val_accuracy: 0.9205
Epoch 20/20
1375/1375 [==============================] - 1s 994us/step - loss: 0.2112 - accuracy: 0.9274 - val_loss: 0.2333 - val_accuracy: 0.9200</code></pre>
</div>
</div>
<div id="cell-81" class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>model_A.save(<span class="st">"my_model_A.h5"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-82" class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>model_B <span class="op">=</span> keras.models.Sequential()</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>model_B.add(keras.layers.Flatten(input_shape<span class="op">=</span>[<span class="dv">28</span>, <span class="dv">28</span>]))</span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n_hidden <span class="kw">in</span> (<span class="dv">300</span>, <span class="dv">100</span>, <span class="dv">50</span>, <span class="dv">50</span>, <span class="dv">50</span>):</span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a>    model_B.add(keras.layers.Dense(n_hidden, activation<span class="op">=</span><span class="st">"selu"</span>))</span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a>model_B.add(keras.layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">"sigmoid"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-83" class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>model_B.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"binary_crossentropy"</span>,</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a>                optimizer<span class="op">=</span>keras.optimizers.SGD(learning_rate<span class="op">=</span><span class="fl">1e-3</span>),</span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a>                metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-84" class="cell" data-execution_count="58">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model_B.fit(X_train_B, y_train_B, epochs<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>                      validation_data<span class="op">=</span>(X_valid_B, y_valid_B))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/20
7/7 [==============================] - 0s 29ms/step - loss: 1.0360 - accuracy: 0.4975 - val_loss: 0.6314 - val_accuracy: 0.6004
Epoch 2/20
7/7 [==============================] - 0s 9ms/step - loss: 0.5883 - accuracy: 0.6971 - val_loss: 0.4784 - val_accuracy: 0.8529
Epoch 3/20
7/7 [==============================] - 0s 10ms/step - loss: 0.4380 - accuracy: 0.8854 - val_loss: 0.4102 - val_accuracy: 0.8945
Epoch 4/20
7/7 [==============================] - 0s 10ms/step - loss: 0.4021 - accuracy: 0.8712 - val_loss: 0.3647 - val_accuracy: 0.9178
Epoch 5/20
7/7 [==============================] - 0s 11ms/step - loss: 0.3361 - accuracy: 0.9348 - val_loss: 0.3300 - val_accuracy: 0.9320
Epoch 6/20
7/7 [==============================] - 0s 11ms/step - loss: 0.3113 - accuracy: 0.9233 - val_loss: 0.3019 - val_accuracy: 0.9402
Epoch 7/20
7/7 [==============================] - 0s 11ms/step - loss: 0.2817 - accuracy: 0.9299 - val_loss: 0.2804 - val_accuracy: 0.9422
Epoch 8/20
7/7 [==============================] - 0s 11ms/step - loss: 0.2632 - accuracy: 0.9379 - val_loss: 0.2606 - val_accuracy: 0.9473
Epoch 9/20
7/7 [==============================] - 0s 10ms/step - loss: 0.2373 - accuracy: 0.9481 - val_loss: 0.2428 - val_accuracy: 0.9523
Epoch 10/20
7/7 [==============================] - 0s 11ms/step - loss: 0.2229 - accuracy: 0.9657 - val_loss: 0.2281 - val_accuracy: 0.9544
Epoch 11/20
7/7 [==============================] - 0s 11ms/step - loss: 0.2155 - accuracy: 0.9590 - val_loss: 0.2150 - val_accuracy: 0.9584
Epoch 12/20
7/7 [==============================] - 0s 11ms/step - loss: 0.1834 - accuracy: 0.9738 - val_loss: 0.2036 - val_accuracy: 0.9584
Epoch 13/20
7/7 [==============================] - 0s 10ms/step - loss: 0.1671 - accuracy: 0.9828 - val_loss: 0.1931 - val_accuracy: 0.9615
Epoch 14/20
7/7 [==============================] - 0s 10ms/step - loss: 0.1527 - accuracy: 0.9915 - val_loss: 0.1838 - val_accuracy: 0.9635
Epoch 15/20
7/7 [==============================] - 0s 10ms/step - loss: 0.1595 - accuracy: 0.9904 - val_loss: 0.1746 - val_accuracy: 0.9686
Epoch 16/20
7/7 [==============================] - 0s 11ms/step - loss: 0.1473 - accuracy: 0.9937 - val_loss: 0.1674 - val_accuracy: 0.9686
Epoch 17/20
7/7 [==============================] - 0s 10ms/step - loss: 0.1412 - accuracy: 0.9944 - val_loss: 0.1604 - val_accuracy: 0.9706
Epoch 18/20
7/7 [==============================] - 0s 11ms/step - loss: 0.1242 - accuracy: 0.9931 - val_loss: 0.1539 - val_accuracy: 0.9706
Epoch 19/20
7/7 [==============================] - 0s 11ms/step - loss: 0.1224 - accuracy: 0.9931 - val_loss: 0.1482 - val_accuracy: 0.9716
Epoch 20/20
7/7 [==============================] - 0s 10ms/step - loss: 0.1096 - accuracy: 0.9912 - val_loss: 0.1431 - val_accuracy: 0.9716</code></pre>
</div>
</div>
<div id="cell-85" class="cell" data-execution_count="59">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>model_B.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "sequential_7"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_5 (Flatten)          (None, 784)               0         
_________________________________________________________________
dense_28 (Dense)             (None, 300)               235500    
_________________________________________________________________
dense_29 (Dense)             (None, 100)               30100     
_________________________________________________________________
dense_30 (Dense)             (None, 50)                5050      
_________________________________________________________________
dense_31 (Dense)             (None, 50)                2550      
_________________________________________________________________
dense_32 (Dense)             (None, 50)                2550      
_________________________________________________________________
dense_33 (Dense)             (None, 1)                 51        
=================================================================
Total params: 275,801
Trainable params: 275,801
Non-trainable params: 0
_________________________________________________________________</code></pre>
</div>
</div>
<div id="cell-86" class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a>model_A <span class="op">=</span> keras.models.load_model(<span class="st">"my_model_A.h5"</span>)</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a>model_B_on_A <span class="op">=</span> keras.models.Sequential(model_A.layers[:<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a>model_B_on_A.add(keras.layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">"sigmoid"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that <code>model_B_on_A</code> and <code>model_A</code> actually share layers now, so when we train one, it will update both models. If we want to avoid that, we need to build <code>model_B_on_A</code> on top of a <em>clone</em> of <code>model_A</code>:</p>
<div id="cell-88" class="cell" data-execution_count="61">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>model_A_clone <span class="op">=</span> keras.models.clone_model(model_A)</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>model_A_clone.set_weights(model_A.get_weights())</span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a>model_B_on_A <span class="op">=</span> keras.models.Sequential(model_A_clone.layers[:<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a>model_B_on_A.add(keras.layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">"sigmoid"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-89" class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> model_B_on_A.layers[:<span class="op">-</span><span class="dv">1</span>]:</span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>    layer.trainable <span class="op">=</span> <span class="va">False</span></span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a>model_B_on_A.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"binary_crossentropy"</span>,</span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a>                     optimizer<span class="op">=</span>keras.optimizers.SGD(learning_rate<span class="op">=</span><span class="fl">1e-3</span>),</span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a>                     metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-90" class="cell" data-execution_count="63">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model_B_on_A.fit(X_train_B, y_train_B, epochs<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>                           validation_data<span class="op">=</span>(X_valid_B, y_valid_B))</span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> model_B_on_A.layers[:<span class="op">-</span><span class="dv">1</span>]:</span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a>    layer.trainable <span class="op">=</span> <span class="va">True</span></span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-7"><a href="#cb90-7" aria-hidden="true" tabindex="-1"></a>model_B_on_A.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"binary_crossentropy"</span>,</span>
<span id="cb90-8"><a href="#cb90-8" aria-hidden="true" tabindex="-1"></a>                     optimizer<span class="op">=</span>keras.optimizers.SGD(learning_rate<span class="op">=</span><span class="fl">1e-3</span>),</span>
<span id="cb90-9"><a href="#cb90-9" aria-hidden="true" tabindex="-1"></a>                     metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span>
<span id="cb90-10"><a href="#cb90-10" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model_B_on_A.fit(X_train_B, y_train_B, epochs<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb90-11"><a href="#cb90-11" aria-hidden="true" tabindex="-1"></a>                           validation_data<span class="op">=</span>(X_valid_B, y_valid_B))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/4
7/7 [==============================] - 0s 29ms/step - loss: 0.2575 - accuracy: 0.9487 - val_loss: 0.2797 - val_accuracy: 0.9270
Epoch 2/4
7/7 [==============================] - 0s 9ms/step - loss: 0.2566 - accuracy: 0.9371 - val_loss: 0.2701 - val_accuracy: 0.9300
Epoch 3/4
7/7 [==============================] - 0s 9ms/step - loss: 0.2473 - accuracy: 0.9332 - val_loss: 0.2613 - val_accuracy: 0.9341
Epoch 4/4
7/7 [==============================] - 0s 10ms/step - loss: 0.2450 - accuracy: 0.9463 - val_loss: 0.2531 - val_accuracy: 0.9391
Epoch 1/16
7/7 [==============================] - 1s 29ms/step - loss: 0.2106 - accuracy: 0.9524 - val_loss: 0.2045 - val_accuracy: 0.9615
Epoch 2/16
7/7 [==============================] - 0s 9ms/step - loss: 0.1738 - accuracy: 0.9526 - val_loss: 0.1719 - val_accuracy: 0.9706
Epoch 3/16
7/7 [==============================] - 0s 9ms/step - loss: 0.1451 - accuracy: 0.9660 - val_loss: 0.1491 - val_accuracy: 0.9807
Epoch 4/16
7/7 [==============================] - 0s 9ms/step - loss: 0.1242 - accuracy: 0.9717 - val_loss: 0.1325 - val_accuracy: 0.9817
Epoch 5/16
7/7 [==============================] - 0s 11ms/step - loss: 0.1078 - accuracy: 0.9855 - val_loss: 0.1200 - val_accuracy: 0.9848
Epoch 6/16
7/7 [==============================] - 0s 11ms/step - loss: 0.1075 - accuracy: 0.9931 - val_loss: 0.1101 - val_accuracy: 0.9858
Epoch 7/16
7/7 [==============================] - 0s 11ms/step - loss: 0.0893 - accuracy: 0.9950 - val_loss: 0.1020 - val_accuracy: 0.9858
Epoch 8/16
7/7 [==============================] - 0s 10ms/step - loss: 0.0815 - accuracy: 0.9950 - val_loss: 0.0953 - val_accuracy: 0.9868
Epoch 9/16
7/7 [==============================] - 0s 10ms/step - loss: 0.0640 - accuracy: 0.9973 - val_loss: 0.0892 - val_accuracy: 0.9868
Epoch 10/16
7/7 [==============================] - 0s 10ms/step - loss: 0.0641 - accuracy: 0.9931 - val_loss: 0.0844 - val_accuracy: 0.9878
Epoch 11/16
7/7 [==============================] - 0s 11ms/step - loss: 0.0609 - accuracy: 0.9931 - val_loss: 0.0800 - val_accuracy: 0.9888
Epoch 12/16
7/7 [==============================] - 0s 11ms/step - loss: 0.0641 - accuracy: 1.0000 - val_loss: 0.0762 - val_accuracy: 0.9888
Epoch 13/16
7/7 [==============================] - 0s 10ms/step - loss: 0.0478 - accuracy: 1.0000 - val_loss: 0.0728 - val_accuracy: 0.9888
Epoch 14/16
7/7 [==============================] - 0s 10ms/step - loss: 0.0444 - accuracy: 1.0000 - val_loss: 0.0700 - val_accuracy: 0.9878
Epoch 15/16
7/7 [==============================] - 0s 11ms/step - loss: 0.0490 - accuracy: 1.0000 - val_loss: 0.0675 - val_accuracy: 0.9878
Epoch 16/16
7/7 [==============================] - 0s 11ms/step - loss: 0.0434 - accuracy: 1.0000 - val_loss: 0.0652 - val_accuracy: 0.9878</code></pre>
</div>
</div>
<p>So, what’s the final verdict?</p>
<div id="cell-92" class="cell" data-execution_count="64">
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>model_B.evaluate(X_test_B, y_test_B)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>63/63 [==============================] - 0s 714us/step - loss: 0.1408 - accuracy: 0.9705</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="64">
<pre><code>[0.1408407837152481, 0.9704999923706055]</code></pre>
</div>
</div>
<div id="cell-93" class="cell" data-execution_count="65">
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>model_B_on_A.evaluate(X_test_B, y_test_B)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>63/63 [==============================] - 0s 751us/step - loss: 0.0562 - accuracy: 0.9940</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="65">
<pre><code>[0.0561506561934948, 0.9940000176429749]</code></pre>
</div>
</div>
<p>Great! We got quite a bit of transfer: the error rate dropped by a factor of 4.9!</p>
<div id="cell-95" class="cell" data-execution_count="66">
<div class="sourceCode cell-code" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a>(<span class="dv">100</span> <span class="op">-</span> <span class="fl">97.05</span>) <span class="op">/</span> (<span class="dv">100</span> <span class="op">-</span> <span class="fl">99.40</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="66">
<pre><code>4.916666666666718</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="faster-optimizers" class="level1">
<h1>Faster Optimizers</h1>
<section id="momentum-optimization" class="level2">
<h2 class="anchored" data-anchor-id="momentum-optimization">Momentum optimization</h2>
<div id="cell-98" class="cell" data-execution_count="67">
<div class="sourceCode cell-code" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> keras.optimizers.SGD(learning_rate<span class="op">=</span><span class="fl">0.001</span>, momentum<span class="op">=</span><span class="fl">0.9</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="nesterov-accelerated-gradient" class="level2">
<h2 class="anchored" data-anchor-id="nesterov-accelerated-gradient">Nesterov Accelerated Gradient</h2>
<div id="cell-100" class="cell" data-execution_count="68">
<div class="sourceCode cell-code" id="cb101"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> keras.optimizers.SGD(learning_rate<span class="op">=</span><span class="fl">0.001</span>, momentum<span class="op">=</span><span class="fl">0.9</span>, nesterov<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="adagrad" class="level2">
<h2 class="anchored" data-anchor-id="adagrad">AdaGrad</h2>
<div id="cell-102" class="cell" data-execution_count="69">
<div class="sourceCode cell-code" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> keras.optimizers.Adagrad(learning_rate<span class="op">=</span><span class="fl">0.001</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="rmsprop" class="level2">
<h2 class="anchored" data-anchor-id="rmsprop">RMSProp</h2>
<div id="cell-104" class="cell" data-execution_count="70">
<div class="sourceCode cell-code" id="cb103"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> keras.optimizers.RMSprop(learning_rate<span class="op">=</span><span class="fl">0.001</span>, rho<span class="op">=</span><span class="fl">0.9</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="adam-optimization" class="level2">
<h2 class="anchored" data-anchor-id="adam-optimization">Adam Optimization</h2>
<div id="cell-106" class="cell" data-execution_count="71">
<div class="sourceCode cell-code" id="cb104"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> keras.optimizers.Adam(learning_rate<span class="op">=</span><span class="fl">0.001</span>, beta_1<span class="op">=</span><span class="fl">0.9</span>, beta_2<span class="op">=</span><span class="fl">0.999</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="adamax-optimization" class="level2">
<h2 class="anchored" data-anchor-id="adamax-optimization">Adamax Optimization</h2>
<div id="cell-108" class="cell" data-execution_count="72">
<div class="sourceCode cell-code" id="cb105"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> keras.optimizers.Adamax(learning_rate<span class="op">=</span><span class="fl">0.001</span>, beta_1<span class="op">=</span><span class="fl">0.9</span>, beta_2<span class="op">=</span><span class="fl">0.999</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="nadam-optimization" class="level2">
<h2 class="anchored" data-anchor-id="nadam-optimization">Nadam Optimization</h2>
<div id="cell-110" class="cell" data-execution_count="73">
<div class="sourceCode cell-code" id="cb106"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> keras.optimizers.Nadam(learning_rate<span class="op">=</span><span class="fl">0.001</span>, beta_1<span class="op">=</span><span class="fl">0.9</span>, beta_2<span class="op">=</span><span class="fl">0.999</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="learning-rate-scheduling" class="level2">
<h2 class="anchored" data-anchor-id="learning-rate-scheduling">Learning Rate Scheduling</h2>
<section id="power-scheduling" class="level3">
<h3 class="anchored" data-anchor-id="power-scheduling">Power Scheduling</h3>
<p><code>lr = lr0 / (1 + steps / s)**c</code> * Keras uses <code>c=1</code> and <code>s = 1 / decay</code></p>
<div id="cell-114" class="cell" data-execution_count="74">
<div class="sourceCode cell-code" id="cb107"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> keras.optimizers.SGD(learning_rate<span class="op">=</span><span class="fl">0.01</span>, decay<span class="op">=</span><span class="fl">1e-4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-115" class="cell" data-execution_count="75">
<div class="sourceCode cell-code" id="cb108"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.models.Sequential([</span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a>    keras.layers.Flatten(input_shape<span class="op">=</span>[<span class="dv">28</span>, <span class="dv">28</span>]),</span>
<span id="cb108-3"><a href="#cb108-3" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">300</span>, activation<span class="op">=</span><span class="st">"selu"</span>, kernel_initializer<span class="op">=</span><span class="st">"lecun_normal"</span>),</span>
<span id="cb108-4"><a href="#cb108-4" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">100</span>, activation<span class="op">=</span><span class="st">"selu"</span>, kernel_initializer<span class="op">=</span><span class="st">"lecun_normal"</span>),</span>
<span id="cb108-5"><a href="#cb108-5" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">"softmax"</span>)</span>
<span id="cb108-6"><a href="#cb108-6" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb108-7"><a href="#cb108-7" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>, optimizer<span class="op">=</span>optimizer, metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-116" class="cell" data-execution_count="76">
<div class="sourceCode cell-code" id="cb109"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a>n_epochs <span class="op">=</span> <span class="dv">25</span></span>
<span id="cb109-2"><a href="#cb109-2" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(X_train_scaled, y_train, epochs<span class="op">=</span>n_epochs,</span>
<span id="cb109-3"><a href="#cb109-3" aria-hidden="true" tabindex="-1"></a>                    validation_data<span class="op">=</span>(X_valid_scaled, y_valid))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.5980 - accuracy: 0.7933 - val_loss: 0.4031 - val_accuracy: 0.8598
Epoch 2/25
1719/1719 [==============================] - 2s 954us/step - loss: 0.3829 - accuracy: 0.8636 - val_loss: 0.3714 - val_accuracy: 0.8720
Epoch 3/25
1719/1719 [==============================] - 2s 943us/step - loss: 0.3491 - accuracy: 0.8771 - val_loss: 0.3746 - val_accuracy: 0.8738
Epoch 4/25
1719/1719 [==============================] - 2s 954us/step - loss: 0.3277 - accuracy: 0.8814 - val_loss: 0.3502 - val_accuracy: 0.8798
Epoch 5/25
1719/1719 [==============================] - 2s 934us/step - loss: 0.3172 - accuracy: 0.8856 - val_loss: 0.3453 - val_accuracy: 0.8780
Epoch 6/25
1719/1719 [==============================] - 2s 919us/step - loss: 0.2922 - accuracy: 0.8940 - val_loss: 0.3419 - val_accuracy: 0.8820
Epoch 7/25
1719/1719 [==============================] - 2s 921us/step - loss: 0.2870 - accuracy: 0.8973 - val_loss: 0.3362 - val_accuracy: 0.8872
Epoch 8/25
1719/1719 [==============================] - 2s 925us/step - loss: 0.2720 - accuracy: 0.9032 - val_loss: 0.3415 - val_accuracy: 0.8830
Epoch 9/25
1719/1719 [==============================] - 2s 929us/step - loss: 0.2730 - accuracy: 0.9004 - val_loss: 0.3297 - val_accuracy: 0.8864
Epoch 10/25
1719/1719 [==============================] - 2s 928us/step - loss: 0.2585 - accuracy: 0.9068 - val_loss: 0.3269 - val_accuracy: 0.8888
Epoch 11/25
1719/1719 [==============================] - 2s 932us/step - loss: 0.2529 - accuracy: 0.9100 - val_loss: 0.3280 - val_accuracy: 0.8878
Epoch 12/25
1719/1719 [==============================] - 2s 954us/step - loss: 0.2485 - accuracy: 0.9101 - val_loss: 0.3343 - val_accuracy: 0.8822
Epoch 13/25
1719/1719 [==============================] - 2s 964us/step - loss: 0.2420 - accuracy: 0.9148 - val_loss: 0.3266 - val_accuracy: 0.8890
Epoch 14/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2373 - accuracy: 0.9144 - val_loss: 0.3299 - val_accuracy: 0.8890
Epoch 15/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2363 - accuracy: 0.9154 - val_loss: 0.3255 - val_accuracy: 0.8874
Epoch 16/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2309 - accuracy: 0.9181 - val_loss: 0.3217 - val_accuracy: 0.8910
Epoch 17/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2235 - accuracy: 0.9211 - val_loss: 0.3248 - val_accuracy: 0.8914
Epoch 18/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2247 - accuracy: 0.9194 - val_loss: 0.3202 - val_accuracy: 0.8934
Epoch 19/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2235 - accuracy: 0.9218 - val_loss: 0.3243 - val_accuracy: 0.8906
Epoch 20/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2227 - accuracy: 0.9225 - val_loss: 0.3224 - val_accuracy: 0.8900
Epoch 21/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2193 - accuracy: 0.9230 - val_loss: 0.3221 - val_accuracy: 0.8912
Epoch 22/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2163 - accuracy: 0.9227 - val_loss: 0.3195 - val_accuracy: 0.8948
Epoch 23/25
1719/1719 [==============================] - 2s 997us/step - loss: 0.2127 - accuracy: 0.9252 - val_loss: 0.3208 - val_accuracy: 0.8908
Epoch 24/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2076 - accuracy: 0.9273 - val_loss: 0.3226 - val_accuracy: 0.8902
Epoch 25/25
1719/1719 [==============================] - 2s 999us/step - loss: 0.2104 - accuracy: 0.9250 - val_loss: 0.3225 - val_accuracy: 0.8924</code></pre>
</div>
</div>
<div id="cell-117" class="cell" data-execution_count="77">
<div class="sourceCode cell-code" id="cb111"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-3"><a href="#cb111-3" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb111-4"><a href="#cb111-4" aria-hidden="true" tabindex="-1"></a>decay <span class="op">=</span> <span class="fl">1e-4</span></span>
<span id="cb111-5"><a href="#cb111-5" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb111-6"><a href="#cb111-6" aria-hidden="true" tabindex="-1"></a>n_steps_per_epoch <span class="op">=</span> math.ceil(<span class="bu">len</span>(X_train) <span class="op">/</span> batch_size)</span>
<span id="cb111-7"><a href="#cb111-7" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> np.arange(n_epochs)</span>
<span id="cb111-8"><a href="#cb111-8" aria-hidden="true" tabindex="-1"></a>lrs <span class="op">=</span> learning_rate <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> decay <span class="op">*</span> epochs <span class="op">*</span> n_steps_per_epoch)</span>
<span id="cb111-9"><a href="#cb111-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-10"><a href="#cb111-10" aria-hidden="true" tabindex="-1"></a>plt.plot(epochs, lrs,  <span class="st">"o-"</span>)</span>
<span id="cb111-11"><a href="#cb111-11" aria-hidden="true" tabindex="-1"></a>plt.axis([<span class="dv">0</span>, n_epochs <span class="op">-</span> <span class="dv">1</span>, <span class="dv">0</span>, <span class="fl">0.01</span>])</span>
<span id="cb111-12"><a href="#cb111-12" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Epoch"</span>)</span>
<span id="cb111-13"><a href="#cb111-13" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Learning Rate"</span>)</span>
<span id="cb111-14"><a href="#cb111-14" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Power Scheduling"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb111-15"><a href="#cb111-15" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb111-16"><a href="#cb111-16" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="11_training_deep_neural_networks_files/figure-html/cell-78-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="exponential-scheduling" class="level3">
<h3 class="anchored" data-anchor-id="exponential-scheduling">Exponential Scheduling</h3>
<p><code>lr = lr0 * 0.1**(epoch / s)</code></p>
<div id="cell-120" class="cell" data-execution_count="78">
<div class="sourceCode cell-code" id="cb112"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> exponential_decay_fn(epoch):</span>
<span id="cb112-2"><a href="#cb112-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">0.01</span> <span class="op">*</span> <span class="fl">0.1</span><span class="op">**</span>(epoch <span class="op">/</span> <span class="dv">20</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-121" class="cell" data-execution_count="79">
<div class="sourceCode cell-code" id="cb113"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> exponential_decay(lr0, s):</span>
<span id="cb113-2"><a href="#cb113-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> exponential_decay_fn(epoch):</span>
<span id="cb113-3"><a href="#cb113-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> lr0 <span class="op">*</span> <span class="fl">0.1</span><span class="op">**</span>(epoch <span class="op">/</span> s)</span>
<span id="cb113-4"><a href="#cb113-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> exponential_decay_fn</span>
<span id="cb113-5"><a href="#cb113-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-6"><a href="#cb113-6" aria-hidden="true" tabindex="-1"></a>exponential_decay_fn <span class="op">=</span> exponential_decay(lr0<span class="op">=</span><span class="fl">0.01</span>, s<span class="op">=</span><span class="dv">20</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-122" class="cell" data-execution_count="80">
<div class="sourceCode cell-code" id="cb114"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.models.Sequential([</span>
<span id="cb114-2"><a href="#cb114-2" aria-hidden="true" tabindex="-1"></a>    keras.layers.Flatten(input_shape<span class="op">=</span>[<span class="dv">28</span>, <span class="dv">28</span>]),</span>
<span id="cb114-3"><a href="#cb114-3" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">300</span>, activation<span class="op">=</span><span class="st">"selu"</span>, kernel_initializer<span class="op">=</span><span class="st">"lecun_normal"</span>),</span>
<span id="cb114-4"><a href="#cb114-4" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">100</span>, activation<span class="op">=</span><span class="st">"selu"</span>, kernel_initializer<span class="op">=</span><span class="st">"lecun_normal"</span>),</span>
<span id="cb114-5"><a href="#cb114-5" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">"softmax"</span>)</span>
<span id="cb114-6"><a href="#cb114-6" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb114-7"><a href="#cb114-7" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>, optimizer<span class="op">=</span><span class="st">"nadam"</span>, metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span>
<span id="cb114-8"><a href="#cb114-8" aria-hidden="true" tabindex="-1"></a>n_epochs <span class="op">=</span> <span class="dv">25</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-123" class="cell" data-execution_count="81">
<div class="sourceCode cell-code" id="cb115"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a>lr_scheduler <span class="op">=</span> keras.callbacks.LearningRateScheduler(exponential_decay_fn)</span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(X_train_scaled, y_train, epochs<span class="op">=</span>n_epochs,</span>
<span id="cb115-3"><a href="#cb115-3" aria-hidden="true" tabindex="-1"></a>                    validation_data<span class="op">=</span>(X_valid_scaled, y_valid),</span>
<span id="cb115-4"><a href="#cb115-4" aria-hidden="true" tabindex="-1"></a>                    callbacks<span class="op">=</span>[lr_scheduler])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/25
1719/1719 [==============================] - 4s 2ms/step - loss: 1.1122 - accuracy: 0.7363 - val_loss: 0.8947 - val_accuracy: 0.7496
Epoch 2/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.7354 - accuracy: 0.7825 - val_loss: 0.6059 - val_accuracy: 0.8122
Epoch 3/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.5973 - accuracy: 0.8175 - val_loss: 0.8195 - val_accuracy: 0.7754
Epoch 4/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.6040 - accuracy: 0.8148 - val_loss: 0.6135 - val_accuracy: 0.8398
Epoch 5/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.5462 - accuracy: 0.8323 - val_loss: 0.5075 - val_accuracy: 0.8490
Epoch 6/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.4479 - accuracy: 0.8555 - val_loss: 0.4538 - val_accuracy: 0.8502
Epoch 7/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.4225 - accuracy: 0.8622 - val_loss: 0.4792 - val_accuracy: 0.8524
Epoch 8/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.3873 - accuracy: 0.8678 - val_loss: 0.5517 - val_accuracy: 0.8448
Epoch 9/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3635 - accuracy: 0.8767 - val_loss: 0.5312 - val_accuracy: 0.8600
Epoch 10/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3353 - accuracy: 0.8840 - val_loss: 0.4671 - val_accuracy: 0.8660
Epoch 11/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.3108 - accuracy: 0.8927 - val_loss: 0.4885 - val_accuracy: 0.8670
Epoch 12/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.2895 - accuracy: 0.8987 - val_loss: 0.4698 - val_accuracy: 0.8636
Epoch 13/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.2660 - accuracy: 0.9071 - val_loss: 0.4558 - val_accuracy: 0.8820
Epoch 14/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.2442 - accuracy: 0.9153 - val_loss: 0.4325 - val_accuracy: 0.8774
Epoch 15/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.2375 - accuracy: 0.9177 - val_loss: 0.4703 - val_accuracy: 0.8800
Epoch 16/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.2196 - accuracy: 0.9231 - val_loss: 0.4657 - val_accuracy: 0.8870
Epoch 17/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.2013 - accuracy: 0.9312 - val_loss: 0.5023 - val_accuracy: 0.8760
Epoch 18/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.1938 - accuracy: 0.9331 - val_loss: 0.4782 - val_accuracy: 0.8856
Epoch 19/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.1774 - accuracy: 0.9394 - val_loss: 0.4815 - val_accuracy: 0.8898
Epoch 20/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.1703 - accuracy: 0.9418 - val_loss: 0.4674 - val_accuracy: 0.8902
Epoch 21/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.1611 - accuracy: 0.9462 - val_loss: 0.5116 - val_accuracy: 0.8930
Epoch 22/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.1530 - accuracy: 0.9481 - val_loss: 0.5326 - val_accuracy: 0.8934
Epoch 23/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.1436 - accuracy: 0.9519 - val_loss: 0.5297 - val_accuracy: 0.8902
Epoch 24/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.1326 - accuracy: 0.9560 - val_loss: 0.5526 - val_accuracy: 0.8930
Epoch 25/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.1308 - accuracy: 0.9560 - val_loss: 0.5699 - val_accuracy: 0.8928</code></pre>
</div>
</div>
<div id="cell-124" class="cell" data-execution_count="82">
<div class="sourceCode cell-code" id="cb117"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a>plt.plot(history.epoch, history.history[<span class="st">"lr"</span>], <span class="st">"o-"</span>)</span>
<span id="cb117-2"><a href="#cb117-2" aria-hidden="true" tabindex="-1"></a>plt.axis([<span class="dv">0</span>, n_epochs <span class="op">-</span> <span class="dv">1</span>, <span class="dv">0</span>, <span class="fl">0.011</span>])</span>
<span id="cb117-3"><a href="#cb117-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Epoch"</span>)</span>
<span id="cb117-4"><a href="#cb117-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Learning Rate"</span>)</span>
<span id="cb117-5"><a href="#cb117-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Exponential Scheduling"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb117-6"><a href="#cb117-6" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb117-7"><a href="#cb117-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="11_training_deep_neural_networks_files/figure-html/cell-83-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The schedule function can take the current learning rate as a second argument:</p>
<div id="cell-126" class="cell" data-execution_count="83">
<div class="sourceCode cell-code" id="cb118"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> exponential_decay_fn(epoch, lr):</span>
<span id="cb118-2"><a href="#cb118-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> lr <span class="op">*</span> <span class="fl">0.1</span><span class="op">**</span>(<span class="dv">1</span> <span class="op">/</span> <span class="dv">20</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If you want to update the learning rate at each iteration rather than at each epoch, you must write your own callback class:</p>
<div id="cell-128" class="cell" data-execution_count="84">
<div class="sourceCode cell-code" id="cb119"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb119-1"><a href="#cb119-1" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> keras.backend</span>
<span id="cb119-2"><a href="#cb119-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-3"><a href="#cb119-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ExponentialDecay(keras.callbacks.Callback):</span>
<span id="cb119-4"><a href="#cb119-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, s<span class="op">=</span><span class="dv">40000</span>):</span>
<span id="cb119-5"><a href="#cb119-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb119-6"><a href="#cb119-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.s <span class="op">=</span> s</span>
<span id="cb119-7"><a href="#cb119-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-8"><a href="#cb119-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> on_batch_begin(<span class="va">self</span>, batch, logs<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb119-9"><a href="#cb119-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Note: the `batch` argument is reset at each epoch</span></span>
<span id="cb119-10"><a href="#cb119-10" aria-hidden="true" tabindex="-1"></a>        lr <span class="op">=</span> K.get_value(<span class="va">self</span>.model.optimizer.learning_rate)</span>
<span id="cb119-11"><a href="#cb119-11" aria-hidden="true" tabindex="-1"></a>        K.set_value(<span class="va">self</span>.model.optimizer.learning_rate, lr <span class="op">*</span> <span class="fl">0.1</span><span class="op">**</span>(<span class="dv">1</span> <span class="op">/</span> <span class="va">self</span>.s))</span>
<span id="cb119-12"><a href="#cb119-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-13"><a href="#cb119-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> on_epoch_end(<span class="va">self</span>, epoch, logs<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb119-14"><a href="#cb119-14" aria-hidden="true" tabindex="-1"></a>        logs <span class="op">=</span> logs <span class="kw">or</span> {}</span>
<span id="cb119-15"><a href="#cb119-15" aria-hidden="true" tabindex="-1"></a>        logs[<span class="st">'lr'</span>] <span class="op">=</span> K.get_value(<span class="va">self</span>.model.optimizer.learning_rate)</span>
<span id="cb119-16"><a href="#cb119-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-17"><a href="#cb119-17" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.models.Sequential([</span>
<span id="cb119-18"><a href="#cb119-18" aria-hidden="true" tabindex="-1"></a>    keras.layers.Flatten(input_shape<span class="op">=</span>[<span class="dv">28</span>, <span class="dv">28</span>]),</span>
<span id="cb119-19"><a href="#cb119-19" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">300</span>, activation<span class="op">=</span><span class="st">"selu"</span>, kernel_initializer<span class="op">=</span><span class="st">"lecun_normal"</span>),</span>
<span id="cb119-20"><a href="#cb119-20" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">100</span>, activation<span class="op">=</span><span class="st">"selu"</span>, kernel_initializer<span class="op">=</span><span class="st">"lecun_normal"</span>),</span>
<span id="cb119-21"><a href="#cb119-21" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">"softmax"</span>)</span>
<span id="cb119-22"><a href="#cb119-22" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb119-23"><a href="#cb119-23" aria-hidden="true" tabindex="-1"></a>lr0 <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb119-24"><a href="#cb119-24" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> keras.optimizers.Nadam(learning_rate<span class="op">=</span>lr0)</span>
<span id="cb119-25"><a href="#cb119-25" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>, optimizer<span class="op">=</span>optimizer, metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span>
<span id="cb119-26"><a href="#cb119-26" aria-hidden="true" tabindex="-1"></a>n_epochs <span class="op">=</span> <span class="dv">25</span></span>
<span id="cb119-27"><a href="#cb119-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-28"><a href="#cb119-28" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> <span class="dv">20</span> <span class="op">*</span> <span class="bu">len</span>(X_train) <span class="op">//</span> <span class="dv">32</span> <span class="co"># number of steps in 20 epochs (batch size = 32)</span></span>
<span id="cb119-29"><a href="#cb119-29" aria-hidden="true" tabindex="-1"></a>exp_decay <span class="op">=</span> ExponentialDecay(s)</span>
<span id="cb119-30"><a href="#cb119-30" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(X_train_scaled, y_train, epochs<span class="op">=</span>n_epochs,</span>
<span id="cb119-31"><a href="#cb119-31" aria-hidden="true" tabindex="-1"></a>                    validation_data<span class="op">=</span>(X_valid_scaled, y_valid),</span>
<span id="cb119-32"><a href="#cb119-32" aria-hidden="true" tabindex="-1"></a>                    callbacks<span class="op">=</span>[exp_decay])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/25
1719/1719 [==============================] - 5s 3ms/step - loss: 1.1153 - accuracy: 0.7390 - val_loss: 0.9588 - val_accuracy: 0.7338
Epoch 2/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.6929 - accuracy: 0.7934 - val_loss: 0.5328 - val_accuracy: 0.8318
Epoch 3/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.6317 - accuracy: 0.8097 - val_loss: 0.7656 - val_accuracy: 0.8278
Epoch 4/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.5827 - accuracy: 0.8258 - val_loss: 0.5585 - val_accuracy: 0.8382
Epoch 5/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.5041 - accuracy: 0.8407 - val_loss: 0.5367 - val_accuracy: 0.8574
Epoch 6/25
1719/1719 [==============================] - 4s 3ms/step - loss: 0.4595 - accuracy: 0.8588 - val_loss: 0.6000 - val_accuracy: 0.8516
Epoch 7/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.4490 - accuracy: 0.8644 - val_loss: 0.4605 - val_accuracy: 0.8648
Epoch 8/25
1719/1719 [==============================] - 4s 3ms/step - loss: 0.3925 - accuracy: 0.8783 - val_loss: 0.5076 - val_accuracy: 0.8616
Epoch 9/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.4085 - accuracy: 0.8797 - val_loss: 0.4577 - val_accuracy: 0.8650
Epoch 10/25
1719/1719 [==============================] - 4s 3ms/step - loss: 0.3440 - accuracy: 0.8927 - val_loss: 0.5309 - val_accuracy: 0.8762
Epoch 11/25
1719/1719 [==============================] - 4s 3ms/step - loss: 0.3267 - accuracy: 0.8948 - val_loss: 0.4652 - val_accuracy: 0.8792
Epoch 12/25
1719/1719 [==============================] - 4s 3ms/step - loss: 0.3046 - accuracy: 0.9033 - val_loss: 0.4863 - val_accuracy: 0.8692
Epoch 13/25
1719/1719 [==============================] - 5s 3ms/step - loss: 0.2811 - accuracy: 0.9087 - val_loss: 0.4726 - val_accuracy: 0.8770
Epoch 14/25
1719/1719 [==============================] - 5s 3ms/step - loss: 0.2684 - accuracy: 0.9145 - val_loss: 0.4526 - val_accuracy: 0.8760
Epoch 15/25
1719/1719 [==============================] - 5s 3ms/step - loss: 0.2478 - accuracy: 0.9209 - val_loss: 0.4926 - val_accuracy: 0.8838
Epoch 16/25
1719/1719 [==============================] - 5s 3ms/step - loss: 0.2315 - accuracy: 0.9253 - val_loss: 0.4686 - val_accuracy: 0.8840
Epoch 17/25
1719/1719 [==============================] - 5s 3ms/step - loss: 0.2164 - accuracy: 0.9318 - val_loss: 0.4845 - val_accuracy: 0.8858
Epoch 18/25
1719/1719 [==============================] - 4s 3ms/step - loss: 0.2093 - accuracy: 0.9346 - val_loss: 0.4923 - val_accuracy: 0.8834
Epoch 19/25
1719/1719 [==============================] - 5s 3ms/step - loss: 0.1929 - accuracy: 0.9396 - val_loss: 0.4779 - val_accuracy: 0.8880
Epoch 20/25
1719/1719 [==============================] - 5s 3ms/step - loss: 0.1852 - accuracy: 0.9439 - val_loss: 0.4886 - val_accuracy: 0.8868
Epoch 21/25
1719/1719 [==============================] - 5s 3ms/step - loss: 0.1740 - accuracy: 0.9470 - val_loss: 0.5097 - val_accuracy: 0.8852
Epoch 22/25
1719/1719 [==============================] - 5s 3ms/step - loss: 0.1668 - accuracy: 0.9474 - val_loss: 0.5161 - val_accuracy: 0.8898
Epoch 23/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.1571 - accuracy: 0.9530 - val_loss: 0.5381 - val_accuracy: 0.8886
Epoch 24/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.1444 - accuracy: 0.9575 - val_loss: 0.5415 - val_accuracy: 0.8910
Epoch 25/25
1719/1719 [==============================] - 5s 3ms/step - loss: 0.1447 - accuracy: 0.9569 - val_loss: 0.5833 - val_accuracy: 0.8880</code></pre>
</div>
</div>
<div id="cell-129" class="cell" data-execution_count="85">
<div class="sourceCode cell-code" id="cb121"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a>n_steps <span class="op">=</span> n_epochs <span class="op">*</span> <span class="bu">len</span>(X_train) <span class="op">//</span> <span class="dv">32</span></span>
<span id="cb121-2"><a href="#cb121-2" aria-hidden="true" tabindex="-1"></a>steps <span class="op">=</span> np.arange(n_steps)</span>
<span id="cb121-3"><a href="#cb121-3" aria-hidden="true" tabindex="-1"></a>lrs <span class="op">=</span> lr0 <span class="op">*</span> <span class="fl">0.1</span><span class="op">**</span>(steps <span class="op">/</span> s)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-130" class="cell" data-scrolled="true" data-execution_count="86">
<div class="sourceCode cell-code" id="cb122"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a>plt.plot(steps, lrs, <span class="st">"-"</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb122-2"><a href="#cb122-2" aria-hidden="true" tabindex="-1"></a>plt.axis([<span class="dv">0</span>, n_steps <span class="op">-</span> <span class="dv">1</span>, <span class="dv">0</span>, lr0 <span class="op">*</span> <span class="fl">1.1</span>])</span>
<span id="cb122-3"><a href="#cb122-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Batch"</span>)</span>
<span id="cb122-4"><a href="#cb122-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Learning Rate"</span>)</span>
<span id="cb122-5"><a href="#cb122-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Exponential Scheduling (per batch)"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb122-6"><a href="#cb122-6" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb122-7"><a href="#cb122-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="11_training_deep_neural_networks_files/figure-html/cell-87-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="piecewise-constant-scheduling" class="level3">
<h3 class="anchored" data-anchor-id="piecewise-constant-scheduling">Piecewise Constant Scheduling</h3>
<div id="cell-132" class="cell" data-execution_count="87">
<div class="sourceCode cell-code" id="cb123"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb123-1"><a href="#cb123-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> piecewise_constant_fn(epoch):</span>
<span id="cb123-2"><a href="#cb123-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> epoch <span class="op">&lt;</span> <span class="dv">5</span>:</span>
<span id="cb123-3"><a href="#cb123-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">0.01</span></span>
<span id="cb123-4"><a href="#cb123-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> epoch <span class="op">&lt;</span> <span class="dv">15</span>:</span>
<span id="cb123-5"><a href="#cb123-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">0.005</span></span>
<span id="cb123-6"><a href="#cb123-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb123-7"><a href="#cb123-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">0.001</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-133" class="cell" data-execution_count="88">
<div class="sourceCode cell-code" id="cb124"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> piecewise_constant(boundaries, values):</span>
<span id="cb124-2"><a href="#cb124-2" aria-hidden="true" tabindex="-1"></a>    boundaries <span class="op">=</span> np.array([<span class="dv">0</span>] <span class="op">+</span> boundaries)</span>
<span id="cb124-3"><a href="#cb124-3" aria-hidden="true" tabindex="-1"></a>    values <span class="op">=</span> np.array(values)</span>
<span id="cb124-4"><a href="#cb124-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> piecewise_constant_fn(epoch):</span>
<span id="cb124-5"><a href="#cb124-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> values[np.argmax(boundaries <span class="op">&gt;</span> epoch) <span class="op">-</span> <span class="dv">1</span>]</span>
<span id="cb124-6"><a href="#cb124-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> piecewise_constant_fn</span>
<span id="cb124-7"><a href="#cb124-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-8"><a href="#cb124-8" aria-hidden="true" tabindex="-1"></a>piecewise_constant_fn <span class="op">=</span> piecewise_constant([<span class="dv">5</span>, <span class="dv">15</span>], [<span class="fl">0.01</span>, <span class="fl">0.005</span>, <span class="fl">0.001</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-134" class="cell" data-execution_count="89">
<div class="sourceCode cell-code" id="cb125"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a>lr_scheduler <span class="op">=</span> keras.callbacks.LearningRateScheduler(piecewise_constant_fn)</span>
<span id="cb125-2"><a href="#cb125-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-3"><a href="#cb125-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.models.Sequential([</span>
<span id="cb125-4"><a href="#cb125-4" aria-hidden="true" tabindex="-1"></a>    keras.layers.Flatten(input_shape<span class="op">=</span>[<span class="dv">28</span>, <span class="dv">28</span>]),</span>
<span id="cb125-5"><a href="#cb125-5" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">300</span>, activation<span class="op">=</span><span class="st">"selu"</span>, kernel_initializer<span class="op">=</span><span class="st">"lecun_normal"</span>),</span>
<span id="cb125-6"><a href="#cb125-6" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">100</span>, activation<span class="op">=</span><span class="st">"selu"</span>, kernel_initializer<span class="op">=</span><span class="st">"lecun_normal"</span>),</span>
<span id="cb125-7"><a href="#cb125-7" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">"softmax"</span>)</span>
<span id="cb125-8"><a href="#cb125-8" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb125-9"><a href="#cb125-9" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>, optimizer<span class="op">=</span><span class="st">"nadam"</span>, metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span>
<span id="cb125-10"><a href="#cb125-10" aria-hidden="true" tabindex="-1"></a>n_epochs <span class="op">=</span> <span class="dv">25</span></span>
<span id="cb125-11"><a href="#cb125-11" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(X_train_scaled, y_train, epochs<span class="op">=</span>n_epochs,</span>
<span id="cb125-12"><a href="#cb125-12" aria-hidden="true" tabindex="-1"></a>                    validation_data<span class="op">=</span>(X_valid_scaled, y_valid),</span>
<span id="cb125-13"><a href="#cb125-13" aria-hidden="true" tabindex="-1"></a>                    callbacks<span class="op">=</span>[lr_scheduler])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/25
1719/1719 [==============================] - 4s 2ms/step - loss: 1.1511 - accuracy: 0.7326 - val_loss: 0.8456 - val_accuracy: 0.7410
Epoch 2/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.7371 - accuracy: 0.7786 - val_loss: 0.6796 - val_accuracy: 0.8092
Epoch 3/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.8055 - accuracy: 0.7700 - val_loss: 1.7429 - val_accuracy: 0.4514
Epoch 4/25
1719/1719 [==============================] - 4s 2ms/step - loss: 1.0351 - accuracy: 0.6826 - val_loss: 0.9870 - val_accuracy: 0.6928
Epoch 5/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.9185 - accuracy: 0.7098 - val_loss: 0.8727 - val_accuracy: 0.6932
Epoch 6/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.6905 - accuracy: 0.7481 - val_loss: 0.6694 - val_accuracy: 0.7696
Epoch 7/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.6115 - accuracy: 0.7713 - val_loss: 0.6956 - val_accuracy: 0.7306
Epoch 8/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.5791 - accuracy: 0.7793 - val_loss: 0.6659 - val_accuracy: 0.7738
Epoch 9/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.5622 - accuracy: 0.7881 - val_loss: 0.7363 - val_accuracy: 0.7850
Epoch 10/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.5253 - accuracy: 0.8470 - val_loss: 0.5484 - val_accuracy: 0.8578
Epoch 11/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.4401 - accuracy: 0.8694 - val_loss: 0.6724 - val_accuracy: 0.8602
Epoch 12/25
1719/1719 [==============================] - 4s 3ms/step - loss: 0.4334 - accuracy: 0.8732 - val_loss: 0.5551 - val_accuracy: 0.8504
Epoch 13/25
1719/1719 [==============================] - 5s 3ms/step - loss: 0.4179 - accuracy: 0.8771 - val_loss: 0.6685 - val_accuracy: 0.8554
Epoch 14/25
1719/1719 [==============================] - 5s 3ms/step - loss: 0.4300 - accuracy: 0.8775 - val_loss: 0.5340 - val_accuracy: 0.8584
Epoch 15/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.4069 - accuracy: 0.8777 - val_loss: 0.6519 - val_accuracy: 0.8478
Epoch 16/25
1719/1719 [==============================] - 5s 3ms/step - loss: 0.3349 - accuracy: 0.8953 - val_loss: 0.4801 - val_accuracy: 0.8778
Epoch 17/25
1719/1719 [==============================] - 4s 3ms/step - loss: 0.2695 - accuracy: 0.9109 - val_loss: 0.4880 - val_accuracy: 0.8786
Epoch 18/25
1719/1719 [==============================] - 5s 3ms/step - loss: 0.2568 - accuracy: 0.9136 - val_loss: 0.4726 - val_accuracy: 0.8822
Epoch 19/25
1719/1719 [==============================] - 5s 3ms/step - loss: 0.2436 - accuracy: 0.9203 - val_loss: 0.4792 - val_accuracy: 0.8842
Epoch 20/25
1719/1719 [==============================] - 4s 3ms/step - loss: 0.2421 - accuracy: 0.9212 - val_loss: 0.5088 - val_accuracy: 0.8838
Epoch 21/25
1719/1719 [==============================] - 4s 3ms/step - loss: 0.2288 - accuracy: 0.9246 - val_loss: 0.5083 - val_accuracy: 0.8830
Epoch 22/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.2215 - accuracy: 0.9270 - val_loss: 0.5217 - val_accuracy: 0.8846
Epoch 23/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.2106 - accuracy: 0.9297 - val_loss: 0.5297 - val_accuracy: 0.8834
Epoch 24/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.2002 - accuracy: 0.9334 - val_loss: 0.5597 - val_accuracy: 0.8864
Epoch 25/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.2005 - accuracy: 0.9350 - val_loss: 0.5533 - val_accuracy: 0.8868</code></pre>
</div>
</div>
<div id="cell-135" class="cell" data-execution_count="90">
<div class="sourceCode cell-code" id="cb127"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb127-1"><a href="#cb127-1" aria-hidden="true" tabindex="-1"></a>plt.plot(history.epoch, [piecewise_constant_fn(epoch) <span class="cf">for</span> epoch <span class="kw">in</span> history.epoch], <span class="st">"o-"</span>)</span>
<span id="cb127-2"><a href="#cb127-2" aria-hidden="true" tabindex="-1"></a>plt.axis([<span class="dv">0</span>, n_epochs <span class="op">-</span> <span class="dv">1</span>, <span class="dv">0</span>, <span class="fl">0.011</span>])</span>
<span id="cb127-3"><a href="#cb127-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Epoch"</span>)</span>
<span id="cb127-4"><a href="#cb127-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Learning Rate"</span>)</span>
<span id="cb127-5"><a href="#cb127-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Piecewise Constant Scheduling"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb127-6"><a href="#cb127-6" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb127-7"><a href="#cb127-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="11_training_deep_neural_networks_files/figure-html/cell-91-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="performance-scheduling" class="level3">
<h3 class="anchored" data-anchor-id="performance-scheduling">Performance Scheduling</h3>
<div id="cell-137" class="cell" data-execution_count="91">
<div class="sourceCode cell-code" id="cb128"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb128-1"><a href="#cb128-1" aria-hidden="true" tabindex="-1"></a>tf.random.set_seed(<span class="dv">42</span>)</span>
<span id="cb128-2"><a href="#cb128-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-138" class="cell" data-execution_count="92">
<div class="sourceCode cell-code" id="cb129"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb129-1"><a href="#cb129-1" aria-hidden="true" tabindex="-1"></a>lr_scheduler <span class="op">=</span> keras.callbacks.ReduceLROnPlateau(factor<span class="op">=</span><span class="fl">0.5</span>, patience<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb129-2"><a href="#cb129-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb129-3"><a href="#cb129-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.models.Sequential([</span>
<span id="cb129-4"><a href="#cb129-4" aria-hidden="true" tabindex="-1"></a>    keras.layers.Flatten(input_shape<span class="op">=</span>[<span class="dv">28</span>, <span class="dv">28</span>]),</span>
<span id="cb129-5"><a href="#cb129-5" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">300</span>, activation<span class="op">=</span><span class="st">"selu"</span>, kernel_initializer<span class="op">=</span><span class="st">"lecun_normal"</span>),</span>
<span id="cb129-6"><a href="#cb129-6" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">100</span>, activation<span class="op">=</span><span class="st">"selu"</span>, kernel_initializer<span class="op">=</span><span class="st">"lecun_normal"</span>),</span>
<span id="cb129-7"><a href="#cb129-7" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">"softmax"</span>)</span>
<span id="cb129-8"><a href="#cb129-8" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb129-9"><a href="#cb129-9" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> keras.optimizers.SGD(learning_rate<span class="op">=</span><span class="fl">0.02</span>, momentum<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb129-10"><a href="#cb129-10" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>, optimizer<span class="op">=</span>optimizer, metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span>
<span id="cb129-11"><a href="#cb129-11" aria-hidden="true" tabindex="-1"></a>n_epochs <span class="op">=</span> <span class="dv">25</span></span>
<span id="cb129-12"><a href="#cb129-12" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(X_train_scaled, y_train, epochs<span class="op">=</span>n_epochs,</span>
<span id="cb129-13"><a href="#cb129-13" aria-hidden="true" tabindex="-1"></a>                    validation_data<span class="op">=</span>(X_valid_scaled, y_valid),</span>
<span id="cb129-14"><a href="#cb129-14" aria-hidden="true" tabindex="-1"></a>                    callbacks<span class="op">=</span>[lr_scheduler])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.7116 - accuracy: 0.7769 - val_loss: 0.4869 - val_accuracy: 0.8478
Epoch 2/25
1719/1719 [==============================] - 2s 947us/step - loss: 0.4912 - accuracy: 0.8390 - val_loss: 0.5958 - val_accuracy: 0.8270
Epoch 3/25
1719/1719 [==============================] - 2s 987us/step - loss: 0.5222 - accuracy: 0.8379 - val_loss: 0.4869 - val_accuracy: 0.8584
Epoch 4/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.5061 - accuracy: 0.8467 - val_loss: 0.4588 - val_accuracy: 0.8548
Epoch 5/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.5216 - accuracy: 0.8469 - val_loss: 0.6096 - val_accuracy: 0.8300
Epoch 6/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.4984 - accuracy: 0.8546 - val_loss: 0.5359 - val_accuracy: 0.8498
Epoch 7/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.5104 - accuracy: 0.8579 - val_loss: 0.5457 - val_accuracy: 0.8522
Epoch 8/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.5375 - accuracy: 0.8538 - val_loss: 0.6445 - val_accuracy: 0.8218
Epoch 9/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.5333 - accuracy: 0.8522 - val_loss: 0.5472 - val_accuracy: 0.8560
Epoch 10/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.3280 - accuracy: 0.8902 - val_loss: 0.3826 - val_accuracy: 0.8876
Epoch 11/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2410 - accuracy: 0.9135 - val_loss: 0.4025 - val_accuracy: 0.8876
Epoch 12/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2266 - accuracy: 0.9180 - val_loss: 0.4540 - val_accuracy: 0.8694
Epoch 13/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2129 - accuracy: 0.9221 - val_loss: 0.4310 - val_accuracy: 0.8866
Epoch 14/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.1959 - accuracy: 0.9270 - val_loss: 0.4406 - val_accuracy: 0.8814
Epoch 15/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.1975 - accuracy: 0.9277 - val_loss: 0.4341 - val_accuracy: 0.8840
Epoch 16/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.1409 - accuracy: 0.9464 - val_loss: 0.4220 - val_accuracy: 0.8932
Epoch 17/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.1181 - accuracy: 0.9542 - val_loss: 0.4409 - val_accuracy: 0.8948
Epoch 18/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.1124 - accuracy: 0.9560 - val_loss: 0.4480 - val_accuracy: 0.8898
Epoch 19/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.1070 - accuracy: 0.9579 - val_loss: 0.4610 - val_accuracy: 0.8932
Epoch 20/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.1016 - accuracy: 0.9606 - val_loss: 0.4845 - val_accuracy: 0.8918
Epoch 21/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.0848 - accuracy: 0.9686 - val_loss: 0.4829 - val_accuracy: 0.8934
Epoch 22/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.0792 - accuracy: 0.9700 - val_loss: 0.4906 - val_accuracy: 0.8952
Epoch 23/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.0751 - accuracy: 0.9720 - val_loss: 0.4951 - val_accuracy: 0.8950
Epoch 24/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.0687 - accuracy: 0.9739 - val_loss: 0.5109 - val_accuracy: 0.8948
Epoch 25/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.0683 - accuracy: 0.9752 - val_loss: 0.5241 - val_accuracy: 0.8936</code></pre>
</div>
</div>
<div id="cell-139" class="cell" data-execution_count="93">
<div class="sourceCode cell-code" id="cb131"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb131-1"><a href="#cb131-1" aria-hidden="true" tabindex="-1"></a>plt.plot(history.epoch, history.history[<span class="st">"lr"</span>], <span class="st">"bo-"</span>)</span>
<span id="cb131-2"><a href="#cb131-2" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Epoch"</span>)</span>
<span id="cb131-3"><a href="#cb131-3" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Learning Rate"</span>, color<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb131-4"><a href="#cb131-4" aria-hidden="true" tabindex="-1"></a>plt.tick_params(<span class="st">'y'</span>, colors<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb131-5"><a href="#cb131-5" aria-hidden="true" tabindex="-1"></a>plt.gca().set_xlim(<span class="dv">0</span>, n_epochs <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb131-6"><a href="#cb131-6" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb131-7"><a href="#cb131-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-8"><a href="#cb131-8" aria-hidden="true" tabindex="-1"></a>ax2 <span class="op">=</span> plt.gca().twinx()</span>
<span id="cb131-9"><a href="#cb131-9" aria-hidden="true" tabindex="-1"></a>ax2.plot(history.epoch, history.history[<span class="st">"val_loss"</span>], <span class="st">"r^-"</span>)</span>
<span id="cb131-10"><a href="#cb131-10" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Validation Loss'</span>, color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb131-11"><a href="#cb131-11" aria-hidden="true" tabindex="-1"></a>ax2.tick_params(<span class="st">'y'</span>, colors<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb131-12"><a href="#cb131-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-13"><a href="#cb131-13" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Reduce LR on Plateau"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb131-14"><a href="#cb131-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="11_training_deep_neural_networks_files/figure-html/cell-94-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="tf.keras-schedulers" class="level3">
<h3 class="anchored" data-anchor-id="tf.keras-schedulers">tf.keras schedulers</h3>
<div id="cell-141" class="cell" data-execution_count="94">
<div class="sourceCode cell-code" id="cb132"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb132-1"><a href="#cb132-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.models.Sequential([</span>
<span id="cb132-2"><a href="#cb132-2" aria-hidden="true" tabindex="-1"></a>    keras.layers.Flatten(input_shape<span class="op">=</span>[<span class="dv">28</span>, <span class="dv">28</span>]),</span>
<span id="cb132-3"><a href="#cb132-3" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">300</span>, activation<span class="op">=</span><span class="st">"selu"</span>, kernel_initializer<span class="op">=</span><span class="st">"lecun_normal"</span>),</span>
<span id="cb132-4"><a href="#cb132-4" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">100</span>, activation<span class="op">=</span><span class="st">"selu"</span>, kernel_initializer<span class="op">=</span><span class="st">"lecun_normal"</span>),</span>
<span id="cb132-5"><a href="#cb132-5" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">"softmax"</span>)</span>
<span id="cb132-6"><a href="#cb132-6" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb132-7"><a href="#cb132-7" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> <span class="dv">20</span> <span class="op">*</span> <span class="bu">len</span>(X_train) <span class="op">//</span> <span class="dv">32</span> <span class="co"># number of steps in 20 epochs (batch size = 32)</span></span>
<span id="cb132-8"><a href="#cb132-8" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> keras.optimizers.schedules.ExponentialDecay(<span class="fl">0.01</span>, s, <span class="fl">0.1</span>)</span>
<span id="cb132-9"><a href="#cb132-9" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> keras.optimizers.SGD(learning_rate)</span>
<span id="cb132-10"><a href="#cb132-10" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>, optimizer<span class="op">=</span>optimizer, metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span>
<span id="cb132-11"><a href="#cb132-11" aria-hidden="true" tabindex="-1"></a>n_epochs <span class="op">=</span> <span class="dv">25</span></span>
<span id="cb132-12"><a href="#cb132-12" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(X_train_scaled, y_train, epochs<span class="op">=</span>n_epochs,</span>
<span id="cb132-13"><a href="#cb132-13" aria-hidden="true" tabindex="-1"></a>                    validation_data<span class="op">=</span>(X_valid_scaled, y_valid))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.5995 - accuracy: 0.7923 - val_loss: 0.4095 - val_accuracy: 0.8606
Epoch 2/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.3890 - accuracy: 0.8613 - val_loss: 0.3738 - val_accuracy: 0.8692
Epoch 3/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.3530 - accuracy: 0.8772 - val_loss: 0.3735 - val_accuracy: 0.8692
Epoch 4/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.3296 - accuracy: 0.8813 - val_loss: 0.3494 - val_accuracy: 0.8798
Epoch 5/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.3178 - accuracy: 0.8867 - val_loss: 0.3430 - val_accuracy: 0.8794
Epoch 6/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2930 - accuracy: 0.8951 - val_loss: 0.3414 - val_accuracy: 0.8826
Epoch 7/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2854 - accuracy: 0.8985 - val_loss: 0.3354 - val_accuracy: 0.8810
Epoch 8/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2714 - accuracy: 0.9039 - val_loss: 0.3364 - val_accuracy: 0.8824
Epoch 9/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2714 - accuracy: 0.9047 - val_loss: 0.3265 - val_accuracy: 0.8846
Epoch 10/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2570 - accuracy: 0.9084 - val_loss: 0.3238 - val_accuracy: 0.8854
Epoch 11/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2502 - accuracy: 0.9117 - val_loss: 0.3250 - val_accuracy: 0.8862
Epoch 12/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2453 - accuracy: 0.9145 - val_loss: 0.3299 - val_accuracy: 0.8830
Epoch 13/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2408 - accuracy: 0.9154 - val_loss: 0.3219 - val_accuracy: 0.8870
Epoch 14/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2380 - accuracy: 0.9154 - val_loss: 0.3221 - val_accuracy: 0.8860
Epoch 15/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2378 - accuracy: 0.9166 - val_loss: 0.3208 - val_accuracy: 0.8864
Epoch 16/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2318 - accuracy: 0.9191 - val_loss: 0.3184 - val_accuracy: 0.8892
Epoch 17/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2266 - accuracy: 0.9212 - val_loss: 0.3197 - val_accuracy: 0.8906
Epoch 18/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2284 - accuracy: 0.9185 - val_loss: 0.3169 - val_accuracy: 0.8906
Epoch 19/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2286 - accuracy: 0.9205 - val_loss: 0.3197 - val_accuracy: 0.8884
Epoch 20/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2288 - accuracy: 0.9211 - val_loss: 0.3169 - val_accuracy: 0.8906
Epoch 21/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2265 - accuracy: 0.9212 - val_loss: 0.3179 - val_accuracy: 0.8904
Epoch 22/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2258 - accuracy: 0.9205 - val_loss: 0.3163 - val_accuracy: 0.8914
Epoch 23/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2224 - accuracy: 0.9226 - val_loss: 0.3170 - val_accuracy: 0.8904
Epoch 24/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2182 - accuracy: 0.9244 - val_loss: 0.3165 - val_accuracy: 0.8898
Epoch 25/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2224 - accuracy: 0.9229 - val_loss: 0.3164 - val_accuracy: 0.8904</code></pre>
</div>
</div>
<p>For piecewise constant scheduling, try this:</p>
<div id="cell-143" class="cell" data-execution_count="95">
<div class="sourceCode cell-code" id="cb134"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> keras.optimizers.schedules.PiecewiseConstantDecay(</span>
<span id="cb134-2"><a href="#cb134-2" aria-hidden="true" tabindex="-1"></a>    boundaries<span class="op">=</span>[<span class="fl">5.</span> <span class="op">*</span> n_steps_per_epoch, <span class="fl">15.</span> <span class="op">*</span> n_steps_per_epoch],</span>
<span id="cb134-3"><a href="#cb134-3" aria-hidden="true" tabindex="-1"></a>    values<span class="op">=</span>[<span class="fl">0.01</span>, <span class="fl">0.005</span>, <span class="fl">0.001</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="cycle-scheduling" class="level3">
<h3 class="anchored" data-anchor-id="cycle-scheduling">1Cycle scheduling</h3>
<div id="cell-145" class="cell" data-execution_count="96">
<div class="sourceCode cell-code" id="cb135"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb135-1"><a href="#cb135-1" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> keras.backend</span>
<span id="cb135-2"><a href="#cb135-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb135-3"><a href="#cb135-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ExponentialLearningRate(keras.callbacks.Callback):</span>
<span id="cb135-4"><a href="#cb135-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, factor):</span>
<span id="cb135-5"><a href="#cb135-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.factor <span class="op">=</span> factor</span>
<span id="cb135-6"><a href="#cb135-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rates <span class="op">=</span> []</span>
<span id="cb135-7"><a href="#cb135-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.losses <span class="op">=</span> []</span>
<span id="cb135-8"><a href="#cb135-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> on_batch_end(<span class="va">self</span>, batch, logs):</span>
<span id="cb135-9"><a href="#cb135-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rates.append(K.get_value(<span class="va">self</span>.model.optimizer.learning_rate))</span>
<span id="cb135-10"><a href="#cb135-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.losses.append(logs[<span class="st">"loss"</span>])</span>
<span id="cb135-11"><a href="#cb135-11" aria-hidden="true" tabindex="-1"></a>        K.set_value(<span class="va">self</span>.model.optimizer.learning_rate, <span class="va">self</span>.model.optimizer.learning_rate <span class="op">*</span> <span class="va">self</span>.factor)</span>
<span id="cb135-12"><a href="#cb135-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb135-13"><a href="#cb135-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> find_learning_rate(model, X, y, epochs<span class="op">=</span><span class="dv">1</span>, batch_size<span class="op">=</span><span class="dv">32</span>, min_rate<span class="op">=</span><span class="dv">10</span><span class="op">**-</span><span class="dv">5</span>, max_rate<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb135-14"><a href="#cb135-14" aria-hidden="true" tabindex="-1"></a>    init_weights <span class="op">=</span> model.get_weights()</span>
<span id="cb135-15"><a href="#cb135-15" aria-hidden="true" tabindex="-1"></a>    iterations <span class="op">=</span> math.ceil(<span class="bu">len</span>(X) <span class="op">/</span> batch_size) <span class="op">*</span> epochs</span>
<span id="cb135-16"><a href="#cb135-16" aria-hidden="true" tabindex="-1"></a>    factor <span class="op">=</span> np.exp(np.log(max_rate <span class="op">/</span> min_rate) <span class="op">/</span> iterations)</span>
<span id="cb135-17"><a href="#cb135-17" aria-hidden="true" tabindex="-1"></a>    init_lr <span class="op">=</span> K.get_value(model.optimizer.learning_rate)</span>
<span id="cb135-18"><a href="#cb135-18" aria-hidden="true" tabindex="-1"></a>    K.set_value(model.optimizer.learning_rate, min_rate)</span>
<span id="cb135-19"><a href="#cb135-19" aria-hidden="true" tabindex="-1"></a>    exp_lr <span class="op">=</span> ExponentialLearningRate(factor)</span>
<span id="cb135-20"><a href="#cb135-20" aria-hidden="true" tabindex="-1"></a>    history <span class="op">=</span> model.fit(X, y, epochs<span class="op">=</span>epochs, batch_size<span class="op">=</span>batch_size,</span>
<span id="cb135-21"><a href="#cb135-21" aria-hidden="true" tabindex="-1"></a>                        callbacks<span class="op">=</span>[exp_lr])</span>
<span id="cb135-22"><a href="#cb135-22" aria-hidden="true" tabindex="-1"></a>    K.set_value(model.optimizer.learning_rate, init_lr)</span>
<span id="cb135-23"><a href="#cb135-23" aria-hidden="true" tabindex="-1"></a>    model.set_weights(init_weights)</span>
<span id="cb135-24"><a href="#cb135-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> exp_lr.rates, exp_lr.losses</span>
<span id="cb135-25"><a href="#cb135-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb135-26"><a href="#cb135-26" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_lr_vs_loss(rates, losses):</span>
<span id="cb135-27"><a href="#cb135-27" aria-hidden="true" tabindex="-1"></a>    plt.plot(rates, losses)</span>
<span id="cb135-28"><a href="#cb135-28" aria-hidden="true" tabindex="-1"></a>    plt.gca().set_xscale(<span class="st">'log'</span>)</span>
<span id="cb135-29"><a href="#cb135-29" aria-hidden="true" tabindex="-1"></a>    plt.hlines(<span class="bu">min</span>(losses), <span class="bu">min</span>(rates), <span class="bu">max</span>(rates))</span>
<span id="cb135-30"><a href="#cb135-30" aria-hidden="true" tabindex="-1"></a>    plt.axis([<span class="bu">min</span>(rates), <span class="bu">max</span>(rates), <span class="bu">min</span>(losses), (losses[<span class="dv">0</span>] <span class="op">+</span> <span class="bu">min</span>(losses)) <span class="op">/</span> <span class="dv">2</span>])</span>
<span id="cb135-31"><a href="#cb135-31" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">"Learning rate"</span>)</span>
<span id="cb135-32"><a href="#cb135-32" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">"Loss"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Warning</strong>: In the <code>on_batch_end()</code> method, <code>logs["loss"]</code> used to contain the batch loss, but in TensorFlow 2.2.0 it was replaced with the mean loss (since the start of the epoch). This explains why the graph below is much smoother than in the book (if you are using TF 2.2 or above). It also means that there is a lag between the moment the batch loss starts exploding and the moment the explosion becomes clear in the graph. So you should choose a slightly smaller learning rate than you would have chosen with the “noisy” graph. Alternatively, you can tweak the <code>ExponentialLearningRate</code> callback above so it computes the batch loss (based on the current mean loss and the previous mean loss):</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb136-1"><a href="#cb136-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ExponentialLearningRate(keras.callbacks.Callback):</span>
<span id="cb136-2"><a href="#cb136-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, factor):</span>
<span id="cb136-3"><a href="#cb136-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.factor <span class="op">=</span> factor</span>
<span id="cb136-4"><a href="#cb136-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rates <span class="op">=</span> []</span>
<span id="cb136-5"><a href="#cb136-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.losses <span class="op">=</span> []</span>
<span id="cb136-6"><a href="#cb136-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> on_epoch_begin(<span class="va">self</span>, epoch, logs<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb136-7"><a href="#cb136-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.prev_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb136-8"><a href="#cb136-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> on_batch_end(<span class="va">self</span>, batch, logs<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb136-9"><a href="#cb136-9" aria-hidden="true" tabindex="-1"></a>        batch_loss <span class="op">=</span> logs[<span class="st">"loss"</span>] <span class="op">*</span> (batch <span class="op">+</span> <span class="dv">1</span>) <span class="op">-</span> <span class="va">self</span>.prev_loss <span class="op">*</span> batch</span>
<span id="cb136-10"><a href="#cb136-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.prev_loss <span class="op">=</span> logs[<span class="st">"loss"</span>]</span>
<span id="cb136-11"><a href="#cb136-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rates.append(K.get_value(<span class="va">self</span>.model.optimizer.learning_rate))</span>
<span id="cb136-12"><a href="#cb136-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.losses.append(batch_loss)</span>
<span id="cb136-13"><a href="#cb136-13" aria-hidden="true" tabindex="-1"></a>        K.set_value(<span class="va">self</span>.model.optimizer.learning_rate, <span class="va">self</span>.model.optimizer.learning_rate <span class="op">*</span> <span class="va">self</span>.factor)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="cell-147" class="cell" data-execution_count="97">
<div class="sourceCode cell-code" id="cb137"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb137-1"><a href="#cb137-1" aria-hidden="true" tabindex="-1"></a>tf.random.set_seed(<span class="dv">42</span>)</span>
<span id="cb137-2"><a href="#cb137-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb137-3"><a href="#cb137-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb137-4"><a href="#cb137-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.models.Sequential([</span>
<span id="cb137-5"><a href="#cb137-5" aria-hidden="true" tabindex="-1"></a>    keras.layers.Flatten(input_shape<span class="op">=</span>[<span class="dv">28</span>, <span class="dv">28</span>]),</span>
<span id="cb137-6"><a href="#cb137-6" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">300</span>, activation<span class="op">=</span><span class="st">"selu"</span>, kernel_initializer<span class="op">=</span><span class="st">"lecun_normal"</span>),</span>
<span id="cb137-7"><a href="#cb137-7" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">100</span>, activation<span class="op">=</span><span class="st">"selu"</span>, kernel_initializer<span class="op">=</span><span class="st">"lecun_normal"</span>),</span>
<span id="cb137-8"><a href="#cb137-8" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">"softmax"</span>)</span>
<span id="cb137-9"><a href="#cb137-9" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb137-10"><a href="#cb137-10" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>,</span>
<span id="cb137-11"><a href="#cb137-11" aria-hidden="true" tabindex="-1"></a>              optimizer<span class="op">=</span>keras.optimizers.SGD(learning_rate<span class="op">=</span><span class="fl">1e-3</span>),</span>
<span id="cb137-12"><a href="#cb137-12" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-148" class="cell" data-execution_count="98">
<div class="sourceCode cell-code" id="cb138"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb138-1"><a href="#cb138-1" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb138-2"><a href="#cb138-2" aria-hidden="true" tabindex="-1"></a>rates, losses <span class="op">=</span> find_learning_rate(model, X_train_scaled, y_train, epochs<span class="op">=</span><span class="dv">1</span>, batch_size<span class="op">=</span>batch_size)</span>
<span id="cb138-3"><a href="#cb138-3" aria-hidden="true" tabindex="-1"></a>plot_lr_vs_loss(rates, losses)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>430/430 [==============================] - 1s 2ms/step - loss: nan - accuracy: 0.3120</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="11_training_deep_neural_networks_files/figure-html/cell-99-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-149" class="cell" data-execution_count="99">
<div class="sourceCode cell-code" id="cb140"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb140-1"><a href="#cb140-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> OneCycleScheduler(keras.callbacks.Callback):</span>
<span id="cb140-2"><a href="#cb140-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, iterations, max_rate, start_rate<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb140-3"><a href="#cb140-3" aria-hidden="true" tabindex="-1"></a>                 last_iterations<span class="op">=</span><span class="va">None</span>, last_rate<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb140-4"><a href="#cb140-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.iterations <span class="op">=</span> iterations</span>
<span id="cb140-5"><a href="#cb140-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.max_rate <span class="op">=</span> max_rate</span>
<span id="cb140-6"><a href="#cb140-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.start_rate <span class="op">=</span> start_rate <span class="kw">or</span> max_rate <span class="op">/</span> <span class="dv">10</span></span>
<span id="cb140-7"><a href="#cb140-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.last_iterations <span class="op">=</span> last_iterations <span class="kw">or</span> iterations <span class="op">//</span> <span class="dv">10</span> <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb140-8"><a href="#cb140-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.half_iteration <span class="op">=</span> (iterations <span class="op">-</span> <span class="va">self</span>.last_iterations) <span class="op">//</span> <span class="dv">2</span></span>
<span id="cb140-9"><a href="#cb140-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.last_rate <span class="op">=</span> last_rate <span class="kw">or</span> <span class="va">self</span>.start_rate <span class="op">/</span> <span class="dv">1000</span></span>
<span id="cb140-10"><a href="#cb140-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.iteration <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb140-11"><a href="#cb140-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _interpolate(<span class="va">self</span>, iter1, iter2, rate1, rate2):</span>
<span id="cb140-12"><a href="#cb140-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> ((rate2 <span class="op">-</span> rate1) <span class="op">*</span> (<span class="va">self</span>.iteration <span class="op">-</span> iter1)</span>
<span id="cb140-13"><a href="#cb140-13" aria-hidden="true" tabindex="-1"></a>                <span class="op">/</span> (iter2 <span class="op">-</span> iter1) <span class="op">+</span> rate1)</span>
<span id="cb140-14"><a href="#cb140-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> on_batch_begin(<span class="va">self</span>, batch, logs):</span>
<span id="cb140-15"><a href="#cb140-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.iteration <span class="op">&lt;</span> <span class="va">self</span>.half_iteration:</span>
<span id="cb140-16"><a href="#cb140-16" aria-hidden="true" tabindex="-1"></a>            rate <span class="op">=</span> <span class="va">self</span>._interpolate(<span class="dv">0</span>, <span class="va">self</span>.half_iteration, <span class="va">self</span>.start_rate, <span class="va">self</span>.max_rate)</span>
<span id="cb140-17"><a href="#cb140-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="va">self</span>.iteration <span class="op">&lt;</span> <span class="dv">2</span> <span class="op">*</span> <span class="va">self</span>.half_iteration:</span>
<span id="cb140-18"><a href="#cb140-18" aria-hidden="true" tabindex="-1"></a>            rate <span class="op">=</span> <span class="va">self</span>._interpolate(<span class="va">self</span>.half_iteration, <span class="dv">2</span> <span class="op">*</span> <span class="va">self</span>.half_iteration,</span>
<span id="cb140-19"><a href="#cb140-19" aria-hidden="true" tabindex="-1"></a>                                     <span class="va">self</span>.max_rate, <span class="va">self</span>.start_rate)</span>
<span id="cb140-20"><a href="#cb140-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb140-21"><a href="#cb140-21" aria-hidden="true" tabindex="-1"></a>            rate <span class="op">=</span> <span class="va">self</span>._interpolate(<span class="dv">2</span> <span class="op">*</span> <span class="va">self</span>.half_iteration, <span class="va">self</span>.iterations,</span>
<span id="cb140-22"><a href="#cb140-22" aria-hidden="true" tabindex="-1"></a>                                     <span class="va">self</span>.start_rate, <span class="va">self</span>.last_rate)</span>
<span id="cb140-23"><a href="#cb140-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.iteration <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb140-24"><a href="#cb140-24" aria-hidden="true" tabindex="-1"></a>        K.set_value(<span class="va">self</span>.model.optimizer.learning_rate, rate)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-150" class="cell" data-execution_count="100">
<div class="sourceCode cell-code" id="cb141"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb141-1"><a href="#cb141-1" aria-hidden="true" tabindex="-1"></a>n_epochs <span class="op">=</span> <span class="dv">25</span></span>
<span id="cb141-2"><a href="#cb141-2" aria-hidden="true" tabindex="-1"></a>onecycle <span class="op">=</span> OneCycleScheduler(math.ceil(<span class="bu">len</span>(X_train) <span class="op">/</span> batch_size) <span class="op">*</span> n_epochs, max_rate<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb141-3"><a href="#cb141-3" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(X_train_scaled, y_train, epochs<span class="op">=</span>n_epochs, batch_size<span class="op">=</span>batch_size,</span>
<span id="cb141-4"><a href="#cb141-4" aria-hidden="true" tabindex="-1"></a>                    validation_data<span class="op">=</span>(X_valid_scaled, y_valid),</span>
<span id="cb141-5"><a href="#cb141-5" aria-hidden="true" tabindex="-1"></a>                    callbacks<span class="op">=</span>[onecycle])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/25
430/430 [==============================] - 1s 2ms/step - loss: 0.6572 - accuracy: 0.7740 - val_loss: 0.4872 - val_accuracy: 0.8338
Epoch 2/25
430/430 [==============================] - 1s 2ms/step - loss: 0.4580 - accuracy: 0.8397 - val_loss: 0.4274 - val_accuracy: 0.8520
Epoch 3/25
430/430 [==============================] - 1s 2ms/step - loss: 0.4121 - accuracy: 0.8545 - val_loss: 0.4116 - val_accuracy: 0.8588
Epoch 4/25
430/430 [==============================] - 1s 2ms/step - loss: 0.3837 - accuracy: 0.8642 - val_loss: 0.3868 - val_accuracy: 0.8688
Epoch 5/25
430/430 [==============================] - 1s 2ms/step - loss: 0.3639 - accuracy: 0.8719 - val_loss: 0.3766 - val_accuracy: 0.8688
Epoch 6/25
430/430 [==============================] - 1s 2ms/step - loss: 0.3456 - accuracy: 0.8775 - val_loss: 0.3739 - val_accuracy: 0.8706
Epoch 7/25
430/430 [==============================] - 1s 2ms/step - loss: 0.3330 - accuracy: 0.8811 - val_loss: 0.3635 - val_accuracy: 0.8708
Epoch 8/25
430/430 [==============================] - 1s 2ms/step - loss: 0.3184 - accuracy: 0.8861 - val_loss: 0.3959 - val_accuracy: 0.8610
Epoch 9/25
430/430 [==============================] - 1s 2ms/step - loss: 0.3065 - accuracy: 0.8890 - val_loss: 0.3475 - val_accuracy: 0.8770
Epoch 10/25
430/430 [==============================] - 1s 2ms/step - loss: 0.2943 - accuracy: 0.8927 - val_loss: 0.3392 - val_accuracy: 0.8806
Epoch 11/25
430/430 [==============================] - 1s 2ms/step - loss: 0.2838 - accuracy: 0.8963 - val_loss: 0.3467 - val_accuracy: 0.8800
Epoch 12/25
430/430 [==============================] - 1s 2ms/step - loss: 0.2707 - accuracy: 0.9024 - val_loss: 0.3646 - val_accuracy: 0.8696
Epoch 13/25
430/430 [==============================] - 1s 2ms/step - loss: 0.2536 - accuracy: 0.9079 - val_loss: 0.3350 - val_accuracy: 0.8842
Epoch 14/25
430/430 [==============================] - 1s 2ms/step - loss: 0.2405 - accuracy: 0.9135 - val_loss: 0.3465 - val_accuracy: 0.8794
Epoch 15/25
430/430 [==============================] - 1s 2ms/step - loss: 0.2279 - accuracy: 0.9185 - val_loss: 0.3257 - val_accuracy: 0.8830
Epoch 16/25
430/430 [==============================] - 1s 2ms/step - loss: 0.2159 - accuracy: 0.9232 - val_loss: 0.3294 - val_accuracy: 0.8824
Epoch 17/25
430/430 [==============================] - 1s 2ms/step - loss: 0.2062 - accuracy: 0.9263 - val_loss: 0.3333 - val_accuracy: 0.8882
Epoch 18/25
430/430 [==============================] - 1s 2ms/step - loss: 0.1978 - accuracy: 0.9301 - val_loss: 0.3235 - val_accuracy: 0.8898
Epoch 19/25
430/430 [==============================] - 1s 2ms/step - loss: 0.1892 - accuracy: 0.9337 - val_loss: 0.3233 - val_accuracy: 0.8906
Epoch 20/25
430/430 [==============================] - 1s 2ms/step - loss: 0.1821 - accuracy: 0.9365 - val_loss: 0.3224 - val_accuracy: 0.8928
Epoch 21/25
430/430 [==============================] - 1s 2ms/step - loss: 0.1752 - accuracy: 0.9400 - val_loss: 0.3220 - val_accuracy: 0.8908
Epoch 22/25
430/430 [==============================] - 1s 2ms/step - loss: 0.1700 - accuracy: 0.9416 - val_loss: 0.3180 - val_accuracy: 0.8962
Epoch 23/25
430/430 [==============================] - 1s 2ms/step - loss: 0.1655 - accuracy: 0.9438 - val_loss: 0.3187 - val_accuracy: 0.8940
Epoch 24/25
430/430 [==============================] - 1s 2ms/step - loss: 0.1627 - accuracy: 0.9454 - val_loss: 0.3177 - val_accuracy: 0.8932
Epoch 25/25
430/430 [==============================] - 1s 2ms/step - loss: 0.1610 - accuracy: 0.9462 - val_loss: 0.3170 - val_accuracy: 0.8934</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="avoiding-overfitting-through-regularization" class="level1">
<h1>Avoiding Overfitting Through Regularization</h1>
<section id="ell_1-and-ell_2-regularization" class="level2">
<h2 class="anchored" data-anchor-id="ell_1-and-ell_2-regularization"><span class="math inline">\(\ell_1\)</span> and <span class="math inline">\(\ell_2\)</span> regularization</h2>
<div id="cell-153" class="cell" data-execution_count="101">
<div class="sourceCode cell-code" id="cb143"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb143-1"><a href="#cb143-1" aria-hidden="true" tabindex="-1"></a>layer <span class="op">=</span> keras.layers.Dense(<span class="dv">100</span>, activation<span class="op">=</span><span class="st">"elu"</span>,</span>
<span id="cb143-2"><a href="#cb143-2" aria-hidden="true" tabindex="-1"></a>                           kernel_initializer<span class="op">=</span><span class="st">"he_normal"</span>,</span>
<span id="cb143-3"><a href="#cb143-3" aria-hidden="true" tabindex="-1"></a>                           kernel_regularizer<span class="op">=</span>keras.regularizers.l2(<span class="fl">0.01</span>))</span>
<span id="cb143-4"><a href="#cb143-4" aria-hidden="true" tabindex="-1"></a><span class="co"># or l1(0.1) for ℓ1 regularization with a factor of 0.1</span></span>
<span id="cb143-5"><a href="#cb143-5" aria-hidden="true" tabindex="-1"></a><span class="co"># or l1_l2(0.1, 0.01) for both ℓ1 and ℓ2 regularization, with factors 0.1 and 0.01 respectively</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-154" class="cell" data-execution_count="102">
<div class="sourceCode cell-code" id="cb144"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb144-1"><a href="#cb144-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.models.Sequential([</span>
<span id="cb144-2"><a href="#cb144-2" aria-hidden="true" tabindex="-1"></a>    keras.layers.Flatten(input_shape<span class="op">=</span>[<span class="dv">28</span>, <span class="dv">28</span>]),</span>
<span id="cb144-3"><a href="#cb144-3" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">300</span>, activation<span class="op">=</span><span class="st">"elu"</span>,</span>
<span id="cb144-4"><a href="#cb144-4" aria-hidden="true" tabindex="-1"></a>                       kernel_initializer<span class="op">=</span><span class="st">"he_normal"</span>,</span>
<span id="cb144-5"><a href="#cb144-5" aria-hidden="true" tabindex="-1"></a>                       kernel_regularizer<span class="op">=</span>keras.regularizers.l2(<span class="fl">0.01</span>)),</span>
<span id="cb144-6"><a href="#cb144-6" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">100</span>, activation<span class="op">=</span><span class="st">"elu"</span>,</span>
<span id="cb144-7"><a href="#cb144-7" aria-hidden="true" tabindex="-1"></a>                       kernel_initializer<span class="op">=</span><span class="st">"he_normal"</span>,</span>
<span id="cb144-8"><a href="#cb144-8" aria-hidden="true" tabindex="-1"></a>                       kernel_regularizer<span class="op">=</span>keras.regularizers.l2(<span class="fl">0.01</span>)),</span>
<span id="cb144-9"><a href="#cb144-9" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">"softmax"</span>,</span>
<span id="cb144-10"><a href="#cb144-10" aria-hidden="true" tabindex="-1"></a>                       kernel_regularizer<span class="op">=</span>keras.regularizers.l2(<span class="fl">0.01</span>))</span>
<span id="cb144-11"><a href="#cb144-11" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb144-12"><a href="#cb144-12" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>, optimizer<span class="op">=</span><span class="st">"nadam"</span>, metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span>
<span id="cb144-13"><a href="#cb144-13" aria-hidden="true" tabindex="-1"></a>n_epochs <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb144-14"><a href="#cb144-14" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(X_train_scaled, y_train, epochs<span class="op">=</span>n_epochs,</span>
<span id="cb144-15"><a href="#cb144-15" aria-hidden="true" tabindex="-1"></a>                    validation_data<span class="op">=</span>(X_valid_scaled, y_valid))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/2
1719/1719 [==============================] - 6s 3ms/step - loss: 3.2189 - accuracy: 0.7967 - val_loss: 0.7169 - val_accuracy: 0.8340
Epoch 2/2
1719/1719 [==============================] - 5s 3ms/step - loss: 0.7280 - accuracy: 0.8247 - val_loss: 0.6850 - val_accuracy: 0.8376</code></pre>
</div>
</div>
<div id="cell-155" class="cell" data-execution_count="103">
<div class="sourceCode cell-code" id="cb146"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb146-1"><a href="#cb146-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> functools <span class="im">import</span> partial</span>
<span id="cb146-2"><a href="#cb146-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb146-3"><a href="#cb146-3" aria-hidden="true" tabindex="-1"></a>RegularizedDense <span class="op">=</span> partial(keras.layers.Dense,</span>
<span id="cb146-4"><a href="#cb146-4" aria-hidden="true" tabindex="-1"></a>                           activation<span class="op">=</span><span class="st">"elu"</span>,</span>
<span id="cb146-5"><a href="#cb146-5" aria-hidden="true" tabindex="-1"></a>                           kernel_initializer<span class="op">=</span><span class="st">"he_normal"</span>,</span>
<span id="cb146-6"><a href="#cb146-6" aria-hidden="true" tabindex="-1"></a>                           kernel_regularizer<span class="op">=</span>keras.regularizers.l2(<span class="fl">0.01</span>))</span>
<span id="cb146-7"><a href="#cb146-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb146-8"><a href="#cb146-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.models.Sequential([</span>
<span id="cb146-9"><a href="#cb146-9" aria-hidden="true" tabindex="-1"></a>    keras.layers.Flatten(input_shape<span class="op">=</span>[<span class="dv">28</span>, <span class="dv">28</span>]),</span>
<span id="cb146-10"><a href="#cb146-10" aria-hidden="true" tabindex="-1"></a>    RegularizedDense(<span class="dv">300</span>),</span>
<span id="cb146-11"><a href="#cb146-11" aria-hidden="true" tabindex="-1"></a>    RegularizedDense(<span class="dv">100</span>),</span>
<span id="cb146-12"><a href="#cb146-12" aria-hidden="true" tabindex="-1"></a>    RegularizedDense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">"softmax"</span>)</span>
<span id="cb146-13"><a href="#cb146-13" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb146-14"><a href="#cb146-14" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>, optimizer<span class="op">=</span><span class="st">"nadam"</span>, metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span>
<span id="cb146-15"><a href="#cb146-15" aria-hidden="true" tabindex="-1"></a>n_epochs <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb146-16"><a href="#cb146-16" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(X_train_scaled, y_train, epochs<span class="op">=</span>n_epochs,</span>
<span id="cb146-17"><a href="#cb146-17" aria-hidden="true" tabindex="-1"></a>                    validation_data<span class="op">=</span>(X_valid_scaled, y_valid))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/2
1719/1719 [==============================] - 6s 3ms/step - loss: 3.2911 - accuracy: 0.7924 - val_loss: 0.7218 - val_accuracy: 0.8310
Epoch 2/2
1719/1719 [==============================] - 5s 3ms/step - loss: 0.7282 - accuracy: 0.8245 - val_loss: 0.6826 - val_accuracy: 0.8382</code></pre>
</div>
</div>
</section>
<section id="dropout" class="level2">
<h2 class="anchored" data-anchor-id="dropout">Dropout</h2>
<div id="cell-157" class="cell" data-execution_count="104">
<div class="sourceCode cell-code" id="cb148"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb148-1"><a href="#cb148-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.models.Sequential([</span>
<span id="cb148-2"><a href="#cb148-2" aria-hidden="true" tabindex="-1"></a>    keras.layers.Flatten(input_shape<span class="op">=</span>[<span class="dv">28</span>, <span class="dv">28</span>]),</span>
<span id="cb148-3"><a href="#cb148-3" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dropout(rate<span class="op">=</span><span class="fl">0.2</span>),</span>
<span id="cb148-4"><a href="#cb148-4" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">300</span>, activation<span class="op">=</span><span class="st">"elu"</span>, kernel_initializer<span class="op">=</span><span class="st">"he_normal"</span>),</span>
<span id="cb148-5"><a href="#cb148-5" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dropout(rate<span class="op">=</span><span class="fl">0.2</span>),</span>
<span id="cb148-6"><a href="#cb148-6" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">100</span>, activation<span class="op">=</span><span class="st">"elu"</span>, kernel_initializer<span class="op">=</span><span class="st">"he_normal"</span>),</span>
<span id="cb148-7"><a href="#cb148-7" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dropout(rate<span class="op">=</span><span class="fl">0.2</span>),</span>
<span id="cb148-8"><a href="#cb148-8" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">"softmax"</span>)</span>
<span id="cb148-9"><a href="#cb148-9" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb148-10"><a href="#cb148-10" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>, optimizer<span class="op">=</span><span class="st">"nadam"</span>, metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span>
<span id="cb148-11"><a href="#cb148-11" aria-hidden="true" tabindex="-1"></a>n_epochs <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb148-12"><a href="#cb148-12" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(X_train_scaled, y_train, epochs<span class="op">=</span>n_epochs,</span>
<span id="cb148-13"><a href="#cb148-13" aria-hidden="true" tabindex="-1"></a>                    validation_data<span class="op">=</span>(X_valid_scaled, y_valid))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/2
1719/1719 [==============================] - 6s 3ms/step - loss: 0.7611 - accuracy: 0.7576 - val_loss: 0.3730 - val_accuracy: 0.8644
Epoch 2/2
1719/1719 [==============================] - 5s 3ms/step - loss: 0.4306 - accuracy: 0.8401 - val_loss: 0.3395 - val_accuracy: 0.8722</code></pre>
</div>
</div>
</section>
<section id="alpha-dropout" class="level2">
<h2 class="anchored" data-anchor-id="alpha-dropout">Alpha Dropout</h2>
<div id="cell-159" class="cell" data-execution_count="105">
<div class="sourceCode cell-code" id="cb150"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb150-1"><a href="#cb150-1" aria-hidden="true" tabindex="-1"></a>tf.random.set_seed(<span class="dv">42</span>)</span>
<span id="cb150-2"><a href="#cb150-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-160" class="cell" data-execution_count="106">
<div class="sourceCode cell-code" id="cb151"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb151-1"><a href="#cb151-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.models.Sequential([</span>
<span id="cb151-2"><a href="#cb151-2" aria-hidden="true" tabindex="-1"></a>    keras.layers.Flatten(input_shape<span class="op">=</span>[<span class="dv">28</span>, <span class="dv">28</span>]),</span>
<span id="cb151-3"><a href="#cb151-3" aria-hidden="true" tabindex="-1"></a>    keras.layers.AlphaDropout(rate<span class="op">=</span><span class="fl">0.2</span>),</span>
<span id="cb151-4"><a href="#cb151-4" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">300</span>, activation<span class="op">=</span><span class="st">"selu"</span>, kernel_initializer<span class="op">=</span><span class="st">"lecun_normal"</span>),</span>
<span id="cb151-5"><a href="#cb151-5" aria-hidden="true" tabindex="-1"></a>    keras.layers.AlphaDropout(rate<span class="op">=</span><span class="fl">0.2</span>),</span>
<span id="cb151-6"><a href="#cb151-6" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">100</span>, activation<span class="op">=</span><span class="st">"selu"</span>, kernel_initializer<span class="op">=</span><span class="st">"lecun_normal"</span>),</span>
<span id="cb151-7"><a href="#cb151-7" aria-hidden="true" tabindex="-1"></a>    keras.layers.AlphaDropout(rate<span class="op">=</span><span class="fl">0.2</span>),</span>
<span id="cb151-8"><a href="#cb151-8" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">"softmax"</span>)</span>
<span id="cb151-9"><a href="#cb151-9" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb151-10"><a href="#cb151-10" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> keras.optimizers.SGD(learning_rate<span class="op">=</span><span class="fl">0.01</span>, momentum<span class="op">=</span><span class="fl">0.9</span>, nesterov<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb151-11"><a href="#cb151-11" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>, optimizer<span class="op">=</span>optimizer, metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span>
<span id="cb151-12"><a href="#cb151-12" aria-hidden="true" tabindex="-1"></a>n_epochs <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb151-13"><a href="#cb151-13" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(X_train_scaled, y_train, epochs<span class="op">=</span>n_epochs,</span>
<span id="cb151-14"><a href="#cb151-14" aria-hidden="true" tabindex="-1"></a>                    validation_data<span class="op">=</span>(X_valid_scaled, y_valid))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/20
1719/1719 [==============================] - 3s 2ms/step - loss: 0.8023 - accuracy: 0.7146 - val_loss: 0.5781 - val_accuracy: 0.8442
Epoch 2/20
1719/1719 [==============================] - 3s 1ms/step - loss: 0.5663 - accuracy: 0.7905 - val_loss: 0.5182 - val_accuracy: 0.8520
Epoch 3/20
1719/1719 [==============================] - 3s 2ms/step - loss: 0.5264 - accuracy: 0.8054 - val_loss: 0.4874 - val_accuracy: 0.8600
Epoch 4/20
1719/1719 [==============================] - 3s 2ms/step - loss: 0.5126 - accuracy: 0.8092 - val_loss: 0.4890 - val_accuracy: 0.8598
Epoch 5/20
1719/1719 [==============================] - 3s 2ms/step - loss: 0.5071 - accuracy: 0.8133 - val_loss: 0.4266 - val_accuracy: 0.8696
Epoch 6/20
1719/1719 [==============================] - 3s 1ms/step - loss: 0.4793 - accuracy: 0.8198 - val_loss: 0.4585 - val_accuracy: 0.8640
Epoch 7/20
1719/1719 [==============================] - 3s 1ms/step - loss: 0.4724 - accuracy: 0.8262 - val_loss: 0.4740 - val_accuracy: 0.8612
Epoch 8/20
1719/1719 [==============================] - 3s 2ms/step - loss: 0.4570 - accuracy: 0.8297 - val_loss: 0.4295 - val_accuracy: 0.8656
Epoch 9/20
1719/1719 [==============================] - 3s 2ms/step - loss: 0.4632 - accuracy: 0.8286 - val_loss: 0.4357 - val_accuracy: 0.8736
Epoch 10/20
1719/1719 [==============================] - 3s 2ms/step - loss: 0.4552 - accuracy: 0.8340 - val_loss: 0.4366 - val_accuracy: 0.8674
Epoch 11/20
1719/1719 [==============================] - 3s 1ms/step - loss: 0.4461 - accuracy: 0.8346 - val_loss: 0.4278 - val_accuracy: 0.8684
Epoch 12/20
1719/1719 [==============================] - 3s 2ms/step - loss: 0.4419 - accuracy: 0.8351 - val_loss: 0.5086 - val_accuracy: 0.8558
Epoch 13/20
1719/1719 [==============================] - 3s 2ms/step - loss: 0.4329 - accuracy: 0.8385 - val_loss: 0.4280 - val_accuracy: 0.8728
Epoch 14/20
1719/1719 [==============================] - 3s 2ms/step - loss: 0.4305 - accuracy: 0.8399 - val_loss: 0.4460 - val_accuracy: 0.8628
Epoch 15/20
1719/1719 [==============================] - 3s 2ms/step - loss: 0.4315 - accuracy: 0.8397 - val_loss: 0.4361 - val_accuracy: 0.8706
Epoch 16/20
1719/1719 [==============================] - 3s 2ms/step - loss: 0.4251 - accuracy: 0.8403 - val_loss: 0.4280 - val_accuracy: 0.8758
Epoch 17/20
1719/1719 [==============================] - 3s 2ms/step - loss: 0.4207 - accuracy: 0.8427 - val_loss: 0.5336 - val_accuracy: 0.8584
Epoch 18/20
1719/1719 [==============================] - 3s 2ms/step - loss: 0.4365 - accuracy: 0.8387 - val_loss: 0.4769 - val_accuracy: 0.8736
Epoch 19/20
1719/1719 [==============================] - 3s 1ms/step - loss: 0.4262 - accuracy: 0.8409 - val_loss: 0.4636 - val_accuracy: 0.8706
Epoch 20/20
1719/1719 [==============================] - 3s 1ms/step - loss: 0.4189 - accuracy: 0.8421 - val_loss: 0.4388 - val_accuracy: 0.8760</code></pre>
</div>
</div>
<div id="cell-161" class="cell" data-execution_count="107">
<div class="sourceCode cell-code" id="cb153"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb153-1"><a href="#cb153-1" aria-hidden="true" tabindex="-1"></a>model.evaluate(X_test_scaled, y_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>313/313 [==============================] - 0s 834us/step - loss: 0.4723 - accuracy: 0.8639</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="107">
<pre><code>[0.47229740023612976, 0.8639000058174133]</code></pre>
</div>
</div>
<div id="cell-162" class="cell" data-execution_count="108">
<div class="sourceCode cell-code" id="cb156"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb156-1"><a href="#cb156-1" aria-hidden="true" tabindex="-1"></a>model.evaluate(X_train_scaled, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1719/1719 [==============================] - 1s 782us/step - loss: 0.3501 - accuracy: 0.8840</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="108">
<pre><code>[0.3501231074333191, 0.8840363621711731]</code></pre>
</div>
</div>
<div id="cell-163" class="cell" data-execution_count="109">
<div class="sourceCode cell-code" id="cb159"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb159-1"><a href="#cb159-1" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(X_train_scaled, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1719/1719 [==============================] - 2s 1ms/step - loss: 0.4225 - accuracy: 0.8432</code></pre>
</div>
</div>
</section>
<section id="mc-dropout" class="level2">
<h2 class="anchored" data-anchor-id="mc-dropout">MC Dropout</h2>
<div id="cell-165" class="cell" data-execution_count="110">
<div class="sourceCode cell-code" id="cb161"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb161-1"><a href="#cb161-1" aria-hidden="true" tabindex="-1"></a>tf.random.set_seed(<span class="dv">42</span>)</span>
<span id="cb161-2"><a href="#cb161-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-166" class="cell" data-execution_count="111">
<div class="sourceCode cell-code" id="cb162"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb162-1"><a href="#cb162-1" aria-hidden="true" tabindex="-1"></a>y_probas <span class="op">=</span> np.stack([model(X_test_scaled, training<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb162-2"><a href="#cb162-2" aria-hidden="true" tabindex="-1"></a>                     <span class="cf">for</span> sample <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>)])</span>
<span id="cb162-3"><a href="#cb162-3" aria-hidden="true" tabindex="-1"></a>y_proba <span class="op">=</span> y_probas.mean(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb162-4"><a href="#cb162-4" aria-hidden="true" tabindex="-1"></a>y_std <span class="op">=</span> y_probas.std(axis<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-167" class="cell" data-execution_count="112">
<div class="sourceCode cell-code" id="cb163"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb163-1"><a href="#cb163-1" aria-hidden="true" tabindex="-1"></a>np.<span class="bu">round</span>(model.predict(X_test_scaled[:<span class="dv">1</span>]), <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="112">
<pre><code>array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.96]],
      dtype=float32)</code></pre>
</div>
</div>
<div id="cell-168" class="cell" data-execution_count="113">
<div class="sourceCode cell-code" id="cb165"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb165-1"><a href="#cb165-1" aria-hidden="true" tabindex="-1"></a>np.<span class="bu">round</span>(y_probas[:, :<span class="dv">1</span>], <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="113">
<pre><code>array([[[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.57, 0.  , 0.42]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.93, 0.  , 0.05]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.99]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.92]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.46, 0.  , 0.53]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.75, 0.  , 0.24]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.43, 0.  , 0.51]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.13, 0.  , 0.28, 0.  , 0.59]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.12, 0.  , 0.06, 0.  , 0.81]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.02, 0.  , 0.98]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.07, 0.  , 0.92]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.28, 0.  , 0.64]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.29, 0.  , 0.7 ]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.26, 0.  , 0.22, 0.  , 0.52]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.25, 0.  , 0.72]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.3 , 0.  , 0.06, 0.  , 0.65]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.  , 0.  , 0.96]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.81, 0.  , 0.17]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.25, 0.  , 0.75]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.03, 0.  , 0.93]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.84, 0.04, 0.01, 0.  , 0.1 ]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.2 , 0.  , 0.1 , 0.  , 0.7 ]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.28, 0.  , 0.72]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.12, 0.  , 0.12, 0.  , 0.76]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.75, 0.  , 0.21]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.11, 0.  , 0.87]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.28, 0.  , 0.39, 0.  , 0.34]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.04, 0.  , 0.95]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.18, 0.  , 0.51, 0.01, 0.3 ]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.01, 0.  , 0.99]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.59, 0.  , 0.01, 0.  , 0.39]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.12, 0.  , 0.8 , 0.  , 0.08]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.27, 0.  , 0.67]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.77, 0.  , 0.16]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.18, 0.  , 0.81]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.32, 0.  , 0.64]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.75, 0.  , 0.04, 0.  , 0.21]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.24, 0.  , 0.75]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.96]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.44, 0.  , 0.38, 0.  , 0.17]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.02, 0.  , 0.96]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.12, 0.  , 0.82]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.97]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.28, 0.  , 0.71]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.17, 0.  , 0.82]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.23, 0.  , 0.27, 0.  , 0.5 ]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.78, 0.  , 0.16]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.03, 0.  , 0.94]],

       [[0.  , 0.  , 0.01, 0.  , 0.02, 0.21, 0.01, 0.01, 0.  , 0.75]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.06, 0.  , 0.93]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.4 , 0.  , 0.6 ]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.12, 0.  , 0.18, 0.01, 0.69]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.15, 0.  , 0.83]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.24, 0.  , 0.24, 0.  , 0.52]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.11, 0.  , 0.01, 0.  , 0.88]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.2 , 0.  , 0.8 ]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.04, 0.  , 0.91]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.21, 0.  , 0.77]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.26, 0.  , 0.67, 0.  , 0.07]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.27, 0.  , 0.55, 0.  , 0.19]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.47, 0.  , 0.43]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.71, 0.  , 0.22]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.1 , 0.  , 0.63, 0.01, 0.26]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.16, 0.  , 0.81]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.15, 0.  , 0.8 ]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.24, 0.  , 0.75]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.13, 0.  , 0.81]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.42, 0.  , 0.01, 0.  , 0.58]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.15, 0.  , 0.83]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.79, 0.  , 0.19]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.21, 0.  , 0.79]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.93]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.12, 0.  , 0.87]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.06, 0.  , 0.9 ]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.01, 0.  , 0.98]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.08, 0.  , 0.87]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.51, 0.  , 0.46]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.96, 0.  , 0.02, 0.  , 0.02]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.38, 0.  , 0.05, 0.  , 0.57]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.07, 0.  , 0.92]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.06, 0.  , 0.89]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.05, 0.  , 0.94]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.37, 0.  , 0.37, 0.  , 0.26]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.37, 0.  , 0.19, 0.  , 0.44]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.11, 0.  , 0.87]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.01, 0.  , 0.98]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.41, 0.  , 0.59]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.12, 0.  , 0.87]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.21, 0.  , 0.71]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.12, 0.  , 0.13, 0.  , 0.75]]],
      dtype=float32)</code></pre>
</div>
</div>
<div id="cell-169" class="cell" data-execution_count="114">
<div class="sourceCode cell-code" id="cb167"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb167-1"><a href="#cb167-1" aria-hidden="true" tabindex="-1"></a>np.<span class="bu">round</span>(y_proba[:<span class="dv">1</span>], <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="114">
<pre><code>array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.1 , 0.  , 0.22, 0.  , 0.68]],
      dtype=float32)</code></pre>
</div>
</div>
<div id="cell-170" class="cell" data-execution_count="115">
<div class="sourceCode cell-code" id="cb169"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb169-1"><a href="#cb169-1" aria-hidden="true" tabindex="-1"></a>y_std <span class="op">=</span> y_probas.std(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb169-2"><a href="#cb169-2" aria-hidden="true" tabindex="-1"></a>np.<span class="bu">round</span>(y_std[:<span class="dv">1</span>], <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="115">
<pre><code>array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.18, 0.  , 0.24, 0.  , 0.29]],
      dtype=float32)</code></pre>
</div>
</div>
<div id="cell-171" class="cell" data-execution_count="116">
<div class="sourceCode cell-code" id="cb171"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb171-1"><a href="#cb171-1" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> np.argmax(y_proba, axis<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-172" class="cell" data-execution_count="117">
<div class="sourceCode cell-code" id="cb172"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb172-1"><a href="#cb172-1" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> np.<span class="bu">sum</span>(y_pred <span class="op">==</span> y_test) <span class="op">/</span> <span class="bu">len</span>(y_test)</span>
<span id="cb172-2"><a href="#cb172-2" aria-hidden="true" tabindex="-1"></a>accuracy</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="117">
<pre><code>0.8661</code></pre>
</div>
</div>
<div id="cell-173" class="cell" data-execution_count="118">
<div class="sourceCode cell-code" id="cb174"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb174-1"><a href="#cb174-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MCDropout(keras.layers.Dropout):</span>
<span id="cb174-2"><a href="#cb174-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb174-3"><a href="#cb174-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">super</span>().call(inputs, training<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb174-4"><a href="#cb174-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb174-5"><a href="#cb174-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MCAlphaDropout(keras.layers.AlphaDropout):</span>
<span id="cb174-6"><a href="#cb174-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb174-7"><a href="#cb174-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">super</span>().call(inputs, training<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-174" class="cell" data-execution_count="119">
<div class="sourceCode cell-code" id="cb175"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb175-1"><a href="#cb175-1" aria-hidden="true" tabindex="-1"></a>tf.random.set_seed(<span class="dv">42</span>)</span>
<span id="cb175-2"><a href="#cb175-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-175" class="cell" data-execution_count="120">
<div class="sourceCode cell-code" id="cb176"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb176-1"><a href="#cb176-1" aria-hidden="true" tabindex="-1"></a>mc_model <span class="op">=</span> keras.models.Sequential([</span>
<span id="cb176-2"><a href="#cb176-2" aria-hidden="true" tabindex="-1"></a>    MCAlphaDropout(layer.rate) <span class="cf">if</span> <span class="bu">isinstance</span>(layer, keras.layers.AlphaDropout) <span class="cf">else</span> layer</span>
<span id="cb176-3"><a href="#cb176-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> layer <span class="kw">in</span> model.layers</span>
<span id="cb176-4"><a href="#cb176-4" aria-hidden="true" tabindex="-1"></a>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-176" class="cell" data-execution_count="121">
<div class="sourceCode cell-code" id="cb177"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb177-1"><a href="#cb177-1" aria-hidden="true" tabindex="-1"></a>mc_model.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "sequential_20"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_18 (Flatten)         (None, 784)               0         
_________________________________________________________________
mc_alpha_dropout (MCAlphaDro (None, 784)               0         
_________________________________________________________________
dense_262 (Dense)            (None, 300)               235500    
_________________________________________________________________
mc_alpha_dropout_1 (MCAlphaD (None, 300)               0         
_________________________________________________________________
dense_263 (Dense)            (None, 100)               30100     
_________________________________________________________________
mc_alpha_dropout_2 (MCAlphaD (None, 100)               0         
_________________________________________________________________
dense_264 (Dense)            (None, 10)                1010      
=================================================================
Total params: 266,610
Trainable params: 266,610
Non-trainable params: 0
_________________________________________________________________</code></pre>
</div>
</div>
<div id="cell-177" class="cell" data-execution_count="122">
<div class="sourceCode cell-code" id="cb179"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb179-1"><a href="#cb179-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> keras.optimizers.SGD(learning_rate<span class="op">=</span><span class="fl">0.01</span>, momentum<span class="op">=</span><span class="fl">0.9</span>, nesterov<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb179-2"><a href="#cb179-2" aria-hidden="true" tabindex="-1"></a>mc_model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>, optimizer<span class="op">=</span>optimizer, metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-178" class="cell" data-execution_count="123">
<div class="sourceCode cell-code" id="cb180"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb180-1"><a href="#cb180-1" aria-hidden="true" tabindex="-1"></a>mc_model.set_weights(model.get_weights())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can use the model with MC Dropout:</p>
<div id="cell-180" class="cell" data-execution_count="124">
<div class="sourceCode cell-code" id="cb181"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb181-1"><a href="#cb181-1" aria-hidden="true" tabindex="-1"></a>np.<span class="bu">round</span>(np.mean([mc_model.predict(X_test_scaled[:<span class="dv">1</span>]) <span class="cf">for</span> sample <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>)], axis<span class="op">=</span><span class="dv">0</span>), <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="124">
<pre><code>array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.14, 0.  , 0.25, 0.01, 0.61]],
      dtype=float32)</code></pre>
</div>
</div>
</section>
<section id="max-norm" class="level2">
<h2 class="anchored" data-anchor-id="max-norm">Max norm</h2>
<div id="cell-182" class="cell" data-execution_count="125">
<div class="sourceCode cell-code" id="cb183"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb183-1"><a href="#cb183-1" aria-hidden="true" tabindex="-1"></a>layer <span class="op">=</span> keras.layers.Dense(<span class="dv">100</span>, activation<span class="op">=</span><span class="st">"selu"</span>, kernel_initializer<span class="op">=</span><span class="st">"lecun_normal"</span>,</span>
<span id="cb183-2"><a href="#cb183-2" aria-hidden="true" tabindex="-1"></a>                           kernel_constraint<span class="op">=</span>keras.constraints.max_norm(<span class="fl">1.</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-183" class="cell" data-execution_count="126">
<div class="sourceCode cell-code" id="cb184"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb184-1"><a href="#cb184-1" aria-hidden="true" tabindex="-1"></a>MaxNormDense <span class="op">=</span> partial(keras.layers.Dense,</span>
<span id="cb184-2"><a href="#cb184-2" aria-hidden="true" tabindex="-1"></a>                       activation<span class="op">=</span><span class="st">"selu"</span>, kernel_initializer<span class="op">=</span><span class="st">"lecun_normal"</span>,</span>
<span id="cb184-3"><a href="#cb184-3" aria-hidden="true" tabindex="-1"></a>                       kernel_constraint<span class="op">=</span>keras.constraints.max_norm(<span class="fl">1.</span>))</span>
<span id="cb184-4"><a href="#cb184-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb184-5"><a href="#cb184-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.models.Sequential([</span>
<span id="cb184-6"><a href="#cb184-6" aria-hidden="true" tabindex="-1"></a>    keras.layers.Flatten(input_shape<span class="op">=</span>[<span class="dv">28</span>, <span class="dv">28</span>]),</span>
<span id="cb184-7"><a href="#cb184-7" aria-hidden="true" tabindex="-1"></a>    MaxNormDense(<span class="dv">300</span>),</span>
<span id="cb184-8"><a href="#cb184-8" aria-hidden="true" tabindex="-1"></a>    MaxNormDense(<span class="dv">100</span>),</span>
<span id="cb184-9"><a href="#cb184-9" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">"softmax"</span>)</span>
<span id="cb184-10"><a href="#cb184-10" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb184-11"><a href="#cb184-11" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>, optimizer<span class="op">=</span><span class="st">"nadam"</span>, metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span>
<span id="cb184-12"><a href="#cb184-12" aria-hidden="true" tabindex="-1"></a>n_epochs <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb184-13"><a href="#cb184-13" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(X_train_scaled, y_train, epochs<span class="op">=</span>n_epochs,</span>
<span id="cb184-14"><a href="#cb184-14" aria-hidden="true" tabindex="-1"></a>                    validation_data<span class="op">=</span>(X_valid_scaled, y_valid))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/2
1719/1719 [==============================] - 5s 3ms/step - loss: 0.5763 - accuracy: 0.8020 - val_loss: 0.3674 - val_accuracy: 0.8674
Epoch 2/2
1719/1719 [==============================] - 5s 3ms/step - loss: 0.3545 - accuracy: 0.8709 - val_loss: 0.3714 - val_accuracy: 0.8662</code></pre>
</div>
</div>
</section>
</section>
<section id="exercises" class="level1">
<h1>Exercises</h1>
<section id="to-7." class="level2">
<h2 class="anchored" data-anchor-id="to-7.">1. to 7.</h2>
<p>See appendix A.</p>
</section>
<section id="deep-learning-on-cifar10" class="level2">
<h2 class="anchored" data-anchor-id="deep-learning-on-cifar10">8. Deep Learning on CIFAR10</h2>
<section id="a." class="level3">
<h3 class="anchored" data-anchor-id="a.">a.</h3>
<p><em>Exercise: Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function.</em></p>
<div id="cell-189" class="cell" data-execution_count="127">
<div class="sourceCode cell-code" id="cb186"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb186-1"><a href="#cb186-1" aria-hidden="true" tabindex="-1"></a>keras.backend.clear_session()</span>
<span id="cb186-2"><a href="#cb186-2" aria-hidden="true" tabindex="-1"></a>tf.random.set_seed(<span class="dv">42</span>)</span>
<span id="cb186-3"><a href="#cb186-3" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb186-4"><a href="#cb186-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb186-5"><a href="#cb186-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.models.Sequential()</span>
<span id="cb186-6"><a href="#cb186-6" aria-hidden="true" tabindex="-1"></a>model.add(keras.layers.Flatten(input_shape<span class="op">=</span>[<span class="dv">32</span>, <span class="dv">32</span>, <span class="dv">3</span>]))</span>
<span id="cb186-7"><a href="#cb186-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">20</span>):</span>
<span id="cb186-8"><a href="#cb186-8" aria-hidden="true" tabindex="-1"></a>    model.add(keras.layers.Dense(<span class="dv">100</span>,</span>
<span id="cb186-9"><a href="#cb186-9" aria-hidden="true" tabindex="-1"></a>                                 activation<span class="op">=</span><span class="st">"elu"</span>,</span>
<span id="cb186-10"><a href="#cb186-10" aria-hidden="true" tabindex="-1"></a>                                 kernel_initializer<span class="op">=</span><span class="st">"he_normal"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="b." class="level3">
<h3 class="anchored" data-anchor-id="b.">b.</h3>
<p><em>Exercise: Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with <code>keras.datasets.cifar10.load_data()</code>. The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters.</em></p>
<p>Let’s add the output layer to the model:</p>
<div id="cell-192" class="cell" data-execution_count="128">
<div class="sourceCode cell-code" id="cb187"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb187-1"><a href="#cb187-1" aria-hidden="true" tabindex="-1"></a>model.add(keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">"softmax"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s use a Nadam optimizer with a learning rate of 5e-5. I tried learning rates 1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3 and 1e-2, and I compared their learning curves for 10 epochs each (using the TensorBoard callback, below). The learning rates 3e-5 and 1e-4 were pretty good, so I tried 5e-5, which turned out to be slightly better.</p>
<div id="cell-194" class="cell" data-execution_count="129">
<div class="sourceCode cell-code" id="cb188"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb188-1"><a href="#cb188-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> keras.optimizers.Nadam(learning_rate<span class="op">=</span><span class="fl">5e-5</span>)</span>
<span id="cb188-2"><a href="#cb188-2" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>,</span>
<span id="cb188-3"><a href="#cb188-3" aria-hidden="true" tabindex="-1"></a>              optimizer<span class="op">=</span>optimizer,</span>
<span id="cb188-4"><a href="#cb188-4" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s load the CIFAR10 dataset. We also want to use early stopping, so we need a validation set. Let’s use the first 5,000 images of the original training set as the validation set:</p>
<div id="cell-196" class="cell" data-execution_count="130">
<div class="sourceCode cell-code" id="cb189"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb189-1"><a href="#cb189-1" aria-hidden="true" tabindex="-1"></a>(X_train_full, y_train_full), (X_test, y_test) <span class="op">=</span> keras.datasets.cifar10.load_data()</span>
<span id="cb189-2"><a href="#cb189-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb189-3"><a href="#cb189-3" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> X_train_full[<span class="dv">5000</span>:]</span>
<span id="cb189-4"><a href="#cb189-4" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> y_train_full[<span class="dv">5000</span>:]</span>
<span id="cb189-5"><a href="#cb189-5" aria-hidden="true" tabindex="-1"></a>X_valid <span class="op">=</span> X_train_full[:<span class="dv">5000</span>]</span>
<span id="cb189-6"><a href="#cb189-6" aria-hidden="true" tabindex="-1"></a>y_valid <span class="op">=</span> y_train_full[:<span class="dv">5000</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can create the callbacks we need and train the model:</p>
<div id="cell-198" class="cell" data-execution_count="131">
<div class="sourceCode cell-code" id="cb190"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb190-1"><a href="#cb190-1" aria-hidden="true" tabindex="-1"></a>early_stopping_cb <span class="op">=</span> keras.callbacks.EarlyStopping(patience<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb190-2"><a href="#cb190-2" aria-hidden="true" tabindex="-1"></a>model_checkpoint_cb <span class="op">=</span> keras.callbacks.ModelCheckpoint(<span class="st">"my_cifar10_model.h5"</span>, save_best_only<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb190-3"><a href="#cb190-3" aria-hidden="true" tabindex="-1"></a>run_index <span class="op">=</span> <span class="dv">1</span> <span class="co"># increment every time you train the model</span></span>
<span id="cb190-4"><a href="#cb190-4" aria-hidden="true" tabindex="-1"></a>run_logdir <span class="op">=</span> os.path.join(os.curdir, <span class="st">"my_cifar10_logs"</span>, <span class="st">"run_</span><span class="sc">{:03d}</span><span class="st">"</span>.<span class="bu">format</span>(run_index))</span>
<span id="cb190-5"><a href="#cb190-5" aria-hidden="true" tabindex="-1"></a>tensorboard_cb <span class="op">=</span> keras.callbacks.TensorBoard(run_logdir)</span>
<span id="cb190-6"><a href="#cb190-6" aria-hidden="true" tabindex="-1"></a>callbacks <span class="op">=</span> [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-199" class="cell" data-execution_count="132">
<div class="sourceCode cell-code" id="cb191"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb191-1"><a href="#cb191-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>tensorboard <span class="op">--</span>logdir<span class="op">=</span>.<span class="op">/</span>my_cifar10_logs <span class="op">--</span>port<span class="op">=</span><span class="dv">6006</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>ERROR: Failed to launch TensorBoard (exited with 255).
Contents of stderr:
E0213 19:12:55.493896 4621630912 program.py:311] TensorBoard could not bind to port 6006, it was already in use
ERROR: TensorBoard could not bind to port 6006, it was already in use</code></pre>
</div>
</div>
<div id="cell-200" class="cell" data-execution_count="133">
<div class="sourceCode cell-code" id="cb193"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb193-1"><a href="#cb193-1" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train, epochs<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb193-2"><a href="#cb193-2" aria-hidden="true" tabindex="-1"></a>          validation_data<span class="op">=</span>(X_valid, y_valid),</span>
<span id="cb193-3"><a href="#cb193-3" aria-hidden="true" tabindex="-1"></a>          callbacks<span class="op">=</span>callbacks)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/100
1407/1407 [==============================] - 9s 5ms/step - loss: 9.4191 - accuracy: 0.1388 - val_loss: 2.2328 - val_accuracy: 0.2040
Epoch 2/100
1407/1407 [==============================] - 7s 5ms/step - loss: 2.1097 - accuracy: 0.2317 - val_loss: 2.0485 - val_accuracy: 0.2402
Epoch 3/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.9667 - accuracy: 0.2844 - val_loss: 1.9681 - val_accuracy: 0.2964
Epoch 4/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.8740 - accuracy: 0.3149 - val_loss: 1.9178 - val_accuracy: 0.3254
Epoch 5/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.8064 - accuracy: 0.3423 - val_loss: 1.8256 - val_accuracy: 0.3384
Epoch 6/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.7525 - accuracy: 0.3595 - val_loss: 1.7430 - val_accuracy: 0.3692
Epoch 7/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.7116 - accuracy: 0.3819 - val_loss: 1.7199 - val_accuracy: 0.3824
Epoch 8/100
1407/1407 [==============================] - 8s 5ms/step - loss: 1.6782 - accuracy: 0.3935 - val_loss: 1.6746 - val_accuracy: 0.3972
Epoch 9/100
1407/1407 [==============================] - 8s 5ms/step - loss: 1.6517 - accuracy: 0.4025 - val_loss: 1.6622 - val_accuracy: 0.4004
Epoch 10/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.6140 - accuracy: 0.4194 - val_loss: 1.7065 - val_accuracy: 0.3840
Epoch 11/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.5884 - accuracy: 0.4301 - val_loss: 1.6736 - val_accuracy: 0.3914
Epoch 12/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.5640 - accuracy: 0.4378 - val_loss: 1.6220 - val_accuracy: 0.4224
Epoch 13/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.5437 - accuracy: 0.4448 - val_loss: 1.6332 - val_accuracy: 0.4144
Epoch 14/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.5214 - accuracy: 0.4555 - val_loss: 1.5785 - val_accuracy: 0.4326
Epoch 15/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.5117 - accuracy: 0.4564 - val_loss: 1.6267 - val_accuracy: 0.4164
Epoch 16/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.4972 - accuracy: 0.4622 - val_loss: 1.5846 - val_accuracy: 0.4316
Epoch 17/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.4888 - accuracy: 0.4661 - val_loss: 1.5549 - val_accuracy: 0.4420
Epoch 18/100
&lt;&lt;24 more lines&gt;&gt;
1407/1407 [==============================] - 8s 5ms/step - loss: 1.3362 - accuracy: 0.5212 - val_loss: 1.6025 - val_accuracy: 0.4500
Epoch 31/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.3360 - accuracy: 0.5207 - val_loss: 1.5175 - val_accuracy: 0.4602
Epoch 32/100
1407/1407 [==============================] - 8s 5ms/step - loss: 1.3031 - accuracy: 0.5302 - val_loss: 1.5397 - val_accuracy: 0.4572
Epoch 33/100
1407/1407 [==============================] - 8s 5ms/step - loss: 1.3082 - accuracy: 0.5308 - val_loss: 1.4997 - val_accuracy: 0.4776
Epoch 34/100
1407/1407 [==============================] - 8s 5ms/step - loss: 1.2882 - accuracy: 0.5338 - val_loss: 1.5482 - val_accuracy: 0.4620
Epoch 35/100
1407/1407 [==============================] - 8s 5ms/step - loss: 1.2889 - accuracy: 0.5355 - val_loss: 1.5474 - val_accuracy: 0.4604
Epoch 36/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.2761 - accuracy: 0.5410 - val_loss: 1.5434 - val_accuracy: 0.4658
Epoch 37/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.2658 - accuracy: 0.5481 - val_loss: 1.5502 - val_accuracy: 0.4706
Epoch 38/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.2554 - accuracy: 0.5489 - val_loss: 1.5527 - val_accuracy: 0.4624
Epoch 39/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.2504 - accuracy: 0.5471 - val_loss: 1.5482 - val_accuracy: 0.4602
Epoch 40/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.2516 - accuracy: 0.5545 - val_loss: 1.5881 - val_accuracy: 0.4574
Epoch 41/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.2401 - accuracy: 0.5566 - val_loss: 1.5403 - val_accuracy: 0.4670
Epoch 42/100
1407/1407 [==============================] - 8s 5ms/step - loss: 1.2305 - accuracy: 0.5570 - val_loss: 1.5343 - val_accuracy: 0.4790
Epoch 43/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.2228 - accuracy: 0.5615 - val_loss: 1.5344 - val_accuracy: 0.4708
Epoch 44/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.2161 - accuracy: 0.5619 - val_loss: 1.5782 - val_accuracy: 0.4526
Epoch 45/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.2124 - accuracy: 0.5641 - val_loss: 1.5182 - val_accuracy: 0.4794
Epoch 46/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.1870 - accuracy: 0.5766 - val_loss: 1.5435 - val_accuracy: 0.4650
Epoch 47/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.1925 - accuracy: 0.5701 - val_loss: 1.5532 - val_accuracy: 0.4686</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="133">
<pre><code>&lt;tensorflow.python.keras.callbacks.History at 0x7fb1d8ef8790&gt;</code></pre>
</div>
</div>
<div id="cell-201" class="cell" data-execution_count="134">
<div class="sourceCode cell-code" id="cb196"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb196-1"><a href="#cb196-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.models.load_model(<span class="st">"my_cifar10_model.h5"</span>)</span>
<span id="cb196-2"><a href="#cb196-2" aria-hidden="true" tabindex="-1"></a>model.evaluate(X_valid, y_valid)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>157/157 [==============================] - 0s 1ms/step - loss: 1.4960 - accuracy: 0.4762</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="134">
<pre><code>[1.4960416555404663, 0.47620001435279846]</code></pre>
</div>
</div>
<p>The model with the lowest validation loss gets about 47.6% accuracy on the validation set. It took 27 epochs to reach the lowest validation loss, with roughly 8 seconds per epoch on my laptop (without a GPU). Let’s see if we can improve performance using Batch Normalization.</p>
</section>
<section id="c." class="level3">
<h3 class="anchored" data-anchor-id="c.">c.</h3>
<p><em>Exercise: Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?</em></p>
<p>The code below is very similar to the code above, with a few changes:</p>
<ul>
<li>I added a BN layer after every Dense layer (before the activation function), except for the output layer. I also added a BN layer before the first hidden layer.</li>
<li>I changed the learning rate to 5e-4. I experimented with 1e-5, 3e-5, 5e-5, 1e-4, 3e-4, 5e-4, 1e-3 and 3e-3, and I chose the one with the best validation performance after 20 epochs.</li>
<li>I renamed the run directories to run_bn_* and the model file name to my_cifar10_bn_model.h5.</li>
</ul>
<div id="cell-205" class="cell" data-execution_count="135">
<div class="sourceCode cell-code" id="cb199"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb199-1"><a href="#cb199-1" aria-hidden="true" tabindex="-1"></a>keras.backend.clear_session()</span>
<span id="cb199-2"><a href="#cb199-2" aria-hidden="true" tabindex="-1"></a>tf.random.set_seed(<span class="dv">42</span>)</span>
<span id="cb199-3"><a href="#cb199-3" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb199-4"><a href="#cb199-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb199-5"><a href="#cb199-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.models.Sequential()</span>
<span id="cb199-6"><a href="#cb199-6" aria-hidden="true" tabindex="-1"></a>model.add(keras.layers.Flatten(input_shape<span class="op">=</span>[<span class="dv">32</span>, <span class="dv">32</span>, <span class="dv">3</span>]))</span>
<span id="cb199-7"><a href="#cb199-7" aria-hidden="true" tabindex="-1"></a>model.add(keras.layers.BatchNormalization())</span>
<span id="cb199-8"><a href="#cb199-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">20</span>):</span>
<span id="cb199-9"><a href="#cb199-9" aria-hidden="true" tabindex="-1"></a>    model.add(keras.layers.Dense(<span class="dv">100</span>, kernel_initializer<span class="op">=</span><span class="st">"he_normal"</span>))</span>
<span id="cb199-10"><a href="#cb199-10" aria-hidden="true" tabindex="-1"></a>    model.add(keras.layers.BatchNormalization())</span>
<span id="cb199-11"><a href="#cb199-11" aria-hidden="true" tabindex="-1"></a>    model.add(keras.layers.Activation(<span class="st">"elu"</span>))</span>
<span id="cb199-12"><a href="#cb199-12" aria-hidden="true" tabindex="-1"></a>model.add(keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">"softmax"</span>))</span>
<span id="cb199-13"><a href="#cb199-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb199-14"><a href="#cb199-14" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> keras.optimizers.Nadam(learning_rate<span class="op">=</span><span class="fl">5e-4</span>)</span>
<span id="cb199-15"><a href="#cb199-15" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>,</span>
<span id="cb199-16"><a href="#cb199-16" aria-hidden="true" tabindex="-1"></a>              optimizer<span class="op">=</span>optimizer,</span>
<span id="cb199-17"><a href="#cb199-17" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span>
<span id="cb199-18"><a href="#cb199-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb199-19"><a href="#cb199-19" aria-hidden="true" tabindex="-1"></a>early_stopping_cb <span class="op">=</span> keras.callbacks.EarlyStopping(patience<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb199-20"><a href="#cb199-20" aria-hidden="true" tabindex="-1"></a>model_checkpoint_cb <span class="op">=</span> keras.callbacks.ModelCheckpoint(<span class="st">"my_cifar10_bn_model.h5"</span>, save_best_only<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb199-21"><a href="#cb199-21" aria-hidden="true" tabindex="-1"></a>run_index <span class="op">=</span> <span class="dv">1</span> <span class="co"># increment every time you train the model</span></span>
<span id="cb199-22"><a href="#cb199-22" aria-hidden="true" tabindex="-1"></a>run_logdir <span class="op">=</span> os.path.join(os.curdir, <span class="st">"my_cifar10_logs"</span>, <span class="st">"run_bn_</span><span class="sc">{:03d}</span><span class="st">"</span>.<span class="bu">format</span>(run_index))</span>
<span id="cb199-23"><a href="#cb199-23" aria-hidden="true" tabindex="-1"></a>tensorboard_cb <span class="op">=</span> keras.callbacks.TensorBoard(run_logdir)</span>
<span id="cb199-24"><a href="#cb199-24" aria-hidden="true" tabindex="-1"></a>callbacks <span class="op">=</span> [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]</span>
<span id="cb199-25"><a href="#cb199-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb199-26"><a href="#cb199-26" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train, epochs<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb199-27"><a href="#cb199-27" aria-hidden="true" tabindex="-1"></a>          validation_data<span class="op">=</span>(X_valid, y_valid),</span>
<span id="cb199-28"><a href="#cb199-28" aria-hidden="true" tabindex="-1"></a>          callbacks<span class="op">=</span>callbacks)</span>
<span id="cb199-29"><a href="#cb199-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb199-30"><a href="#cb199-30" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.models.load_model(<span class="st">"my_cifar10_bn_model.h5"</span>)</span>
<span id="cb199-31"><a href="#cb199-31" aria-hidden="true" tabindex="-1"></a>model.evaluate(X_valid, y_valid)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/100
1407/1407 [==============================] - 19s 9ms/step - loss: 1.9765 - accuracy: 0.2968 - val_loss: 1.6602 - val_accuracy: 0.4042
Epoch 2/100
1407/1407 [==============================] - 11s 8ms/step - loss: 1.6787 - accuracy: 0.4056 - val_loss: 1.5887 - val_accuracy: 0.4304
Epoch 3/100
1407/1407 [==============================] - 11s 8ms/step - loss: 1.6097 - accuracy: 0.4274 - val_loss: 1.5781 - val_accuracy: 0.4326
Epoch 4/100
1407/1407 [==============================] - 11s 8ms/step - loss: 1.5574 - accuracy: 0.4486 - val_loss: 1.5064 - val_accuracy: 0.4676
Epoch 5/100
1407/1407 [==============================] - 11s 8ms/step - loss: 1.5075 - accuracy: 0.4642 - val_loss: 1.4412 - val_accuracy: 0.4844
Epoch 6/100
1407/1407 [==============================] - 11s 8ms/step - loss: 1.4664 - accuracy: 0.4787 - val_loss: 1.4179 - val_accuracy: 0.4984
Epoch 7/100
1407/1407 [==============================] - 11s 8ms/step - loss: 1.4334 - accuracy: 0.4932 - val_loss: 1.4277 - val_accuracy: 0.4906
Epoch 8/100
1407/1407 [==============================] - 12s 8ms/step - loss: 1.4054 - accuracy: 0.5038 - val_loss: 1.3843 - val_accuracy: 0.5130
Epoch 9/100
1407/1407 [==============================] - 12s 8ms/step - loss: 1.3816 - accuracy: 0.5106 - val_loss: 1.3691 - val_accuracy: 0.5108
Epoch 10/100
1407/1407 [==============================] - 12s 8ms/step - loss: 1.3547 - accuracy: 0.5206 - val_loss: 1.3552 - val_accuracy: 0.5226
Epoch 11/100
1407/1407 [==============================] - 12s 9ms/step - loss: 1.3244 - accuracy: 0.5371 - val_loss: 1.3678 - val_accuracy: 0.5142
Epoch 12/100
1407/1407 [==============================] - 12s 8ms/step - loss: 1.3078 - accuracy: 0.5393 - val_loss: 1.3844 - val_accuracy: 0.5080
Epoch 13/100
1407/1407 [==============================] - 12s 9ms/step - loss: 1.2889 - accuracy: 0.5431 - val_loss: 1.3566 - val_accuracy: 0.5164
Epoch 14/100
1407/1407 [==============================] - 12s 9ms/step - loss: 1.2607 - accuracy: 0.5559 - val_loss: 1.3626 - val_accuracy: 0.5248
Epoch 15/100
1407/1407 [==============================] - 12s 8ms/step - loss: 1.2580 - accuracy: 0.5587 - val_loss: 1.3616 - val_accuracy: 0.5276
Epoch 16/100
1407/1407 [==============================] - 12s 8ms/step - loss: 1.2441 - accuracy: 0.5586 - val_loss: 1.3350 - val_accuracy: 0.5286
Epoch 17/100
1407/1407 [==============================] - 12s 8ms/step - loss: 1.2241 - accuracy: 0.5676 - val_loss: 1.3370 - val_accuracy: 0.5408
Epoch 18/100
&lt;&lt;29 more lines&gt;&gt;
Epoch 33/100
1407/1407 [==============================] - 12s 8ms/step - loss: 1.0336 - accuracy: 0.6369 - val_loss: 1.3682 - val_accuracy: 0.5450
Epoch 34/100
1407/1407 [==============================] - 11s 8ms/step - loss: 1.0228 - accuracy: 0.6388 - val_loss: 1.3348 - val_accuracy: 0.5458
Epoch 35/100
1407/1407 [==============================] - 12s 8ms/step - loss: 1.0205 - accuracy: 0.6407 - val_loss: 1.3490 - val_accuracy: 0.5440
Epoch 36/100
1407/1407 [==============================] - 12s 9ms/step - loss: 1.0008 - accuracy: 0.6489 - val_loss: 1.3568 - val_accuracy: 0.5408
Epoch 37/100
1407/1407 [==============================] - 12s 9ms/step - loss: 0.9785 - accuracy: 0.6543 - val_loss: 1.3628 - val_accuracy: 0.5396
Epoch 38/100
1407/1407 [==============================] - 12s 9ms/step - loss: 0.9832 - accuracy: 0.6592 - val_loss: 1.3617 - val_accuracy: 0.5482
Epoch 39/100
1407/1407 [==============================] - 12s 8ms/step - loss: 0.9707 - accuracy: 0.6581 - val_loss: 1.3767 - val_accuracy: 0.5446
Epoch 40/100
1407/1407 [==============================] - 12s 9ms/step - loss: 0.9590 - accuracy: 0.6651 - val_loss: 1.4200 - val_accuracy: 0.5314
Epoch 41/100
1407/1407 [==============================] - 12s 9ms/step - loss: 0.9548 - accuracy: 0.6668 - val_loss: 1.3692 - val_accuracy: 0.5450
Epoch 42/100
1407/1407 [==============================] - 12s 9ms/step - loss: 0.9480 - accuracy: 0.6667 - val_loss: 1.3841 - val_accuracy: 0.5310
Epoch 43/100
1407/1407 [==============================] - 12s 9ms/step - loss: 0.9411 - accuracy: 0.6716 - val_loss: 1.4036 - val_accuracy: 0.5382
Epoch 44/100
1407/1407 [==============================] - 12s 9ms/step - loss: 0.9383 - accuracy: 0.6708 - val_loss: 1.4114 - val_accuracy: 0.5236
Epoch 45/100
1407/1407 [==============================] - 12s 9ms/step - loss: 0.9258 - accuracy: 0.6769 - val_loss: 1.4224 - val_accuracy: 0.5324
Epoch 46/100
1407/1407 [==============================] - 12s 9ms/step - loss: 0.9072 - accuracy: 0.6836 - val_loss: 1.3875 - val_accuracy: 0.5442
Epoch 47/100
1407/1407 [==============================] - 12s 9ms/step - loss: 0.8996 - accuracy: 0.6850 - val_loss: 1.4449 - val_accuracy: 0.5280
Epoch 48/100
1407/1407 [==============================] - 13s 9ms/step - loss: 0.9050 - accuracy: 0.6835 - val_loss: 1.4167 - val_accuracy: 0.5338
Epoch 49/100
1407/1407 [==============================] - 12s 9ms/step - loss: 0.8934 - accuracy: 0.6880 - val_loss: 1.4260 - val_accuracy: 0.5294
157/157 [==============================] - 1s 2ms/step - loss: 1.3344 - accuracy: 0.5398</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="135">
<pre><code>[1.3343921899795532, 0.5397999882698059]</code></pre>
</div>
</div>
<ul>
<li><em>Is the model converging faster than before?</em> Much faster! The previous model took 27 epochs to reach the lowest validation loss, while the new model achieved that same loss in just 5 epochs and continued to make progress until the 16th epoch. The BN layers stabilized training and allowed us to use a much larger learning rate, so convergence was faster.</li>
<li><em>Does BN produce a better model?</em> Yes! The final model is also much better, with 54.0% accuracy instead of 47.6%. It’s still not a very good model, but at least it’s much better than before (a Convolutional Neural Network would do much better, but that’s a different topic, see chapter 14).</li>
<li><em>How does BN affect training speed?</em> Although the model converged much faster, each epoch took about 12s instead of 8s, because of the extra computations required by the BN layers. But overall the training time (wall time) was shortened significantly!</li>
</ul>
</section>
<section id="d." class="level3">
<h3 class="anchored" data-anchor-id="d.">d.</h3>
<p><em>Exercise: Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).</em></p>
<div id="cell-208" class="cell" data-scrolled="true" data-execution_count="136">
<div class="sourceCode cell-code" id="cb202"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb202-1"><a href="#cb202-1" aria-hidden="true" tabindex="-1"></a>keras.backend.clear_session()</span>
<span id="cb202-2"><a href="#cb202-2" aria-hidden="true" tabindex="-1"></a>tf.random.set_seed(<span class="dv">42</span>)</span>
<span id="cb202-3"><a href="#cb202-3" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb202-4"><a href="#cb202-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb202-5"><a href="#cb202-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.models.Sequential()</span>
<span id="cb202-6"><a href="#cb202-6" aria-hidden="true" tabindex="-1"></a>model.add(keras.layers.Flatten(input_shape<span class="op">=</span>[<span class="dv">32</span>, <span class="dv">32</span>, <span class="dv">3</span>]))</span>
<span id="cb202-7"><a href="#cb202-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">20</span>):</span>
<span id="cb202-8"><a href="#cb202-8" aria-hidden="true" tabindex="-1"></a>    model.add(keras.layers.Dense(<span class="dv">100</span>,</span>
<span id="cb202-9"><a href="#cb202-9" aria-hidden="true" tabindex="-1"></a>                                 kernel_initializer<span class="op">=</span><span class="st">"lecun_normal"</span>,</span>
<span id="cb202-10"><a href="#cb202-10" aria-hidden="true" tabindex="-1"></a>                                 activation<span class="op">=</span><span class="st">"selu"</span>))</span>
<span id="cb202-11"><a href="#cb202-11" aria-hidden="true" tabindex="-1"></a>model.add(keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">"softmax"</span>))</span>
<span id="cb202-12"><a href="#cb202-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb202-13"><a href="#cb202-13" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> keras.optimizers.Nadam(learning_rate<span class="op">=</span><span class="fl">7e-4</span>)</span>
<span id="cb202-14"><a href="#cb202-14" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>,</span>
<span id="cb202-15"><a href="#cb202-15" aria-hidden="true" tabindex="-1"></a>              optimizer<span class="op">=</span>optimizer,</span>
<span id="cb202-16"><a href="#cb202-16" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span>
<span id="cb202-17"><a href="#cb202-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb202-18"><a href="#cb202-18" aria-hidden="true" tabindex="-1"></a>early_stopping_cb <span class="op">=</span> keras.callbacks.EarlyStopping(patience<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb202-19"><a href="#cb202-19" aria-hidden="true" tabindex="-1"></a>model_checkpoint_cb <span class="op">=</span> keras.callbacks.ModelCheckpoint(<span class="st">"my_cifar10_selu_model.h5"</span>, save_best_only<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb202-20"><a href="#cb202-20" aria-hidden="true" tabindex="-1"></a>run_index <span class="op">=</span> <span class="dv">1</span> <span class="co"># increment every time you train the model</span></span>
<span id="cb202-21"><a href="#cb202-21" aria-hidden="true" tabindex="-1"></a>run_logdir <span class="op">=</span> os.path.join(os.curdir, <span class="st">"my_cifar10_logs"</span>, <span class="st">"run_selu_</span><span class="sc">{:03d}</span><span class="st">"</span>.<span class="bu">format</span>(run_index))</span>
<span id="cb202-22"><a href="#cb202-22" aria-hidden="true" tabindex="-1"></a>tensorboard_cb <span class="op">=</span> keras.callbacks.TensorBoard(run_logdir)</span>
<span id="cb202-23"><a href="#cb202-23" aria-hidden="true" tabindex="-1"></a>callbacks <span class="op">=</span> [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]</span>
<span id="cb202-24"><a href="#cb202-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb202-25"><a href="#cb202-25" aria-hidden="true" tabindex="-1"></a>X_means <span class="op">=</span> X_train.mean(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb202-26"><a href="#cb202-26" aria-hidden="true" tabindex="-1"></a>X_stds <span class="op">=</span> X_train.std(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb202-27"><a href="#cb202-27" aria-hidden="true" tabindex="-1"></a>X_train_scaled <span class="op">=</span> (X_train <span class="op">-</span> X_means) <span class="op">/</span> X_stds</span>
<span id="cb202-28"><a href="#cb202-28" aria-hidden="true" tabindex="-1"></a>X_valid_scaled <span class="op">=</span> (X_valid <span class="op">-</span> X_means) <span class="op">/</span> X_stds</span>
<span id="cb202-29"><a href="#cb202-29" aria-hidden="true" tabindex="-1"></a>X_test_scaled <span class="op">=</span> (X_test <span class="op">-</span> X_means) <span class="op">/</span> X_stds</span>
<span id="cb202-30"><a href="#cb202-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb202-31"><a href="#cb202-31" aria-hidden="true" tabindex="-1"></a>model.fit(X_train_scaled, y_train, epochs<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb202-32"><a href="#cb202-32" aria-hidden="true" tabindex="-1"></a>          validation_data<span class="op">=</span>(X_valid_scaled, y_valid),</span>
<span id="cb202-33"><a href="#cb202-33" aria-hidden="true" tabindex="-1"></a>          callbacks<span class="op">=</span>callbacks)</span>
<span id="cb202-34"><a href="#cb202-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb202-35"><a href="#cb202-35" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.models.load_model(<span class="st">"my_cifar10_selu_model.h5"</span>)</span>
<span id="cb202-36"><a href="#cb202-36" aria-hidden="true" tabindex="-1"></a>model.evaluate(X_valid_scaled, y_valid)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/100
1407/1407 [==============================] - 10s 5ms/step - loss: 2.0622 - accuracy: 0.2631 - val_loss: 1.7878 - val_accuracy: 0.3552
Epoch 2/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.7328 - accuracy: 0.3830 - val_loss: 1.7028 - val_accuracy: 0.3828
Epoch 3/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.6342 - accuracy: 0.4279 - val_loss: 1.6692 - val_accuracy: 0.4022
Epoch 4/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.5524 - accuracy: 0.4538 - val_loss: 1.6350 - val_accuracy: 0.4300
Epoch 5/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.4979 - accuracy: 0.4756 - val_loss: 1.5773 - val_accuracy: 0.4356
Epoch 6/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.4428 - accuracy: 0.4902 - val_loss: 1.5529 - val_accuracy: 0.4630
Epoch 7/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.3966 - accuracy: 0.5126 - val_loss: 1.5290 - val_accuracy: 0.4682
Epoch 8/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.3549 - accuracy: 0.5232 - val_loss: 1.4633 - val_accuracy: 0.4792
Epoch 9/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.3162 - accuracy: 0.5444 - val_loss: 1.4787 - val_accuracy: 0.4776
Epoch 10/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.2825 - accuracy: 0.5534 - val_loss: 1.4794 - val_accuracy: 0.4934
Epoch 11/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.2529 - accuracy: 0.5682 - val_loss: 1.5529 - val_accuracy: 0.4982
Epoch 12/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.2256 - accuracy: 0.5784 - val_loss: 1.4942 - val_accuracy: 0.4902
Epoch 13/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.2049 - accuracy: 0.5823 - val_loss: 1.4868 - val_accuracy: 0.5024
Epoch 14/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.1627 - accuracy: 0.6012 - val_loss: 1.4839 - val_accuracy: 0.5082
Epoch 15/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.1543 - accuracy: 0.6034 - val_loss: 1.5097 - val_accuracy: 0.4968
Epoch 16/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.1200 - accuracy: 0.6135 - val_loss: 1.5001 - val_accuracy: 0.5120
Epoch 17/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.1028 - accuracy: 0.6199 - val_loss: 1.4856 - val_accuracy: 0.5056
Epoch 18/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.0863 - accuracy: 0.6265 - val_loss: 1.5116 - val_accuracy: 0.4966
Epoch 19/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.0715 - accuracy: 0.6345 - val_loss: 1.5787 - val_accuracy: 0.5070
Epoch 20/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.0342 - accuracy: 0.6453 - val_loss: 1.4987 - val_accuracy: 0.5144
Epoch 21/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.0169 - accuracy: 0.6531 - val_loss: 1.6292 - val_accuracy: 0.4462
Epoch 22/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.1346 - accuracy: 0.6074 - val_loss: 1.5280 - val_accuracy: 0.5136
Epoch 23/100
1407/1407 [==============================] - 7s 5ms/step - loss: 0.9820 - accuracy: 0.6678 - val_loss: 1.5392 - val_accuracy: 0.5040
Epoch 24/100
1407/1407 [==============================] - 7s 5ms/step - loss: 0.9701 - accuracy: 0.6679 - val_loss: 1.5505 - val_accuracy: 0.5170
Epoch 25/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.3604 - accuracy: 0.6753 - val_loss: 1.5468 - val_accuracy: 0.4992
Epoch 26/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.0177 - accuracy: 0.6510 - val_loss: 1.5474 - val_accuracy: 0.5020
Epoch 27/100
1407/1407 [==============================] - 7s 5ms/step - loss: 0.9425 - accuracy: 0.6798 - val_loss: 1.5545 - val_accuracy: 0.5076
Epoch 28/100
1407/1407 [==============================] - 7s 5ms/step - loss: 0.9005 - accuracy: 0.6902 - val_loss: 1.5659 - val_accuracy: 0.5138
157/157 [==============================] - 0s 1ms/step - loss: 1.4633 - accuracy: 0.4792</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="136">
<pre><code>[1.4633383750915527, 0.47920000553131104]</code></pre>
</div>
</div>
<div id="cell-209" class="cell" data-execution_count="137">
<div class="sourceCode cell-code" id="cb205"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb205-1"><a href="#cb205-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.models.load_model(<span class="st">"my_cifar10_selu_model.h5"</span>)</span>
<span id="cb205-2"><a href="#cb205-2" aria-hidden="true" tabindex="-1"></a>model.evaluate(X_valid_scaled, y_valid)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>157/157 [==============================] - 0s 1ms/step - loss: 1.4633 - accuracy: 0.4792</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="137">
<pre><code>[1.4633383750915527, 0.47920000553131104]</code></pre>
</div>
</div>
<p>We get 47.9% accuracy, which is not much better than the original model (47.6%), and not as good as the model using batch normalization (54.0%). However, convergence was almost as fast as with the BN model, plus each epoch took only 7 seconds. So it’s by far the fastest model to train so far.</p>
</section>
<section id="e." class="level3">
<h3 class="anchored" data-anchor-id="e.">e.</h3>
<p><em>Exercise: Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout.</em></p>
<div id="cell-212" class="cell" data-execution_count="138">
<div class="sourceCode cell-code" id="cb208"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb208-1"><a href="#cb208-1" aria-hidden="true" tabindex="-1"></a>keras.backend.clear_session()</span>
<span id="cb208-2"><a href="#cb208-2" aria-hidden="true" tabindex="-1"></a>tf.random.set_seed(<span class="dv">42</span>)</span>
<span id="cb208-3"><a href="#cb208-3" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb208-4"><a href="#cb208-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb208-5"><a href="#cb208-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.models.Sequential()</span>
<span id="cb208-6"><a href="#cb208-6" aria-hidden="true" tabindex="-1"></a>model.add(keras.layers.Flatten(input_shape<span class="op">=</span>[<span class="dv">32</span>, <span class="dv">32</span>, <span class="dv">3</span>]))</span>
<span id="cb208-7"><a href="#cb208-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">20</span>):</span>
<span id="cb208-8"><a href="#cb208-8" aria-hidden="true" tabindex="-1"></a>    model.add(keras.layers.Dense(<span class="dv">100</span>,</span>
<span id="cb208-9"><a href="#cb208-9" aria-hidden="true" tabindex="-1"></a>                                 kernel_initializer<span class="op">=</span><span class="st">"lecun_normal"</span>,</span>
<span id="cb208-10"><a href="#cb208-10" aria-hidden="true" tabindex="-1"></a>                                 activation<span class="op">=</span><span class="st">"selu"</span>))</span>
<span id="cb208-11"><a href="#cb208-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb208-12"><a href="#cb208-12" aria-hidden="true" tabindex="-1"></a>model.add(keras.layers.AlphaDropout(rate<span class="op">=</span><span class="fl">0.1</span>))</span>
<span id="cb208-13"><a href="#cb208-13" aria-hidden="true" tabindex="-1"></a>model.add(keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">"softmax"</span>))</span>
<span id="cb208-14"><a href="#cb208-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb208-15"><a href="#cb208-15" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> keras.optimizers.Nadam(learning_rate<span class="op">=</span><span class="fl">5e-4</span>)</span>
<span id="cb208-16"><a href="#cb208-16" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>,</span>
<span id="cb208-17"><a href="#cb208-17" aria-hidden="true" tabindex="-1"></a>              optimizer<span class="op">=</span>optimizer,</span>
<span id="cb208-18"><a href="#cb208-18" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span>
<span id="cb208-19"><a href="#cb208-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb208-20"><a href="#cb208-20" aria-hidden="true" tabindex="-1"></a>early_stopping_cb <span class="op">=</span> keras.callbacks.EarlyStopping(patience<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb208-21"><a href="#cb208-21" aria-hidden="true" tabindex="-1"></a>model_checkpoint_cb <span class="op">=</span> keras.callbacks.ModelCheckpoint(<span class="st">"my_cifar10_alpha_dropout_model.h5"</span>, save_best_only<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb208-22"><a href="#cb208-22" aria-hidden="true" tabindex="-1"></a>run_index <span class="op">=</span> <span class="dv">1</span> <span class="co"># increment every time you train the model</span></span>
<span id="cb208-23"><a href="#cb208-23" aria-hidden="true" tabindex="-1"></a>run_logdir <span class="op">=</span> os.path.join(os.curdir, <span class="st">"my_cifar10_logs"</span>, <span class="st">"run_alpha_dropout_</span><span class="sc">{:03d}</span><span class="st">"</span>.<span class="bu">format</span>(run_index))</span>
<span id="cb208-24"><a href="#cb208-24" aria-hidden="true" tabindex="-1"></a>tensorboard_cb <span class="op">=</span> keras.callbacks.TensorBoard(run_logdir)</span>
<span id="cb208-25"><a href="#cb208-25" aria-hidden="true" tabindex="-1"></a>callbacks <span class="op">=</span> [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]</span>
<span id="cb208-26"><a href="#cb208-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb208-27"><a href="#cb208-27" aria-hidden="true" tabindex="-1"></a>X_means <span class="op">=</span> X_train.mean(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb208-28"><a href="#cb208-28" aria-hidden="true" tabindex="-1"></a>X_stds <span class="op">=</span> X_train.std(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb208-29"><a href="#cb208-29" aria-hidden="true" tabindex="-1"></a>X_train_scaled <span class="op">=</span> (X_train <span class="op">-</span> X_means) <span class="op">/</span> X_stds</span>
<span id="cb208-30"><a href="#cb208-30" aria-hidden="true" tabindex="-1"></a>X_valid_scaled <span class="op">=</span> (X_valid <span class="op">-</span> X_means) <span class="op">/</span> X_stds</span>
<span id="cb208-31"><a href="#cb208-31" aria-hidden="true" tabindex="-1"></a>X_test_scaled <span class="op">=</span> (X_test <span class="op">-</span> X_means) <span class="op">/</span> X_stds</span>
<span id="cb208-32"><a href="#cb208-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb208-33"><a href="#cb208-33" aria-hidden="true" tabindex="-1"></a>model.fit(X_train_scaled, y_train, epochs<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb208-34"><a href="#cb208-34" aria-hidden="true" tabindex="-1"></a>          validation_data<span class="op">=</span>(X_valid_scaled, y_valid),</span>
<span id="cb208-35"><a href="#cb208-35" aria-hidden="true" tabindex="-1"></a>          callbacks<span class="op">=</span>callbacks)</span>
<span id="cb208-36"><a href="#cb208-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb208-37"><a href="#cb208-37" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.models.load_model(<span class="st">"my_cifar10_alpha_dropout_model.h5"</span>)</span>
<span id="cb208-38"><a href="#cb208-38" aria-hidden="true" tabindex="-1"></a>model.evaluate(X_valid_scaled, y_valid)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/100
1407/1407 [==============================] - 9s 5ms/step - loss: 2.0583 - accuracy: 0.2742 - val_loss: 1.7429 - val_accuracy: 0.3858
Epoch 2/100
1407/1407 [==============================] - 6s 5ms/step - loss: 1.6852 - accuracy: 0.4008 - val_loss: 1.7055 - val_accuracy: 0.3792
Epoch 3/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.5963 - accuracy: 0.4413 - val_loss: 1.7401 - val_accuracy: 0.4072
Epoch 4/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.5231 - accuracy: 0.4634 - val_loss: 1.5728 - val_accuracy: 0.4584
Epoch 5/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.4619 - accuracy: 0.4887 - val_loss: 1.5448 - val_accuracy: 0.4702
Epoch 6/100
1407/1407 [==============================] - 6s 5ms/step - loss: 1.4074 - accuracy: 0.5061 - val_loss: 1.5678 - val_accuracy: 0.4664
Epoch 7/100
1407/1407 [==============================] - 6s 5ms/step - loss: 1.3718 - accuracy: 0.5222 - val_loss: 1.5764 - val_accuracy: 0.4824
Epoch 8/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.3220 - accuracy: 0.5387 - val_loss: 1.4805 - val_accuracy: 0.4890
Epoch 9/100
1407/1407 [==============================] - 6s 5ms/step - loss: 1.2908 - accuracy: 0.5487 - val_loss: 1.5521 - val_accuracy: 0.4638
Epoch 10/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.2537 - accuracy: 0.5607 - val_loss: 1.5281 - val_accuracy: 0.4924
Epoch 11/100
1407/1407 [==============================] - 6s 5ms/step - loss: 1.2215 - accuracy: 0.5782 - val_loss: 1.5147 - val_accuracy: 0.5046
Epoch 12/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.1910 - accuracy: 0.5831 - val_loss: 1.5248 - val_accuracy: 0.5002
Epoch 13/100
1407/1407 [==============================] - 6s 5ms/step - loss: 1.1659 - accuracy: 0.5982 - val_loss: 1.5620 - val_accuracy: 0.5066
Epoch 14/100
1407/1407 [==============================] - 6s 5ms/step - loss: 1.1282 - accuracy: 0.6120 - val_loss: 1.5440 - val_accuracy: 0.5180
Epoch 15/100
1407/1407 [==============================] - 6s 5ms/step - loss: 1.1127 - accuracy: 0.6133 - val_loss: 1.5782 - val_accuracy: 0.5146
Epoch 16/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.0917 - accuracy: 0.6266 - val_loss: 1.6182 - val_accuracy: 0.5182
Epoch 17/100
1407/1407 [==============================] - 6s 5ms/step - loss: 1.0620 - accuracy: 0.6331 - val_loss: 1.6285 - val_accuracy: 0.5126
Epoch 18/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.0433 - accuracy: 0.6413 - val_loss: 1.6299 - val_accuracy: 0.5158
Epoch 19/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.0087 - accuracy: 0.6549 - val_loss: 1.7172 - val_accuracy: 0.5062
Epoch 20/100
1407/1407 [==============================] - 6s 5ms/step - loss: 0.9950 - accuracy: 0.6571 - val_loss: 1.6524 - val_accuracy: 0.5098
Epoch 21/100
1407/1407 [==============================] - 7s 5ms/step - loss: 0.9848 - accuracy: 0.6652 - val_loss: 1.7686 - val_accuracy: 0.5038
Epoch 22/100
1407/1407 [==============================] - 7s 5ms/step - loss: 0.9597 - accuracy: 0.6744 - val_loss: 1.6177 - val_accuracy: 0.5084
Epoch 23/100
1407/1407 [==============================] - 7s 5ms/step - loss: 0.9399 - accuracy: 0.6790 - val_loss: 1.7095 - val_accuracy: 0.5082
Epoch 24/100
1407/1407 [==============================] - 7s 5ms/step - loss: 0.9148 - accuracy: 0.6884 - val_loss: 1.7160 - val_accuracy: 0.5150
Epoch 25/100
1407/1407 [==============================] - 6s 5ms/step - loss: 0.9023 - accuracy: 0.6949 - val_loss: 1.7017 - val_accuracy: 0.5152
Epoch 26/100
1407/1407 [==============================] - 7s 5ms/step - loss: 0.8732 - accuracy: 0.7031 - val_loss: 1.7274 - val_accuracy: 0.5088
Epoch 27/100
1407/1407 [==============================] - 6s 5ms/step - loss: 0.8542 - accuracy: 0.7091 - val_loss: 1.7648 - val_accuracy: 0.5166
Epoch 28/100
1407/1407 [==============================] - 7s 5ms/step - loss: 0.8499 - accuracy: 0.7118 - val_loss: 1.7973 - val_accuracy: 0.5000
157/157 [==============================] - 0s 1ms/step - loss: 1.4805 - accuracy: 0.4890</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="138">
<pre><code>[1.4804893732070923, 0.48899999260902405]</code></pre>
</div>
</div>
<p>The model reaches 48.9% accuracy on the validation set. That’s very slightly better than without dropout (47.6%). With an extensive hyperparameter search, it might be possible to do better (I tried dropout rates of 5%, 10%, 20% and 40%, and learning rates 1e-4, 3e-4, 5e-4, and 1e-3), but probably not much better in this case.</p>
<p>Let’s use MC Dropout now. We will need the <code>MCAlphaDropout</code> class we used earlier, so let’s just copy it here for convenience:</p>
<div id="cell-215" class="cell" data-execution_count="139">
<div class="sourceCode cell-code" id="cb211"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb211-1"><a href="#cb211-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MCAlphaDropout(keras.layers.AlphaDropout):</span>
<span id="cb211-2"><a href="#cb211-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb211-3"><a href="#cb211-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">super</span>().call(inputs, training<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now let’s create a new model, identical to the one we just trained (with the same weights), but with <code>MCAlphaDropout</code> dropout layers instead of <code>AlphaDropout</code> layers:</p>
<div id="cell-217" class="cell" data-execution_count="140">
<div class="sourceCode cell-code" id="cb212"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb212-1"><a href="#cb212-1" aria-hidden="true" tabindex="-1"></a>mc_model <span class="op">=</span> keras.models.Sequential([</span>
<span id="cb212-2"><a href="#cb212-2" aria-hidden="true" tabindex="-1"></a>    MCAlphaDropout(layer.rate) <span class="cf">if</span> <span class="bu">isinstance</span>(layer, keras.layers.AlphaDropout) <span class="cf">else</span> layer</span>
<span id="cb212-3"><a href="#cb212-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> layer <span class="kw">in</span> model.layers</span>
<span id="cb212-4"><a href="#cb212-4" aria-hidden="true" tabindex="-1"></a>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Then let’s add a couple utility functions. The first will run the model many times (10 by default) and it will return the mean predicted class probabilities. The second will use these mean probabilities to predict the most likely class for each instance:</p>
<div id="cell-219" class="cell" data-execution_count="141">
<div class="sourceCode cell-code" id="cb213"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb213-1"><a href="#cb213-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mc_dropout_predict_probas(mc_model, X, n_samples<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb213-2"><a href="#cb213-2" aria-hidden="true" tabindex="-1"></a>    Y_probas <span class="op">=</span> [mc_model.predict(X) <span class="cf">for</span> sample <span class="kw">in</span> <span class="bu">range</span>(n_samples)]</span>
<span id="cb213-3"><a href="#cb213-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean(Y_probas, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb213-4"><a href="#cb213-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb213-5"><a href="#cb213-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mc_dropout_predict_classes(mc_model, X, n_samples<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb213-6"><a href="#cb213-6" aria-hidden="true" tabindex="-1"></a>    Y_probas <span class="op">=</span> mc_dropout_predict_probas(mc_model, X, n_samples)</span>
<span id="cb213-7"><a href="#cb213-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.argmax(Y_probas, axis<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now let’s make predictions for all the instances in the validation set, and compute the accuracy:</p>
<div id="cell-221" class="cell" data-execution_count="142">
<div class="sourceCode cell-code" id="cb214"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb214-1"><a href="#cb214-1" aria-hidden="true" tabindex="-1"></a>keras.backend.clear_session()</span>
<span id="cb214-2"><a href="#cb214-2" aria-hidden="true" tabindex="-1"></a>tf.random.set_seed(<span class="dv">42</span>)</span>
<span id="cb214-3"><a href="#cb214-3" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb214-4"><a href="#cb214-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb214-5"><a href="#cb214-5" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> mc_dropout_predict_classes(mc_model, X_valid_scaled)</span>
<span id="cb214-6"><a href="#cb214-6" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> np.mean(y_pred <span class="op">==</span> y_valid[:, <span class="dv">0</span>])</span>
<span id="cb214-7"><a href="#cb214-7" aria-hidden="true" tabindex="-1"></a>accuracy</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="142">
<pre><code>0.4892</code></pre>
</div>
</div>
<p>We get no accuracy improvement in this case (we’re still at 48.9% accuracy).</p>
<p>So the best model we got in this exercise is the Batch Normalization model.</p>
</section>
<section id="f." class="level3">
<h3 class="anchored" data-anchor-id="f.">f.</h3>
<p><em>Exercise: Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy.</em></p>
<div id="cell-224" class="cell" data-execution_count="143">
<div class="sourceCode cell-code" id="cb216"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb216-1"><a href="#cb216-1" aria-hidden="true" tabindex="-1"></a>keras.backend.clear_session()</span>
<span id="cb216-2"><a href="#cb216-2" aria-hidden="true" tabindex="-1"></a>tf.random.set_seed(<span class="dv">42</span>)</span>
<span id="cb216-3"><a href="#cb216-3" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb216-4"><a href="#cb216-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb216-5"><a href="#cb216-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.models.Sequential()</span>
<span id="cb216-6"><a href="#cb216-6" aria-hidden="true" tabindex="-1"></a>model.add(keras.layers.Flatten(input_shape<span class="op">=</span>[<span class="dv">32</span>, <span class="dv">32</span>, <span class="dv">3</span>]))</span>
<span id="cb216-7"><a href="#cb216-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">20</span>):</span>
<span id="cb216-8"><a href="#cb216-8" aria-hidden="true" tabindex="-1"></a>    model.add(keras.layers.Dense(<span class="dv">100</span>,</span>
<span id="cb216-9"><a href="#cb216-9" aria-hidden="true" tabindex="-1"></a>                                 kernel_initializer<span class="op">=</span><span class="st">"lecun_normal"</span>,</span>
<span id="cb216-10"><a href="#cb216-10" aria-hidden="true" tabindex="-1"></a>                                 activation<span class="op">=</span><span class="st">"selu"</span>))</span>
<span id="cb216-11"><a href="#cb216-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb216-12"><a href="#cb216-12" aria-hidden="true" tabindex="-1"></a>model.add(keras.layers.AlphaDropout(rate<span class="op">=</span><span class="fl">0.1</span>))</span>
<span id="cb216-13"><a href="#cb216-13" aria-hidden="true" tabindex="-1"></a>model.add(keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">"softmax"</span>))</span>
<span id="cb216-14"><a href="#cb216-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb216-15"><a href="#cb216-15" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> keras.optimizers.SGD(learning_rate<span class="op">=</span><span class="fl">1e-3</span>)</span>
<span id="cb216-16"><a href="#cb216-16" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>,</span>
<span id="cb216-17"><a href="#cb216-17" aria-hidden="true" tabindex="-1"></a>              optimizer<span class="op">=</span>optimizer,</span>
<span id="cb216-18"><a href="#cb216-18" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-225" class="cell" data-execution_count="144">
<div class="sourceCode cell-code" id="cb217"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb217-1"><a href="#cb217-1" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb217-2"><a href="#cb217-2" aria-hidden="true" tabindex="-1"></a>rates, losses <span class="op">=</span> find_learning_rate(model, X_train_scaled, y_train, epochs<span class="op">=</span><span class="dv">1</span>, batch_size<span class="op">=</span>batch_size)</span>
<span id="cb217-3"><a href="#cb217-3" aria-hidden="true" tabindex="-1"></a>plot_lr_vs_loss(rates, losses)</span>
<span id="cb217-4"><a href="#cb217-4" aria-hidden="true" tabindex="-1"></a>plt.axis([<span class="bu">min</span>(rates), <span class="bu">max</span>(rates), <span class="bu">min</span>(losses), (losses[<span class="dv">0</span>] <span class="op">+</span> <span class="bu">min</span>(losses)) <span class="op">/</span> <span class="fl">1.4</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>352/352 [==============================] - 2s 6ms/step - loss: nan - accuracy: 0.1255</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="144">
<pre><code>(9.999999747378752e-06,
 9.999868392944336,
 2.6167492866516113,
 3.9354368618556435)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="11_training_deep_neural_networks_files/figure-html/cell-145-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-226" class="cell" data-execution_count="145">
<div class="sourceCode cell-code" id="cb220"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb220-1"><a href="#cb220-1" aria-hidden="true" tabindex="-1"></a>keras.backend.clear_session()</span>
<span id="cb220-2"><a href="#cb220-2" aria-hidden="true" tabindex="-1"></a>tf.random.set_seed(<span class="dv">42</span>)</span>
<span id="cb220-3"><a href="#cb220-3" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb220-4"><a href="#cb220-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-5"><a href="#cb220-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.models.Sequential()</span>
<span id="cb220-6"><a href="#cb220-6" aria-hidden="true" tabindex="-1"></a>model.add(keras.layers.Flatten(input_shape<span class="op">=</span>[<span class="dv">32</span>, <span class="dv">32</span>, <span class="dv">3</span>]))</span>
<span id="cb220-7"><a href="#cb220-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">20</span>):</span>
<span id="cb220-8"><a href="#cb220-8" aria-hidden="true" tabindex="-1"></a>    model.add(keras.layers.Dense(<span class="dv">100</span>,</span>
<span id="cb220-9"><a href="#cb220-9" aria-hidden="true" tabindex="-1"></a>                                 kernel_initializer<span class="op">=</span><span class="st">"lecun_normal"</span>,</span>
<span id="cb220-10"><a href="#cb220-10" aria-hidden="true" tabindex="-1"></a>                                 activation<span class="op">=</span><span class="st">"selu"</span>))</span>
<span id="cb220-11"><a href="#cb220-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-12"><a href="#cb220-12" aria-hidden="true" tabindex="-1"></a>model.add(keras.layers.AlphaDropout(rate<span class="op">=</span><span class="fl">0.1</span>))</span>
<span id="cb220-13"><a href="#cb220-13" aria-hidden="true" tabindex="-1"></a>model.add(keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">"softmax"</span>))</span>
<span id="cb220-14"><a href="#cb220-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-15"><a href="#cb220-15" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> keras.optimizers.SGD(learning_rate<span class="op">=</span><span class="fl">1e-2</span>)</span>
<span id="cb220-16"><a href="#cb220-16" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>,</span>
<span id="cb220-17"><a href="#cb220-17" aria-hidden="true" tabindex="-1"></a>              optimizer<span class="op">=</span>optimizer,</span>
<span id="cb220-18"><a href="#cb220-18" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-227" class="cell" data-execution_count="146">
<div class="sourceCode cell-code" id="cb221"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb221-1"><a href="#cb221-1" aria-hidden="true" tabindex="-1"></a>n_epochs <span class="op">=</span> <span class="dv">15</span></span>
<span id="cb221-2"><a href="#cb221-2" aria-hidden="true" tabindex="-1"></a>onecycle <span class="op">=</span> OneCycleScheduler(math.ceil(<span class="bu">len</span>(X_train_scaled) <span class="op">/</span> batch_size) <span class="op">*</span> n_epochs, max_rate<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb221-3"><a href="#cb221-3" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(X_train_scaled, y_train, epochs<span class="op">=</span>n_epochs, batch_size<span class="op">=</span>batch_size,</span>
<span id="cb221-4"><a href="#cb221-4" aria-hidden="true" tabindex="-1"></a>                    validation_data<span class="op">=</span>(X_valid_scaled, y_valid),</span>
<span id="cb221-5"><a href="#cb221-5" aria-hidden="true" tabindex="-1"></a>                    callbacks<span class="op">=</span>[onecycle])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/15
352/352 [==============================] - 3s 6ms/step - loss: 2.2298 - accuracy: 0.2349 - val_loss: 1.7841 - val_accuracy: 0.3834
Epoch 2/15
352/352 [==============================] - 2s 6ms/step - loss: 1.7928 - accuracy: 0.3689 - val_loss: 1.6806 - val_accuracy: 0.4086
Epoch 3/15
352/352 [==============================] - 2s 6ms/step - loss: 1.6475 - accuracy: 0.4190 - val_loss: 1.6378 - val_accuracy: 0.4350
Epoch 4/15
352/352 [==============================] - 2s 6ms/step - loss: 1.5428 - accuracy: 0.4543 - val_loss: 1.6266 - val_accuracy: 0.4390
Epoch 5/15
352/352 [==============================] - 2s 6ms/step - loss: 1.4865 - accuracy: 0.4769 - val_loss: 1.6158 - val_accuracy: 0.4384
Epoch 6/15
352/352 [==============================] - 2s 6ms/step - loss: 1.4339 - accuracy: 0.4866 - val_loss: 1.5850 - val_accuracy: 0.4412
Epoch 7/15
352/352 [==============================] - 2s 6ms/step - loss: 1.4042 - accuracy: 0.5056 - val_loss: 1.6146 - val_accuracy: 0.4384
Epoch 8/15
352/352 [==============================] - 2s 6ms/step - loss: 1.3437 - accuracy: 0.5229 - val_loss: 1.5299 - val_accuracy: 0.4846
Epoch 9/15
352/352 [==============================] - 2s 5ms/step - loss: 1.2721 - accuracy: 0.5459 - val_loss: 1.5145 - val_accuracy: 0.4874
Epoch 10/15
352/352 [==============================] - 2s 6ms/step - loss: 1.1942 - accuracy: 0.5698 - val_loss: 1.4958 - val_accuracy: 0.5040
Epoch 11/15
352/352 [==============================] - 2s 6ms/step - loss: 1.1211 - accuracy: 0.6033 - val_loss: 1.5406 - val_accuracy: 0.4984
Epoch 12/15
352/352 [==============================] - 2s 6ms/step - loss: 1.0673 - accuracy: 0.6161 - val_loss: 1.5284 - val_accuracy: 0.5144
Epoch 13/15
352/352 [==============================] - 2s 6ms/step - loss: 0.9927 - accuracy: 0.6435 - val_loss: 1.5449 - val_accuracy: 0.5140
Epoch 14/15
352/352 [==============================] - 2s 6ms/step - loss: 0.9205 - accuracy: 0.6703 - val_loss: 1.5652 - val_accuracy: 0.5224
Epoch 15/15
352/352 [==============================] - 2s 6ms/step - loss: 0.8936 - accuracy: 0.6801 - val_loss: 1.5912 - val_accuracy: 0.5198</code></pre>
</div>
</div>
<p>One cycle allowed us to train the model in just 15 epochs, each taking only 2 seconds (thanks to the larger batch size). This is several times faster than the fastest model we trained so far. Moreover, we improved the model’s performance (from 47.6% to 52.0%). The batch normalized model reaches a slightly better performance (54%), but it’s much slower to train.</p>
</section>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>