<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Chapter 4 – Training Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="04_training_linear_models_files/libs/clipboard/clipboard.min.js"></script>
<script src="04_training_linear_models_files/libs/quarto-html/quarto.js"></script>
<script src="04_training_linear_models_files/libs/quarto-html/popper.min.js"></script>
<script src="04_training_linear_models_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="04_training_linear_models_files/libs/quarto-html/anchor.min.js"></script>
<link href="04_training_linear_models_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="04_training_linear_models_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="04_training_linear_models_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="04_training_linear_models_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="04_training_linear_models_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#setup" id="toc-setup" class="nav-link active" data-scroll-target="#setup">Setup</a></li>
  <li><a href="#linear-regression" id="toc-linear-regression" class="nav-link" data-scroll-target="#linear-regression">Linear Regression</a>
  <ul class="collapse">
  <li><a href="#the-normal-equation" id="toc-the-normal-equation" class="nav-link" data-scroll-target="#the-normal-equation">The Normal Equation</a></li>
  </ul></li>
  <li><a href="#gradient-descent" id="toc-gradient-descent" class="nav-link" data-scroll-target="#gradient-descent">Gradient Descent</a>
  <ul class="collapse">
  <li><a href="#batch-gradient-descent" id="toc-batch-gradient-descent" class="nav-link" data-scroll-target="#batch-gradient-descent">Batch Gradient Descent</a></li>
  <li><a href="#stochastic-gradient-descent" id="toc-stochastic-gradient-descent" class="nav-link" data-scroll-target="#stochastic-gradient-descent">Stochastic Gradient Descent</a></li>
  <li><a href="#mini-batch-gradient-descent" id="toc-mini-batch-gradient-descent" class="nav-link" data-scroll-target="#mini-batch-gradient-descent">Mini-batch gradient descent</a></li>
  </ul></li>
  <li><a href="#polynomial-regression" id="toc-polynomial-regression" class="nav-link" data-scroll-target="#polynomial-regression">Polynomial Regression</a></li>
  <li><a href="#learning-curves" id="toc-learning-curves" class="nav-link" data-scroll-target="#learning-curves">Learning Curves</a></li>
  <li><a href="#regularized-linear-models" id="toc-regularized-linear-models" class="nav-link" data-scroll-target="#regularized-linear-models">Regularized Linear Models</a>
  <ul class="collapse">
  <li><a href="#ridge-regression" id="toc-ridge-regression" class="nav-link" data-scroll-target="#ridge-regression">Ridge Regression</a></li>
  <li><a href="#lasso-regression" id="toc-lasso-regression" class="nav-link" data-scroll-target="#lasso-regression">Lasso Regression</a></li>
  <li><a href="#elastic-net" id="toc-elastic-net" class="nav-link" data-scroll-target="#elastic-net">Elastic Net</a></li>
  <li><a href="#early-stopping" id="toc-early-stopping" class="nav-link" data-scroll-target="#early-stopping">Early Stopping</a></li>
  </ul></li>
  <li><a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression">Logistic Regression</a>
  <ul class="collapse">
  <li><a href="#decision-boundaries" id="toc-decision-boundaries" class="nav-link" data-scroll-target="#decision-boundaries">Decision Boundaries</a></li>
  <li><a href="#softmax-regression" id="toc-softmax-regression" class="nav-link" data-scroll-target="#softmax-regression">Softmax Regression</a></li>
  </ul></li>
  <li><a href="#exercise-solutions" id="toc-exercise-solutions" class="nav-link" data-scroll-target="#exercise-solutions">Exercise solutions</a>
  <ul class="collapse">
  <li><a href="#to-11." id="toc-to-11." class="nav-link" data-scroll-target="#to-11.">1. to 11.</a></li>
  <li><a href="#batch-gradient-descent-with-early-stopping-for-softmax-regression" id="toc-batch-gradient-descent-with-early-stopping-for-softmax-regression" class="nav-link" data-scroll-target="#batch-gradient-descent-with-early-stopping-for-softmax-regression">12. Batch Gradient Descent with early stopping for Softmax Regression</a></li>
  </ul></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Chapter 4 – Training Models</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p><em>This notebook contains all the sample code and solutions to the exercises in chapter 4.</em></p>
<section id="setup" class="level1">
<h1>Setup</h1>
<p>First, let’s import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Python ≥3.5 is required</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> sys.version_info <span class="op">&gt;=</span> (<span class="dv">3</span>, <span class="dv">5</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Scikit-Learn ≥0.20 is required</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> sklearn.__version__ <span class="op">&gt;=</span> <span class="st">"0.20"</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Common imports</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># to make this notebook's output stable across runs</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># To plot pretty figures</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>mpl.rc(<span class="st">'axes'</span>, labelsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>mpl.rc(<span class="st">'xtick'</span>, labelsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>mpl.rc(<span class="st">'ytick'</span>, labelsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Where to save the figures</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>PROJECT_ROOT_DIR <span class="op">=</span> <span class="st">"."</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>CHAPTER_ID <span class="op">=</span> <span class="st">"training_linear_models"</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>IMAGES_PATH <span class="op">=</span> os.path.join(PROJECT_ROOT_DIR, <span class="st">"images"</span>, CHAPTER_ID)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>os.makedirs(IMAGES_PATH, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> save_fig(fig_id, tight_layout<span class="op">=</span><span class="va">True</span>, fig_extension<span class="op">=</span><span class="st">"png"</span>, resolution<span class="op">=</span><span class="dv">300</span>):</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    path <span class="op">=</span> os.path.join(IMAGES_PATH, fig_id <span class="op">+</span> <span class="st">"."</span> <span class="op">+</span> fig_extension)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Saving figure"</span>, fig_id)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> tight_layout:</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>        plt.tight_layout()</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    plt.savefig(path, <span class="bu">format</span><span class="op">=</span>fig_extension, dpi<span class="op">=</span>resolution)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="linear-regression" class="level1">
<h1>Linear Regression</h1>
<section id="the-normal-equation" class="level2">
<h2 class="anchored" data-anchor-id="the-normal-equation">The Normal Equation</h2>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> np.random.rand(<span class="dv">100</span>, <span class="dv">1</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">4</span> <span class="op">+</span> <span class="dv">3</span> <span class="op">*</span> X <span class="op">+</span> np.random.randn(<span class="dv">100</span>, <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>plt.plot(X, y, <span class="st">"b."</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$x_1$"</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$y$"</span>, rotation<span class="op">=</span><span class="dv">0</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>plt.axis([<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">15</span>])</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>save_fig(<span class="st">"generated_data_plot"</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Saving figure generated_data_plot</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="04_training_linear_models_files/figure-html/cell-4-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>X_b <span class="op">=</span> np.c_[np.ones((<span class="dv">100</span>, <span class="dv">1</span>)), X]  <span class="co"># add x0 = 1 to each instance</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>theta_best <span class="op">=</span> np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>theta_best</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>array([[4.21509616],
       [2.77011339]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>X_new <span class="op">=</span> np.array([[<span class="dv">0</span>], [<span class="dv">2</span>]])</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>X_new_b <span class="op">=</span> np.c_[np.ones((<span class="dv">2</span>, <span class="dv">1</span>)), X_new]  <span class="co"># add x0 = 1 to each instance</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>y_predict <span class="op">=</span> X_new_b.dot(theta_best)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>y_predict</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>array([[4.21509616],
       [9.75532293]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>plt.plot(X_new, y_predict, <span class="st">"r-"</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>plt.plot(X, y, <span class="st">"b."</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>plt.axis([<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">15</span>])</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="04_training_linear_models_files/figure-html/cell-8-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The figure in the book actually corresponds to the following code, with a legend and axis labels:</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>plt.plot(X_new, y_predict, <span class="st">"r-"</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">"Predictions"</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>plt.plot(X, y, <span class="st">"b."</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$x_1$"</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$y$"</span>, rotation<span class="op">=</span><span class="dv">0</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">"upper left"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>plt.axis([<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">15</span>])</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>save_fig(<span class="st">"linear_model_predictions_plot"</span>)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Saving figure linear_model_predictions_plot</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="04_training_linear_models_files/figure-html/cell-9-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>lin_reg <span class="op">=</span> LinearRegression()</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>lin_reg.fit(X, y)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>lin_reg.intercept_, lin_reg.coef_</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>(array([4.21509616]), array([[2.77011339]]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>lin_reg.predict(X_new)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>array([[4.21509616],
       [9.75532293]])</code></pre>
</div>
</div>
<p>The <code>LinearRegression</code> class is based on the <code>scipy.linalg.lstsq()</code> function (the name stands for “least squares”), which you could call directly:</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>theta_best_svd, residuals, rank, s <span class="op">=</span> np.linalg.lstsq(X_b, y, rcond<span class="op">=</span><span class="fl">1e-6</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>theta_best_svd</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>array([[4.21509616],
       [2.77011339]])</code></pre>
</div>
</div>
<p>This function computes <span class="math inline">\(\mathbf{X}^+\mathbf{y}\)</span>, where <span class="math inline">\(\mathbf{X}^{+}\)</span> is the <em>pseudoinverse</em> of <span class="math inline">\(\mathbf{X}\)</span> (specifically the Moore-Penrose inverse). You can use <code>np.linalg.pinv()</code> to compute the pseudoinverse directly:</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>np.linalg.pinv(X_b).dot(y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>array([[4.21509616],
       [2.77011339]])</code></pre>
</div>
</div>
</section>
</section>
<section id="gradient-descent" class="level1">
<h1>Gradient Descent</h1>
<section id="batch-gradient-descent" class="level2">
<h2 class="anchored" data-anchor-id="batch-gradient-descent">Batch Gradient Descent</h2>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>eta <span class="op">=</span> <span class="fl">0.1</span>  <span class="co"># learning rate</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>n_iterations <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>theta <span class="op">=</span> np.random.randn(<span class="dv">2</span>,<span class="dv">1</span>)  <span class="co"># random initialization</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> iteration <span class="kw">in</span> <span class="bu">range</span>(n_iterations):</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>    gradients <span class="op">=</span> <span class="dv">2</span><span class="op">/</span>m <span class="op">*</span> X_b.T.dot(X_b.dot(theta) <span class="op">-</span> y)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> theta <span class="op">-</span> eta <span class="op">*</span> gradients</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>theta</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>array([[4.21509616],
       [2.77011339]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>X_new_b.dot(theta)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>array([[4.21509616],
       [9.75532293]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>theta_path_bgd <span class="op">=</span> []</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_gradient_descent(theta, eta, theta_path<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> <span class="bu">len</span>(X_b)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    plt.plot(X, y, <span class="st">"b."</span>)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>    n_iterations <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> iteration <span class="kw">in</span> <span class="bu">range</span>(n_iterations):</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> iteration <span class="op">&lt;</span> <span class="dv">10</span>:</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>            y_predict <span class="op">=</span> X_new_b.dot(theta)</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>            style <span class="op">=</span> <span class="st">"b-"</span> <span class="cf">if</span> iteration <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="st">"r--"</span></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>            plt.plot(X_new, y_predict, style)</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>        gradients <span class="op">=</span> <span class="dv">2</span><span class="op">/</span>m <span class="op">*</span> X_b.T.dot(X_b.dot(theta) <span class="op">-</span> y)</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">=</span> theta <span class="op">-</span> eta <span class="op">*</span> gradients</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> theta_path <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>            theta_path.append(theta)</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">"$x_1$"</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>    plt.axis([<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">15</span>])</span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="vs">r"$\eta = </span><span class="sc">{}</span><span class="vs">$"</span>.<span class="bu">format</span>(eta), fontsize<span class="op">=</span><span class="dv">16</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>theta <span class="op">=</span> np.random.randn(<span class="dv">2</span>,<span class="dv">1</span>)  <span class="co"># random initialization</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">4</span>))</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">131</span>)<span class="op">;</span> plot_gradient_descent(theta, eta<span class="op">=</span><span class="fl">0.02</span>)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$y$"</span>, rotation<span class="op">=</span><span class="dv">0</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">132</span>)<span class="op">;</span> plot_gradient_descent(theta, eta<span class="op">=</span><span class="fl">0.1</span>, theta_path<span class="op">=</span>theta_path_bgd)</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">133</span>)<span class="op">;</span> plot_gradient_descent(theta, eta<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>save_fig(<span class="st">"gradient_descent_plot"</span>)</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Saving figure gradient_descent_plot</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="04_training_linear_models_files/figure-html/cell-18-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="stochastic-gradient-descent" class="level2">
<h2 class="anchored" data-anchor-id="stochastic-gradient-descent">Stochastic Gradient Descent</h2>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>theta_path_sgd <span class="op">=</span> []</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="bu">len</span>(X_b)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>n_epochs <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>t0, t1 <span class="op">=</span> <span class="dv">5</span>, <span class="dv">50</span>  <span class="co"># learning schedule hyperparameters</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> learning_schedule(t):</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> t0 <span class="op">/</span> (t <span class="op">+</span> t1)</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>theta <span class="op">=</span> np.random.randn(<span class="dv">2</span>,<span class="dv">1</span>)  <span class="co"># random initialization</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(n_epochs):</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(m):</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> epoch <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> i <span class="op">&lt;</span> <span class="dv">20</span>:                    <span class="co"># not shown in the book</span></span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>            y_predict <span class="op">=</span> X_new_b.dot(theta)           <span class="co"># not shown</span></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>            style <span class="op">=</span> <span class="st">"b-"</span> <span class="cf">if</span> i <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="st">"r--"</span>         <span class="co"># not shown</span></span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>            plt.plot(X_new, y_predict, style)        <span class="co"># not shown</span></span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>        random_index <span class="op">=</span> np.random.randint(m)</span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>        xi <span class="op">=</span> X_b[random_index:random_index<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>        yi <span class="op">=</span> y[random_index:random_index<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>        gradients <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> xi.T.dot(xi.dot(theta) <span class="op">-</span> yi)</span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>        eta <span class="op">=</span> learning_schedule(epoch <span class="op">*</span> m <span class="op">+</span> i)</span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">=</span> theta <span class="op">-</span> eta <span class="op">*</span> gradients</span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a>        theta_path_sgd.append(theta)                 <span class="co"># not shown</span></span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a>plt.plot(X, y, <span class="st">"b."</span>)                                 <span class="co"># not shown</span></span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$x_1$"</span>, fontsize<span class="op">=</span><span class="dv">18</span>)                     <span class="co"># not shown</span></span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$y$"</span>, rotation<span class="op">=</span><span class="dv">0</span>, fontsize<span class="op">=</span><span class="dv">18</span>)           <span class="co"># not shown</span></span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a>plt.axis([<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">15</span>])                              <span class="co"># not shown</span></span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a>save_fig(<span class="st">"sgd_plot"</span>)                                 <span class="co"># not shown</span></span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a>plt.show()                                           <span class="co"># not shown</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Saving figure sgd_plot</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="04_training_linear_models_files/figure-html/cell-20-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-scrolled="true" data-execution_count="20">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>theta</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>array([[4.21076011],
       [2.74856079]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> SGDRegressor</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>sgd_reg <span class="op">=</span> SGDRegressor(max_iter<span class="op">=</span><span class="dv">1000</span>, tol<span class="op">=</span><span class="fl">1e-3</span>, penalty<span class="op">=</span><span class="va">None</span>, eta0<span class="op">=</span><span class="fl">0.1</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>sgd_reg.fit(X, y.ravel())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>SGDRegressor(eta0=0.1, penalty=None, random_state=42)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>sgd_reg.intercept_, sgd_reg.coef_</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="22">
<pre><code>(array([4.24365286]), array([2.8250878]))</code></pre>
</div>
</div>
</section>
<section id="mini-batch-gradient-descent" class="level2">
<h2 class="anchored" data-anchor-id="mini-batch-gradient-descent">Mini-batch gradient descent</h2>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>theta_path_mgd <span class="op">=</span> []</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>n_iterations <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>minibatch_size <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>theta <span class="op">=</span> np.random.randn(<span class="dv">2</span>,<span class="dv">1</span>)  <span class="co"># random initialization</span></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>t0, t1 <span class="op">=</span> <span class="dv">200</span>, <span class="dv">1000</span></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> learning_schedule(t):</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> t0 <span class="op">/</span> (t <span class="op">+</span> t1)</span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(n_iterations):</span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a>    shuffled_indices <span class="op">=</span> np.random.permutation(m)</span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a>    X_b_shuffled <span class="op">=</span> X_b[shuffled_indices]</span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a>    y_shuffled <span class="op">=</span> y[shuffled_indices]</span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, m, minibatch_size):</span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a>        t <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a>        xi <span class="op">=</span> X_b_shuffled[i:i<span class="op">+</span>minibatch_size]</span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a>        yi <span class="op">=</span> y_shuffled[i:i<span class="op">+</span>minibatch_size]</span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a>        gradients <span class="op">=</span> <span class="dv">2</span><span class="op">/</span>minibatch_size <span class="op">*</span> xi.T.dot(xi.dot(theta) <span class="op">-</span> yi)</span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a>        eta <span class="op">=</span> learning_schedule(t)</span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">=</span> theta <span class="op">-</span> eta <span class="op">*</span> gradients</span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a>        theta_path_mgd.append(theta)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>theta</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>array([[4.25214635],
       [2.7896408 ]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>theta_path_bgd <span class="op">=</span> np.array(theta_path_bgd)</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>theta_path_sgd <span class="op">=</span> np.array(theta_path_sgd)</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>theta_path_mgd <span class="op">=</span> np.array(theta_path_mgd)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>,<span class="dv">4</span>))</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>plt.plot(theta_path_sgd[:, <span class="dv">0</span>], theta_path_sgd[:, <span class="dv">1</span>], <span class="st">"r-s"</span>, linewidth<span class="op">=</span><span class="dv">1</span>, label<span class="op">=</span><span class="st">"Stochastic"</span>)</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>plt.plot(theta_path_mgd[:, <span class="dv">0</span>], theta_path_mgd[:, <span class="dv">1</span>], <span class="st">"g-+"</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">"Mini-batch"</span>)</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>plt.plot(theta_path_bgd[:, <span class="dv">0</span>], theta_path_bgd[:, <span class="dv">1</span>], <span class="st">"b-o"</span>, linewidth<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">"Batch"</span>)</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">"upper left"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"$\theta_0$"</span>, fontsize<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"$\theta_1$   "</span>, fontsize<span class="op">=</span><span class="dv">20</span>, rotation<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>plt.axis([<span class="fl">2.5</span>, <span class="fl">4.5</span>, <span class="fl">2.3</span>, <span class="fl">3.9</span>])</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>save_fig(<span class="st">"gradient_descent_paths_plot"</span>)</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Saving figure gradient_descent_paths_plot</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="04_training_linear_models_files/figure-html/cell-27-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="polynomial-regression" class="level1">
<h1>Polynomial Regression</h1>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy.random <span class="im">as</span> rnd</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> <span class="dv">6</span> <span class="op">*</span> np.random.rand(m, <span class="dv">1</span>) <span class="op">-</span> <span class="dv">3</span></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> X<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> X <span class="op">+</span> <span class="dv">2</span> <span class="op">+</span> np.random.randn(m, <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>plt.plot(X, y, <span class="st">"b."</span>)</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$x_1$"</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$y$"</span>, rotation<span class="op">=</span><span class="dv">0</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>plt.axis([<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">10</span>])</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>save_fig(<span class="st">"quadratic_data_plot"</span>)</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Saving figure quadratic_data_plot</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="04_training_linear_models_files/figure-html/cell-30-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> PolynomialFeatures</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>poly_features <span class="op">=</span> PolynomialFeatures(degree<span class="op">=</span><span class="dv">2</span>, include_bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>X_poly <span class="op">=</span> poly_features.fit_transform(X)</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>X[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>array([-0.75275929])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>X_poly[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>array([-0.75275929,  0.56664654])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>lin_reg <span class="op">=</span> LinearRegression()</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>lin_reg.fit(X_poly, y)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>lin_reg.intercept_, lin_reg.coef_</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>(array([1.78134581]), array([[0.93366893, 0.56456263]]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>X_new<span class="op">=</span>np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">100</span>).reshape(<span class="dv">100</span>, <span class="dv">1</span>)</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>X_new_poly <span class="op">=</span> poly_features.transform(X_new)</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>y_new <span class="op">=</span> lin_reg.predict(X_new_poly)</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>plt.plot(X, y, <span class="st">"b."</span>)</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>plt.plot(X_new, y_new, <span class="st">"r-"</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">"Predictions"</span>)</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$x_1$"</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$y$"</span>, rotation<span class="op">=</span><span class="dv">0</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">"upper left"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>plt.axis([<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">10</span>])</span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a>save_fig(<span class="st">"quadratic_predictions_plot"</span>)</span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Saving figure quadratic_predictions_plot</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="04_training_linear_models_files/figure-html/cell-34-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> style, width, degree <span class="kw">in</span> ((<span class="st">"g-"</span>, <span class="dv">1</span>, <span class="dv">300</span>), (<span class="st">"b--"</span>, <span class="dv">2</span>, <span class="dv">2</span>), (<span class="st">"r-+"</span>, <span class="dv">2</span>, <span class="dv">1</span>)):</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>    polybig_features <span class="op">=</span> PolynomialFeatures(degree<span class="op">=</span>degree, include_bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>    std_scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>    lin_reg <span class="op">=</span> LinearRegression()</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>    polynomial_regression <span class="op">=</span> Pipeline([</span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a>            (<span class="st">"poly_features"</span>, polybig_features),</span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>            (<span class="st">"std_scaler"</span>, std_scaler),</span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a>            (<span class="st">"lin_reg"</span>, lin_reg),</span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a>    polynomial_regression.fit(X, y)</span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a>    y_newbig <span class="op">=</span> polynomial_regression.predict(X_new)</span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true" tabindex="-1"></a>    plt.plot(X_new, y_newbig, style, label<span class="op">=</span><span class="bu">str</span>(degree), linewidth<span class="op">=</span>width)</span>
<span id="cb56-16"><a href="#cb56-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-17"><a href="#cb56-17" aria-hidden="true" tabindex="-1"></a>plt.plot(X, y, <span class="st">"b."</span>, linewidth<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb56-18"><a href="#cb56-18" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">"upper left"</span>)</span>
<span id="cb56-19"><a href="#cb56-19" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$x_1$"</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb56-20"><a href="#cb56-20" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$y$"</span>, rotation<span class="op">=</span><span class="dv">0</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb56-21"><a href="#cb56-21" aria-hidden="true" tabindex="-1"></a>plt.axis([<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">10</span>])</span>
<span id="cb56-22"><a href="#cb56-22" aria-hidden="true" tabindex="-1"></a>save_fig(<span class="st">"high_degree_polynomials_plot"</span>)</span>
<span id="cb56-23"><a href="#cb56-23" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Saving figure high_degree_polynomials_plot</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="04_training_linear_models_files/figure-html/cell-35-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="learning-curves" class="level1">
<h1>Learning Curves</h1>
<div class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_learning_curves(model, X, y):</span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>    X_train, X_val, y_train, y_val <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>    train_errors, val_errors <span class="op">=</span> [], []</span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> m <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(X_train) <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a>        model.fit(X_train[:m], y_train[:m])</span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a>        y_train_predict <span class="op">=</span> model.predict(X_train[:m])</span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>        y_val_predict <span class="op">=</span> model.predict(X_val)</span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a>        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))</span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a>        val_errors.append(mean_squared_error(y_val, y_val_predict))</span>
<span id="cb58-13"><a href="#cb58-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-14"><a href="#cb58-14" aria-hidden="true" tabindex="-1"></a>    plt.plot(np.sqrt(train_errors), <span class="st">"r-+"</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">"train"</span>)</span>
<span id="cb58-15"><a href="#cb58-15" aria-hidden="true" tabindex="-1"></a>    plt.plot(np.sqrt(val_errors), <span class="st">"b-"</span>, linewidth<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">"val"</span>)</span>
<span id="cb58-16"><a href="#cb58-16" aria-hidden="true" tabindex="-1"></a>    plt.legend(loc<span class="op">=</span><span class="st">"upper right"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)   <span class="co"># not shown in the book</span></span>
<span id="cb58-17"><a href="#cb58-17" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">"Training set size"</span>, fontsize<span class="op">=</span><span class="dv">14</span>) <span class="co"># not shown</span></span>
<span id="cb58-18"><a href="#cb58-18" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">"RMSE"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)              <span class="co"># not shown</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>lin_reg <span class="op">=</span> LinearRegression()</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>plot_learning_curves(lin_reg, X, y)</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>plt.axis([<span class="dv">0</span>, <span class="dv">80</span>, <span class="dv">0</span>, <span class="dv">3</span>])                         <span class="co"># not shown in the book</span></span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>save_fig(<span class="st">"underfitting_learning_curves_plot"</span>)   <span class="co"># not shown</span></span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>plt.show()                                      <span class="co"># not shown</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Saving figure underfitting_learning_curves_plot</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="04_training_linear_models_files/figure-html/cell-37-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>polynomial_regression <span class="op">=</span> Pipeline([</span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>        (<span class="st">"poly_features"</span>, PolynomialFeatures(degree<span class="op">=</span><span class="dv">10</span>, include_bias<span class="op">=</span><span class="va">False</span>)),</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>        (<span class="st">"lin_reg"</span>, LinearRegression()),</span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>    ])</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>plot_learning_curves(polynomial_regression, X, y)</span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a>plt.axis([<span class="dv">0</span>, <span class="dv">80</span>, <span class="dv">0</span>, <span class="dv">3</span>])           <span class="co"># not shown</span></span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a>save_fig(<span class="st">"learning_curves_plot"</span>)  <span class="co"># not shown</span></span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a>plt.show()                        <span class="co"># not shown</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Saving figure learning_curves_plot</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="04_training_linear_models_files/figure-html/cell-38-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="regularized-linear-models" class="level1">
<h1>Regularized Linear Models</h1>
<section id="ridge-regression" class="level2">
<h2 class="anchored" data-anchor-id="ridge-regression">Ridge Regression</h2>
<div class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> <span class="dv">3</span> <span class="op">*</span> np.random.rand(m, <span class="dv">1</span>)</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">1</span> <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> X <span class="op">+</span> np.random.randn(m, <span class="dv">1</span>) <span class="op">/</span> <span class="fl">1.5</span></span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>X_new <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">100</span>).reshape(<span class="dv">100</span>, <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Ridge</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>ridge_reg <span class="op">=</span> Ridge(alpha<span class="op">=</span><span class="dv">1</span>, solver<span class="op">=</span><span class="st">"cholesky"</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>ridge_reg.fit(X, y)</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>ridge_reg.predict([[<span class="fl">1.5</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="39">
<pre><code>array([[1.55071465]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>ridge_reg <span class="op">=</span> Ridge(alpha<span class="op">=</span><span class="dv">1</span>, solver<span class="op">=</span><span class="st">"sag"</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>ridge_reg.fit(X, y)</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>ridge_reg.predict([[<span class="fl">1.5</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="40">
<pre><code>array([[1.5507201]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Ridge</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_model(model_class, polynomial, alphas, <span class="op">**</span>model_kargs):</span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> alpha, style <span class="kw">in</span> <span class="bu">zip</span>(alphas, (<span class="st">"b-"</span>, <span class="st">"g--"</span>, <span class="st">"r:"</span>)):</span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> model_class(alpha, <span class="op">**</span>model_kargs) <span class="cf">if</span> alpha <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> LinearRegression()</span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> polynomial:</span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a>            model <span class="op">=</span> Pipeline([</span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a>                    (<span class="st">"poly_features"</span>, PolynomialFeatures(degree<span class="op">=</span><span class="dv">10</span>, include_bias<span class="op">=</span><span class="va">False</span>)),</span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a>                    (<span class="st">"std_scaler"</span>, StandardScaler()),</span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a>                    (<span class="st">"regul_reg"</span>, model),</span>
<span id="cb68-11"><a href="#cb68-11" aria-hidden="true" tabindex="-1"></a>                ])</span>
<span id="cb68-12"><a href="#cb68-12" aria-hidden="true" tabindex="-1"></a>        model.fit(X, y)</span>
<span id="cb68-13"><a href="#cb68-13" aria-hidden="true" tabindex="-1"></a>        y_new_regul <span class="op">=</span> model.predict(X_new)</span>
<span id="cb68-14"><a href="#cb68-14" aria-hidden="true" tabindex="-1"></a>        lw <span class="op">=</span> <span class="dv">2</span> <span class="cf">if</span> alpha <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">1</span></span>
<span id="cb68-15"><a href="#cb68-15" aria-hidden="true" tabindex="-1"></a>        plt.plot(X_new, y_new_regul, style, linewidth<span class="op">=</span>lw, label<span class="op">=</span><span class="vs">r"$\alpha = </span><span class="sc">{}</span><span class="vs">$"</span>.<span class="bu">format</span>(alpha))</span>
<span id="cb68-16"><a href="#cb68-16" aria-hidden="true" tabindex="-1"></a>    plt.plot(X, y, <span class="st">"b."</span>, linewidth<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb68-17"><a href="#cb68-17" aria-hidden="true" tabindex="-1"></a>    plt.legend(loc<span class="op">=</span><span class="st">"upper left"</span>, fontsize<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb68-18"><a href="#cb68-18" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">"$x_1$"</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb68-19"><a href="#cb68-19" aria-hidden="true" tabindex="-1"></a>    plt.axis([<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">4</span>])</span>
<span id="cb68-20"><a href="#cb68-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-21"><a href="#cb68-21" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">4</span>))</span>
<span id="cb68-22"><a href="#cb68-22" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">121</span>)</span>
<span id="cb68-23"><a href="#cb68-23" aria-hidden="true" tabindex="-1"></a>plot_model(Ridge, polynomial<span class="op">=</span><span class="va">False</span>, alphas<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">100</span>), random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb68-24"><a href="#cb68-24" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$y$"</span>, rotation<span class="op">=</span><span class="dv">0</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb68-25"><a href="#cb68-25" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">122</span>)</span>
<span id="cb68-26"><a href="#cb68-26" aria-hidden="true" tabindex="-1"></a>plot_model(Ridge, polynomial<span class="op">=</span><span class="va">True</span>, alphas<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">10</span><span class="op">**-</span><span class="dv">5</span>, <span class="dv">1</span>), random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb68-27"><a href="#cb68-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-28"><a href="#cb68-28" aria-hidden="true" tabindex="-1"></a>save_fig(<span class="st">"ridge_regression_plot"</span>)</span>
<span id="cb68-29"><a href="#cb68-29" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Saving figure ridge_regression_plot</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="04_training_linear_models_files/figure-html/cell-42-output-2.png" class="img-fluid"></p>
</div>
</div>
<p><strong>Note</strong>: to be future-proof, we set <code>max_iter=1000</code> and <code>tol=1e-3</code> because these will be the default values in Scikit-Learn 0.21.</p>
<div class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>sgd_reg <span class="op">=</span> SGDRegressor(penalty<span class="op">=</span><span class="st">"l2"</span>, max_iter<span class="op">=</span><span class="dv">1000</span>, tol<span class="op">=</span><span class="fl">1e-3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>sgd_reg.fit(X, y.ravel())</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>sgd_reg.predict([[<span class="fl">1.5</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="42">
<pre><code>array([1.47012588])</code></pre>
</div>
</div>
</section>
<section id="lasso-regression" class="level2">
<h2 class="anchored" data-anchor-id="lasso-regression">Lasso Regression</h2>
<div class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Lasso</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">4</span>))</span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">121</span>)</span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a>plot_model(Lasso, polynomial<span class="op">=</span><span class="va">False</span>, alphas<span class="op">=</span>(<span class="dv">0</span>, <span class="fl">0.1</span>, <span class="dv">1</span>), random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$y$"</span>, rotation<span class="op">=</span><span class="dv">0</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">122</span>)</span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a>plot_model(Lasso, polynomial<span class="op">=</span><span class="va">True</span>, alphas<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">10</span><span class="op">**-</span><span class="dv">7</span>, <span class="dv">1</span>), random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb72-9"><a href="#cb72-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-10"><a href="#cb72-10" aria-hidden="true" tabindex="-1"></a>save_fig(<span class="st">"lasso_regression_plot"</span>)</span>
<span id="cb72-11"><a href="#cb72-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Saving figure lasso_regression_plot</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="04_training_linear_models_files/figure-html/cell-44-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Lasso</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>lasso_reg <span class="op">=</span> Lasso(alpha<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>lasso_reg.fit(X, y)</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>lasso_reg.predict([[<span class="fl">1.5</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="44">
<pre><code>array([1.53788174])</code></pre>
</div>
</div>
</section>
<section id="elastic-net" class="level2">
<h2 class="anchored" data-anchor-id="elastic-net">Elastic Net</h2>
<div class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> ElasticNet</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>elastic_net <span class="op">=</span> ElasticNet(alpha<span class="op">=</span><span class="fl">0.1</span>, l1_ratio<span class="op">=</span><span class="fl">0.5</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>elastic_net.fit(X, y)</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>elastic_net.predict([[<span class="fl">1.5</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="45">
<pre><code>array([1.54333232])</code></pre>
</div>
</div>
</section>
<section id="early-stopping" class="level2">
<h2 class="anchored" data-anchor-id="early-stopping">Early Stopping</h2>
<div class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> <span class="dv">6</span> <span class="op">*</span> np.random.rand(m, <span class="dv">1</span>) <span class="op">-</span> <span class="dv">3</span></span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">2</span> <span class="op">+</span> X <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> X<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> np.random.randn(m, <span class="dv">1</span>)</span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>X_train, X_val, y_train, y_val <span class="op">=</span> train_test_split(X[:<span class="dv">50</span>], y[:<span class="dv">50</span>].ravel(), test_size<span class="op">=</span><span class="fl">0.5</span>, random_state<span class="op">=</span><span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-scrolled="true" data-execution_count="47">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> copy <span class="im">import</span> deepcopy</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>poly_scaler <span class="op">=</span> Pipeline([</span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>        (<span class="st">"poly_features"</span>, PolynomialFeatures(degree<span class="op">=</span><span class="dv">90</span>, include_bias<span class="op">=</span><span class="va">False</span>)),</span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a>        (<span class="st">"std_scaler"</span>, StandardScaler())</span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a>    ])</span>
<span id="cb79-7"><a href="#cb79-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-8"><a href="#cb79-8" aria-hidden="true" tabindex="-1"></a>X_train_poly_scaled <span class="op">=</span> poly_scaler.fit_transform(X_train)</span>
<span id="cb79-9"><a href="#cb79-9" aria-hidden="true" tabindex="-1"></a>X_val_poly_scaled <span class="op">=</span> poly_scaler.transform(X_val)</span>
<span id="cb79-10"><a href="#cb79-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-11"><a href="#cb79-11" aria-hidden="true" tabindex="-1"></a>sgd_reg <span class="op">=</span> SGDRegressor(max_iter<span class="op">=</span><span class="dv">1</span>, tol<span class="op">=-</span>np.infty, warm_start<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb79-12"><a href="#cb79-12" aria-hidden="true" tabindex="-1"></a>                       penalty<span class="op">=</span><span class="va">None</span>, learning_rate<span class="op">=</span><span class="st">"constant"</span>, eta0<span class="op">=</span><span class="fl">0.0005</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb79-13"><a href="#cb79-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-14"><a href="#cb79-14" aria-hidden="true" tabindex="-1"></a>minimum_val_error <span class="op">=</span> <span class="bu">float</span>(<span class="st">"inf"</span>)</span>
<span id="cb79-15"><a href="#cb79-15" aria-hidden="true" tabindex="-1"></a>best_epoch <span class="op">=</span> <span class="va">None</span></span>
<span id="cb79-16"><a href="#cb79-16" aria-hidden="true" tabindex="-1"></a>best_model <span class="op">=</span> <span class="va">None</span></span>
<span id="cb79-17"><a href="#cb79-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb79-18"><a href="#cb79-18" aria-hidden="true" tabindex="-1"></a>    sgd_reg.fit(X_train_poly_scaled, y_train)  <span class="co"># continues where it left off</span></span>
<span id="cb79-19"><a href="#cb79-19" aria-hidden="true" tabindex="-1"></a>    y_val_predict <span class="op">=</span> sgd_reg.predict(X_val_poly_scaled)</span>
<span id="cb79-20"><a href="#cb79-20" aria-hidden="true" tabindex="-1"></a>    val_error <span class="op">=</span> mean_squared_error(y_val, y_val_predict)</span>
<span id="cb79-21"><a href="#cb79-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> val_error <span class="op">&lt;</span> minimum_val_error:</span>
<span id="cb79-22"><a href="#cb79-22" aria-hidden="true" tabindex="-1"></a>        minimum_val_error <span class="op">=</span> val_error</span>
<span id="cb79-23"><a href="#cb79-23" aria-hidden="true" tabindex="-1"></a>        best_epoch <span class="op">=</span> epoch</span>
<span id="cb79-24"><a href="#cb79-24" aria-hidden="true" tabindex="-1"></a>        best_model <span class="op">=</span> deepcopy(sgd_reg)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Create the graph:</p>
<div class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>sgd_reg <span class="op">=</span> SGDRegressor(max_iter<span class="op">=</span><span class="dv">1</span>, tol<span class="op">=-</span>np.infty, warm_start<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>                       penalty<span class="op">=</span><span class="va">None</span>, learning_rate<span class="op">=</span><span class="st">"constant"</span>, eta0<span class="op">=</span><span class="fl">0.0005</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a>n_epochs <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>train_errors, val_errors <span class="op">=</span> [], []</span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(n_epochs):</span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a>    sgd_reg.fit(X_train_poly_scaled, y_train)</span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a>    y_train_predict <span class="op">=</span> sgd_reg.predict(X_train_poly_scaled)</span>
<span id="cb80-9"><a href="#cb80-9" aria-hidden="true" tabindex="-1"></a>    y_val_predict <span class="op">=</span> sgd_reg.predict(X_val_poly_scaled)</span>
<span id="cb80-10"><a href="#cb80-10" aria-hidden="true" tabindex="-1"></a>    train_errors.append(mean_squared_error(y_train, y_train_predict))</span>
<span id="cb80-11"><a href="#cb80-11" aria-hidden="true" tabindex="-1"></a>    val_errors.append(mean_squared_error(y_val, y_val_predict))</span>
<span id="cb80-12"><a href="#cb80-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-13"><a href="#cb80-13" aria-hidden="true" tabindex="-1"></a>best_epoch <span class="op">=</span> np.argmin(val_errors)</span>
<span id="cb80-14"><a href="#cb80-14" aria-hidden="true" tabindex="-1"></a>best_val_rmse <span class="op">=</span> np.sqrt(val_errors[best_epoch])</span>
<span id="cb80-15"><a href="#cb80-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-16"><a href="#cb80-16" aria-hidden="true" tabindex="-1"></a>plt.annotate(<span class="st">'Best model'</span>,</span>
<span id="cb80-17"><a href="#cb80-17" aria-hidden="true" tabindex="-1"></a>             xy<span class="op">=</span>(best_epoch, best_val_rmse),</span>
<span id="cb80-18"><a href="#cb80-18" aria-hidden="true" tabindex="-1"></a>             xytext<span class="op">=</span>(best_epoch, best_val_rmse <span class="op">+</span> <span class="dv">1</span>),</span>
<span id="cb80-19"><a href="#cb80-19" aria-hidden="true" tabindex="-1"></a>             ha<span class="op">=</span><span class="st">"center"</span>,</span>
<span id="cb80-20"><a href="#cb80-20" aria-hidden="true" tabindex="-1"></a>             arrowprops<span class="op">=</span><span class="bu">dict</span>(facecolor<span class="op">=</span><span class="st">'black'</span>, shrink<span class="op">=</span><span class="fl">0.05</span>),</span>
<span id="cb80-21"><a href="#cb80-21" aria-hidden="true" tabindex="-1"></a>             fontsize<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb80-22"><a href="#cb80-22" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb80-23"><a href="#cb80-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-24"><a href="#cb80-24" aria-hidden="true" tabindex="-1"></a>best_val_rmse <span class="op">-=</span> <span class="fl">0.03</span>  <span class="co"># just to make the graph look better</span></span>
<span id="cb80-25"><a href="#cb80-25" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="dv">0</span>, n_epochs], [best_val_rmse, best_val_rmse], <span class="st">"k:"</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb80-26"><a href="#cb80-26" aria-hidden="true" tabindex="-1"></a>plt.plot(np.sqrt(val_errors), <span class="st">"b-"</span>, linewidth<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">"Validation set"</span>)</span>
<span id="cb80-27"><a href="#cb80-27" aria-hidden="true" tabindex="-1"></a>plt.plot(np.sqrt(train_errors), <span class="st">"r--"</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">"Training set"</span>)</span>
<span id="cb80-28"><a href="#cb80-28" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">"upper right"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb80-29"><a href="#cb80-29" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Epoch"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb80-30"><a href="#cb80-30" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"RMSE"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb80-31"><a href="#cb80-31" aria-hidden="true" tabindex="-1"></a>save_fig(<span class="st">"early_stopping_plot"</span>)</span>
<span id="cb80-32"><a href="#cb80-32" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Saving figure early_stopping_plot</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="04_training_linear_models_files/figure-html/cell-49-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>best_epoch, best_model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="49">
<pre><code>(239,
 SGDRegressor(eta0=0.0005, learning_rate='constant', max_iter=1, penalty=None,
              random_state=42, tol=-inf, warm_start=True))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>t1a, t1b, t2a, t2b <span class="op">=</span> <span class="op">-</span><span class="dv">1</span>, <span class="dv">3</span>, <span class="op">-</span><span class="fl">1.5</span>, <span class="fl">1.5</span></span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a>t1s <span class="op">=</span> np.linspace(t1a, t1b, <span class="dv">500</span>)</span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a>t2s <span class="op">=</span> np.linspace(t2a, t2b, <span class="dv">500</span>)</span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a>t1, t2 <span class="op">=</span> np.meshgrid(t1s, t2s)</span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> np.c_[t1.ravel(), t2.ravel()]</span>
<span id="cb85-7"><a href="#cb85-7" aria-hidden="true" tabindex="-1"></a>Xr <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="fl">0.5</span>]])</span>
<span id="cb85-8"><a href="#cb85-8" aria-hidden="true" tabindex="-1"></a>yr <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> Xr[:, :<span class="dv">1</span>] <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> Xr[:, <span class="dv">1</span>:]</span>
<span id="cb85-9"><a href="#cb85-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-10"><a href="#cb85-10" aria-hidden="true" tabindex="-1"></a>J <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span><span class="bu">len</span>(Xr) <span class="op">*</span> np.<span class="bu">sum</span>((T.dot(Xr.T) <span class="op">-</span> yr.T)<span class="op">**</span><span class="dv">2</span>, axis<span class="op">=</span><span class="dv">1</span>)).reshape(t1.shape)</span>
<span id="cb85-11"><a href="#cb85-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-12"><a href="#cb85-12" aria-hidden="true" tabindex="-1"></a>N1 <span class="op">=</span> np.linalg.norm(T, <span class="bu">ord</span><span class="op">=</span><span class="dv">1</span>, axis<span class="op">=</span><span class="dv">1</span>).reshape(t1.shape)</span>
<span id="cb85-13"><a href="#cb85-13" aria-hidden="true" tabindex="-1"></a>N2 <span class="op">=</span> np.linalg.norm(T, <span class="bu">ord</span><span class="op">=</span><span class="dv">2</span>, axis<span class="op">=</span><span class="dv">1</span>).reshape(t1.shape)</span>
<span id="cb85-14"><a href="#cb85-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-15"><a href="#cb85-15" aria-hidden="true" tabindex="-1"></a>t_min_idx <span class="op">=</span> np.unravel_index(np.argmin(J), J.shape)</span>
<span id="cb85-16"><a href="#cb85-16" aria-hidden="true" tabindex="-1"></a>t1_min, t2_min <span class="op">=</span> t1[t_min_idx], t2[t_min_idx]</span>
<span id="cb85-17"><a href="#cb85-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-18"><a href="#cb85-18" aria-hidden="true" tabindex="-1"></a>t_init <span class="op">=</span> np.array([[<span class="fl">0.25</span>], [<span class="op">-</span><span class="dv">1</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bgd_path(theta, X, y, l1, l2, core <span class="op">=</span> <span class="dv">1</span>, eta <span class="op">=</span> <span class="fl">0.05</span>, n_iterations <span class="op">=</span> <span class="dv">200</span>):</span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>    path <span class="op">=</span> [theta]</span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> iteration <span class="kw">in</span> <span class="bu">range</span>(n_iterations):</span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a>        gradients <span class="op">=</span> core <span class="op">*</span> <span class="dv">2</span><span class="op">/</span><span class="bu">len</span>(X) <span class="op">*</span> X.T.dot(X.dot(theta) <span class="op">-</span> y) <span class="op">+</span> l1 <span class="op">*</span> np.sign(theta) <span class="op">+</span> l2 <span class="op">*</span> theta</span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">=</span> theta <span class="op">-</span> eta <span class="op">*</span> gradients</span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a>        path.append(theta)</span>
<span id="cb86-7"><a href="#cb86-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(path)</span>
<span id="cb86-8"><a href="#cb86-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-9"><a href="#cb86-9" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span>, figsize<span class="op">=</span>(<span class="fl">10.1</span>, <span class="dv">8</span>))</span>
<span id="cb86-10"><a href="#cb86-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, N, l1, l2, title <span class="kw">in</span> ((<span class="dv">0</span>, N1, <span class="fl">2.</span>, <span class="dv">0</span>, <span class="st">"Lasso"</span>), (<span class="dv">1</span>, N2, <span class="dv">0</span>,  <span class="fl">2.</span>, <span class="st">"Ridge"</span>)):</span>
<span id="cb86-11"><a href="#cb86-11" aria-hidden="true" tabindex="-1"></a>    JR <span class="op">=</span> J <span class="op">+</span> l1 <span class="op">*</span> N1 <span class="op">+</span> l2 <span class="op">*</span> <span class="fl">0.5</span> <span class="op">*</span> N2<span class="op">**</span><span class="dv">2</span></span>
<span id="cb86-12"><a href="#cb86-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb86-13"><a href="#cb86-13" aria-hidden="true" tabindex="-1"></a>    tr_min_idx <span class="op">=</span> np.unravel_index(np.argmin(JR), JR.shape)</span>
<span id="cb86-14"><a href="#cb86-14" aria-hidden="true" tabindex="-1"></a>    t1r_min, t2r_min <span class="op">=</span> t1[tr_min_idx], t2[tr_min_idx]</span>
<span id="cb86-15"><a href="#cb86-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-16"><a href="#cb86-16" aria-hidden="true" tabindex="-1"></a>    levelsJ<span class="op">=</span>(np.exp(np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">20</span>)) <span class="op">-</span> <span class="dv">1</span>) <span class="op">*</span> (np.<span class="bu">max</span>(J) <span class="op">-</span> np.<span class="bu">min</span>(J)) <span class="op">+</span> np.<span class="bu">min</span>(J)</span>
<span id="cb86-17"><a href="#cb86-17" aria-hidden="true" tabindex="-1"></a>    levelsJR<span class="op">=</span>(np.exp(np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">20</span>)) <span class="op">-</span> <span class="dv">1</span>) <span class="op">*</span> (np.<span class="bu">max</span>(JR) <span class="op">-</span> np.<span class="bu">min</span>(JR)) <span class="op">+</span> np.<span class="bu">min</span>(JR)</span>
<span id="cb86-18"><a href="#cb86-18" aria-hidden="true" tabindex="-1"></a>    levelsN<span class="op">=</span>np.linspace(<span class="dv">0</span>, np.<span class="bu">max</span>(N), <span class="dv">10</span>)</span>
<span id="cb86-19"><a href="#cb86-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb86-20"><a href="#cb86-20" aria-hidden="true" tabindex="-1"></a>    path_J <span class="op">=</span> bgd_path(t_init, Xr, yr, l1<span class="op">=</span><span class="dv">0</span>, l2<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb86-21"><a href="#cb86-21" aria-hidden="true" tabindex="-1"></a>    path_JR <span class="op">=</span> bgd_path(t_init, Xr, yr, l1, l2)</span>
<span id="cb86-22"><a href="#cb86-22" aria-hidden="true" tabindex="-1"></a>    path_N <span class="op">=</span> bgd_path(np.array([[<span class="fl">2.0</span>], [<span class="fl">0.5</span>]]), Xr, yr, np.sign(l1)<span class="op">/</span><span class="dv">3</span>, np.sign(l2), core<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb86-23"><a href="#cb86-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-24"><a href="#cb86-24" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> axes[i, <span class="dv">0</span>]</span>
<span id="cb86-25"><a href="#cb86-25" aria-hidden="true" tabindex="-1"></a>    ax.grid(<span class="va">True</span>)</span>
<span id="cb86-26"><a href="#cb86-26" aria-hidden="true" tabindex="-1"></a>    ax.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb86-27"><a href="#cb86-27" aria-hidden="true" tabindex="-1"></a>    ax.axvline(x<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb86-28"><a href="#cb86-28" aria-hidden="true" tabindex="-1"></a>    ax.contourf(t1, t2, N <span class="op">/</span> <span class="fl">2.</span>, levels<span class="op">=</span>levelsN)</span>
<span id="cb86-29"><a href="#cb86-29" aria-hidden="true" tabindex="-1"></a>    ax.plot(path_N[:, <span class="dv">0</span>], path_N[:, <span class="dv">1</span>], <span class="st">"y--"</span>)</span>
<span id="cb86-30"><a href="#cb86-30" aria-hidden="true" tabindex="-1"></a>    ax.plot(<span class="dv">0</span>, <span class="dv">0</span>, <span class="st">"ys"</span>)</span>
<span id="cb86-31"><a href="#cb86-31" aria-hidden="true" tabindex="-1"></a>    ax.plot(t1_min, t2_min, <span class="st">"ys"</span>)</span>
<span id="cb86-32"><a href="#cb86-32" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="vs">r"$\ell_</span><span class="sc">{}</span><span class="vs">$ penalty"</span>.<span class="bu">format</span>(i <span class="op">+</span> <span class="dv">1</span>), fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb86-33"><a href="#cb86-33" aria-hidden="true" tabindex="-1"></a>    ax.axis([t1a, t1b, t2a, t2b])</span>
<span id="cb86-34"><a href="#cb86-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb86-35"><a href="#cb86-35" aria-hidden="true" tabindex="-1"></a>        ax.set_xlabel(<span class="vs">r"$\theta_1$"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb86-36"><a href="#cb86-36" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="vs">r"$\theta_2$"</span>, fontsize<span class="op">=</span><span class="dv">16</span>, rotation<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb86-37"><a href="#cb86-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-38"><a href="#cb86-38" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> axes[i, <span class="dv">1</span>]</span>
<span id="cb86-39"><a href="#cb86-39" aria-hidden="true" tabindex="-1"></a>    ax.grid(<span class="va">True</span>)</span>
<span id="cb86-40"><a href="#cb86-40" aria-hidden="true" tabindex="-1"></a>    ax.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb86-41"><a href="#cb86-41" aria-hidden="true" tabindex="-1"></a>    ax.axvline(x<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb86-42"><a href="#cb86-42" aria-hidden="true" tabindex="-1"></a>    ax.contourf(t1, t2, JR, levels<span class="op">=</span>levelsJR, alpha<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb86-43"><a href="#cb86-43" aria-hidden="true" tabindex="-1"></a>    ax.plot(path_JR[:, <span class="dv">0</span>], path_JR[:, <span class="dv">1</span>], <span class="st">"w-o"</span>)</span>
<span id="cb86-44"><a href="#cb86-44" aria-hidden="true" tabindex="-1"></a>    ax.plot(path_N[:, <span class="dv">0</span>], path_N[:, <span class="dv">1</span>], <span class="st">"y--"</span>)</span>
<span id="cb86-45"><a href="#cb86-45" aria-hidden="true" tabindex="-1"></a>    ax.plot(<span class="dv">0</span>, <span class="dv">0</span>, <span class="st">"ys"</span>)</span>
<span id="cb86-46"><a href="#cb86-46" aria-hidden="true" tabindex="-1"></a>    ax.plot(t1_min, t2_min, <span class="st">"ys"</span>)</span>
<span id="cb86-47"><a href="#cb86-47" aria-hidden="true" tabindex="-1"></a>    ax.plot(t1r_min, t2r_min, <span class="st">"rs"</span>)</span>
<span id="cb86-48"><a href="#cb86-48" aria-hidden="true" tabindex="-1"></a>    ax.set_title(title, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb86-49"><a href="#cb86-49" aria-hidden="true" tabindex="-1"></a>    ax.axis([t1a, t1b, t2a, t2b])</span>
<span id="cb86-50"><a href="#cb86-50" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb86-51"><a href="#cb86-51" aria-hidden="true" tabindex="-1"></a>        ax.set_xlabel(<span class="vs">r"$\theta_1$"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb86-52"><a href="#cb86-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-53"><a href="#cb86-53" aria-hidden="true" tabindex="-1"></a>save_fig(<span class="st">"lasso_vs_ridge_plot"</span>)</span>
<span id="cb86-54"><a href="#cb86-54" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Saving figure lasso_vs_ridge_plot</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="04_training_linear_models_files/figure-html/cell-53-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="logistic-regression" class="level1">
<h1>Logistic Regression</h1>
<section id="decision-boundaries" class="level2">
<h2 class="anchored" data-anchor-id="decision-boundaries">Decision Boundaries</h2>
<div class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>sig <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>t))</span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">9</span>, <span class="dv">3</span>))</span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>], [<span class="dv">0</span>, <span class="dv">0</span>], <span class="st">"k-"</span>)</span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>], [<span class="fl">0.5</span>, <span class="fl">0.5</span>], <span class="st">"k:"</span>)</span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>], [<span class="dv">1</span>, <span class="dv">1</span>], <span class="st">"k:"</span>)</span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="dv">0</span>, <span class="dv">0</span>], [<span class="op">-</span><span class="fl">1.1</span>, <span class="fl">1.1</span>], <span class="st">"k-"</span>)</span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a>plt.plot(t, sig, <span class="st">"b-"</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="vs">r"$\sigma(t) = \frac</span><span class="sc">{1}</span><span class="vs">{1 + e^{-t</span><span class="sc">}}</span><span class="vs">$"</span>)</span>
<span id="cb88-9"><a href="#cb88-9" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"t"</span>)</span>
<span id="cb88-10"><a href="#cb88-10" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">"upper left"</span>, fontsize<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb88-11"><a href="#cb88-11" aria-hidden="true" tabindex="-1"></a>plt.axis([<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="op">-</span><span class="fl">0.1</span>, <span class="fl">1.1</span>])</span>
<span id="cb88-12"><a href="#cb88-12" aria-hidden="true" tabindex="-1"></a>save_fig(<span class="st">"logistic_function_plot"</span>)</span>
<span id="cb88-13"><a href="#cb88-13" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Saving figure logistic_function_plot</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="04_training_linear_models_files/figure-html/cell-54-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> datasets</span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>iris <span class="op">=</span> datasets.load_iris()</span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a><span class="bu">list</span>(iris.keys())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="54">
<pre><code>['data',
 'target',
 'frame',
 'target_names',
 'DESCR',
 'feature_names',
 'filename']</code></pre>
</div>
</div>
<div class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(iris.DESCR)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>.. _iris_dataset:

Iris plants dataset
--------------------

**Data Set Characteristics:**

    :Number of Instances: 150 (50 in each of three classes)
    :Number of Attributes: 4 numeric, predictive attributes and the class
    :Attribute Information:
        - sepal length in cm
        - sepal width in cm
        - petal length in cm
        - petal width in cm
        - class:
                - Iris-Setosa
                - Iris-Versicolour
                - Iris-Virginica
                
    :Summary Statistics:

    ============== ==== ==== ======= ===== ====================
                    Min  Max   Mean    SD   Class Correlation
    ============== ==== ==== ======= ===== ====================
    sepal length:   4.3  7.9   5.84   0.83    0.7826
    sepal width:    2.0  4.4   3.05   0.43   -0.4194
    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)
    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)
    ============== ==== ==== ======= ===== ====================

    :Missing Attribute Values: None
    :Class Distribution: 33.3% for each of 3 classes.
    :Creator: R.A. Fisher
    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)
    :Date: July, 1988

The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken
from Fisher's paper. Note that it's the same as in R, but not as in the UCI
Machine Learning Repository, which has two wrong data points.

This is perhaps the best known database to be found in the
pattern recognition literature.  Fisher's paper is a classic in the field and
is referenced frequently to this day.  (See Duda &amp; Hart, for example.)  The
data set contains 3 classes of 50 instances each, where each class refers to a
type of iris plant.  One class is linearly separable from the other 2; the
latter are NOT linearly separable from each other.

.. topic:: References

   - Fisher, R.A. "The use of multiple measurements in taxonomic problems"
     Annual Eugenics, 7, Part II, 179-188 (1936); also in "Contributions to
     Mathematical Statistics" (John Wiley, NY, 1950).
   - Duda, R.O., &amp; Hart, P.E. (1973) Pattern Classification and Scene Analysis.
     (Q327.D83) John Wiley &amp; Sons.  ISBN 0-471-22361-1.  See page 218.
   - Dasarathy, B.V. (1980) "Nosing Around the Neighborhood: A New System
     Structure and Classification Rule for Recognition in Partially Exposed
     Environments".  IEEE Transactions on Pattern Analysis and Machine
     Intelligence, Vol. PAMI-2, No. 1, 67-71.
   - Gates, G.W. (1972) "The Reduced Nearest Neighbor Rule".  IEEE Transactions
     on Information Theory, May 1972, 431-433.
   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al"s AUTOCLASS II
     conceptual clustering system finds 3 classes in the data.
   - Many, many more ...</code></pre>
</div>
</div>
<div class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> iris[<span class="st">"data"</span>][:, <span class="dv">3</span>:]  <span class="co"># petal width</span></span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> (iris[<span class="st">"target"</span>] <span class="op">==</span> <span class="dv">2</span>).astype(np.<span class="bu">int</span>)  <span class="co"># 1 if Iris virginica, else 0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Note</strong>: To be future-proof we set <code>solver="lbfgs"</code> since this will be the default value in Scikit-Learn 0.22.</p>
<div class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>log_reg <span class="op">=</span> LogisticRegression(solver<span class="op">=</span><span class="st">"lbfgs"</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a>log_reg.fit(X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="57">
<pre><code>LogisticRegression(random_state=42)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="58">
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a>X_new <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">1000</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a>y_proba <span class="op">=</span> log_reg.predict_proba(X_new)</span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a>plt.plot(X_new, y_proba[:, <span class="dv">1</span>], <span class="st">"g-"</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">"Iris virginica"</span>)</span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a>plt.plot(X_new, y_proba[:, <span class="dv">0</span>], <span class="st">"b--"</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">"Not Iris virginica"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="04_training_linear_models_files/figure-html/cell-59-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The figure in the book actually is actually a bit fancier:</p>
<div class="cell" data-execution_count="59">
<div class="sourceCode cell-code" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a>X_new <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">1000</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a>y_proba <span class="op">=</span> log_reg.predict_proba(X_new)</span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true" tabindex="-1"></a>decision_boundary <span class="op">=</span> X_new[y_proba[:, <span class="dv">1</span>] <span class="op">&gt;=</span> <span class="fl">0.5</span>][<span class="dv">0</span>]</span>
<span id="cb98-4"><a href="#cb98-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-5"><a href="#cb98-5" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">3</span>))</span>
<span id="cb98-6"><a href="#cb98-6" aria-hidden="true" tabindex="-1"></a>plt.plot(X[y<span class="op">==</span><span class="dv">0</span>], y[y<span class="op">==</span><span class="dv">0</span>], <span class="st">"bs"</span>)</span>
<span id="cb98-7"><a href="#cb98-7" aria-hidden="true" tabindex="-1"></a>plt.plot(X[y<span class="op">==</span><span class="dv">1</span>], y[y<span class="op">==</span><span class="dv">1</span>], <span class="st">"g^"</span>)</span>
<span id="cb98-8"><a href="#cb98-8" aria-hidden="true" tabindex="-1"></a>plt.plot([decision_boundary, decision_boundary], [<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>], <span class="st">"k:"</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb98-9"><a href="#cb98-9" aria-hidden="true" tabindex="-1"></a>plt.plot(X_new, y_proba[:, <span class="dv">1</span>], <span class="st">"g-"</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">"Iris virginica"</span>)</span>
<span id="cb98-10"><a href="#cb98-10" aria-hidden="true" tabindex="-1"></a>plt.plot(X_new, y_proba[:, <span class="dv">0</span>], <span class="st">"b--"</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">"Not Iris virginica"</span>)</span>
<span id="cb98-11"><a href="#cb98-11" aria-hidden="true" tabindex="-1"></a>plt.text(decision_boundary<span class="op">+</span><span class="fl">0.02</span>, <span class="fl">0.15</span>, <span class="st">"Decision  boundary"</span>, fontsize<span class="op">=</span><span class="dv">14</span>, color<span class="op">=</span><span class="st">"k"</span>, ha<span class="op">=</span><span class="st">"center"</span>)</span>
<span id="cb98-12"><a href="#cb98-12" aria-hidden="true" tabindex="-1"></a>plt.arrow(decision_boundary, <span class="fl">0.08</span>, <span class="op">-</span><span class="fl">0.3</span>, <span class="dv">0</span>, head_width<span class="op">=</span><span class="fl">0.05</span>, head_length<span class="op">=</span><span class="fl">0.1</span>, fc<span class="op">=</span><span class="st">'b'</span>, ec<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb98-13"><a href="#cb98-13" aria-hidden="true" tabindex="-1"></a>plt.arrow(decision_boundary, <span class="fl">0.92</span>, <span class="fl">0.3</span>, <span class="dv">0</span>, head_width<span class="op">=</span><span class="fl">0.05</span>, head_length<span class="op">=</span><span class="fl">0.1</span>, fc<span class="op">=</span><span class="st">'g'</span>, ec<span class="op">=</span><span class="st">'g'</span>)</span>
<span id="cb98-14"><a href="#cb98-14" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Petal width (cm)"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb98-15"><a href="#cb98-15" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Probability"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb98-16"><a href="#cb98-16" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">"center left"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb98-17"><a href="#cb98-17" aria-hidden="true" tabindex="-1"></a>plt.axis([<span class="dv">0</span>, <span class="dv">3</span>, <span class="op">-</span><span class="fl">0.02</span>, <span class="fl">1.02</span>])</span>
<span id="cb98-18"><a href="#cb98-18" aria-hidden="true" tabindex="-1"></a>save_fig(<span class="st">"logistic_regression_plot"</span>)</span>
<span id="cb98-19"><a href="#cb98-19" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Saving figure logistic_regression_plot</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="04_training_linear_models_files/figure-html/cell-60-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>decision_boundary</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="60">
<pre><code>array([1.66066066])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="61">
<div class="sourceCode cell-code" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>log_reg.predict([[<span class="fl">1.7</span>], [<span class="fl">1.5</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="61">
<pre><code>array([1, 0])</code></pre>
</div>
</div>
</section>
<section id="softmax-regression" class="level2">
<h2 class="anchored" data-anchor-id="softmax-regression">Softmax Regression</h2>
<div class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb104"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb104-2"><a href="#cb104-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-3"><a href="#cb104-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> iris[<span class="st">"data"</span>][:, (<span class="dv">2</span>, <span class="dv">3</span>)]  <span class="co"># petal length, petal width</span></span>
<span id="cb104-4"><a href="#cb104-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> (iris[<span class="st">"target"</span>] <span class="op">==</span> <span class="dv">2</span>).astype(np.<span class="bu">int</span>)</span>
<span id="cb104-5"><a href="#cb104-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-6"><a href="#cb104-6" aria-hidden="true" tabindex="-1"></a>log_reg <span class="op">=</span> LogisticRegression(solver<span class="op">=</span><span class="st">"lbfgs"</span>, C<span class="op">=</span><span class="dv">10</span><span class="op">**</span><span class="dv">10</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb104-7"><a href="#cb104-7" aria-hidden="true" tabindex="-1"></a>log_reg.fit(X, y)</span>
<span id="cb104-8"><a href="#cb104-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-9"><a href="#cb104-9" aria-hidden="true" tabindex="-1"></a>x0, x1 <span class="op">=</span> np.meshgrid(</span>
<span id="cb104-10"><a href="#cb104-10" aria-hidden="true" tabindex="-1"></a>        np.linspace(<span class="fl">2.9</span>, <span class="dv">7</span>, <span class="dv">500</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb104-11"><a href="#cb104-11" aria-hidden="true" tabindex="-1"></a>        np.linspace(<span class="fl">0.8</span>, <span class="fl">2.7</span>, <span class="dv">200</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb104-12"><a href="#cb104-12" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb104-13"><a href="#cb104-13" aria-hidden="true" tabindex="-1"></a>X_new <span class="op">=</span> np.c_[x0.ravel(), x1.ravel()]</span>
<span id="cb104-14"><a href="#cb104-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-15"><a href="#cb104-15" aria-hidden="true" tabindex="-1"></a>y_proba <span class="op">=</span> log_reg.predict_proba(X_new)</span>
<span id="cb104-16"><a href="#cb104-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-17"><a href="#cb104-17" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">4</span>))</span>
<span id="cb104-18"><a href="#cb104-18" aria-hidden="true" tabindex="-1"></a>plt.plot(X[y<span class="op">==</span><span class="dv">0</span>, <span class="dv">0</span>], X[y<span class="op">==</span><span class="dv">0</span>, <span class="dv">1</span>], <span class="st">"bs"</span>)</span>
<span id="cb104-19"><a href="#cb104-19" aria-hidden="true" tabindex="-1"></a>plt.plot(X[y<span class="op">==</span><span class="dv">1</span>, <span class="dv">0</span>], X[y<span class="op">==</span><span class="dv">1</span>, <span class="dv">1</span>], <span class="st">"g^"</span>)</span>
<span id="cb104-20"><a href="#cb104-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-21"><a href="#cb104-21" aria-hidden="true" tabindex="-1"></a>zz <span class="op">=</span> y_proba[:, <span class="dv">1</span>].reshape(x0.shape)</span>
<span id="cb104-22"><a href="#cb104-22" aria-hidden="true" tabindex="-1"></a>contour <span class="op">=</span> plt.contour(x0, x1, zz, cmap<span class="op">=</span>plt.cm.brg)</span>
<span id="cb104-23"><a href="#cb104-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-24"><a href="#cb104-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-25"><a href="#cb104-25" aria-hidden="true" tabindex="-1"></a>left_right <span class="op">=</span> np.array([<span class="fl">2.9</span>, <span class="dv">7</span>])</span>
<span id="cb104-26"><a href="#cb104-26" aria-hidden="true" tabindex="-1"></a>boundary <span class="op">=</span> <span class="op">-</span>(log_reg.coef_[<span class="dv">0</span>][<span class="dv">0</span>] <span class="op">*</span> left_right <span class="op">+</span> log_reg.intercept_[<span class="dv">0</span>]) <span class="op">/</span> log_reg.coef_[<span class="dv">0</span>][<span class="dv">1</span>]</span>
<span id="cb104-27"><a href="#cb104-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-28"><a href="#cb104-28" aria-hidden="true" tabindex="-1"></a>plt.clabel(contour, inline<span class="op">=</span><span class="dv">1</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb104-29"><a href="#cb104-29" aria-hidden="true" tabindex="-1"></a>plt.plot(left_right, boundary, <span class="st">"k--"</span>, linewidth<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb104-30"><a href="#cb104-30" aria-hidden="true" tabindex="-1"></a>plt.text(<span class="fl">3.5</span>, <span class="fl">1.5</span>, <span class="st">"Not Iris virginica"</span>, fontsize<span class="op">=</span><span class="dv">14</span>, color<span class="op">=</span><span class="st">"b"</span>, ha<span class="op">=</span><span class="st">"center"</span>)</span>
<span id="cb104-31"><a href="#cb104-31" aria-hidden="true" tabindex="-1"></a>plt.text(<span class="fl">6.5</span>, <span class="fl">2.3</span>, <span class="st">"Iris virginica"</span>, fontsize<span class="op">=</span><span class="dv">14</span>, color<span class="op">=</span><span class="st">"g"</span>, ha<span class="op">=</span><span class="st">"center"</span>)</span>
<span id="cb104-32"><a href="#cb104-32" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Petal length"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb104-33"><a href="#cb104-33" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Petal width"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb104-34"><a href="#cb104-34" aria-hidden="true" tabindex="-1"></a>plt.axis([<span class="fl">2.9</span>, <span class="dv">7</span>, <span class="fl">0.8</span>, <span class="fl">2.7</span>])</span>
<span id="cb104-35"><a href="#cb104-35" aria-hidden="true" tabindex="-1"></a>save_fig(<span class="st">"logistic_regression_contour_plot"</span>)</span>
<span id="cb104-36"><a href="#cb104-36" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Saving figure logistic_regression_contour_plot</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="04_training_linear_models_files/figure-html/cell-63-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="63">
<div class="sourceCode cell-code" id="cb106"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> iris[<span class="st">"data"</span>][:, (<span class="dv">2</span>, <span class="dv">3</span>)]  <span class="co"># petal length, petal width</span></span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> iris[<span class="st">"target"</span>]</span>
<span id="cb106-3"><a href="#cb106-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-4"><a href="#cb106-4" aria-hidden="true" tabindex="-1"></a>softmax_reg <span class="op">=</span> LogisticRegression(multi_class<span class="op">=</span><span class="st">"multinomial"</span>,solver<span class="op">=</span><span class="st">"lbfgs"</span>, C<span class="op">=</span><span class="dv">10</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb106-5"><a href="#cb106-5" aria-hidden="true" tabindex="-1"></a>softmax_reg.fit(X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="63">
<pre><code>LogisticRegression(C=10, multi_class='multinomial', random_state=42)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="64">
<div class="sourceCode cell-code" id="cb108"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a>x0, x1 <span class="op">=</span> np.meshgrid(</span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a>        np.linspace(<span class="dv">0</span>, <span class="dv">8</span>, <span class="dv">500</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb108-3"><a href="#cb108-3" aria-hidden="true" tabindex="-1"></a>        np.linspace(<span class="dv">0</span>, <span class="fl">3.5</span>, <span class="dv">200</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb108-4"><a href="#cb108-4" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb108-5"><a href="#cb108-5" aria-hidden="true" tabindex="-1"></a>X_new <span class="op">=</span> np.c_[x0.ravel(), x1.ravel()]</span>
<span id="cb108-6"><a href="#cb108-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-7"><a href="#cb108-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-8"><a href="#cb108-8" aria-hidden="true" tabindex="-1"></a>y_proba <span class="op">=</span> softmax_reg.predict_proba(X_new)</span>
<span id="cb108-9"><a href="#cb108-9" aria-hidden="true" tabindex="-1"></a>y_predict <span class="op">=</span> softmax_reg.predict(X_new)</span>
<span id="cb108-10"><a href="#cb108-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-11"><a href="#cb108-11" aria-hidden="true" tabindex="-1"></a>zz1 <span class="op">=</span> y_proba[:, <span class="dv">1</span>].reshape(x0.shape)</span>
<span id="cb108-12"><a href="#cb108-12" aria-hidden="true" tabindex="-1"></a>zz <span class="op">=</span> y_predict.reshape(x0.shape)</span>
<span id="cb108-13"><a href="#cb108-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-14"><a href="#cb108-14" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">4</span>))</span>
<span id="cb108-15"><a href="#cb108-15" aria-hidden="true" tabindex="-1"></a>plt.plot(X[y<span class="op">==</span><span class="dv">2</span>, <span class="dv">0</span>], X[y<span class="op">==</span><span class="dv">2</span>, <span class="dv">1</span>], <span class="st">"g^"</span>, label<span class="op">=</span><span class="st">"Iris virginica"</span>)</span>
<span id="cb108-16"><a href="#cb108-16" aria-hidden="true" tabindex="-1"></a>plt.plot(X[y<span class="op">==</span><span class="dv">1</span>, <span class="dv">0</span>], X[y<span class="op">==</span><span class="dv">1</span>, <span class="dv">1</span>], <span class="st">"bs"</span>, label<span class="op">=</span><span class="st">"Iris versicolor"</span>)</span>
<span id="cb108-17"><a href="#cb108-17" aria-hidden="true" tabindex="-1"></a>plt.plot(X[y<span class="op">==</span><span class="dv">0</span>, <span class="dv">0</span>], X[y<span class="op">==</span><span class="dv">0</span>, <span class="dv">1</span>], <span class="st">"yo"</span>, label<span class="op">=</span><span class="st">"Iris setosa"</span>)</span>
<span id="cb108-18"><a href="#cb108-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-19"><a href="#cb108-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.colors <span class="im">import</span> ListedColormap</span>
<span id="cb108-20"><a href="#cb108-20" aria-hidden="true" tabindex="-1"></a>custom_cmap <span class="op">=</span> ListedColormap([<span class="st">'#fafab0'</span>,<span class="st">'#9898ff'</span>,<span class="st">'#a0faa0'</span>])</span>
<span id="cb108-21"><a href="#cb108-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-22"><a href="#cb108-22" aria-hidden="true" tabindex="-1"></a>plt.contourf(x0, x1, zz, cmap<span class="op">=</span>custom_cmap)</span>
<span id="cb108-23"><a href="#cb108-23" aria-hidden="true" tabindex="-1"></a>contour <span class="op">=</span> plt.contour(x0, x1, zz1, cmap<span class="op">=</span>plt.cm.brg)</span>
<span id="cb108-24"><a href="#cb108-24" aria-hidden="true" tabindex="-1"></a>plt.clabel(contour, inline<span class="op">=</span><span class="dv">1</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb108-25"><a href="#cb108-25" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Petal length"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb108-26"><a href="#cb108-26" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Petal width"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb108-27"><a href="#cb108-27" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">"center left"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb108-28"><a href="#cb108-28" aria-hidden="true" tabindex="-1"></a>plt.axis([<span class="dv">0</span>, <span class="dv">7</span>, <span class="dv">0</span>, <span class="fl">3.5</span>])</span>
<span id="cb108-29"><a href="#cb108-29" aria-hidden="true" tabindex="-1"></a>save_fig(<span class="st">"softmax_regression_contour_plot"</span>)</span>
<span id="cb108-30"><a href="#cb108-30" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Saving figure softmax_regression_contour_plot</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="04_training_linear_models_files/figure-html/cell-65-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="65">
<div class="sourceCode cell-code" id="cb110"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a>softmax_reg.predict([[<span class="dv">5</span>, <span class="dv">2</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="65">
<pre><code>array([2])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="66">
<div class="sourceCode cell-code" id="cb112"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a>softmax_reg.predict_proba([[<span class="dv">5</span>, <span class="dv">2</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="66">
<pre><code>array([[6.38014896e-07, 5.74929995e-02, 9.42506362e-01]])</code></pre>
</div>
</div>
</section>
</section>
<section id="exercise-solutions" class="level1">
<h1>Exercise solutions</h1>
<section id="to-11." class="level2">
<h2 class="anchored" data-anchor-id="to-11.">1. to 11.</h2>
<p>See appendix A.</p>
</section>
<section id="batch-gradient-descent-with-early-stopping-for-softmax-regression" class="level2">
<h2 class="anchored" data-anchor-id="batch-gradient-descent-with-early-stopping-for-softmax-regression">12. Batch Gradient Descent with early stopping for Softmax Regression</h2>
<p>(without using Scikit-Learn)</p>
<p>Let’s start by loading the data. We will just reuse the Iris dataset we loaded earlier.</p>
<div class="cell" data-execution_count="67">
<div class="sourceCode cell-code" id="cb114"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> iris[<span class="st">"data"</span>][:, (<span class="dv">2</span>, <span class="dv">3</span>)]  <span class="co"># petal length, petal width</span></span>
<span id="cb114-2"><a href="#cb114-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> iris[<span class="st">"target"</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We need to add the bias term for every instance (<span class="math inline">\(x_0 = 1\)</span>):</p>
<div class="cell" data-execution_count="68">
<div class="sourceCode cell-code" id="cb115"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a>X_with_bias <span class="op">=</span> np.c_[np.ones([<span class="bu">len</span>(X), <span class="dv">1</span>]), X]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>And let’s set the random seed so the output of this exercise solution is reproducible:</p>
<div class="cell" data-execution_count="69">
<div class="sourceCode cell-code" id="cb116"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">2042</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The easiest option to split the dataset into a training set, a validation set and a test set would be to use Scikit-Learn’s <code>train_test_split()</code> function, but the point of this exercise is to try understand the algorithms by implementing them manually. So here is one possible implementation:</p>
<div class="cell" data-execution_count="70">
<div class="sourceCode cell-code" id="cb117"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a>test_ratio <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb117-2"><a href="#cb117-2" aria-hidden="true" tabindex="-1"></a>validation_ratio <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb117-3"><a href="#cb117-3" aria-hidden="true" tabindex="-1"></a>total_size <span class="op">=</span> <span class="bu">len</span>(X_with_bias)</span>
<span id="cb117-4"><a href="#cb117-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb117-5"><a href="#cb117-5" aria-hidden="true" tabindex="-1"></a>test_size <span class="op">=</span> <span class="bu">int</span>(total_size <span class="op">*</span> test_ratio)</span>
<span id="cb117-6"><a href="#cb117-6" aria-hidden="true" tabindex="-1"></a>validation_size <span class="op">=</span> <span class="bu">int</span>(total_size <span class="op">*</span> validation_ratio)</span>
<span id="cb117-7"><a href="#cb117-7" aria-hidden="true" tabindex="-1"></a>train_size <span class="op">=</span> total_size <span class="op">-</span> test_size <span class="op">-</span> validation_size</span>
<span id="cb117-8"><a href="#cb117-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb117-9"><a href="#cb117-9" aria-hidden="true" tabindex="-1"></a>rnd_indices <span class="op">=</span> np.random.permutation(total_size)</span>
<span id="cb117-10"><a href="#cb117-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb117-11"><a href="#cb117-11" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> X_with_bias[rnd_indices[:train_size]]</span>
<span id="cb117-12"><a href="#cb117-12" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> y[rnd_indices[:train_size]]</span>
<span id="cb117-13"><a href="#cb117-13" aria-hidden="true" tabindex="-1"></a>X_valid <span class="op">=</span> X_with_bias[rnd_indices[train_size:<span class="op">-</span>test_size]]</span>
<span id="cb117-14"><a href="#cb117-14" aria-hidden="true" tabindex="-1"></a>y_valid <span class="op">=</span> y[rnd_indices[train_size:<span class="op">-</span>test_size]]</span>
<span id="cb117-15"><a href="#cb117-15" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> X_with_bias[rnd_indices[<span class="op">-</span>test_size:]]</span>
<span id="cb117-16"><a href="#cb117-16" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> y[rnd_indices[<span class="op">-</span>test_size:]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The targets are currently class indices (0, 1 or 2), but we need target class probabilities to train the Softmax Regression model. Each instance will have target class probabilities equal to 0.0 for all classes except for the target class which will have a probability of 1.0 (in other words, the vector of class probabilities for ay given instance is a one-hot vector). Let’s write a small function to convert the vector of class indices into a matrix containing a one-hot vector for each instance:</p>
<div class="cell" data-execution_count="71">
<div class="sourceCode cell-code" id="cb118"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> to_one_hot(y):</span>
<span id="cb118-2"><a href="#cb118-2" aria-hidden="true" tabindex="-1"></a>    n_classes <span class="op">=</span> y.<span class="bu">max</span>() <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb118-3"><a href="#cb118-3" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> <span class="bu">len</span>(y)</span>
<span id="cb118-4"><a href="#cb118-4" aria-hidden="true" tabindex="-1"></a>    Y_one_hot <span class="op">=</span> np.zeros((m, n_classes))</span>
<span id="cb118-5"><a href="#cb118-5" aria-hidden="true" tabindex="-1"></a>    Y_one_hot[np.arange(m), y] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb118-6"><a href="#cb118-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Y_one_hot</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s test this function on the first 10 instances:</p>
<div class="cell" data-execution_count="72">
<div class="sourceCode cell-code" id="cb119"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb119-1"><a href="#cb119-1" aria-hidden="true" tabindex="-1"></a>y_train[:<span class="dv">10</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="72">
<pre><code>array([0, 1, 2, 1, 1, 0, 1, 1, 1, 0])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="73">
<div class="sourceCode cell-code" id="cb121"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a>to_one_hot(y_train[:<span class="dv">10</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="73">
<pre><code>array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.],
       [0., 1., 0.],
       [0., 1., 0.],
       [1., 0., 0.],
       [0., 1., 0.],
       [0., 1., 0.],
       [0., 1., 0.],
       [1., 0., 0.]])</code></pre>
</div>
</div>
<p>Looks good, so let’s create the target class probabilities matrix for the training set and the test set:</p>
<div class="cell" data-execution_count="74">
<div class="sourceCode cell-code" id="cb123"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb123-1"><a href="#cb123-1" aria-hidden="true" tabindex="-1"></a>Y_train_one_hot <span class="op">=</span> to_one_hot(y_train)</span>
<span id="cb123-2"><a href="#cb123-2" aria-hidden="true" tabindex="-1"></a>Y_valid_one_hot <span class="op">=</span> to_one_hot(y_valid)</span>
<span id="cb123-3"><a href="#cb123-3" aria-hidden="true" tabindex="-1"></a>Y_test_one_hot <span class="op">=</span> to_one_hot(y_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now let’s implement the Softmax function. Recall that it is defined by the following equation:</p>
<p><span class="math inline">\(\sigma\left(\mathbf{s}(\mathbf{x})\right)_k = \dfrac{\exp\left(s_k(\mathbf{x})\right)}{\sum\limits_{j=1}^{K}{\exp\left(s_j(\mathbf{x})\right)}}\)</span></p>
<div class="cell" data-execution_count="75">
<div class="sourceCode cell-code" id="cb124"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> softmax(logits):</span>
<span id="cb124-2"><a href="#cb124-2" aria-hidden="true" tabindex="-1"></a>    exps <span class="op">=</span> np.exp(logits)</span>
<span id="cb124-3"><a href="#cb124-3" aria-hidden="true" tabindex="-1"></a>    exp_sums <span class="op">=</span> np.<span class="bu">sum</span>(exps, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb124-4"><a href="#cb124-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> exps <span class="op">/</span> exp_sums</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We are almost ready to start training. Let’s define the number of inputs and outputs:</p>
<div class="cell" data-execution_count="76">
<div class="sourceCode cell-code" id="cb125"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a>n_inputs <span class="op">=</span> X_train.shape[<span class="dv">1</span>] <span class="co"># == 3 (2 features plus the bias term)</span></span>
<span id="cb125-2"><a href="#cb125-2" aria-hidden="true" tabindex="-1"></a>n_outputs <span class="op">=</span> <span class="bu">len</span>(np.unique(y_train))   <span class="co"># == 3 (3 iris classes)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now here comes the hardest part: training! Theoretically, it’s simple: it’s just a matter of translating the math equations into Python code. But in practice, it can be quite tricky: in particular, it’s easy to mix up the order of the terms, or the indices. You can even end up with code that looks like it’s working but is actually not computing exactly the right thing. When unsure, you should write down the shape of each term in the equation and make sure the corresponding terms in your code match closely. It can also help to evaluate each term independently and print them out. The good news it that you won’t have to do this everyday, since all this is well implemented by Scikit-Learn, but it will help you understand what’s going on under the hood.</p>
<p>So the equations we will need are the cost function:</p>
<p><span class="math inline">\(J(\mathbf{\Theta}) = - \dfrac{1}{m}\sum\limits_{i=1}^{m}\sum\limits_{k=1}^{K}{y_k^{(i)}\log\left(\hat{p}_k^{(i)}\right)}\)</span></p>
<p>And the equation for the gradients:</p>
<p><span class="math inline">\(\nabla_{\mathbf{\theta}^{(k)}} \, J(\mathbf{\Theta}) = \dfrac{1}{m} \sum\limits_{i=1}^{m}{ \left ( \hat{p}^{(i)}_k - y_k^{(i)} \right ) \mathbf{x}^{(i)}}\)</span></p>
<p>Note that <span class="math inline">\(\log\left(\hat{p}_k^{(i)}\right)\)</span> may not be computable if <span class="math inline">\(\hat{p}_k^{(i)} = 0\)</span>. So we will add a tiny value <span class="math inline">\(\epsilon\)</span> to <span class="math inline">\(\log\left(\hat{p}_k^{(i)}\right)\)</span> to avoid getting <code>nan</code> values.</p>
<div class="cell" data-execution_count="77">
<div class="sourceCode cell-code" id="cb126"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb126-1"><a href="#cb126-1" aria-hidden="true" tabindex="-1"></a>eta <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb126-2"><a href="#cb126-2" aria-hidden="true" tabindex="-1"></a>n_iterations <span class="op">=</span> <span class="dv">5001</span></span>
<span id="cb126-3"><a href="#cb126-3" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="bu">len</span>(X_train)</span>
<span id="cb126-4"><a href="#cb126-4" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">1e-7</span></span>
<span id="cb126-5"><a href="#cb126-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb126-6"><a href="#cb126-6" aria-hidden="true" tabindex="-1"></a>Theta <span class="op">=</span> np.random.randn(n_inputs, n_outputs)</span>
<span id="cb126-7"><a href="#cb126-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb126-8"><a href="#cb126-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> iteration <span class="kw">in</span> <span class="bu">range</span>(n_iterations):</span>
<span id="cb126-9"><a href="#cb126-9" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> X_train.dot(Theta)</span>
<span id="cb126-10"><a href="#cb126-10" aria-hidden="true" tabindex="-1"></a>    Y_proba <span class="op">=</span> softmax(logits)</span>
<span id="cb126-11"><a href="#cb126-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> iteration <span class="op">%</span> <span class="dv">500</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb126-12"><a href="#cb126-12" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> <span class="op">-</span>np.mean(np.<span class="bu">sum</span>(Y_train_one_hot <span class="op">*</span> np.log(Y_proba <span class="op">+</span> epsilon), axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb126-13"><a href="#cb126-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(iteration, loss)</span>
<span id="cb126-14"><a href="#cb126-14" aria-hidden="true" tabindex="-1"></a>    error <span class="op">=</span> Y_proba <span class="op">-</span> Y_train_one_hot</span>
<span id="cb126-15"><a href="#cb126-15" aria-hidden="true" tabindex="-1"></a>    gradients <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>m <span class="op">*</span> X_train.T.dot(error)</span>
<span id="cb126-16"><a href="#cb126-16" aria-hidden="true" tabindex="-1"></a>    Theta <span class="op">=</span> Theta <span class="op">-</span> eta <span class="op">*</span> gradients</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0 5.446205811872683
500 0.8350062641405651
1000 0.6878801447192402
1500 0.6012379137693314
2000 0.5444496861981872
2500 0.5038530181431525
3000 0.47292289721922487
3500 0.44824244188957774
4000 0.4278651093928793
4500 0.41060071429187134
5000 0.3956780375390374</code></pre>
</div>
</div>
<p>And that’s it! The Softmax model is trained. Let’s look at the model parameters:</p>
<div class="cell" data-execution_count="78">
<div class="sourceCode cell-code" id="cb128"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb128-1"><a href="#cb128-1" aria-hidden="true" tabindex="-1"></a>Theta</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="78">
<pre><code>array([[ 3.32094157, -0.6501102 , -2.99979416],
       [-1.1718465 ,  0.11706172,  0.10507543],
       [-0.70224261, -0.09527802,  1.4786383 ]])</code></pre>
</div>
</div>
<p>Let’s make predictions for the validation set and check the accuracy score:</p>
<div class="cell" data-execution_count="79">
<div class="sourceCode cell-code" id="cb130"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb130-1"><a href="#cb130-1" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> X_valid.dot(Theta)</span>
<span id="cb130-2"><a href="#cb130-2" aria-hidden="true" tabindex="-1"></a>Y_proba <span class="op">=</span> softmax(logits)</span>
<span id="cb130-3"><a href="#cb130-3" aria-hidden="true" tabindex="-1"></a>y_predict <span class="op">=</span> np.argmax(Y_proba, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb130-4"><a href="#cb130-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb130-5"><a href="#cb130-5" aria-hidden="true" tabindex="-1"></a>accuracy_score <span class="op">=</span> np.mean(y_predict <span class="op">==</span> y_valid)</span>
<span id="cb130-6"><a href="#cb130-6" aria-hidden="true" tabindex="-1"></a>accuracy_score</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="79">
<pre><code>0.9666666666666667</code></pre>
</div>
</div>
<p>Well, this model looks pretty good. For the sake of the exercise, let’s add a bit of <span class="math inline">\(\ell_2\)</span> regularization. The following training code is similar to the one above, but the loss now has an additional <span class="math inline">\(\ell_2\)</span> penalty, and the gradients have the proper additional term (note that we don’t regularize the first element of <code>Theta</code> since this corresponds to the bias term). Also, let’s try increasing the learning rate <code>eta</code>.</p>
<div class="cell" data-execution_count="80">
<div class="sourceCode cell-code" id="cb132"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb132-1"><a href="#cb132-1" aria-hidden="true" tabindex="-1"></a>eta <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb132-2"><a href="#cb132-2" aria-hidden="true" tabindex="-1"></a>n_iterations <span class="op">=</span> <span class="dv">5001</span></span>
<span id="cb132-3"><a href="#cb132-3" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="bu">len</span>(X_train)</span>
<span id="cb132-4"><a href="#cb132-4" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">1e-7</span></span>
<span id="cb132-5"><a href="#cb132-5" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.1</span>  <span class="co"># regularization hyperparameter</span></span>
<span id="cb132-6"><a href="#cb132-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-7"><a href="#cb132-7" aria-hidden="true" tabindex="-1"></a>Theta <span class="op">=</span> np.random.randn(n_inputs, n_outputs)</span>
<span id="cb132-8"><a href="#cb132-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-9"><a href="#cb132-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> iteration <span class="kw">in</span> <span class="bu">range</span>(n_iterations):</span>
<span id="cb132-10"><a href="#cb132-10" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> X_train.dot(Theta)</span>
<span id="cb132-11"><a href="#cb132-11" aria-hidden="true" tabindex="-1"></a>    Y_proba <span class="op">=</span> softmax(logits)</span>
<span id="cb132-12"><a href="#cb132-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> iteration <span class="op">%</span> <span class="dv">500</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb132-13"><a href="#cb132-13" aria-hidden="true" tabindex="-1"></a>        xentropy_loss <span class="op">=</span> <span class="op">-</span>np.mean(np.<span class="bu">sum</span>(Y_train_one_hot <span class="op">*</span> np.log(Y_proba <span class="op">+</span> epsilon), axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb132-14"><a href="#cb132-14" aria-hidden="true" tabindex="-1"></a>        l2_loss <span class="op">=</span> <span class="dv">1</span><span class="op">/</span><span class="dv">2</span> <span class="op">*</span> np.<span class="bu">sum</span>(np.square(Theta[<span class="dv">1</span>:]))</span>
<span id="cb132-15"><a href="#cb132-15" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> xentropy_loss <span class="op">+</span> alpha <span class="op">*</span> l2_loss</span>
<span id="cb132-16"><a href="#cb132-16" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(iteration, loss)</span>
<span id="cb132-17"><a href="#cb132-17" aria-hidden="true" tabindex="-1"></a>    error <span class="op">=</span> Y_proba <span class="op">-</span> Y_train_one_hot</span>
<span id="cb132-18"><a href="#cb132-18" aria-hidden="true" tabindex="-1"></a>    gradients <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>m <span class="op">*</span> X_train.T.dot(error) <span class="op">+</span> np.r_[np.zeros([<span class="dv">1</span>, n_outputs]), alpha <span class="op">*</span> Theta[<span class="dv">1</span>:]]</span>
<span id="cb132-19"><a href="#cb132-19" aria-hidden="true" tabindex="-1"></a>    Theta <span class="op">=</span> Theta <span class="op">-</span> eta <span class="op">*</span> gradients</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0 6.629842469083912
500 0.5339667976629505
1000 0.5036400750148942
1500 0.49468910594603216
2000 0.4912968418075476
2500 0.48989924700933296
3000 0.4892990598451198
3500 0.4890351244397859
4000 0.4889173621830818
4500 0.4888643337449303
5000 0.4888403120738818</code></pre>
</div>
</div>
<p>Because of the additional <span class="math inline">\(\ell_2\)</span> penalty, the loss seems greater than earlier, but perhaps this model will perform better? Let’s find out:</p>
<div class="cell" data-execution_count="81">
<div class="sourceCode cell-code" id="cb134"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> X_valid.dot(Theta)</span>
<span id="cb134-2"><a href="#cb134-2" aria-hidden="true" tabindex="-1"></a>Y_proba <span class="op">=</span> softmax(logits)</span>
<span id="cb134-3"><a href="#cb134-3" aria-hidden="true" tabindex="-1"></a>y_predict <span class="op">=</span> np.argmax(Y_proba, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb134-4"><a href="#cb134-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-5"><a href="#cb134-5" aria-hidden="true" tabindex="-1"></a>accuracy_score <span class="op">=</span> np.mean(y_predict <span class="op">==</span> y_valid)</span>
<span id="cb134-6"><a href="#cb134-6" aria-hidden="true" tabindex="-1"></a>accuracy_score</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="81">
<pre><code>1.0</code></pre>
</div>
</div>
<p>Cool, perfect accuracy! We probably just got lucky with this validation set, but still, it’s pleasant.</p>
<p>Now let’s add early stopping. For this we just need to measure the loss on the validation set at every iteration and stop when the error starts growing.</p>
<div class="cell" data-execution_count="82">
<div class="sourceCode cell-code" id="cb136"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb136-1"><a href="#cb136-1" aria-hidden="true" tabindex="-1"></a>eta <span class="op">=</span> <span class="fl">0.1</span> </span>
<span id="cb136-2"><a href="#cb136-2" aria-hidden="true" tabindex="-1"></a>n_iterations <span class="op">=</span> <span class="dv">5001</span></span>
<span id="cb136-3"><a href="#cb136-3" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="bu">len</span>(X_train)</span>
<span id="cb136-4"><a href="#cb136-4" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">1e-7</span></span>
<span id="cb136-5"><a href="#cb136-5" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.1</span>  <span class="co"># regularization hyperparameter</span></span>
<span id="cb136-6"><a href="#cb136-6" aria-hidden="true" tabindex="-1"></a>best_loss <span class="op">=</span> np.infty</span>
<span id="cb136-7"><a href="#cb136-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-8"><a href="#cb136-8" aria-hidden="true" tabindex="-1"></a>Theta <span class="op">=</span> np.random.randn(n_inputs, n_outputs)</span>
<span id="cb136-9"><a href="#cb136-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-10"><a href="#cb136-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> iteration <span class="kw">in</span> <span class="bu">range</span>(n_iterations):</span>
<span id="cb136-11"><a href="#cb136-11" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> X_train.dot(Theta)</span>
<span id="cb136-12"><a href="#cb136-12" aria-hidden="true" tabindex="-1"></a>    Y_proba <span class="op">=</span> softmax(logits)</span>
<span id="cb136-13"><a href="#cb136-13" aria-hidden="true" tabindex="-1"></a>    error <span class="op">=</span> Y_proba <span class="op">-</span> Y_train_one_hot</span>
<span id="cb136-14"><a href="#cb136-14" aria-hidden="true" tabindex="-1"></a>    gradients <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>m <span class="op">*</span> X_train.T.dot(error) <span class="op">+</span> np.r_[np.zeros([<span class="dv">1</span>, n_outputs]), alpha <span class="op">*</span> Theta[<span class="dv">1</span>:]]</span>
<span id="cb136-15"><a href="#cb136-15" aria-hidden="true" tabindex="-1"></a>    Theta <span class="op">=</span> Theta <span class="op">-</span> eta <span class="op">*</span> gradients</span>
<span id="cb136-16"><a href="#cb136-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-17"><a href="#cb136-17" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> X_valid.dot(Theta)</span>
<span id="cb136-18"><a href="#cb136-18" aria-hidden="true" tabindex="-1"></a>    Y_proba <span class="op">=</span> softmax(logits)</span>
<span id="cb136-19"><a href="#cb136-19" aria-hidden="true" tabindex="-1"></a>    xentropy_loss <span class="op">=</span> <span class="op">-</span>np.mean(np.<span class="bu">sum</span>(Y_valid_one_hot <span class="op">*</span> np.log(Y_proba <span class="op">+</span> epsilon), axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb136-20"><a href="#cb136-20" aria-hidden="true" tabindex="-1"></a>    l2_loss <span class="op">=</span> <span class="dv">1</span><span class="op">/</span><span class="dv">2</span> <span class="op">*</span> np.<span class="bu">sum</span>(np.square(Theta[<span class="dv">1</span>:]))</span>
<span id="cb136-21"><a href="#cb136-21" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> xentropy_loss <span class="op">+</span> alpha <span class="op">*</span> l2_loss</span>
<span id="cb136-22"><a href="#cb136-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> iteration <span class="op">%</span> <span class="dv">500</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb136-23"><a href="#cb136-23" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(iteration, loss)</span>
<span id="cb136-24"><a href="#cb136-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> loss <span class="op">&lt;</span> best_loss:</span>
<span id="cb136-25"><a href="#cb136-25" aria-hidden="true" tabindex="-1"></a>        best_loss <span class="op">=</span> loss</span>
<span id="cb136-26"><a href="#cb136-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb136-27"><a href="#cb136-27" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(iteration <span class="op">-</span> <span class="dv">1</span>, best_loss)</span>
<span id="cb136-28"><a href="#cb136-28" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(iteration, loss, <span class="st">"early stopping!"</span>)</span>
<span id="cb136-29"><a href="#cb136-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0 4.7096017363419875
500 0.5739711987633519
1000 0.5435638529109128
1500 0.5355752782580262
2000 0.5331959249285545
2500 0.5325946767399382
2765 0.5325460966791898
2766 0.5325460971327978 early stopping!</code></pre>
</div>
</div>
<div class="cell" data-execution_count="83">
<div class="sourceCode cell-code" id="cb138"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb138-1"><a href="#cb138-1" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> X_valid.dot(Theta)</span>
<span id="cb138-2"><a href="#cb138-2" aria-hidden="true" tabindex="-1"></a>Y_proba <span class="op">=</span> softmax(logits)</span>
<span id="cb138-3"><a href="#cb138-3" aria-hidden="true" tabindex="-1"></a>y_predict <span class="op">=</span> np.argmax(Y_proba, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb138-4"><a href="#cb138-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb138-5"><a href="#cb138-5" aria-hidden="true" tabindex="-1"></a>accuracy_score <span class="op">=</span> np.mean(y_predict <span class="op">==</span> y_valid)</span>
<span id="cb138-6"><a href="#cb138-6" aria-hidden="true" tabindex="-1"></a>accuracy_score</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="83">
<pre><code>1.0</code></pre>
</div>
</div>
<p>Still perfect, but faster.</p>
<p>Now let’s plot the model’s predictions on the whole dataset:</p>
<div class="cell" data-execution_count="84">
<div class="sourceCode cell-code" id="cb140"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb140-1"><a href="#cb140-1" aria-hidden="true" tabindex="-1"></a>x0, x1 <span class="op">=</span> np.meshgrid(</span>
<span id="cb140-2"><a href="#cb140-2" aria-hidden="true" tabindex="-1"></a>        np.linspace(<span class="dv">0</span>, <span class="dv">8</span>, <span class="dv">500</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb140-3"><a href="#cb140-3" aria-hidden="true" tabindex="-1"></a>        np.linspace(<span class="dv">0</span>, <span class="fl">3.5</span>, <span class="dv">200</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb140-4"><a href="#cb140-4" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb140-5"><a href="#cb140-5" aria-hidden="true" tabindex="-1"></a>X_new <span class="op">=</span> np.c_[x0.ravel(), x1.ravel()]</span>
<span id="cb140-6"><a href="#cb140-6" aria-hidden="true" tabindex="-1"></a>X_new_with_bias <span class="op">=</span> np.c_[np.ones([<span class="bu">len</span>(X_new), <span class="dv">1</span>]), X_new]</span>
<span id="cb140-7"><a href="#cb140-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-8"><a href="#cb140-8" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> X_new_with_bias.dot(Theta)</span>
<span id="cb140-9"><a href="#cb140-9" aria-hidden="true" tabindex="-1"></a>Y_proba <span class="op">=</span> softmax(logits)</span>
<span id="cb140-10"><a href="#cb140-10" aria-hidden="true" tabindex="-1"></a>y_predict <span class="op">=</span> np.argmax(Y_proba, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb140-11"><a href="#cb140-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-12"><a href="#cb140-12" aria-hidden="true" tabindex="-1"></a>zz1 <span class="op">=</span> Y_proba[:, <span class="dv">1</span>].reshape(x0.shape)</span>
<span id="cb140-13"><a href="#cb140-13" aria-hidden="true" tabindex="-1"></a>zz <span class="op">=</span> y_predict.reshape(x0.shape)</span>
<span id="cb140-14"><a href="#cb140-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-15"><a href="#cb140-15" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">4</span>))</span>
<span id="cb140-16"><a href="#cb140-16" aria-hidden="true" tabindex="-1"></a>plt.plot(X[y<span class="op">==</span><span class="dv">2</span>, <span class="dv">0</span>], X[y<span class="op">==</span><span class="dv">2</span>, <span class="dv">1</span>], <span class="st">"g^"</span>, label<span class="op">=</span><span class="st">"Iris virginica"</span>)</span>
<span id="cb140-17"><a href="#cb140-17" aria-hidden="true" tabindex="-1"></a>plt.plot(X[y<span class="op">==</span><span class="dv">1</span>, <span class="dv">0</span>], X[y<span class="op">==</span><span class="dv">1</span>, <span class="dv">1</span>], <span class="st">"bs"</span>, label<span class="op">=</span><span class="st">"Iris versicolor"</span>)</span>
<span id="cb140-18"><a href="#cb140-18" aria-hidden="true" tabindex="-1"></a>plt.plot(X[y<span class="op">==</span><span class="dv">0</span>, <span class="dv">0</span>], X[y<span class="op">==</span><span class="dv">0</span>, <span class="dv">1</span>], <span class="st">"yo"</span>, label<span class="op">=</span><span class="st">"Iris setosa"</span>)</span>
<span id="cb140-19"><a href="#cb140-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-20"><a href="#cb140-20" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.colors <span class="im">import</span> ListedColormap</span>
<span id="cb140-21"><a href="#cb140-21" aria-hidden="true" tabindex="-1"></a>custom_cmap <span class="op">=</span> ListedColormap([<span class="st">'#fafab0'</span>,<span class="st">'#9898ff'</span>,<span class="st">'#a0faa0'</span>])</span>
<span id="cb140-22"><a href="#cb140-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-23"><a href="#cb140-23" aria-hidden="true" tabindex="-1"></a>plt.contourf(x0, x1, zz, cmap<span class="op">=</span>custom_cmap)</span>
<span id="cb140-24"><a href="#cb140-24" aria-hidden="true" tabindex="-1"></a>contour <span class="op">=</span> plt.contour(x0, x1, zz1, cmap<span class="op">=</span>plt.cm.brg)</span>
<span id="cb140-25"><a href="#cb140-25" aria-hidden="true" tabindex="-1"></a>plt.clabel(contour, inline<span class="op">=</span><span class="dv">1</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb140-26"><a href="#cb140-26" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Petal length"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb140-27"><a href="#cb140-27" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Petal width"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb140-28"><a href="#cb140-28" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">"upper left"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb140-29"><a href="#cb140-29" aria-hidden="true" tabindex="-1"></a>plt.axis([<span class="dv">0</span>, <span class="dv">7</span>, <span class="dv">0</span>, <span class="fl">3.5</span>])</span>
<span id="cb140-30"><a href="#cb140-30" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="04_training_linear_models_files/figure-html/cell-85-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>And now let’s measure the final model’s accuracy on the test set:</p>
<div class="cell" data-execution_count="85">
<div class="sourceCode cell-code" id="cb141"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb141-1"><a href="#cb141-1" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> X_test.dot(Theta)</span>
<span id="cb141-2"><a href="#cb141-2" aria-hidden="true" tabindex="-1"></a>Y_proba <span class="op">=</span> softmax(logits)</span>
<span id="cb141-3"><a href="#cb141-3" aria-hidden="true" tabindex="-1"></a>y_predict <span class="op">=</span> np.argmax(Y_proba, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb141-4"><a href="#cb141-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb141-5"><a href="#cb141-5" aria-hidden="true" tabindex="-1"></a>accuracy_score <span class="op">=</span> np.mean(y_predict <span class="op">==</span> y_test)</span>
<span id="cb141-6"><a href="#cb141-6" aria-hidden="true" tabindex="-1"></a>accuracy_score</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="85">
<pre><code>0.9333333333333333</code></pre>
</div>
</div>
<p>Our perfect model turns out to have slight imperfections. This variability is likely due to the very small size of the dataset: depending on how you sample the training set, validation set and the test set, you can get quite different results. Try changing the random seed and running the code again a few times, you will see that the results will vary.</p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>