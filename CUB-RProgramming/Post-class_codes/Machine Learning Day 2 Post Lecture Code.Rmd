---
title: "Machine Learning Day 2 Lecture"
author: "Dr. Osita Onyejekwe"
date: "11/02/2022"
output:
  html_document: default
  pdf_document: default
---

# Prevalence, Sensitivity and Specificity

Assuming we develop a prediction rule to predict `Male` if the student is taller than 65 inches. Given that the average female is about 65 inches, this prediction rule is bound to make many errors. If a student is the height of the average female, shouldn't we predict `Female`? A closer look at the confusion matrix reveals the problem. We look at the proportion of calls for each sex:

```{r, message=FALSE, warning=FALSE}
test_set %>% 
  mutate(y_hat = y_hat) %>%
  group_by(sex) %>% 
  summarize(accuracy = mean(y_hat == sex))
```

There is an imbalance in the accuracy for males and females: too many females are predicted to be male. The reason this does not affect our overall accuracy is because the _prevalence_ of males in this dataset is high:

```{r}
prev <- mean(y == "Male")
prev
```

So the high percentage of mistakes made for females is outweighed by the gains in correct calls for men. This can be a big problem in machine learning. **If your training data is biased in some way, you are likely to develop algorithms that are biased as well. This is one of the reasons we look at metrics other than overall accuracy when evaluating a machine learning algorithm**.


To evaluate an algorithm in a way that prevalence does not cloud our assessment, we can study _sensitivity_ and _specificity_ separately. These terms are defined for a specific category. Once we specify a category of interest then we can talk about positive outcomes, $Y=1$, and negative outcomes, $Y=0$.

In general, _sensitivity_ is defined as the ability of an algorithm to predict a positive outcome when the actual outcome is $Y=1$. Because an algorithm that calls everything $\hat{Y}=1$ has perfect sensitivity, sensitivity on its own is not enough to judge an algorithm. For this reason we also examine _specificity_, which is generally defined as the ability of an algorithm to *only* call a $\hat{Y}=1$ when the case is actually $Y=1$. We can summarize in the following way:

* High sensitivity: $Y=1 \implies \hat{Y}=1$
* High specificity: $\hat{Y} = 1 \implies Y=1$ or equivalently $Y=0 \implies \hat{Y}=0$

To provide a precise definition we name the four entries of the confusion matrix:

```{r, echo=FALSE}
mat <- matrix(c("True positives (TP)", "False negatives (FN)", 
                "False positives (FP)", "True negatives (TN)"), 2, 2)
colnames(mat) <- c("Positive", "Negative")
rownames(mat) <- c("Predicted positve", "Predicted negative")
as.data.frame(mat) %>% knitr::kable()
```

Sensitivity is typically quantified by $TP/(TP+FN)$, or the proportion of positives `TP+FN` that are called positives `TP`. This quantity is referred to as the _true positive rate_ (TPR) or _**recall**_. 

Specificity is typically quantified as $TN/(TN+FP)$ or the proportion of negatives `TN+FP` that are called negatives `TN`. This quantity is called the true negative rate (TNR). Specificity is sometimes quantified with $TP/(TP+FP)$, the proportion of outcomes called positives $TP+FP$ that are actaully positives $TP$. This quantity is referred to as _**precision**_. Note that unlike TPR and TNR, precision depends on prevelance since higher prevalence implies you can get higher precision, even when guessing.

The caret function `confusionMatrix` computes all these metrics for us once we define what a positive is. The function expects factors as input and coerces characters into factors. The first level is considered the positives. Here `Female` is the first level because it comes before `Male` alphabetically.


```{r}
library(e1071)
confusionMatrix(data = as.factor(y_hat), reference = test_set$sex)
```

We can see that the high accuracy is possible despite relatively low sensitivity. The reason this can happen is the low prevalence: because the proportion of females is low, incorrectly classifying them as males does not lower the accuracy as much as it is increased by most males being predicted as males. This is an example of why it's important to examine sensitivity and specificity and not just accuracy. 

However, it is often useful to have one number summary, for example for optimization purposes. One metric is simply the average of specificity and sensitivity, referred to as  _**balanced accuracy**_. However, because these are rates, it is more appropriate to compute the harmonic average of specificity and sensitivity. In fact the _**F$_1$-score**_, a widely used one number summary, is the harmonic average of precision and recall:

$$
\frac{1}{\frac{1}{2}\left(\frac{1}{\mbox{recall}} + 
    \frac{1}{\mbox{precision}}\right) }
$$

which can be rewritten as

$$
2\frac{\mbox{precision} \cdot \mbox{recall}}
{\mbox{precision} + \mbox{recall}}
$$

However, depending on the context, some types of errors are more costly than others. For example, in the case of plane safety it is much more important to maximize sensitivity over specificity: failing to predict a plane will malfunction before it crashes is a much more costly error than grounding a plane when in fact the plane is in perfect condition. In a capital murder criminal case the opposite is true: a false positive can lead to killing an innocent person. 

The F$_1$-score can be adapted to weigh specificity and sensitivity differently. The way it is implemented is by defining $\beta$ to represent how much more important sensitivity is compared to specificity and consider a weighted harmonic average:

$$
\frac{1}{\frac{\beta^2}{1+\beta^2}\frac{1}{\mbox{recall}} + 
    \frac{1}{1+\beta^2}\frac{1}{\mbox{precision}} }
$$


The `F_meas` function in the caret package computes this summary with `beta` defaulting to 1.


We can reassess our algorithm above using the F-score instead:

```{r}
cutoff <- seq(60, 70)
F_1 <- map_dbl(cutoff, function(x){
  y_hat <- ifelse(train_set$height > x, "Male", "Female")
  F_meas(data = factor(y_hat), reference = factor(train_set$sex))
})
```

As before, we can plot these $F_1$ measures versus the cutoffs:

```{r, echo=FALSE}
data.frame(cutoff, F_1) %>% 
  ggplot(aes(cutoff, F_1)) + 
  geom_point() + 
  geom_line() 
```

We see that it is maximized at:

```{r}
max(F_1)
```

when we use cutoff:
```{r}
best_cutoff <- cutoff[which.max(F_1)]
best_cutoff
```

However, a cutoff of 66 makes more sense than 65. Furthermore, it balances the specificity and sensitivity of our confusion matrix a bit better:

```{r}
y_hat <- ifelse(test_set$height > 66, "Male", "Female")
confusionMatrix(data = as.factor(y_hat), reference = test_set$sex)
```

# Conditional probabilities

The machine learning classification challenge can be thought of as trying to estimate the probability of $Y$ being any of the $K$ possible  categories given a set of predictors $\mathbf{X}=(X_1,\dots,X_p)$. We can quantify this by saying that we are interested in the _conditional probabilities_: 

$$
p_k(x) = \mbox{Pr}(Y=k \mid \mathbf{X}=\mathbf{x}), \, k=1,\dots,K
$$

If we know $p_k(x)$ then we can optimize our predictions by, for example, predicting the $k$ with the largest probability: 

$$\hat{Y} = \max_k p_k(x)$$

As discussed above, sensitivity and specificity may differ in importance in different contexts. For this reason, maximizing the probability is not always optimal in practice and depends on the context. But even in these cases, knowing $p_k(x)$ will suffice to build optimal prediction models, since we can inform our predictions based on these probabilities and control specificity and sensitivty however we wish. For example, we can simply change the cutoffs used to predict one outcome or the other. In the plane example given above, we may ground the plane anytime the probability of malfunction is higher than 1/1000 as opposed to the default 1/2 used when error types are equally undesired. 

To simplify the expression below, let's consider the predicting sex example which is a binary data case. 

For binary data, you can think of the probability $\mbox{Pr}(Y=1 \mid \mathbf{X}=\mathbf{x})$ as the proportion of 1s in the stratum of the population for which $\mathbf{X}=\mathbf{x}$. 

# Logistic Regression

If we define the outcome $Y$ as 1 for females and 0 for males and $X$ as the height, then we are interested in the conditional probability:

$$
p(x) = \mbox{Pr}( Y = 1 \mid X = x)
$$

As an example, let's provide a prediction for a student that is 66 inches tall.  What is the conditional probability of being female if you are 66 inches tall? In our dataset we can estimate this by rounding to the nearest inch and computing:

```{r}
heights %>% 
  filter(round(height)==66) %>%
  summarize(mean(sex=="Female"))
```

Using data exploration let's see what this estimate of our conditional probability looks like for several values of $x$. We will remove values of $x$ with few data points using the `n()` function.

```{r, message=FALSE, warning=FALSE}
heights %>% 
  mutate(x = round(height)) %>%
  group_by(x) %>%
  filter(n() >= 10) %>%
  summarize(prob = mean(sex == "Female")) %>%
  ggplot(aes(x, prob)) +
  geom_point()
```


This plot suggests that the conditional probability decreases with $x$
and, at least in some parts, appears to be a linear function of $x$. Let's assume that this is the case for now and use the following model:

$$
p(x) = \mbox{Pr}( Y = 1 | X=x)  = \beta_0 + \beta_1 x
$$

If we can estimate $\beta_0$ and $\beta_1$ we will have an estimate of $p(x)$ which will permit us to build a classification algorithm.
We will use least squares (linear regression).

```{r}
library(broom)
betas <- train_set %>% 
  mutate(y = as.numeric(sex == "Female")) %>% 
  do(tidy(lm(y ~ height, data = .))) %>%
  .$estimate
```

The model fits the data relatively well:

```{r conditional-prob-is-approx-linear}
heights %>% 
  mutate(x = round(height)) %>%
  group_by(x) %>%
  filter(n() >= 10) %>%
  summarize(prob = mean(sex == "Female")) %>%
  ggplot(aes(x, prob)) +
  geom_point() +
  geom_abline(intercept = betas[1], slope = betas[2])
```

and we can define an actual prediction rule, for example: define

$$
\hat{p}(x) = \hat{\beta}_0+ \hat{\beta}_1 x
$$

and predict `Female` if $\hat{p}(x) > 0.5$. Here is the confusion matrix and relevant statistics:

```{r}
p_hat <- betas[1] + betas[2]*test_set$height
prediction <- ifelse(p_hat > 0.5, "Female", "Male")
confusionMatrix(data = as.factor(prediction), reference = test_set$sex)
```

As before we see an imbalance of specificity and sensitivity. In this case we can fix this by changing the probability cutoff. 

One problem with this approach is that $\hat{p}(x)$ can be outside the [0,1] range:

```{r}
range(betas[1] + betas[2]*test_set$height)
```


An extension of the regression model that permits us to continue using a regression-like approach is to apply transformations that guarantee that $\hat{p}(x)$ will be between 0 and 1. 

In the case of binary data the most common approach is to fit a _logistic regression_ model which makes use of the _logistic_ transformation 

$$ g(p) = \log \frac{p}{1-p}$$

This logistic transformation converts probability to log odds. As discussed in the data visualization lecture, the odds tell us how much more likely something will happen compared to not happening. So $p=0.5$ means the odds are 1 to 1. If $p=0.75$ the odds are 3 to 1. A nice characteristic of this transformation is that it transforms probabilities to be symmetric around 0. Here is a plot of $g(p)$ versus $p$:

```{r p-versus-logistic-of-p, echo=FALSE}
p <- seq(0.01,.99,len=100)
qplot(p, log( p/(1-p) ), geom="line")
```

With _logistic regression_ we model the conditional probability with:

$$ 
g\left\{ \mbox{Pr}(Y = 1 \mid X=x) \right\} = \beta_0 + \beta_1 x
$$

With this model, we can no longer use least squares. Instead we compute the _maximum likelihood estimate_ (MLE). You can learn more about this concept in a [statistical theory text](http://www.amazon.com/Mathematical-Statistics-Analysis-Available-Enhanced/dp/0534399428). 

In R we can fit the logistic regression model with the function `glm`: generalized linear models. This function is more general than logistic regression so we need to specify the model we want through the `family` parameter:

```{r}
glm_fit <- train_set %>% 
  mutate(y = as.numeric(sex == "Female")) %>%
  glm(y ~ height, data=., family = "binomial")
```

We can obtain predictions using the predict function:

```{r}
p_hat_logit <- predict(glm_fit, newdata = test_set, type="response")
```

Note that this model fits the data slightly better than the line:

```{r conditional-prob-glm-fit, echo=FALSE, message=FALSE, warning=FALSE }
tmp <- heights %>% 
  mutate(x = round(height)) %>%
  group_by(x) %>%
  filter(n() >= 10) %>%
  summarize(prob = mean(sex == "Female")) 
logistic_curve <- data.frame(x = seq(min(tmp$x), max(tmp$x))) %>%
  mutate(p_hat = plogis(glm_fit$coef[1] + glm_fit$coef[2]*x))
tmp %>% 
  ggplot(aes(x, prob)) +
  geom_point() +
  geom_line(data = logistic_curve,
             mapping = aes(x, p_hat), lty = 2)
```


Because we have an estimate $\hat{p}(x)$ we can obtain predictions.

```{r}
y_hat_logit <- ifelse(p_hat_logit > 0.5, "Female", "Male")
confusionMatrix(data = as.factor(y_hat_logit), reference = test_set$sex)
```

# Naive Bayes

The best we can do in a Machine Learning problem is when we actually know

$$
p(x) = \mbox{Pr}(Y=1 \mid \mathbf{X}=\mathbf{x}) 
$$

This gives us what we call _Bayes' Rule_. However, in practice we don't know Bayes' Rule, since estimating $p(x)$ is the main challenge.

_Naive Bayes_ is an approach that tries to estimate $p(x)$ using Bayes theorem.

In this particular example we know that the normal distribution works rather well for the heights $X$ for both classes $y=1$ (female) and $y=0$ (male). This implies that we can approximate the conditional distributions $f_{X|Y=1}$ and $f_{X|Y=0}$. These are the height distributions for females and males respectively and we can easily estimate all the necessary parameters from the data:

```{r, message=FALSE, warning=FALSE}
params <- train_set %>% 
  group_by(sex) %>% 
  summarize(avg = mean(height), sd = sd(height))
params
```


Then using Bayes rule we can compute:

$$
p(x) = \mbox{Pr}(Y=1|X=x) = \frac{f_{X|Y=1}(x) \mbox{Pr}(Y=1)}
{ f_{X|Y=0}(x)\mbox{Pr}(Y=0)  + f_{X|Y=1}(x)\mbox{Pr}(Y=1) }
$$

The prevalence, which we will denote with $\pi = \mbox{Pr}(Y=1)$, can be estimated with 

```{r}
pi <- train_set %>% 
  summarize(pi = mean(sex == "Female")) %>% 
  .$pi
pi
```

Now we can use our estimates of average and standard deviation to get an actual rule:

```{r}
x <- test_set$height
f0 <- dnorm(x, params$avg[2], params$sd[2])
f1 <- dnorm(x, params$avg[1], params$sd[1])
p_hat_bayes <- f1*pi / (f1*pi + f0*(1 - pi))
```

Our naive Bayes estimate $\hat{p}(x)$ looks a lot like our logistic regression estimate:

```{r conditional-prob-glm-fit-2, echo=FALSE, warning=FALSE, message=FALSE}
tmp <- heights %>% 
  mutate(x = round(height)) %>%
  group_by(x) %>%
  filter(n() >= 10) %>%
  summarize(prob = mean(sex == "Female")) 
naive_bayes_curve <- data.frame(x = seq(min(tmp$x), max(tmp$x))) %>%
  mutate(p_hat = dnorm(x, params$avg[1], params$sd[1])*pi/
           (dnorm(x, params$avg[1], params$sd[1])*pi +
              dnorm(x, params$avg[2], params$sd[2])*(1-pi)))
tmp %>% 
  ggplot(aes(x, prob)) +
  geom_point() +
  geom_line(data = naive_bayes_curve,
             mapping = aes(x, p_hat), lty = 3) 
```


In fact, we can show that the naive Bayes approach is similar to the logistic regression prediction mathematically. However, we leave the demonstration to a more advanced text: such as [this one](https://web.stanford.edu/~hastie/Papers/ESLII.pdf). We can see that they are similar empirically:

```{r}
qplot(p_hat_logit, p_hat_bayes) + geom_abline()
```

Let's look at the confusion matrix to see how well we are performing. Our results are very close to what we got for our logistic regression model. 
```{r}
y_hat_bayes <- ifelse(p_hat_bayes > 0.5, "Female", "Male")
confusionMatrix(data = as.factor(y_hat_bayes), reference = test_set$sex)
```


