---
title: "Machine Learning Day 2 Lecture"
author: "Dr. Osita Onyejekwe"
date: "11/02/2022"
output: html_document
---

# Prevalence, Sensitivity and Specificity

Assuming we develop a prediction rule to predict `Male` if the student is taller than 65 inches. Given that the average female is about 65 inches, this prediction rule is bound to make many errors. If a student is the height of the average female, shouldn't we predict `Female`? A closer look at the confusion matrix reveals the problem. We look at the proportion of calls for each sex:

```{r, message=FALSE, warning=FALSE}

# Need to have day1 ML class notes loaded for this file to work
test_set %>% mutate(y_hat = y_hat) %>% group_by(sex) %>% summarize(accuracy = mean(y_hat==sex))

```

There is an imbalance in the accuracy for males and females: too many females are predicted to be male. The reason this does not affect our overall accuracy is because the *prevalence* of males in this dataset is high:

```{r}
# prevalence
prev_male <- mean(y=="Male")       #prevalence of males in "y" data
prev_male

prev_female <- mean(y=="Female")  # prevalence of females in "y" data
prev_female

```

So the high percentage of mistakes made for females is outweighed by the gains in correct calls for men. This can be a big problem in machine learning. **If your training data is biased in some way, you are likely to develop algorithms that are biased as well. This is one of the reasons we look at metrics other than overall accuracy when evaluating a machine learning algorithm**.

To evaluate an algorithm in a way that prevalence does not cloud our assessment, we can study *sensitivity* and *specificity* separately. These terms are defined for a specific category. Once we specify a category of interest then we can talk about positive outcomes, $Y=1$, and negative outcomes, $Y=0$. (*binary outcome*)

# Definition of Sensitivity and Specificity

In general, *sensitivity* is defined as the ability of an algorithm to predict a positive outcome when the actual outcome is $Y=1$ (predicting positive when the outcome is actually positive).

Because an algorithm that calls everything $\hat{Y}=1$ has perfect sensitivity, sensitivity on its own is not enough to judge an algorithm.

For this reason we also examine *specificity*, which is generally defined as the ability of an algorithm to *only* call a $\hat{Y}=1$ when the case is actually $Y=1$. We can summarize in the following way:

-   High sensitivity: $Y=1 \implies \hat{Y}=1$
-   High specificity: $\hat{Y} = 1 \implies Y=1$ or equivalently $Y=0 \implies \hat{Y}=0$

To provide a precise definition we name the four entries of the confusion matrix:

```{r, echo=FALSE}

mat <- matrix(c("True positive (TP)", "False negatives (FN)", "False positives (FP)", "True negatives (TN)"), 2,2)
colnames(mat) <- c("Positive", "Negative")
rownames(mat) <- c("Predicted Positive","Predicted Negative")
as.data.frame(mat) %>% knitr::kable()
```

**Sensitivity** is typically quantified by $TP/(TP+FN)$, or the proportion of positives `TP+FN` that are called positives `TP`. This quantity is referred to as the *true positive rate* (TPR) or ***recall***.

**Specificity** is typically quantified as $TN/(TN+FP)$ or the proportion of negatives `TN+FP` that are called negatives `TN`. This quantity is called the true negative rate (TNR).

[**Specificity** is sometimes quantified with $TP/(TP+FP)$, the proportion of outcomes called positives $TP+FP$ that are actually positives $TP$.]

This quantity is referred to as ***precision***. Note that unlike TPR and TNR, precision depends on prevalence since higher prevalence implies you can get higher precision, even when guessing.

The caret function `confusionMatrix` computes all these metrics for us once we define what a positive is. The function expects factors as input and coerces characters into factors. The first level is considered the positives. Here `Female` is the first level because it comes before `Male` alphabetically.

```{r}
library(e1071)
confusionMatrix(data=as.factor(y_hat), reference = test_set$sex)


```

We can see that the high accuracy is possible despite relatively low sensitivity. The reason this can happen is the low prevalence: because the proportion of females is low, incorrectly classifying them as males does not lower the accuracy as much as it is increased by most males being predicted as males. This is an example of why it's important to examine sensitivity and specificity and not just accuracy.

However, it is often useful to have one number summary, for example for optimization purposes. One metric is simply the average of specificity and sensitivity, referred to as ***balanced accuracy***. However, because these are rates, it is more appropriate to compute the harmonic average of specificity and sensitivity. In fact the ***F***$_1$-score, a widely used one number summary, is the harmonic average of precision and recall:

$$
\frac{1}{\frac{1}{2}\left(\frac{1}{\mbox{recall}} + 
    \frac{1}{\mbox{precision}}\right) }
$$

which can be rewritten as

$$
2\frac{\mbox{precision} \cdot \mbox{recall}}
{\mbox{precision} + \mbox{recall}}
$$

However, depending on the context, some types of errors are more costly than others. For example, in the case of plane safety it is much more important to maximize sensitivity over specificity: failing to predict a plane will malfunction before it crashes is a much more costly error than grounding a plane when in fact the plane is in perfect condition. In a capital murder criminal case the opposite is true: a false positive can lead to an innocent person getting convicted (Type 1 Error).

The F$_1$-score can be adapted to weigh specificity and sensitivity differently. The way it is implemented is by defining $\beta$ to represent how much more important sensitivity is compared to specificity and consider a weighted harmonic average:

$$
\frac{1}{\frac{\beta^2}{1+\beta^2}\frac{1}{\mbox{recall}} + 
    \frac{1}{1+\beta^2}\frac{1}{\mbox{precision}} }
$$

The `F_meas` function in the caret package computes this summary with `beta` defaulting to 1.

**Note that, in statistical analysis of binary classification, the F-score or F-measure of a test's accuracy. It is calculated from the precision and recall of the test, such that the precision is the number of true positive results divided by the number of all positive results, including those not correctly identified, and the recall is the number of true positive results divided by the number of all samples that should have been identified as positive.**

The $F_1$ score is the `harmonic mean` of the precision and recall. The more generic $F_{\beta}$ score applies to weights, value one of precision or recall more than the other.

The highest possible value for an F-score is 1.0, indicating perfect precision and recall. The lowest possible F-score is 0.0, and this occurs if both precision and recall are zero.

We can reassess our algorithm above using the F-score instead:

```{r}

cutoff <- seq(60,70)

F_1 <- map_dbl(cutoff,function(x){
  y_hat <- ifelse(train_set$height > x, "Male","Female")
  F_meas(data = factor(y_hat), reference = factor(train_set$sex))
  
  
})

```

As before, we can plot these $F_1$ measures versus the cutoffs:

```{r, echo=FALSE}

data.frame(cutoff, F_1) %>% 
  ggplot(aes(cutoff,F_1)) +
  geom_point() +
  geom_line()

 
```

We see that it is maximized at:

```{r}
max(F_1)

```

when we use cutoff:

```{r}

best_cutoff <- cutoff[which.max(F_1)]
best_cutoff

```

However, a cutoff of 66 makes more sense than 65. Furthermore, it balances the specificity and sensitivity of our confusion matrix a bit better:

```{r}

y_hat <- ifelse(test_set$height > 66, "Male", "Female")
confusionMatrix(data = as.factor(y_hat), reference = test_set$sex)


```

# Conditional probabilities

The machine learning classification challenge can be thought of as trying to estimate the probability of $Y$ being any of the $K$ possible categories given a set of predictors $\mathbf{X}=(X_1,\dots,X_p)$. We can quantify this by saying that we are interested in the *conditional probabilities*:

$$
p_k(x) = \mbox{Pr}(Y=k \mid \mathbf{X}=\mathbf{x}), \, k=1,\dots,K
$$

If we know $p_k(x)$ then we can optimize our predictions by, for example, predicting the $k$ with the largest probability:

$$\hat{Y} = \max_k p_k(x)$$

As discussed above, sensitivity and specificity may differ in importance in different contexts. For this reason, maximizing the probability is not always optimal in practice and depends on the context. 

But even in these cases, knowing $p_k(x)$ will suffice to build optimal prediction models, since we can inform our predictions based on these probabilities and control specificity and sensitivity however we wish. 

For example, we can simply change the cutoffs used to predict one outcome or the other. In the plane example given above, we may ground the plane anytime the probability of malfunction is higher than 1/1000 as opposed to the default 1/2 used when error types are equally undesired.

To simplify the expression below, let's consider the predicting sex example which is a binary data case.

For binary data, you can think of the probability $\mbox{Pr}(Y=1 \mid \mathbf{X}=\mathbf{x})$ as the proportion of 1s in the stratum of the population for which $\mathbf{X}=\mathbf{x}$.

# Logistic Regression

If we define the outcome $Y$ as 1 for females and 0 for males and $X$ as the height, then we are interested in the conditional probability:

$$
p(x) = \mbox{Pr}( Y = 1 \mid X = x)
$$

As an example, let's provide a prediction for a student that is 66 inches tall. What is the conditional probability of being female if you are 66 inches tall? In our dataset we can estimate this by rounding to the nearest inch and computing:

```{r}

heights %>% 
  filter(round(height) == 66) %>% 
  summarize(mean(sex == "Female"))

```

Using data exploration let's see what this estimate of our conditional probability looks like for several values of $x$. We will remove values of $x$ with few data points using the `n()` function.

```{r, message=FALSE, warning=FALSE}

heights %>% 
  mutate(x = round(height)) %>% 
  group_by(x) %>% 
  filter(n() >= 10) %>% 
  summarize(prob = mean(sex=="Female")) %>% 
  ggplot(aes(x,prob)) +
  geom_point() 


```

This plot suggests that the conditional probability decreases with $x$ and, at least in some parts, appears to be a linear function of $x$. Let's assume that this is the case for now and use the following model:

$$
p(x) = \mbox{Pr}( Y = 1 | X=x)  = \beta_0 + \beta_1 x
$$

If we can estimate $\beta_0$ and $\beta_1$ we will have an estimate of $p(x)$ which will permit us to build a classification algorithm. We will use least squares (linear regression).

```{r}
library(broom) # the broom library takes the messy output of a built-in R function such as (linear model (lm)) and turns them into tidy tibbles. (The tibble is a package in the R programming language that is used to manipulate and print data frames).

betas <- train_set %>% 
  mutate(y = as.numeric(sex == "Female")) %>% 
  do(tidy(lm(y~height, data = .))) %>%     # period (in this case) = all data
  .$estimate

betas

```
# Matrix

(R-studio definition) A matrix is a special case of an atomic vector that has 2-dimension. This means that an entire matrix has to have a single data type which makes them useful for algebraic computations/operations. It can also make numeric operations faster in some cases. (Note that if you are careful enough with a data frame, it does not make much of a difference)

**Matrix is efficient, but harder to implement because it enforces restrictions on the data.**


# Data Frame

A data frame is a `list` of equal length vectors. This means that adding a column is as easy as adding a vector to a list. It also means that each column has its own data type. Note that the columns can have **different data types** for a data frame. This is really good because it makes data frames useful for data storage.

**A data frame is trading a little of its efficiency for convenience and clarity.**


# Tibble

A `tibble` is a modernized version of a `data frame` used in the `tidyverse` such that several techniques are implemented in tibbles to enable them to operate in a smarter manner (types of loading... etc.)

**Tibble is trading more of the efficiency even more for convenience while trying to mask said tradeoff technique that attempt to postpone the computation to a time when it doesn't appear to be its fault.**

1) Tibble can never change the input type (you don't need to worry of characters being automatically turned into strings)

2) Tibbles can have columns that are lists.

3) Tibbles can also have non-standard variable names

4)   "     can start with a number or contain spaces

5)   "     never create row names

I.E. TIBBLES ARE RE-IMAGINING DATA FRAMES :) i.e. they are a modern reimagining of the data frame, such that you keep what is proven to be effective and discarding what is not effective, thus retaining all the important features of a data frame and leaving out all the dated features.


You can use the `do()` function to perform arbitrary computations, returning either a data frame or arbitrary objects which will be stored as a list. This is particularly useful when working with model: You can fit models per group with `do()` and then flexibly extract components with either another `do()` or `summarise()`. Note that `do()` always returns a data frame. 


The model fits the data relatively well:

```{r conditional-prob-is-approx-linear}

heights %>% mutate(x = round(height)) %>% 
  group_by(x) %>% 
  filter(n() > 10) %>% 
  summarize(prob = mean(sex=="Female")) %>% 
  ggplot(aes(x,prob)) +
  geom_point() +
  geom_abline(intercept = betas[1], slope = betas[2])



```

and we can define an actual prediction rule, for example: define

$$
\hat{p}(x) = \hat{\beta}_0+ \hat{\beta}_1 x
$$

and predict `Female` if $\hat{p}(x) > 0.5$. Here is the confusion matrix and relevant statistics:

```{r}

p_hat = betas[1] + betas[2]*test_set$height    # the equation above
prediction <- ifelse(p_hat > 0.5, "Female", "Male")
confusionMatrix(data = as.factor(prediction), reference = test_set$sex)

#note that in R, the as.factor() function is used to convert a vector to a factor, i.e a column from numeric to factor.


```
Sensitivity tells us the ability of the model to correctly predict the positive class. The specificity tells us the ability of the model to correctly predict the negative class.


As before we see an imbalance of specificity and sensitivity. In this case we can fix this by changing the probability cutoff.

One problem with this approach is that $\hat{p}(x)$ can be outside the [0,1] range:

```{r}



```

An extension of the regression model that permits us to continue using a regression-like approach is to apply transformations that guarantee that $\hat{p}(x)$ will be between 0 and 1.

In the case of binary data the most common approach is to fit a *logistic regression* model which makes use of the *logistic* transformation

$$ g(p) = \log \frac{p}{1-p}$$

This logistic transformation converts probability to log odds. As discussed in the data visualization lecture, the odds tell us how much more likely something will happen compared to not happening. So $p=0.5$ means the odds are 1 to 1. If $p=0.75$ the odds are 3 to 1. A nice characteristic of this transformation is that it transforms probabilities to be symmetric around 0. Here is a plot of $g(p)$ versus $p$:

```{r p-versus-logistic-of-p, echo=FALSE}




```

With *logistic regression* we model the conditional probability with:

$$ 
g\left\{ \mbox{Pr}(Y = 1 \mid X=x) \right\} = \beta_0 + \beta_1 x
$$

With this model, we can no longer use least squares. Instead we compute the *maximum likelihood estimate* (MLE). You can learn more about this concept in a [statistical theory text](http://www.amazon.com/Mathematical-Statistics-Analysis-Available-Enhanced/dp/0534399428).

In R we can fit the logistic regression model with the function `glm`: generalized linear models. This function is more general than logistic regression so we need to specify the model we want through the `family` parameter:

```{r}




```

We can obtain predictions using the predict function:

```{r}


```

Note that this model fits the data slightly better than the line:

```{r conditional-prob-glm-fit, echo=FALSE, message=FALSE, warning=FALSE }






```

Because we have an estimate $\hat{p}(x)$ we can obtain predictions.

```{r}




```

# Naive Bayes

The best we can do in a Machine Learning problem is when we actually know

$$
p(x) = \mbox{Pr}(Y=1 \mid \mathbf{X}=\mathbf{x}) 
$$

This gives us what we call *Bayes' Rule*. However, in practice we don't know Bayes' Rule, since estimating $p(x)$ is the main challenge.

*Naive Bayes* is an approach that tries to estimate $p(x)$ using Bayes theorem.

In this particular example we know that the normal distribution works rather well for the heights $X$ for both classes $y=1$ (female) and $y=0$ (male). This implies that we can approximate the conditional distributions $f_{X|Y=1}$ and $f_{X|Y=0}$. These are the height distributions for females and males respectively and we can easily estimate all the necessary parameters from the data:

```{r, message=FALSE, warning=FALSE}

params <- train_set %>% 
  group_by(sex) %>% 
  summarize(avg = mean(height), sd = sd(height))

params


```

Then using Bayes rule we can compute:

$$
p(x) = \mbox{Pr}(Y=1|X=x) = \frac{f_{X|Y=1}(x) \mbox{Pr}(Y=1)}
{ f_{X|Y=0}(x)\mbox{Pr}(Y=0)  + f_{X|Y=1}(x)\mbox{Pr}(Y=1) }
$$

The prevalence, which we will denote with $\pi = \mbox{Pr}(Y=1)$, can be estimated with

```{r}

pi <- train_set %>% 
  summarize(pi=mean(sex == "Male")) %>% 
  .$pi

pi

```

Now we can use our estimates of average and standard deviation to get an actual rule:

```{r}

x <- test_set$height
f0 <- dnorm(x, params$avg[2],params$sd[2])   # note that dnorm is for the density distribution function in random generation for the normal distribution with mean = mean and standard deviation = sd
f1 <- dnorm(x, params$avg[1],params$sd[1])

p_hat_bayes <- f1*pi / (f1*pi + f0*(1-pi))
p_hat_bayes

```

Our naive Bayes estimate $\hat{p}(x)$ looks a lot like our logistic regression estimate:

```{r conditional-prob-glm-fit-2, echo=FALSE, warning=FALSE, message=FALSE}

tmp <- heights %>% 
  mutate(x = round(height)) %>% 
  group_by(x) %>% 
  filter(n() >= 10) %>% 
  summarize(prob = mean(sex == "Female"))


naive_bayes_curve <- data.frame(seq(min(tmp$x),max(tmp$x))) %>% 
  mutate(p_hat = dnorm(x, params$avg[1], params$sd[1]*pi)/(dnorm(x,params$avg[1],params$sd[1]*pi) + dnorm(x,params$avg[2],params$sd[2]*(1-pi))))

tmp %>% 
  ggplot(aes(x,prob)) + 
  geom_point() + 
  geom_line(data = naive_bayes_curve, mapping = aes(x, p_hat), lty = 3)
```

In fact, we can show that the naive Bayes approach is similar to the logistic regression prediction mathematically. However, we leave the demonstration to a more advanced text: such as [this one](https://web.stanford.edu/~hastie/Papers/ESLII.pdf). We can see that they are similar empirically:

```{r}

qplot(p_hat_logit, p_hat_bayes) + geom_abline()

```

Let's look at the confusion matrix to see how well we are performing. Our results are very close to what we got for our logistic regression model.

```{r}

y_hat_bayes <- ifelse(p_hat_bayes > 0.5, "Female", "Male")
confusionMatrix(data = as.factor(y_hat_bayes),reference = test_set$sex)




```
