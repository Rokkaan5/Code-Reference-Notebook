{"cells":[{"cell_type":"markdown","source":"# Naive Bayes Classification","metadata":{"tags":[],"cell_id":"44ac7930157a41ccb55af185986308a9","deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":1},"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Introduction","metadata":{"tags":[],"cell_id":"d78d6421e72d4637bf6a93a0c18b35a2","deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":7},"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"It is a classification algorithm that works based on the Bayes theorem. Naive Bayes is the most straightforward and fast classification algorithm, which is suitable for a large chunk of data. Naive Bayes classifier is successfully used in various applications such as spam filtering, text classification, sentiment analysis, and recommender systems. It uses Bayes's theorem of probability for the prediction of an unknown class.","metadata":{"tags":[],"cell_id":"24f920d0b4664ce58610161c2fe99f4f","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":13},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"Before explaining Naive Bayes, first, we should discuss Bayes Theorem. Bayes theorem is used to find the probability of a hypothesis with given evidence. Conditional Probability:","metadata":{"tags":[],"cell_id":"65ac761d5ea041659c91631ccefd237b","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":16},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"P(A/B)= (P(B/A) * P(A)) / P(B)\nIn this, using the Bayes theorem we can find the probability of A, given that B occurred. A is the hypothesis and B is the evidence.","metadata":{"tags":[],"cell_id":"619f69dcb3674645914664b49134fe46","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":30,"fromCodePoint":0}],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":19},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"P(B|A) is the probability of B given that A is True.","metadata":{"tags":[],"cell_id":"d7276a08e5f34372806b67cd121dc9c8","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":22},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"P(A) and P(B) are the independent probabilities of A and B.","metadata":{"tags":[],"cell_id":"81fe3540f6044477afbd9d29d2379cf8","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":25},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"## Naive Bayes with Binary Labels","metadata":{"tags":[],"cell_id":"7baf7afe22ab400c84772d59b80d1d0a","deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":28},"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"How does Naive Bayes Classifier work?\nLetâ€™s understand the working of Naive Bayes through an example. Given an example of weather conditions and playing sports. You need to calculate the probability of playing sports. Now, you need to classify whether players will play or not, based on the weather condition.","metadata":{"tags":[],"cell_id":"1762213b902e4ae8ac895cc30b1a04a9","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":37,"fromCodePoint":0}],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":34},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"Defining Dataset\nIn this example, you can use the dummy dataset with three columns: weather, temperature, and play. The first two are features(weather, temperature) and the other is the label.","metadata":{"tags":[],"cell_id":"4e1d598142bf4b68a1c950db5495afc7","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":16,"fromCodePoint":0}],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":37},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"# Assigning features and label variables\nweather=['Sunny','Sunny','Overcast','Rainy','Rainy','Rainy','Overcast','Sunny','Sunny',\n'Rainy','Sunny','Overcast','Overcast','Rainy']\ntemp=['Hot','Hot','Hot','Mild','Cool','Cool','Cool','Mild','Cool','Mild','Mild','Mild','Hot','Mild']\n\nplay=['No','No','Yes','Yes','Yes','No','Yes','No','Yes','Yes','Yes','Yes','Yes','No']","metadata":{"tags":[],"cell_id":"0354405c02f8425da0a29112b4806c76","source_hash":"33a8bde7","execution_start":1664584035430,"execution_millis":2,"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":40},"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Encoding Features\nFirst, you need to convert these string labels into numbers. for example: 'Overcast', 'Rainy', 'Sunny' as 0, 1, 2. This is known as label encoding. Scikit-learn provides LabelEncoder library for encoding labels with a value between 0 and one less than the number of discrete classes.","metadata":{"tags":[],"cell_id":"25a2e820bfed49f1be81c1f8d302d559","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":17,"fromCodePoint":0}],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":46},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"# Import LabelEncoder\nfrom sklearn import preprocessing\n#creating labelEncoder\nle = preprocessing.LabelEncoder()\n# Converting string labels into numbers.\nweather_encoded=le.fit_transform(weather)\nprint (weather_encoded)","metadata":{"tags":[],"cell_id":"819f0993481e4a8a9761739125d7a77a","source_hash":"4eb72ea2","execution_start":1664584035479,"execution_millis":933,"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":49},"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"[2 2 0 1 1 1 0 2 2 1 2 0 0 1]\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"Similarly, you can also encode temp and play columns.","metadata":{"tags":[],"cell_id":"788bdce412ea422cb23ce8deacfb01ce","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":55},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"# Converting string labels into numbers\ntemp_encoded=le.fit_transform(temp)\nlabel=le.fit_transform(play)\nprint (\"Temp:\",temp_encoded)\nprint (\"Play:\",label)","metadata":{"tags":[],"cell_id":"94865040d3364d19bae654561b01fdfa","source_hash":"2887a65d","execution_start":1664584036014,"execution_millis":449,"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":58},"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Temp: [1 1 1 2 0 0 0 2 0 2 2 2 1 2]\nPlay: [0 0 1 1 1 0 1 0 1 1 1 1 1 0]\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"Now combine both the features (weather and temp) in a single variable (list of tuples).","metadata":{"tags":[],"cell_id":"c2ee522c4cce40e49f52c98ebab6ecf0","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":64},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"#Combinig weather and temp into single listof tuples\ndef merge(list1, list2):\n      \n    merged_list = [(list1[i], list2[i]) for i in range(0, len(list1))]\n    return merged_list\nfeatures=merge(weather_encoded,temp_encoded)\nprint(features)","metadata":{"tags":[],"cell_id":"ddffb003312a435c87fdfcb94401eef4","source_hash":"95b191e3","execution_start":1664584036019,"execution_millis":453,"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":67},"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"[(2, 1), (2, 1), (0, 1), (1, 2), (1, 0), (1, 0), (0, 0), (2, 2), (2, 0), (1, 2), (2, 2), (0, 2), (0, 1), (1, 2)]\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"Generating Model","metadata":{"tags":[],"cell_id":"6d33dfc54a7c456a947a3e2ee04e6971","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":16,"fromCodePoint":0}],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":73},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"\nGenerate a model using naive bayes classifier in the following steps:\n1) Create naive bayes classifier\n2) Fit the dataset on classifier\n3) Perform prediction","metadata":{"tags":[],"cell_id":"65046a707dac42cfabaec41d2cc54335","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":76},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"#Import Gaussian Naive Bayes model\nfrom sklearn.naive_bayes import GaussianNB\n\n#Create a Gaussian Classifier\nmodel = GaussianNB()\n\n# Train the model using the training sets\nmodel.fit(features,label)\n\n#Predict Output\npredicted= model.predict([[0,2]]) # 0:Overcast, 2:Mild\nprint(\"Predicted Value:\", predicted)","metadata":{"tags":[],"cell_id":"d850ffb3279a4cfc87a559348ece0b7b","source_hash":"804a84a7","execution_start":1664584036026,"execution_millis":448,"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":79},"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Predicted Value: [1]\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"Here, 1 indicates that players can 'play'.","metadata":{"tags":[],"cell_id":"10b151d07b67432dbe472193b37bc56d","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":85},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"## Naive Bayes with Multiple Labels","metadata":{"tags":[],"cell_id":"237fbda35c8342d48ac5d5d0aa4b0af0","deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":88},"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Naive Bayes with Multiple Labels\nTill now you have learned Naive Bayes classification with binary labels. Now you will learn about multiple class classification in Naive Bayes. Which is known as multinomial Naive Bayes classification. For example, if you want to classify a news article about technology, entertainment, politics, or sports.","metadata":{"tags":[],"cell_id":"73ae243b4a8e456c917a835276eb8406","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":32,"fromCodePoint":0}],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":94},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":" In model building part, you can use wine dataset which is a very famous multi-class classification problem. \"This dataset is the result of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars.\" (UC Irvine) Dataset comprises of 13 features (alcohol, malic_acid, ash, alcalinity_of_ash, magnesium, total_phenols, flavanoids, nonflavanoid_phenols, proanthocyanins, color_intensity, hue, od280/od315_of_diluted_wines, proline) and type of wine cultivar. ","metadata":{"tags":[],"cell_id":"9f5bba85864f45229edae79c15d58400","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":97},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"This data has three type of wine Class_0, Class_1, and Class_3. Here you can build a model to classify the type of wine. The dataset is available in the scikit-learn library.","metadata":{"tags":[],"cell_id":"1a4beea2bdc34e8a8a113c2e4d467519","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":100},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"Loading Data\nLet's first load the required wine dataset from scikit-learn datasets.","metadata":{"tags":[],"cell_id":"0ee6e2b259ce40b1bff7a80f4c89658a","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":12,"fromCodePoint":0}],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":103},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"#Import scikit-learn dataset library\nfrom sklearn import datasets\n\n#Load dataset\nwine = datasets.load_wine()","metadata":{"tags":[],"cell_id":"07699eb80d064ff4a97ec86fe3395cf6","source_hash":"94a45c33","execution_start":1664584036074,"execution_millis":45,"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":106},"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Exploring Data\nYou can print the target and feature names, to make sure you have the right dataset, as such:","metadata":{"tags":[],"cell_id":"c8fc281b2e4a46fab114f1c56f984bfd","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":14,"fromCodePoint":0}],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":112},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"# print the names of the 13 features\nprint (\"Features: \", wine.feature_names)\n\n# print the label type of wine(class_0, class_1, class_2)\nprint (\"Labels: \", wine.target_names)","metadata":{"tags":[],"cell_id":"6a70152c498343b2ae4b2451da4cabb7","source_hash":"e7e69e3a","execution_start":1664584036119,"execution_millis":356,"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":115},"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Features:  ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\nLabels:  ['class_0' 'class_1' 'class_2']\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"It's a good idea to always explore your data a bit, so you know what you're working with. Here, you can see the first five rows of the dataset are printed, as well as the target variable for the whole dataset.\n","metadata":{"tags":[],"cell_id":"807789429e224c0693dabbafb25e3d9e","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":211,"fromCodePoint":209}],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":121},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"# print data(feature)shape\nwine.data.shape","metadata":{"tags":[],"cell_id":"c8bdb2ca55164cdfa2b208d67c2a0c12","source_hash":"8bd40e10","execution_start":1664584036121,"execution_millis":354,"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":124},"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"(178, 13)"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"# print the wine labels (0:Class_0, 1:class_2, 2:class_2)\nprint (wine.target)","metadata":{"tags":[],"cell_id":"0ab5d3cb756040cab8115ddb25eedf9a","source_hash":"7474a5d","execution_start":1664584036164,"execution_millis":311,"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":130},"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"Splitting Data\nFirst, you separate the columns into dependent and independent variables(or features and label). Then you split those variables into train and test set.","metadata":{"tags":[],"cell_id":"e68f55f36be242f9845acfd2fa06baaf","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":15,"fromCodePoint":0}],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":136},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"# Import train_test_split function\n\nfrom sklearn.model_selection  import train_test_split\n\n# Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.3,random_state=109) # 70% training and 30% test","metadata":{"tags":[],"cell_id":"77380f36e1234be9bbe0191b245b03be","source_hash":"250eb4d6","execution_start":1664584036165,"execution_millis":43,"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":139},"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Model Generation\nAfter splitting, you will generate a random forest model on the training set and perform prediction on test set features.","metadata":{"tags":[],"cell_id":"314372c44a764d4db345e82dae40bd18","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":16,"fromCodePoint":0}],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":145},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"#Import Gaussian Naive Bayes model\nfrom sklearn.naive_bayes import GaussianNB\n\n#Create a Gaussian Classifier\ngnb = GaussianNB()\n\n#Train the model using the training sets\ngnb.fit(X_train, y_train)\n\n#Predict the response for test dataset\ny_pred = gnb.predict(X_test)","metadata":{"tags":[],"cell_id":"ecb265cc89374c1a82ad16b1c23fde2f","source_hash":"330bc88f","execution_start":1664584036208,"execution_millis":0,"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":148},"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Evaluating Model\nAfter model generation, check the accuracy using actual and predicted values.","metadata":{"tags":[],"cell_id":"6608a8a882c54f6f871533546f7b4247","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":17,"fromCodePoint":0}],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":154},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","metadata":{"tags":[],"cell_id":"4bd1509cac5f4d22bcd3a96aec679b78","source_hash":"e2b72751","execution_start":1664584036209,"execution_millis":267,"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":157},"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Accuracy: 0.9074074074074074\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## Discussions","metadata":{"tags":[],"cell_id":"119cd01a3d744c0c8372f7651b8217bf","deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":163},"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Advantages","metadata":{"tags":[],"cell_id":"63039ede35634848978be38219e751f3","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":10,"fromCodePoint":0}],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":169},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"- It is not only a simple approach but also a fast and accurate method for prediction.","metadata":{"tags":[],"cell_id":"8232c77123d14900b49fbddcd482b816","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":172},"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"- Naive Bayes has very low computation cost.","metadata":{"tags":[],"cell_id":"d7de77fe25434983a4c2948f2d865781","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":175},"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"- It can be used with multiple class prediction problems.","metadata":{"tags":[],"cell_id":"9d6fae9059ca44888464e2642480f552","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":178},"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"- When the assumption of independence holds, a Naive Bayes classifier performs better compared to other models like logistic regression.","metadata":{"tags":[],"cell_id":"0d53945498184b529de747f1ac7958cc","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":181},"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"Disadvantages","metadata":{"tags":[],"cell_id":"ff3b5998dfc145a9b19bf320f480a9c8","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":13,"fromCodePoint":0}],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":184},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"- The assumption of independent features. In practice, it is almost impossible that model will get a set of predictors which are entirely independent.","metadata":{"tags":[],"cell_id":"95d2f04e62b04c1c870e33bf84e15056","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":187},"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"- If there is no training tuple of a particular class, this causes zero posterior probability. In this case, the model is unable to make predictions. This problem is known as Zero Probability/Frequency Problem.","metadata":{"tags":[],"cell_id":"76ffd904dbdc4809a7adff6735a5a15f","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":190},"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"References:","metadata":{"tags":[],"cell_id":"4bc4f06e80de4b4883f8e78b562a531f","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":10,"fromCodePoint":0}],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":193},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"1)https://www.datacamp.com/tutorial/naive-bayes-scikit-learn","metadata":{"tags":[],"cell_id":"66392e32a2054f1eabc7c21d6caad0f7","is_collapsed":false,"formattedRanges":[{"url":"https://www.datacamp.com/tutorial/naive-bayes-scikit-learn","type":"link","ranges":[],"toCodePoint":60,"fromCodePoint":2}],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":196},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"2)https://www.analyticsvidhya.com/blog/2021/01/a-guide-to-the-naive-bayes-algorithm/","metadata":{"tags":[],"cell_id":"739ad971816d4b5bb3233d39c21d3659","is_collapsed":false,"formattedRanges":[{"url":"https://www.analyticsvidhya.com/blog/2021/01/a-guide-to-the-naive-bayes-algorithm/","type":"link","ranges":[],"toCodePoint":84,"fromCodePoint":2}],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":199},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"3)https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/","metadata":{"tags":[],"cell_id":"4d023ed23f264df69faaec23b44d8123","is_collapsed":false,"formattedRanges":[{"url":"https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/","type":"link","ranges":[],"toCodePoint":75,"fromCodePoint":2}],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":202},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"## Credit\n\nThis tutorial is prepared by Bhawneet Singh","metadata":{"tags":[],"cell_id":"61b592f8b266403dabcaacb5ff9c3f74","deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":205},"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=713499d1-09e0-49b6-bbde-4d748e3da645' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_app_layout":"article","deepnote_notebook_id":"f1853fcc761544be859ca766d1d6418c","deepnote_execution_queue":[],"deepnote_persisted_session":{"createdAt":"2022-10-01T00:46:33.274Z"}}}