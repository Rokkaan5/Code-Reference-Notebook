---
title: "Decison Trees - Example1 (R)"
author: Professor Ami Gates
format:
    html:
        code-fold: false
execute:
    output: true
    warning: false
toc: true
---

#  Decision Tree Tutorial - Gates
##  Dataset
<https://drive.google.com/file/d/1wxcPBcYHUFh7gPSC0ApPSnQZ6VRUe-C3/view?usp=sharing>

## LIBRARIES
```{r libraries, warning=FALSE, message=FALSE}
library(rpart)   ## FOR Decision Trees
library(rattle)  ## FOR Decision Tree Vis
library(rpart.plot)
library(RColorBrewer)
library(Cairo)
library(network)
library(ggplot2)
##If you install from the source....
#Sys.setenv(NOAWT=TRUE)
## ONCE: install.packages("wordcloud")
library(wordcloud)
## ONCE: install.packages("tm")

library(slam)
library(quanteda)
## ONCE: install.packages("quanteda")
## Note - this includes SnowballC
#library(SnowballC)

library(proxy)
## ONCE: if needed:  install.packages("stringr")
library(stringr)
## ONCE: install.packages("textmineR")
library(textmineR)
library(igraph)
library(caret)
#library(lsa)
```


## Read in the dataset you want to work with
```{r working directory, eval=FALSE}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# JK note: in this case I set the wd to the current file source location
```
```{r load data}
RecordDatasetName="../../data_files/Labeled_ThreeClasses_Mammals_Fish_Reptiles3.csv"
RecordDF_A<-read.csv(RecordDatasetName, stringsAsFactors=TRUE)
head(RecordDF_A)
```


#  Let's start with the record dataset

We need to split it into a TRAINING and a TESTING set  

**AND** 

we will remove the label and save it.

Finally, we will remove and save the Name of the animal.

While we do this - let's check data types
```{r}
str(RecordDF_A)
#RecordDF_A$label<-as.factor(RecordDF_A$label)
```

We MUST convert the label (called label) into type FACTOR!

If you do not do this, your modeling will not work as well (or at all). I did this above using `stringsAsFactors=TRUE`


Our data is already clean and it is MIXED data. I will not normalize it.

What is mixed data? It is data made up of many data types

However, let's explore just a little bit to look for BALANCE in the variables AND in the label

!!!!!! We will also need to remove the "name" column

Why??


## Simple tables
```{r}
apply(RecordDF_A, 2, table)  # 2 means columns
```

**NOTE:** Our data and label are balanced pretty well.

Think about what you see. Are there columns to remove from the data?



Here is a fancy method to use a function to create a bar graph for each variable. 
```{r}
## Define the function on any dataframe input x
GoPlot <- function(x) {
  
  G <-ggplot(data=RecordDF_A, aes(.data[[x]], y="") ) +
    geom_bar(stat="identity", aes(fill =.data[[x]])) 
  
  return(G)
}

## Use the function in lappy
lapply(names(RecordDF_A), function(x) GoPlot(x))
```



## Remove unwanted columns
Next - let's look at the DF and remove columns we do not want to use in our model




We MUST remove the name column or the model will not be good. Think about why.

```{r remove name column}
## RecordDF_A

(AnimalName <- RecordDF_A$name)
(RecordDF_A<-RecordDF_A[-c(1)])
```

## Training and Testing Data

Next - split into TRAIN and TEST data

!!!! Sampling Matters !!!

In our case, we will use random sampling without replacement.

Why without replacement?

!!!! IMPORTANT - always clean, prepare, etc. **BEFORE** splitting data into train and test. NEVER after.


```{r training testing initial parameters}
(DataSize=nrow(RecordDF_A)) ## how many rows?
(TrainingSet_Size<-floor(DataSize*(3/4))) ## Size for training set
(TestSet_Size <- DataSize - TrainingSet_Size) ## Size for testing set
```

Random sample WITHOUT replacement (why?)
```{r training rows}
## set a seed if you want it to be the same each time you
## run the code. The number (like 1234) does not matter
set.seed(1234)

## This is the sample of row numbers
(MyTrainSample <- sample(nrow(RecordDF_A),
                         TrainingSet_Size,replace=FALSE))
```

Use the sample of row numbers to grab those rows only from the dataframe....
```{r training set}
(MyTrainingSET <- RecordDF_A[MyTrainSample,])
table(MyTrainingSET$label)
```

Use the NOT those row numbers (called `-`) to get the other row numbers not in the training to use to create the test set.

Training and Testing datasets MUST be disjoint. Why?
```{r testing set}
(MyTestSET <- RecordDF_A[-MyTrainSample,])
table(MyTestSET$label)
```

## Make sure your Training and Testing datasets are BALANCED

NEXT -REMOVE THE LABELS from the test set!!! - and keep them

```{r}
(TestKnownLabels <- MyTestSET$label)
(MyTestSET <- MyTestSET[ , -which(names(MyTestSET) %in% c("label"))])
```


# Decision Trees

First - train the model with your training data

Second - test the model - get predictions - compare to the known labels you have.
```{r}
MyTrainingSET
str(MyTrainingSET)
```

# Tree 1
This code uses `rpart` to create decision tree

Here, the `~ .`  means to train using all data variables

The `MyTrainingSET$label` tells it what the label is called

In this dataset, the label is called "label".
```{r tree1}
DT <- rpart(MyTrainingSET$label ~ ., data = MyTrainingSET, method="class")
summary(DT)
```

# Tree 2 (with `cp`)

Let's make another tree...here we will use `cp`

The smaller `cp` the larger the tree, if `cp` is too small you have overfitting.
```{r tree2}
DT2<-rpart(MyTrainingSET$label ~ ., data = MyTrainingSET,cp=.27, method="class")
summary(DT2)
```

## cp plot
```{r plot cp}
plotcp(DT2) ## This is the cp plot
```
# Tree 3

Let's make a third tree - here we use `cp = 0` and "information" instead of the default which is GINI
```{r}
DT3<-rpart(MyTrainingSET$label ~ ., 
           data = MyTrainingSET,cp=0, method="class",
           parms = list(split="information"),minsplit=2)
## The small cp the larger the tree if cp is too small you have overfitting
summary(DT3)
```

# Tree 4 (less data)
Let's make a 4th tree - but here, we will only use SOME of the variables in the dataset to train the model
```{r}
DT4<-rpart(MyTrainingSET$label ~ feathers + livebirth + blood, 
           data = MyTrainingSET,cp=0, method="class",
           parms = list(split="information"),minsplit=2)
## The small cp the larger the tree if cp is too small you have overfitting
summary(DT4)
```

# Extra notes about the output/summary
- Root Node Error x (X Error) is the cross-validated error rate, which is a more objective measure of predictive accuracy
- Root Node Error x (Rel Error) is the resubstitution error rate (the error rate computed on the training sample).

**Variable Importance:** The values are calculate by summing up all the improvement measures that each variable contributes

**RE:** the sum of the goodness of split measures for each split for which it was the primary variable

in Summary, the variable importance sums to 100

## Variable importance
NOTE: `variable.importance` is a named numeric vector giving the importance of each variable. (Only present if there are any splits.) 

When printed by `summary.rpart` these are rescaled to add to 100.

```{r}
DT3$variable.importance  ## before re-eval to add to 100
```

# Predict the Testset using all 4 trees - 

Let's see what we get. 

We will build a tree and a confusion matrix for all 4
## DT
```{r DT prediction}
(DT_Prediction= predict(DT, MyTestSET, type="class"))
```
### Confusion Matrix
```{r DT confusion matrix}
table(DT_Prediction,TestKnownLabels) ## one way to make a confu mat
```
### VIS
```{r}
fancyRpartPlot(DT)
```

## DT2

Example two with cp - a lower cp value is a bigger tree
```{r DT2 prediction}
(DT_Prediction2= predict(DT2, MyTestSET, type = "class"))
```

## ANother way to make a confusion matrix
```{r confusion matrix2}
caret::confusionMatrix(DT_Prediction2, TestKnownLabels, positive="true")
fancyRpartPlot(DT2)
```
## DT3
Example three with information gain and lower cp
```{r DT3 prediction}
(DT_Prediction3= predict(DT3, MyTestSET, type = "class"))

confusionMatrix(DT_Prediction3, TestKnownLabels, positive="true")
rattle::fancyRpartPlot(DT3,main="Decision Tree", cex=.5)
```

## DT4
```{r}
(DT_Prediction4= predict(DT4, MyTestSET, type = "class"))
confusionMatrix(DT_Prediction4, TestKnownLabels, positive="true")
rattle::fancyRpartPlot(DT4,main="Decision Tree", cex=.5)
```

# NOTES (on cross-validation):
In this code, we only created one training and one testing set

If we repeat the above process fully for a different training and testing set - and if we do that say 5 times it is called 5-fold cross validation.


## READ MORE about cross-validation:
1) <http://inferate.blogspot.com/2015/05/k-fold-cross-validation-with-decision.html>
2) This one is mathy and complicated for those who are interested: <https://rafalab.github.io/dsbook/cross-validation.html>
3) I like this one: <https://www.kaggle.com/satishgunjal/tutorial-k-fold-cross-validation>

