{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: 'Python - Decision Trees, Naive Bayes, SVM (Part 1)'\n",
        "author: Professor Ami Gates\n",
        "format:\n",
        "  html:\n",
        "    code-fold: false\n",
        "execute:\n",
        "  output: true\n",
        "  warning: false\n",
        "toc: true\n",
        "---"
      ],
      "id": "53782c6d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Supervised Learning Example Code\n",
        "- Naive Bayes\n",
        "- Bernoulli\n",
        "\n",
        "**Note 1:** It is better to make your own data and update the code for YOUR data.\n",
        "\n",
        "**Note 2:** This code focuses on text data but can easily be updated for record data. \n",
        "\n",
        "Datasets (corpus) can be found HERE:\n",
        "- [DOG](https://drive.google.com/drive/folders/1UZSCzKyTKOOKqD2dPrmjIxS_IA9nnsDi?usp=sharing)\n",
        "- [HIKE](https://drive.google.com/drive/folders/141yq5AosVWiQSgk0fixcOn9RWIWmj2DH?usp=sharing)\n",
        "\n",
        "Note that the other datasets for Record data are below in that section....\n",
        "\n",
        "# Textmining Naive Bayes Example"
      ],
      "id": "a8730edc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import nltk\n",
        "from sklearn import preprocessing\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import re  \n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "## For Stemming\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random as rd\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import tree\n",
        "## conda install python-graphviz\n",
        "## restart kernel (click the little red x next to the Console)\n",
        "import graphviz \n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.decomposition import PCA\n",
        "#from mpl_toolkits.mplot3d import Axes3D \n",
        "## conda install python-graphviz\n",
        "## restart kernel (click the little red x next to the Console)\n",
        "import graphviz \n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from sklearn.tree import export_graphviz\n",
        "#from sklearn.externals.six import StringIO  \n",
        "from IPython.display import Image  \n",
        "## conda install pydotplus\n",
        "import pydotplus\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "#from nltk.stem import WordNetLemmatizer \n",
        "#LEMMER = WordNetLemmatizer() \n",
        "\n",
        "from nltk.stem.porter import PorterStemmer"
      ],
      "id": "ff064623",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A `Stemmer` is a function that truncates and combines words such as hikings and hikers. \n"
      ],
      "id": "08215c2d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "STEMMER=PorterStemmer()\n",
        "print(STEMMER.stem(\"fishings\"))"
      ],
      "id": "6187aafd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Use NLTK's PorterStemmer in a function"
      ],
      "id": "97c41089"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def MY_STEMMER(str_input):   #I like dogs a lot111 !!\"\n",
        "    words = re.sub(r\"[^A-Za-z\\-]\", \" \", str_input).lower().split()   # I, like, dogs, a\n",
        "    words = [STEMMER.stem(w) for w in words]\n",
        "    return words"
      ],
      "id": "ba6dc909",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stemming and Lemming\n",
        "\n",
        "Stemming is different to Lemmatization in the approach it uses to produce root forms of words and the word produced.\n",
        "\n",
        "!!! Stemming can result in words that are not actually words. \n",
        "\n",
        "\"trouble\", \"troubling\", \"troubled\", \"troubles\" .... all become \"troubl\"\n",
        "\n",
        "Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. Lemmatization is similar to stemming but it brings context to the words. \n",
        "So it links words with similar meaning to one word. \n",
        "\n",
        "\n",
        "# Vectorizing\n",
        "\n",
        "Here, I am creating a few vectorizers - some with stemming and some without. One for Bernoulli, and some that normalize using Tf-Idf\n"
      ],
      "id": "7bfb4b35"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "MyVect_STEM=CountVectorizer(input='filename',\n",
        "                        analyzer = 'word',\n",
        "                        stop_words='english',\n",
        "                        ##stop_words=[\"and\", \"or\", \"but\"],\n",
        "                        #token_pattern='(?u)[a-zA-Z]+',\n",
        "                        #token_pattern=pattern,\n",
        "                        tokenizer=MY_STEMMER,\n",
        "                        #strip_accents = 'unicode', \n",
        "                        lowercase = True\n",
        "                        )\n",
        "\n",
        "\n",
        "MyVect_STEM_Bern=CountVectorizer(input='filename',\n",
        "                        analyzer = 'word',\n",
        "                        stop_words='english',\n",
        "                        ##stop_words=[\"and\", \"or\", \"but\"],\n",
        "                        #token_pattern='(?u)[a-zA-Z]+',\n",
        "                        #token_pattern=pattern,\n",
        "                        tokenizer=MY_STEMMER,\n",
        "                        #strip_accents = 'unicode', \n",
        "                        lowercase = True,\n",
        "                        binary=True    #  0 if the word is not in the doc  and a 1 if it is\n",
        "                        )\n",
        "\n",
        "\n",
        "\n",
        "MyVect_IFIDF=TfidfVectorizer(input='filename',\n",
        "                        analyzer = 'word',\n",
        "                        stop_words='english',\n",
        "                        lowercase = True,\n",
        "                        #binary=True\n",
        "                        )\n",
        "\n",
        "MyVect_IFIDF_STEM=TfidfVectorizer(input='filename',\n",
        "                        analyzer = 'word',\n",
        "                        stop_words='english',\n",
        "                        tokenizer=MY_STEMMER,\n",
        "                        #strip_accents = 'unicode', \n",
        "                        lowercase = True,\n",
        "                        #binary=True\n",
        "                        )"
      ],
      "id": "c216d97c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## New Dataframes\n",
        "We will be creating new data frames - one for NB and one for Bern. \n",
        "\n",
        "These are the two new and currently empty DFs\n",
        "\n",
        "Notice there are FOUR (4) of these because I have 4 vectorizers"
      ],
      "id": "a8cd0165"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "FinalDF_STEM=pd.DataFrame()\n",
        "FinalDF_STEM_Bern=pd.DataFrame()\n",
        "FinalDF_TFIDF=pd.DataFrame()\n",
        "FinalDF_TFIDF_STEM=pd.DataFrame()"
      ],
      "id": "33857038",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You will need to know where things are on your computer.\n",
        "\n",
        "This code assumes that it is in the same folder/location as the folders DOG and HIKE. It will loop through the files in these two folders and will build the list needed to use `CounterVectorizer`. \n",
        "\n",
        "**NOTICE:** My loop has a path in it. This is for MY computer - not yours! You will need to adjust the path."
      ],
      "id": "6fcf8ede"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "os.getcwd()"
      ],
      "id": "ab2958b1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for name in [\"DOG\", \"HIKE\"]:\n",
        "\n",
        "    builder=name+\"DF\"    #DOGDF\n",
        "    #print(builder)\n",
        "    builderB=name+\"DFB\"\n",
        "    \n",
        "    path= os.path.abspath(os.path.join('../..','data_files',name))\n",
        "    \n",
        "    FileList=[]\n",
        "    for item in os.listdir(path):\n",
        "        #print(path+ \"\\\\\" + item)\n",
        "        next1=path+ \"/\" + item\n",
        "        FileList.append(next1)  \n",
        "        print(\"full list...\")\n",
        "        #print(FileList)\n",
        "        \n",
        "        ## Do for all three\n",
        "        ## MyVect_STEM  and MyVect_IFIDF and MyVect_IFIDF_STEM\n",
        "        X1=MyVect_STEM.fit_transform(FileList)\n",
        "        X2=MyVect_IFIDF.fit_transform(FileList)\n",
        "        X3=MyVect_IFIDF_STEM.fit_transform(FileList)\n",
        "        XB=MyVect_STEM_Bern.fit_transform(FileList)\n",
        "        \n",
        "        \n",
        "        ColumnNames1=MyVect_STEM.get_feature_names_out()\n",
        "        NumFeatures1=len(ColumnNames1)\n",
        "        ColumnNames2=MyVect_IFIDF.get_feature_names_out()\n",
        "        NumFeatures2=len(ColumnNames2)\n",
        "        ColumnNames3=MyVect_IFIDF_STEM.get_feature_names_out()\n",
        "        NumFeatures3=len(ColumnNames3)\n",
        "        ColumnNamesB=MyVect_STEM_Bern.get_feature_names_out()\n",
        "        NumFeatures4=len(ColumnNamesB)\n",
        "        #print(\"Column names: \", ColumnNames2)\n",
        "        #Create a name\n",
        "        \n",
        "   \n",
        "    builderS=pd.DataFrame(X1.toarray(),columns=ColumnNames1)\n",
        "    builderT=pd.DataFrame(X2.toarray(),columns=ColumnNames2)\n",
        "    builderTS=pd.DataFrame(X3.toarray(),columns=ColumnNames3)\n",
        "    builderB=pd.DataFrame(XB.toarray(),columns=ColumnNamesB)\n",
        "    \n",
        "    ## Add column\n",
        "    #print(\"Adding new column....\")\n",
        "    builderS[\"Label\"]=name\n",
        "    builderT[\"Label\"]=name\n",
        "    builderTS[\"Label\"]=name\n",
        "    builderB[\"Label\"]=name\n",
        "    #print(builderS)\n",
        "    \n",
        "    FinalDF_STEM= FinalDF_STEM.append(builderS)\n",
        "    FinalDF_STEM_Bern= FinalDF_STEM_Bern.append(builderB)\n",
        "    FinalDF_TFIDF= FinalDF_TFIDF.append(builderT)\n",
        "    FinalDF_TFIDF_STEM= FinalDF_TFIDF_STEM.append(builderTS)\n",
        "   \n",
        "    #print(FinalDF_STEM.head())"
      ],
      "id": "bced5040",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dealing with NaN\n",
        "Replace the NaN with 0 because it actually means none in this case"
      ],
      "id": "d4e170cd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "FinalDF_STEM=FinalDF_STEM.fillna(0)\n",
        "FinalDF_STEM_Bern=FinalDF_STEM_Bern.fillna(0)\n",
        "FinalDF_TFIDF=FinalDF_TFIDF.fillna(0)\n",
        "FinalDF_TFIDF_STEM=FinalDF_TFIDF_STEM.fillna(0)"
      ],
      "id": "febfa469",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## REMOVE number columns\n",
        "\n",
        "Remove columns with number from this one\n"
      ],
      "id": "d2e8cd1a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "##-------------------------------------------------------------------\n",
        "####### Create a function that removes columns that are/contain nums\n",
        "##-------------------------------------------------------------------\n",
        "def RemoveNums(SomeDF):\n",
        "    #print(SomeDF)\n",
        "    print(\"Running Remove Numbers function....\\n\")\n",
        "    temp=SomeDF\n",
        "    MyList=[]\n",
        "    for col in temp.columns:\n",
        "        #print(col)\n",
        "        #Logical1=col.isdigit()  ## is a num\n",
        "        Logical2=str.isalpha(col) ## this checks for anything\n",
        "        ## that is not a letter\n",
        "        if(Logical2==False):# or Logical2==True):\n",
        "            #print(col)\n",
        "            MyList.append(str(col))\n",
        "            #print(MyList)       \n",
        "    temp.drop(MyList, axis=1, inplace=True)\n",
        "            #print(temp)\n",
        "            #return temp\n",
        "       \n",
        "    return temp"
      ],
      "id": "55d847e0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Call the function ...."
      ],
      "id": "ff10c390"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "FinalDF_STEM=RemoveNums(FinalDF_STEM)\n",
        "FinalDF_STEM_Bern=RemoveNums(FinalDF_STEM_Bern)\n",
        "FinalDF_TFIDF=RemoveNums(FinalDF_TFIDF)\n",
        "FinalDF_TFIDF_STEM=RemoveNums(FinalDF_TFIDF_STEM)"
      ],
      "id": "880fbebe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Have a look:\n"
      ],
      "id": "ae111158"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## These print statements help you to see where you are\n",
        "#print(FinalDF_STEM)\n",
        "## Remove columns that contain \"-\"  HOW TO....\n",
        "#cols = [c for c in FinalDF_STEM.columns if \"-\" in c[:] ]\n",
        "#FinalDF_STEM=FinalDF_STEM.drop(cols, axis = 1) \n",
        "print(FinalDF_STEM)  #1\n",
        "print(FinalDF_STEM_Bern) #4\n",
        "print(FinalDF_TFIDF)  #2\n",
        "print(FinalDF_TFIDF_STEM) #3\n",
        "\n",
        "##################################################\n",
        "##\n",
        "##        Now we have 4 labeled dataframes!\n",
        "##\n",
        "##        Let's model them.....\n",
        "##\n",
        "######################################################"
      ],
      "id": "dac7b9fc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create the testing set \n",
        "grab a sample from the training set. \n",
        "\n",
        "Be careful. Notice that right now, our train set is sorted by label.\n",
        "\n",
        "If your train set is large enough, you can take a random sample."
      ],
      "id": "db98acc6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import random as rd\n",
        "#rd.seed(1234)\n",
        "TrainDF1, TestDF1 = train_test_split(FinalDF_STEM, test_size=0.3)\n",
        "print(TrainDF1)\n",
        "print(TestDF1)\n",
        "TrainDF2, TestDF2 = train_test_split(FinalDF_TFIDF, test_size=0.3)\n",
        "TrainDF3, TestDF3 = train_test_split(FinalDF_TFIDF_STEM, test_size=0.3)\n",
        "TrainDF4, TestDF4 = train_test_split(FinalDF_STEM_Bern, test_size=0.4)\n",
        "print(TestDF4)"
      ],
      "id": "7f477141",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "OK - at this point we have Train and Test data for the text data in DOG and HIKE. \n",
        "\n",
        "Of course, this can be updated to work from sentiment (like POS and NEG) and can be update for multiple folders or one folder..\n",
        "\n",
        "\n",
        "## For all FOUR DFs - separate LABELS\n",
        "### TEST\n",
        "**IMPORTANT** - YOU CANNOT LEAVE LABELS ON THE TEST SET\n",
        "#### Save labels"
      ],
      "id": "288b2b92"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Test1Labels=TestDF1[\"Label\"]\n",
        "print(Test1Labels)\n",
        "Test2Labels=TestDF2[\"Label\"]\n",
        "Test3Labels=TestDF3[\"Label\"]\n",
        "Test4Labels=TestDF4[\"Label\"]\n",
        "print(Test2Labels)"
      ],
      "id": "4d1e4cdc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### remove labels"
      ],
      "id": "c8ed0e2f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "TestDF1 = TestDF1.drop([\"Label\"], axis=1)\n",
        "TestDF2 = TestDF2.drop([\"Label\"], axis=1)\n",
        "TestDF3 = TestDF3.drop([\"Label\"], axis=1)\n",
        "TestDF4 = TestDF4.drop([\"Label\"], axis=1)\n",
        "print(TestDF1)"
      ],
      "id": "9811e1d0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TRAIN"
      ],
      "id": "d5cdf06f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Train1Labels=TrainDF1[\"Label\"]\n",
        "Train2Labels=TrainDF2[\"Label\"]\n",
        "Train3Labels=TrainDF3[\"Label\"]\n",
        "Train4Labels=TrainDF4[\"Label\"]\n",
        "print(Train3Labels)"
      ],
      "id": "09785289",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### remove labels"
      ],
      "id": "8cf26446"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "TrainDF1 = TrainDF1.drop([\"Label\"], axis=1)\n",
        "TrainDF2 = TrainDF2.drop([\"Label\"], axis=1)\n",
        "TrainDF3 = TrainDF3.drop([\"Label\"], axis=1)\n",
        "TrainDF4 = TrainDF4.drop([\"Label\"], axis=1)\n",
        "print(TrainDF3)"
      ],
      "id": "a3cdc2a5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Naive Bayes "
      ],
      "id": "86b314ee"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "#https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB.fit\n",
        "#Create the modeler\n",
        "MyModelNB= MultinomialNB()"
      ],
      "id": "8bea6878",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run on all four Dfs."
      ],
      "id": "7c5f2553"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "NB1=MyModelNB.fit(TrainDF1, Train1Labels)\n",
        "Prediction1 = MyModelNB.predict(TestDF1)\n",
        "print(np.round(MyModelNB.predict_proba(TestDF1),2))"
      ],
      "id": "191c97ef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "NB2=MyModelNB.fit(TrainDF2, Train2Labels)\n",
        "Prediction2 = MyModelNB.predict(TestDF2)\n",
        "print(np.round(MyModelNB.predict_proba(TestDF2),2))"
      ],
      "id": "eeb7e338",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "NB3=MyModelNB.fit(TrainDF3, Train3Labels)\n",
        "Prediction3 = MyModelNB.predict(TestDF3)\n",
        "print(np.round(MyModelNB.predict_proba(TestDF3),2))"
      ],
      "id": "2727e1c5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "NB4=MyModelNB.fit(TrainDF4, Train4Labels)\n",
        "Prediction4 = MyModelNB.predict(TestDF4)\n",
        "print(np.round(MyModelNB.predict_proba(TestDF4),2))"
      ],
      "id": "c290f6c1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\nThe prediction from NB is:\")\n",
        "print(Prediction1)\n",
        "print(\"\\nThe actual labels are:\")\n",
        "print(Test1Labels)"
      ],
      "id": "cf229fd4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\nThe prediction from NB is:\")\n",
        "print(Prediction2)\n",
        "print(\"\\nThe actual labels are:\")\n",
        "print(Test2Labels)"
      ],
      "id": "7c9cf637",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\nThe prediction from NB is:\")\n",
        "print(Prediction3)\n",
        "print(\"\\nThe actual labels are:\")\n",
        "print(Test3Labels)"
      ],
      "id": "49718c4a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\nThe prediction from NB is:\")\n",
        "print(Prediction4)\n",
        "print(\"\\nThe actual labels are:\")\n",
        "print(Test4Labels)"
      ],
      "id": "eab6db1d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## confusion matrix\n",
        "\n",
        "The confusion matrix is square and is labels X labels\n",
        "\n",
        "We have two labels, so ours will be 2X2\n",
        "\n",
        "The matrix shows\n",
        "- rows are the true labels\n",
        "- columns are predicted\n",
        "- it is alphabetical\n",
        "- The numbers are how many "
      ],
      "id": "65999c5d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cnf_matrix1 = confusion_matrix(Test1Labels, Prediction1)\n",
        "print(\"\\nThe confusion matrix is:\")\n",
        "print(cnf_matrix1)"
      ],
      "id": "8fe8efa7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cnf_matrix2 = confusion_matrix(Test2Labels, Prediction2)\n",
        "print(\"\\nThe confusion matrix is:\")\n",
        "print(cnf_matrix2)"
      ],
      "id": "80814683",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cnf_matrix3 = confusion_matrix(Test3Labels, Prediction3)\n",
        "print(\"\\nThe confusion matrix is:\")\n",
        "print(cnf_matrix3)"
      ],
      "id": "fa9ec6f7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cnf_matrix4 = confusion_matrix(Test4Labels, Prediction4)\n",
        "print(\"\\nThe confusion matrix is:\")\n",
        "print(cnf_matrix4)"
      ],
      "id": "1f0e0dad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bernoulli \n"
      ],
      "id": "f5b965ac"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "BernModel = BernoulliNB()\n",
        "BernModel.fit(TrainDF4, Train4Labels)\n",
        "##BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
        "print(\"\\nBernoulli prediction:\\n\")\n",
        "Prediction=BernModel.predict(TestDF4)"
      ],
      "id": "06595b03",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\nActual:\")\n",
        "print(Test4Labels)\n",
        "print(\"\\The prediction\\n\")\n",
        "print(Prediction)"
      ],
      "id": "f2f5ca96",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "bn_matrix = confusion_matrix(Test4Labels, Prediction)\n",
        "print(\"\\nThe confusion matrix is:\")\n",
        "print(bn_matrix)"
      ],
      "id": "17586c0e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Decision Trees\n",
        "<https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html>"
      ],
      "id": "87b6a669"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import tree\n",
        "import matplotlib.pyplot as plt\n",
        "## conda install python-graphviz\n",
        "## restart kernel (click the little red x next to the Console)\n",
        "import graphviz \n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "MyDT=DecisionTreeClassifier(criterion='entropy', ##\"entropy\" or \"gini\"\n",
        "                            splitter='best',  ## or \"random\" or \"best\"\n",
        "                            max_depth=None, \n",
        "                            min_samples_split=2, \n",
        "                            min_samples_leaf=1, \n",
        "                            min_weight_fraction_leaf=0.0, \n",
        "                            max_features=None, \n",
        "                            random_state=None, \n",
        "                            max_leaf_nodes=None, \n",
        "                            min_impurity_decrease=0.0, \n",
        "                            #min_impurity_split=None, \n",
        "                            class_weight=None)"
      ],
      "id": "65f548d9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(TrainDF1)"
      ],
      "id": "ebcaa762",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This for loop will fit and predict Decision Trees for all 4 of the dataframes. Notice that this uses dynamic variables and eval\n"
      ],
      "id": "b96f9b09"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for i in [1,2,3,4]:\n",
        "    temp1=str(\"TrainDF\"+str(i))   ##  TrainDF1\n",
        "    temp2=str(\"Train\"+str(i)+\"Labels\")  #Train1Labels\n",
        "    temp3=str(\"TestDF\"+str(i))  #TestDF1\n",
        "    temp4=str(\"Test\"+str(i)+\"Labels\") # Test1Labels\n",
        "    \n",
        "    ## perform DT\n",
        "    #MyDT.fit(TrainDF1, Train1Labels)\n",
        "    MyDT.fit(eval(temp1), eval(temp2))\n",
        "    ## plot the tree\n",
        "    tree.plot_tree(MyDT)\n",
        "    plt.savefig(temp1)\n",
        "    feature_names=eval(str(temp1+\".columns\"))\n",
        "    dot_data = tree.export_graphviz(MyDT, out_file=None,\n",
        "                    ## The following creates TrainDF.columns for each\n",
        "                    ## which are the feature names.\n",
        "                      feature_names=eval(str(temp1+\".columns\")),  \n",
        "                      #class_names=MyDT.class_names,  \n",
        "                      filled=True, rounded=True,  \n",
        "                      special_characters=True)                                    \n",
        "    graph = graphviz.Source(dot_data) \n",
        "    ## Create dynamic graph name\n",
        "    tempname=str(\"Graph\" + str(i))\n",
        "    graph.render(tempname) \n",
        "    ## Show the predictions from the DT on the test set\n",
        "    print(\"\\nActual for DataFrame: \", i, \"\\n\")\n",
        "    print(eval(temp2))\n",
        "    print(\"Prediction\\n\")\n",
        "    DT_pred=MyDT.predict(eval(temp3))\n",
        "    print(DT_pred)\n",
        "    ## Show the confusion matrix\n",
        "    bn_matrix = confusion_matrix(eval(temp4), DT_pred)\n",
        "    print(\"\\nThe confusion matrix is:\")\n",
        "    print(bn_matrix)\n",
        "    FeatureImp=MyDT.feature_importances_   \n",
        "    indices = np.argsort(FeatureImp)[::-1]\n",
        "    ## print out the important features.....\n",
        "    for f in range(TrainDF4.shape[1]):\n",
        "        if FeatureImp[indices[f]] > 0:\n",
        "            print(\"%d. feature %d (%f)\" % (f + 1, indices[f], FeatureImp[indices[f]]))\n",
        "            print (\"feature name: \", feature_names[indices[f]])\n",
        "\n",
        "## FYI for small datasets you can zip features....\n",
        "# print(dict(zip(iris_pd.columns, clf.feature_importances_)))"
      ],
      "id": "577a76a4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Confusion Matrix Visual Code (quickly updated)"
      ],
      "id": "5bf54a15"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt     \n",
        "\n",
        "##########################################################\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "cm = confusion_matrix(Test4Labels, DT_pred, labels=MyDT.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=MyDT.classes_)                         \n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "id": "d9fe70fd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Visualize Decision Trees plotting paired surfaces\n"
      ],
      "id": "a58286f4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "\n",
        "f1=TrainDF1.columns.get_loc(\"dog\") \n",
        "f2=TrainDF1.columns.get_loc(\"hike\") \n",
        "f3=TrainDF1.columns.get_loc(\"workout\") \n",
        "f4=TrainDF1.columns.get_loc(\"happi\") "
      ],
      "id": "0ec66e1c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "n_classes =2\n",
        "plot_colors = \"ryb\"\n",
        "plot_step = 0.02\n",
        "\n",
        "for pairidx, pair in enumerate([[f1, f2], [f1, f3], [f1, f4],\n",
        "                                [f2,f3], [f3, f4]]):\n",
        "    #print(TrainDF1.iloc[:,pair])\n",
        "    X = TrainDF1.iloc[:, pair]\n",
        "    ## Because we are plotting, using our GOD and HIKE labels will not work\n",
        "    ## we need to change them to 0 and 1\n",
        "    y = Train1Labels\n",
        "    print(y)\n",
        "    oldy=y\n",
        "    #print(type(y))\n",
        "    y=y.replace(\"DOG\", 1)\n",
        "    y=y.replace(\"HIKE\", 0)\n",
        "    \n",
        "    print(y)\n",
        "    # Train\n",
        "    DT = DecisionTreeClassifier().fit(X, y)\n",
        "    # Plot the decision boundary\n",
        "    plt.subplot(2, 3, pairidx + 1)\n",
        "\n",
        "    x_min, x_max = X.iloc[:, 0].min() - 1, X.iloc[:, 0].max() + 1\n",
        "    print(x_min)\n",
        "    y_min, y_max = X.iloc[:, 1].min() - 1, X.iloc[:, 1].max() + 1\n",
        "   \n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max,plot_step),\n",
        "                         np.arange(y_min, y_max,plot_step))\n",
        "    \n",
        "    #print(yy)\n",
        "    \n",
        "    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
        "#\n",
        "    Z = DT.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    print(Z)\n",
        "    \n",
        "    \n",
        "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n",
        "       \n",
        "    plt.scatter(X.iloc[:, 0], X.iloc[:, 1], s=30, label=oldy,edgecolor='black', \n",
        "                    #c=color, s=15)\n",
        "                    #label=y[i],\n",
        "                    cmap=plt.cm.RdYlBu)\n",
        "###---------------------------end for loop ----------------------------------\n",
        "#plt.suptitle(\"Decision surface of a decision tree using paired features\")\n",
        "#plt.legend(loc='lower right', borderpad=0, handletextpad=0)\n",
        "#plt.axis(\"tight\")\n",
        "#\n",
        "#plt.figure()"
      ],
      "id": "82f67226",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Random Forest for Text Data\n"
      ],
      "id": "5762699f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "RF = RandomForestClassifier()\n",
        "RF.fit(TrainDF1, Train1Labels)\n",
        "RF_pred=RF.predict(TestDF1)\n",
        "\n",
        "bn_matrix_RF_text = confusion_matrix(Test1Labels, RF_pred)\n",
        "print(\"\\nThe confusion matrix is:\")\n",
        "print(bn_matrix_RF_text)"
      ],
      "id": "a383489e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## VIS RF\n",
        "### FEATURE NAMES"
      ],
      "id": "d259e17f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "FeaturesT=TrainDF1.columns.tolist()\n",
        "#Targets=StudentTestLabels_Num\n",
        "\n",
        "figT, axesT = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=800)\n",
        "\n",
        "tree.plot_tree(RF.estimators_[0],\n",
        "               feature_names = FeaturesT, \n",
        "               #class_names=Targets,\n",
        "               filled = True)"
      ],
      "id": "3384ee67",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "figT.savefig('RF_Tree_Text')  ## creates png"
      ],
      "id": "266af62d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### View estimator Trees in RF"
      ],
      "id": "58086ed4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "figT2, axesT2 = plt.subplots(nrows = 1,ncols = 5,figsize = (10,2), dpi=900)\n",
        "\n",
        "for index in range(0, 5):\n",
        "    tree.plot_tree(RF.estimators_[index],\n",
        "                   feature_names = FeaturesT, \n",
        "                   filled = True,\n",
        "                   ax = axesT2[index])\n",
        "\n",
        "    axesT2[index].set_title('Estimator: ' + str(index), fontsize = 11)"
      ],
      "id": "bc462842",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " Save it"
      ],
      "id": "befaf2de"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "figT2.savefig('FIVEtrees_RF.png')"
      ],
      "id": "8989214f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature importance in RF\n",
        "\n",
        "Recall that FeaturesT are the columns names - the words in this case."
      ],
      "id": "b44eee63"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "FeatureImpRF=RF.feature_importances_   \n",
        "indicesRF = np.argsort(FeatureImpRF)[::-1]"
      ],
      "id": "c973df6c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### print out the important features"
      ],
      "id": "82ad3b94"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for f2 in range(TrainDF1.shape[1]):   ##TrainDF1.shape[1] is number of columns\n",
        "    if FeatureImpRF[indicesRF[f2]] >= 0.01:\n",
        "        print(\"%d. feature %d (%.2f)\" % (f2 + 1, indicesRF[f2], FeatureImpRF[indicesRF[f2]]))\n",
        "        print (\"feature name: \", FeaturesT[indicesRF[f2]])"
      ],
      "id": "367fc024",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PLOT THE TOP 10 FEATURES"
      ],
      "id": "3b1d345a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "top_ten_arg = indicesRF[:10]\n",
        "#print(top_ten_arg)\n",
        "plt.title('Feature Importances Dog and Hike')\n",
        "plt.barh(range(len(top_ten_arg)), FeatureImpRF[top_ten_arg], color='b', align='center')\n",
        "plt.yticks(range(len(top_ten_arg)), [FeaturesT[i] for i in top_ten_arg])\n",
        "plt.xlabel('Relative Importance')\n",
        "plt.show()"
      ],
      "id": "0bd12512",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NN"
      ],
      "id": "1eb49d15"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "MyNN = MLPClassifier(solver='lbfgs', \n",
        "                    #solver='adam',\n",
        "                    #solver='sgd',\n",
        "                     alpha=1,  \n",
        "                     \n",
        "                   hidden_layer_sizes=(1, 2), random_state=1)\n",
        "\n",
        "MyNN.fit(TrainDF1, Train1Labels)\n",
        "NNPrediction = MyNN.predict(TestDF1)\n",
        "print(\"confusion matrix for NN\\n\")\n",
        "NN_matrix = confusion_matrix(Test1Labels, NNPrediction)\n",
        "print(NN_matrix)"
      ],
      "id": "3ab1ce14",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SVM"
      ],
      "id": "adafcd39"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#from sklearn.svm import LinearSVC\n",
        "SVM_Model=LinearSVC(C=1)\n",
        "\n",
        "SVM_Model.fit(TrainDF1, Train1Labels)\n",
        "\n",
        "#print(\"SVM prediction:\\n\", SVM_Model.predict(TestDF1))\n",
        "#print(\"Actual:\")\n",
        "#print(Test1Labels)\n",
        "\n",
        "SVM_matrix = confusion_matrix(Test1Labels, SVM_Model.predict(TestDF1))\n",
        "print(\"\\nThe confusion matrix is:\")\n",
        "print(SVM_matrix)\n",
        "print(\"\\n\\n\")"
      ],
      "id": "b5f44692",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SVMs do not run on qualitative data.\n",
        "\n",
        "ALWAYS remove the Labels from the Test and Train data\n",
        "\n",
        "Here is what we have from above:"
      ],
      "id": "a0c827ef"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## TrainDF_nolabels, TrainLabels\n",
        "### TestDF, TestLabels"
      ],
      "id": "914ca777",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Rather than creating copies of everything"
      ],
      "id": "023ac7da"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "### !!!!!!!!!!!!!!!!!\n",
        "##  You can set the variables: TRAIN, TRAIN_Labels\n",
        "##                            TEST and TEST_Labels\n",
        "## to whatever you wish\n",
        "######################################################"
      ],
      "id": "0816a713",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "TRAIN= TrainDF1   ## As noted above - this can also be TrainDF2, etc.\n",
        "print(TRAIN)\n",
        "TRAIN_Labels= Train1Labels\n",
        "print(TRAIN_Labels)\n",
        "TEST= TestDF1\n",
        "TEST_Labels= Test1Labels"
      ],
      "id": "e595074b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "SVM_Model1=LinearSVC(C=1)\n",
        "SVM_Model1.fit(TRAIN, TRAIN_Labels)"
      ],
      "id": "c36bbfb7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#print(\"SVM prediction:\\n\", SVM_Model1.predict(TEST))\n",
        "#print(\"Actual:\")\n",
        "#print(TEST_Labels)"
      ],
      "id": "95f19d21",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "SVM_matrix = confusion_matrix(TEST_Labels, SVM_Model1.predict(TEST))\n",
        "print(\"\\nThe confusion matrix is:\")\n",
        "print(SVM_matrix)\n",
        "print(\"\\n\\n\")"
      ],
      "id": "abdccadf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## other kernels\n",
        "### RBF"
      ],
      "id": "4655dc5a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "SVM_Model2=sklearn.svm.SVC(C=1, kernel='rbf', \n",
        "                           verbose=True, gamma=\"auto\")\n",
        "SVM_Model2.fit(TRAIN, TRAIN_Labels)"
      ],
      "id": "c1b86977",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#print(\"SVM prediction:\\n\", SVM_Model2.predict(TEST))\n",
        "#print(\"Actual:\")\n",
        "#print(TEST_Labels)\n",
        "\n",
        "print(\"RBF  :\\n\")\n",
        "SVM_matrix2 = confusion_matrix(TEST_Labels, SVM_Model2.predict(TEST))\n",
        "print(\"\\nThe confusion matrix is:\")\n",
        "print(SVM_matrix2)\n",
        "print(\"\\n\\n\")"
      ],
      "id": "02a973c7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### POLY"
      ],
      "id": "3f68a0b9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "SVM_Model3=sklearn.svm.SVC(C=100, kernel='poly',degree=3,\n",
        "                           gamma=\"auto\", verbose=True)\n",
        "\n",
        "#print(SVM_Model3)\n",
        "SVM_Model3.fit(TRAIN, TRAIN_Labels)"
      ],
      "id": "3613220f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#print(\"SVM prediction:\\n\", SVM_Model3.predict(TEST))\n",
        "#print(\"Actual:\")\n",
        "#print(TEST_Labels)\n",
        "print(\"POLY Degree 2:\\n\")\n",
        "SVM_matrix3 = confusion_matrix(TEST_Labels, SVM_Model3.predict(TEST))\n",
        "print(\"\\nThe confusion matrix is:\")\n",
        "print(SVM_matrix3)\n",
        "print(\"\\n\\n\")"
      ],
      "id": "eb59ebe0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualizing the top features\n",
        "\n",
        "Then Visualizing the margin with the top 2 in 2D\n"
      ],
      "id": "f514f2cb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "## Credit: https://medium.com/@aneesha/visualising-top-features-in-linear-svm-with-scikit-learn-and-matplotlib-3454ab18a14d\n",
        "## Define a function to visualize the TOP words (variables)\n",
        "def plot_coefficients(MODEL=SVM_Model, COLNAMES=TrainDF1.columns, top_features=10):\n",
        "    ## Model if SVM MUST be SVC, RE: SVM_Model=LinearSVC(C=10)\n",
        "    coef = MODEL.coef_.ravel()\n",
        "    top_positive_coefficients = np.argsort(coef,axis=0)[-top_features:]\n",
        "    print(top_positive_coefficients)\n",
        "    top_negative_coefficients = np.argsort(coef,axis=0)[:top_features]\n",
        "    print(top_negative_coefficients)\n",
        "    top_coefficients = np.hstack([top_negative_coefficients, top_positive_coefficients])\n",
        "    # create plot\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    colors = [\"red\" if c < 0 else \"blue\" for c in coef[top_coefficients]]\n",
        "    plt.bar(  x=  np.arange(2 * top_features)  , height=coef[top_coefficients], width=.5,  color=colors)\n",
        "    feature_names = np.array(COLNAMES)\n",
        "    plt.xticks(np.arange(0, (2*top_features)), feature_names[top_coefficients], rotation=60, ha=\"right\")\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "plot_coefficients()"
      ],
      "id": "b06c1412",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the top 2 features from above let's look at the margin of the SVM"
      ],
      "id": "e7fa2fc7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.svm import SVC\n",
        "X = np.array([TRAIN[\"dog\"], TRAIN[\"hike\"]])\n",
        "X = X.transpose()\n",
        "print(X)"
      ],
      "id": "241e83c3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The classes of the training data"
      ],
      "id": "50e3a6e8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y = TRAIN_Labels\n",
        "print(y)\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn import preprocessing\n",
        "lb = preprocessing.LabelBinarizer()\n",
        "y=lb.fit_transform(y)\n",
        "\n",
        "y = np.array(y)\n",
        "y = y.ravel()  ## to make it the right 1D array type\n",
        "\n",
        "print(y)"
      ],
      "id": "2a3766e9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here - we need to make y into 0 or 1 so it will plot\n",
        "\n",
        "### TRAIN\n",
        "Define the model with SVC\n",
        "\n",
        "Fit SVM with training data"
      ],
      "id": "3051f3ef"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "clf = SVC(C=1, kernel=\"linear\")\n",
        "clf.fit(X, y) \n",
        "\n",
        "\n",
        "margin = 2 / np.sqrt(np.sum(clf.coef_ ** 2))"
      ],
      "id": "4beea7a9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "get the separating hyperplane\n"
      ],
      "id": "a9d55557"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# The weights vector w\n",
        "w = clf.coef_[0]\n",
        "#print(\"The weight vector \", w)"
      ],
      "id": "b42983e4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#The slope of the SVM sep line\n",
        "a = -w[0] / w[1]\n",
        "#print(\"The slope of the SVM sep line is \", a)"
      ],
      "id": "5db3f5e2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Create a variable xx that are values between 4 and 8\n",
        "xx = np.linspace(0, 10)"
      ],
      "id": "8dff9814",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Equation of sep line in 2D\n",
        "$ x_1  = - \\frac{b}{w_1}  - (\\frac{w_0}{w_1})(x_0)$\n",
        "- Note that `clf.intercept_[0]` is \"b\"\n",
        "- Note that `a  = -w0/w1` and `xx` are a bunch of x values\n",
        "\n",
        "This is the y values for the main sep line"
      ],
      "id": "21366455"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "yy = a * xx - (clf.intercept_[0]) / w[1]"
      ],
      "id": "6cde9b9d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These plot the two parellel margin lines plot the parallel lines to the separating hyperplane that pass through the support vectors and note the `margin = 2 / np.sqrt(np.sum(clf.coef_ ** 2))`\n",
        "\n",
        "translate the location of the center sep line by adding or subtracting a fraaction of the margin "
      ],
      "id": "52e4133c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "yy_down = yy + .5*margin\n",
        "yy_up = yy - .5*margin"
      ],
      "id": "6927daf1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "plot the line, the points, and the nearest vectors to the plane"
      ],
      "id": "b1cd4e0c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#plt.figure(fignum, figsize=(4, 3))\n",
        "plt.clf()\n",
        "plt.plot(xx, yy, 'r-')\n",
        "plt.plot(xx, yy_down, 'k--')\n",
        "plt.plot(xx, yy_up, 'k--')\n",
        "\n",
        "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=10,\n",
        "                facecolors='none', zorder=5)\n",
        "# cmap is the color map\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, zorder=5, cmap=plt.cm.Paired)\n",
        "\n",
        "plt.axis('tight')"
      ],
      "id": "9f8cdd64",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}