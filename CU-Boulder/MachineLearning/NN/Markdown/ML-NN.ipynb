{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Machine Learning - Neural Networks Module\n",
        "author: Professor Ami Gates\n",
        "format:\n",
        "  html:\n",
        "    code-fold: false\n",
        "execute:\n",
        "  output: true\n",
        "  warning: false\n",
        "toc: true\n",
        "---"
      ],
      "id": "6eb3c719"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Networks Example Code From Machine Learning Course\n",
        "\n",
        "ANN - CNN - RNN"
      ],
      "id": "aa64bafd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import re  \n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "## For Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import os\n",
        "\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import string\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import Counter\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "#nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "id": "bf54d9d9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Movies Dataset:\n",
        " <https://www.kaggle.com/datasets/columbine/imdb-dataset-sentiment-analysis-in-csv-format>\n"
      ],
      "id": "b96b8489"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "path = os.path.dirname(os.path.realpath(__file__))\n",
        "\n",
        "TrainData = pd.read_csv(str(path+\"Train.csv\"))\n",
        "# print(TrainData.shape)\n",
        "print(TrainData.head(10))\n",
        "# print(type(TrainData))"
      ],
      "id": "7f4a76e8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "TestData = pd.read_csv(str(path+\"Test.csv\"))\n",
        "#print(TestData.shape)\n",
        "TestData.head(10)"
      ],
      "id": "b124fcc2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ValidData = pd.read_csv(str(path+\"Valid.csv\"))\n",
        "#print(ValidData.shape)\n",
        "ValidData.head(10)"
      ],
      "id": "aa3344cb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## Concat requires a list\n",
        "## Place all data from above into one dataframe\n",
        "FullDataset=pd.concat([TrainData,TestData, ValidData])\n",
        "#print(FullDataset.shape)"
      ],
      "id": "0b806eeb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code represents a morehands-on option for tokenizing/vectorizing the data. I am leaving it here as a reference\n",
        "\n",
        "BELOW this comment area - `CountVectorizer` is used to perform the same tasks. \n"
      ],
      "id": "3d656bf9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# def remove_html(text):\n",
        "#     bs = BeautifulSoup(text, \"html.parser\")\n",
        "#     return ' ' + bs.get_text() + ' '\n",
        " \n",
        "# def keep_only_letters(text):\n",
        "#     text=re.sub(r'[^a-zA-Z\\s]',' ',text)\n",
        "#     return text\n",
        " \n",
        "# def convert_to_lowercase(text):\n",
        "#     return text.lower()\n",
        "\n",
        "# def remove_small_words(text):\n",
        "#     text=\" \".join(word for word in text.split() if len(word)>=3)\n",
        "#     return text\n",
        "    \n",
        "    \n",
        "# def clean_reviews(text):\n",
        "#     text = remove_html(text)\n",
        "#     text = keep_only_letters(text)\n",
        "#     text = convert_to_lowercase(text)\n",
        "#     text = remove_small_words(text)\n",
        "#     return text\n",
        "\n",
        "# def returnNewDF(oldDF):\n",
        "#     newDF=pd.DataFrame(columns=[\"key\", \"value\"])\n",
        "#     if not oldDF[\"key\"] in stopwords.words():\n",
        "#         newDF[\"key\"] = oldDF[\"value\"]\n",
        "#     return newDF\n",
        "\n",
        "\n",
        " \n",
        "# TrainData[\"text\"] = TrainData[\"text\"].apply(lambda text: clean_reviews(text))\n",
        "# print(TrainData.head(30))\n",
        "\n",
        "# TestData[\"text\"] = TestData[\"text\"].apply(lambda text: clean_reviews(text))\n",
        "# print(TestData.head(30))\n",
        "\n",
        "# ValidData[\"text\"] = ValidData[\"text\"].apply(lambda text: clean_reviews(text))\n",
        "# print(ValidData.head(30))\n",
        "\n",
        "\n",
        "# ## Create Vocab\n",
        "# counter = Counter([words for reviews in TrainData[\"text\"] for words in reviews.split()])\n",
        "# df = pd.DataFrame()\n",
        "# df['key'] = counter.keys()\n",
        "# df['value'] = counter.values()\n",
        "# df.sort_values(by='value', ascending=False, inplace=True)\n",
        "# print(df.head(10))\n",
        "\n",
        "# ## Drop all the stopwords - OPTIONAL - .............\n",
        "# #df = df[~df.key.isin(stopwords.words())]\n",
        "# #print(stopwords.words())"
      ],
      "id": "1926a704",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean Up TrainData\n",
        "\n",
        "Get the vocab"
      ],
      "id": "5d0c8e5a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#print(TrainData.head())\n",
        "# Testing iterating the columns \n",
        "for col in TrainData.columns: \n",
        "    print(col) "
      ],
      "id": "73610a8a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check Content  "
      ],
      "id": "286f30cd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(TrainData[\"text\"])\n",
        "print(TrainData[\"label\"]) ##0 is negative, 1 is positive"
      ],
      "id": "1df248b7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenize and Vectorize \n",
        "\n",
        "- Create the list \n",
        "- Keep the labels"
      ],
      "id": "c4b91fd3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ReviewsLIST=[]  ## from the text column\n",
        "LabelLIST=[]    \n",
        "\n",
        "for nextreview, nextlabel in zip(TrainData[\"text\"], TrainData[\"label\"]):\n",
        "    ReviewsLIST.append(nextreview)\n",
        "    LabelLIST.append(nextlabel)"
      ],
      "id": "ee015a34",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"A Look at some of the reviews list is:\\n\")\n",
        "print(ReviewsLIST[0:20])"
      ],
      "id": "a0383ed0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"A Look at some of the labels list is:\\n\")\n",
        "print(LabelLIST[0:20])"
      ],
      "id": "12f1bff2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional - for Stemming the data\n",
        "\n",
        "Instantiate it"
      ],
      "id": "d0fa3a48"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "A_STEMMER=PorterStemmer()"
      ],
      "id": "400fcbb0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test it"
      ],
      "id": "ee3142c7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(A_STEMMER.stem(\"fishers\"))"
      ],
      "id": "701b2b26",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Use NLTK's `PorterStemmer` in a function - DEFINE THE FUNCTION\n"
      ],
      "id": "98a8c2c8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def MY_STEMMER(str_input):\n",
        "    ## Only use letters, no punct, no nums, make lowercase...\n",
        "    words = re.sub(r\"[^A-Za-z\\-]\", \" \", str_input).lower().split()\n",
        "    words = [A_STEMMER.stem(word) for word in words] ## Use the Stemmer...\n",
        "    return words"
      ],
      "id": "9528a6b5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Build the labeled dataframe\n",
        "Get the Vocab  - here keeping top 10,000\n",
        "\n",
        "### Vectorize\n"
      ],
      "id": "b8178e90"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Instantiate your CV\n",
        "MyCountV=CountVectorizer(\n",
        "        input=\"content\",  \n",
        "        lowercase=True, \n",
        "        #stop_words = \"english\", ## This is optional\n",
        "        #tokenizer=MY_STEMMER, ## Stemming is optional\n",
        "        max_features=11000  ## This can be updated\n",
        "        )\n",
        "\n",
        "## Use your CV \n",
        "MyDTM = MyCountV.fit_transform(ReviewsLIST)  # create a sparse matrix\n",
        "print(type(MyDTM))"
      ],
      "id": "f159bef8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ColumnNames=MyCountV.get_feature_names() ## This is the vocab\n",
        "print(ColumnNames)\n",
        "print(type(ColumnNames))"
      ],
      "id": "86166b09",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we can clean up the columns"
      ],
      "id": "9ee8408e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## Build the data frame\n",
        "MyDTM_DF=pd.DataFrame(MyDTM.toarray(),columns=ColumnNames)\n",
        "\n",
        "## Convert the labels from list to df\n",
        "Labels_DF = pd.DataFrame(LabelLIST,columns=['LABEL'])\n",
        "\n",
        "## Check your new DF and you new Labels df:\n",
        "# print(\"Labels\\n\")\n",
        "print(Labels_DF)\n",
        "# print(\"DF\\n\")\n",
        "print(MyDTM_DF.iloc[:,0:20])\n",
        "print(MyDTM_DF.shape) ## 40,000 by 11000"
      ],
      "id": "c8855b7a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remove any columns that contain numbers\n",
        "\n",
        "Remove columns with words not the size you want. For example, words $<3$ chars\n"
      ],
      "id": "49516a14"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# DEFINE A FUNCTION that returns True if numbers are in a string \n",
        "def Logical_Numbers_Present(anyString):\n",
        "    return any(char.isdigit() for char in anyString)"
      ],
      "id": "381b9f1d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for nextcol in MyDTM_DF.columns:\n",
        "    #print(nextcol)\n",
        "    ## Remove unwanted columns\n",
        "    #Result=str.isdigit(nextcol) ## Fast way to check numbers\n",
        "    #print(Result)\n",
        "    \n",
        "    ##-------------call the function -------\n",
        "    LogResult=Logical_Numbers_Present(nextcol)\n",
        "    #print(LogResult)\n",
        "    ## The above returns a logical of True or False\n",
        "    \n",
        "    ## The following will remove all columns that contains numbers\n",
        "    if(LogResult==True):\n",
        "        #print(LogResult)\n",
        "        #print(nextcol)\n",
        "        MyDTM_DF=MyDTM_DF.drop([nextcol], axis=1)\n",
        "\n",
        "    ## The following will remove any column with name\n",
        "    ## of 3 or smaller - like \"it\" or \"of\" or \"pre\".\n",
        "    ## print(len(nextcol))  ## check it first\n",
        "    ## NOTE: You can also use this code to CONTROL\n",
        "    ## the words in the columns. For example - you can\n",
        "    ## have only words between lengths 5 and 9. \n",
        "    ## In this case, we remove columns with words <= 3.\n",
        "    elif(len(str(nextcol))<3):\n",
        "        print(nextcol)\n",
        "        MyDTM_DF=MyDTM_DF.drop([nextcol], axis=1)"
      ],
      "id": "cbec4c80",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save original DF - without the lables"
      ],
      "id": "fc0f1536"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "My_Orig_DF=MyDTM_DF\n",
        "print(My_Orig_DF)"
      ],
      "id": "8c944eac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Now - let's create a complete and labeled dataframe:"
      ],
      "id": "8bad0153"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dfs = [Labels_DF, MyDTM_DF]\n",
        "print(dfs)\n",
        "print(\"shape of labels\\n\", Labels_DF)\n",
        "print(\"shape of data\\n\", MyDTM_DF)\n",
        "\n",
        "Final_DF_Labeled = pd.concat(dfs,axis=1, join='inner')"
      ],
      "id": "41ed3e30",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DF with labels"
      ],
      "id": "ab48ccfd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(Final_DF_Labeled.iloc[:, 0:2])\n",
        "print(Final_DF_Labeled.shape)"
      ],
      "id": "cd14a6f0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**FYI**\n",
        "\n",
        "An alternative option for most frequent 10,000 words \n",
        "\n",
        "Not needed here as we used CountVectorizer with option `max_features`"
      ],
      "id": "52854c49"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# print (df.shape[0])\n",
        "# print (df[:10000].value.sum()/df.value.sum())\n",
        "# top_words = list(df[:10000].key.values)\n",
        "# print(top_words)\n",
        "# ## Example using index\n",
        "# index = top_words.index(\"humiliating\")\n",
        "# print(index)"
      ],
      "id": "15237c1b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create list of all words"
      ],
      "id": "1cf06f47"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(Final_DF_Labeled.columns[0])\n",
        "NumCols=Final_DF_Labeled.shape[1]\n",
        "print(NumCols)\n",
        "print(len(list(Final_DF_Labeled.columns)))"
      ],
      "id": "ef17fce2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exclude the Label"
      ],
      "id": "c9044f58"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "top_words=list(Final_DF_Labeled.columns[1:NumCols+1])\n",
        "\n",
        "print(top_words[0])\n",
        "print(top_words[-1])"
      ],
      "id": "429f7a79",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(type(top_words))\n",
        "print(top_words.index(\"aamir\")) ## index 0 in top_words\n",
        "print(top_words.index(\"zucco\")) #index NumCols - 2 in top_words"
      ],
      "id": "61c927ac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Encoding the data"
      ],
      "id": "a985bc53"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def Encode(review):\n",
        "    words = review.split()\n",
        "   # print(words)\n",
        "    if len(words) > 500:\n",
        "        words = words[:500]\n",
        "        #print(words)\n",
        "    encoding = []\n",
        "    for word in words:\n",
        "        try:\n",
        "            index = top_words.index(word)\n",
        "        except:\n",
        "            index = (NumCols - 1)\n",
        "        encoding.append(index)\n",
        "    while len(encoding) < 500:\n",
        "        encoding.append(NumCols)\n",
        "    return encoding"
      ],
      "id": "855a4ae6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the code to assure that it is doing what you think it should"
      ],
      "id": "0fea2f4f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result1 = Encode(\"aaron aamir abbey abbott abilities zucco \")\n",
        "print(result1)\n",
        "result2 = Encode(\"york young younger youngest youngsters youth youthful youtube zach zane zany zealand zellweger\")\n",
        "print(result2)\n",
        "print(len(result2)) ## Will be 500 because we set it that way above"
      ],
      "id": "f2a6de3e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final Training and Testing data and labels\n",
        "Now we are ready to encode all of our reviews - which are called \"text\" in our dataset. \n",
        "\n",
        "Using `vocab` from above `i` -  convert reviews (text) into numerical form \n",
        "\n",
        "Replacing each word with its corresponding integer index value from the vocabulary. Words not in the vocab will be assigned as the max length of the `vocab + 1`\n",
        "\n",
        "## Encode our training and testing datasets with same vocab."
      ],
      "id": "835e9b25"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(TestData.head(10))\n",
        "print(TestData.shape)\n",
        "print(TrainData.shape)"
      ],
      "id": "53fe9d6e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "training_data = np.array([Encode(review) for review in TrainData[\"text\"]])\n",
        "print(training_data[20])\n",
        "print(training_data.shape)"
      ],
      "id": "19ade8fa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "testing_data = np.array([Encode(review) for review in TestData['text']])\n",
        "print(testing_data[20])"
      ],
      "id": "8500b30d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "validation_data = np.array([Encode(review) for review in ValidData['text']])\n",
        "print (training_data.shape, testing_data.shape)"
      ],
      "id": "6d735c95",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prepare the labels if they are not already 0 and 1. In our case they are so these lines are commented out and just FYI"
      ],
      "id": "e370232c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#train_labels = [1 if label=='positive' else 0 for sentiment in TrainData['label']]\n",
        "#test_labels = [1 if label=='positive' else 0 for sentiment in TestData['label']]"
      ],
      "id": "6b78cfe1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Labels"
      ],
      "id": "6300ca21"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "train_labels = np.array([TrainData['label']])\n",
        "train_labels=train_labels.T\n",
        "print(train_labels.shape)\n",
        "test_labels = np.array([TestData['label']])\n",
        "test_labels=test_labels.T\n",
        "print(test_labels.shape)"
      ],
      "id": "15fa8bfc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ANN\n",
        "\n",
        "Simple Dense NN for sentiment analysis (classification 0 neg, 1 pos)\n",
        "\n",
        "First layer: Embedding Layer (Keras Embedding Layer) that will learn embeddings for different words.\n",
        "\n",
        "RE: <https://keras.io/api/layers/core_layers/embedding/>\n",
        "\n",
        "- `input_dim`: Integer. Size of the vocabulary\n",
        "- `input_length`: Length of input sequences, when it is constant\n"
      ],
      "id": "d9d15d5a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.layers import Activation"
      ],
      "id": "9df0f046",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<https://www.tensorflow.org/api_docs/python/tf/keras/Input>"
      ],
      "id": "4cc9b303"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(NumCols)   \n",
        "input_dim = NumCols + 1 \n",
        "input_data = tensorflow.keras.layers.Input(shape=(500))"
      ],
      "id": "394365b3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding>\n"
      ],
      "id": "5ca0241c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data = tensorflow.keras.layers.Embedding(input_dim=input_dim, output_dim=64, input_length=500)(input_data)"
      ],
      "id": "e59e29c5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Good tutorial for this concept:\n",
        "<https://medium.com/analytics-vidhya/understanding-embedding-layer-in-keras-bbe3ff1327ce>\n",
        "- `input_dim`: Integer. Size of the vocabulary, i.e. maximum integer `index + 1`\n",
        "- `output_dim`: Integer. Dimension of the dense embedding.\n",
        "    - This is the size of the vector space in which words will be embedded. \n",
        "    - It defines the size of the output vectors from this layer for each word. \n",
        "        - For example, it could be 32 or 100 or even larger.\n",
        "        - <https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/>\n",
        "\n",
        "In an embedding, words are represented by dense vectors where a vector represents the projection of the word into a continuous vector space.\n",
        "\n",
        "The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used.\n",
        "\n",
        "The position of a word in the learned vector space is referred to as its embedding.\n",
        "\n",
        "```\n",
        "data = tensorflow.keras.layers.Flatten()(data)\n",
        "```\n",
        "\n",
        "Dense layers require inputs as `(batch_size, input_size)` "
      ],
      "id": "2d8d8731"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data = tensorflow.keras.layers.Dense(16)(data)\n",
        "data = tensorflow.keras.layers.Activation('relu')(data)\n",
        "#data = tensorflow.keras.layers.Dropout(0.5)(data)\n",
        " \n",
        "data = tensorflow.keras.layers.Dense(8)(data)\n",
        "data = tensorflow.keras.layers.Activation('relu')(data)\n",
        "\n",
        "#data = tensorflow.keras.layers.Dropout(0.5)(data)\n",
        " \n",
        "data = tensorflow.keras.layers.Dense(4)(data)\n",
        "data = tensorflow.keras.layers.Activation('sigmoid')(data)\n",
        "#data = tensorflow.keras.layers.Dropout(0.5)(data)\n",
        " \n",
        "data = tensorflow.keras.layers.Dense(1)(data)\n",
        "output_data = tensorflow.keras.layers.Activation('sigmoid')(data)\n",
        " \n",
        "model = tensorflow.keras.models.Model(inputs=input_data, outputs=output_data)\n",
        " \n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics='accuracy')\n",
        "model.summary()"
      ],
      "id": "e61e78a3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(training_data[0:3, 0:3])\n",
        "print(training_data.shape)\n",
        "model.fit(training_data, train_labels, epochs=10, batch_size=256, validation_data=(testing_data, test_labels))"
      ],
      "id": "b3842ad1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RNN\n"
      ],
      "id": "91866c2f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.layers import Activation"
      ],
      "id": "22c08823",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "input_data = tensorflow.keras.layers.Input(shape=(500))\n",
        " \n",
        "data = tensorflow.keras.layers.Embedding(input_dim=input_dim, output_dim=32, input_length=500)(input_data)"
      ],
      "id": "53aeaf66",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional>\n"
      ],
      "id": "4f6d73d8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data = tensorflow.keras.layers.Bidirectional(tensorflow.keras.layers.SimpleRNN(50))(data)\n",
        " \n",
        "data = tensorflow.keras.layers.Dense(1)(data)\n",
        "output_data = tensorflow.keras.layers.Activation('sigmoid')(data)\n",
        " \n",
        "model = tensorflow.keras.models.Model(inputs=input_data, outputs=output_data)\n",
        " \n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics='accuracy')\n",
        "model.summary()\n",
        "\n",
        "model.fit(training_data, train_labels, epochs=10, batch_size=256, validation_data=(testing_data, test_labels))"
      ],
      "id": "70575ee2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LSTM\n"
      ],
      "id": "3aa32f7b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.layers import Activation\n",
        " \n",
        "input_data = tensorflow.keras.layers.Input(shape=(500))\n",
        " \n",
        "data = tensorflow.keras.layers.Embedding(input_dim=input_dim, output_dim=32, input_length=500)(input_data)\n",
        " \n",
        "data = tensorflow.keras.layers.Bidirectional(tensorflow.keras.layers.LSTM(50))(data)\n",
        " \n",
        "data = tensorflow.keras.layers.Dense(1)(data)\n",
        "output_data = tensorflow.keras.layers.Activation('sigmoid')(data)\n",
        " \n",
        "model = tensorflow.keras.models.Model(inputs=input_data, outputs=output_data)\n",
        " \n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics='accuracy')\n",
        "model.summary()\n",
        "\n",
        "model.fit(training_data, train_labels, epochs=10, batch_size=128, validation_data=(testing_data, test_labels))"
      ],
      "id": "4645b1ef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CNN\n"
      ],
      "id": "25b4f7b5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import tensorflow\n",
        " \n",
        "input_data = tensorflow.keras.layers.Input(shape=(500))\n",
        " \n",
        "data = tensorflow.keras.layers.Embedding(input_dim=input_dim, output_dim=32, input_length=500)(input_data)\n",
        " \n",
        "data = tensorflow.keras.layers.Conv1D(50, kernel_size=3, activation='relu')(data)\n",
        "data = tensorflow.keras.layers.MaxPool1D(pool_size=2)(data)\n",
        " \n",
        "data = tensorflow.keras.layers.Conv1D(40, kernel_size=3, activation='relu')(data)\n",
        "data = tensorflow.keras.layers.MaxPool1D(pool_size=2)(data)\n",
        " \n",
        "data = tensorflow.keras.layers.Conv1D(30, kernel_size=3, activation='relu')(data)\n",
        "data = tensorflow.keras.layers.MaxPool1D(pool_size=2)(data)\n",
        " \n",
        "data = tensorflow.keras.layers.Conv1D(30, kernel_size=3, activation='relu')(data)\n",
        "data = tensorflow.keras.layers.MaxPool1D(pool_size=2)(data)\n",
        " \n",
        "data = tensorflow.keras.layers.Flatten()(data)\n",
        " \n",
        "data = tensorflow.keras.layers.Dense(20)(data)\n",
        "data = tensorflow.keras.layers.Dropout(0.5)(data)\n",
        " \n",
        "data = tensorflow.keras.layers.Dense(1)(data)\n",
        "output_data = tensorflow.keras.layers.Activation('sigmoid')(data)\n",
        " \n",
        "model = tensorflow.keras.models.Model(inputs=input_data, outputs=output_data)\n",
        " \n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics='accuracy')\n",
        "model.summary()\n",
        "\n",
        "model.fit(training_data, train_labels, epochs=10, batch_size=256, validation_data=(testing_data, test_labels))"
      ],
      "id": "7f449010",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Evaluate model on test data\")\n",
        "results = model.evaluate(testing_data, test_labels, batch_size=256)\n",
        "print(\"test loss, test acc:\", results)"
      ],
      "id": "02a2fd06",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate a prediction using `model.predict()` and calculate it's shape"
      ],
      "id": "8a07461b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Generate a prediction\")\n",
        "prediction = model.predict(testing_data)\n",
        "print(prediction)\n",
        "print(\"prediction shape:\", prediction.shape)\n",
        "print(type(prediction))\n",
        "prediction[prediction > .5] = 1\n",
        "prediction[prediction <= .5] = 0\n",
        "print(prediction)"
      ],
      "id": "64dadd9a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Confusion Matrix"
      ],
      "id": "6bfb3062"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(prediction, test_labels))"
      ],
      "id": "9f661928",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}