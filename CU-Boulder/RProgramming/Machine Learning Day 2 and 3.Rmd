---
title: "Machine Learning Day 2 and 3"
author: "Dr. Osita Onyejekwe"
date: "10/31/2022"
output: html_document
---

## Controlling prevalence

One nice feature of the naive Bayes approach is that it includes a parameter to account for differences in prevalence. Using our sample we estimated $f_{X|Y=1}$, $f_{X|Y=0}$ and $\pi$. If we use hats to denote the estimates we can write $\hat{p}(x)$ as

$$
\hat{p}(x)= \frac{\hat{f}_{X|Y=1}(x) \hat{\pi}}
{ \hat{f}_{X|Y=0}(x)(1-\hat{\pi}) + \hat{f}_{X|Y=1}(x)\hat{\pi} }
$$
As we discussed, our sample has a much lower prevalence, `r pi`, than the general population. So if we use the rule $\hat{p}(x)>0.5$ to predict females our accuracy will be affected due to the low sensitivity: 

```{r}
y_hat_bayes <- ifelse(p_hat_bayes > 0.5, "Female", "Male")
sensitivity(data = factor(y_hat_bayes), reference = factor(test_set$sex))
```

Again, this is because the algorithm gives more weight to specificity to account for the low prevalence:
```{r}
specificity(data = factor(y_hat_bayes), reference = factor(test_set$sex))
```

This is due mainly to the fact that $\hat{\pi}$ is substantially less than 0.5 so we tend to predict `Male` more often. It makes sense for a machine learning algorithm to do this in our sample, because we do have a higher percentage of males. But if we were to extrapolate this to a general population our overall accuracy would be affected by the low sensitivity. 

The naive Bayes approach gives us a direct way to correct this since we can simply force $\hat{pi}$ to be, for example, $\pi$. So to balance specificity and sensitivity, instead of changing the cutoff in the decision rule we could simply change $\hat{\pi}$:

```{r}
p_hat_bayes_unbiased <- f1*0.5 / (f1*0.5 + f0*(1-0.5)) 
y_hat_bayes_unbiased <- ifelse(p_hat_bayes_unbiased > 0.5, "Female", "Male")
```

Note the difference in sensitivity with a better balance:

```{r}
confusionMatrix(data = as.factor(y_hat_bayes_unbiased), reference = test_set$sex)
```


The new rule also gives us a very intuitive cutoff of 67, which is about the middle of the female and male average heights:


```{r}
qplot(x, p_hat_bayes_unbiased) + geom_hline(yintercept = 0.5) + geom_vline(xintercept = 67)
```

Now that we know how to perform Naive Bayes by hand and understand the math behind it, we can use the `naiveBayes` function from the `e1071` package to do these calculations automatically. Here, `naiveBayes` outputs the prediction class automatically with a default cutoff of 0.5 for the probabilities. Thus, we don't need to classify a value as Female or Male using a threshold by hand.

We see we get the exact same confusion matrix we got before we changed the prevalence.

```{r}
nb_fit   <- naiveBayes(sex ~ height, data = train_set)
y_hat_nb <- predict(nb_fit, test_set)
confusionMatrix(data = as.factor(y_hat_nb), reference = test_set$sex)
```





# Two Predictors  

In the two simple examples above we only had one predictor. We actually do not consider these machine learning challenges, which are characterized by including many predictors. Let's go back to the digits example in which we had 784 predictors. 
For illustrative purposes we will build an example with 2 features and only two classes, 2s and 7s. Then we will go back to the original 784 feature example.

```{r, echo=FALSE}
if(!exists("digits")){
  url <- "https://raw.githubusercontent.com/datasciencelabs/data/master/hand-written-digits-train.csv"
digits <- read_csv(url)
}
```
First let's filter to include only 2s and 7s and change the labels from numbers to factors. This second step is important to assure that R does not treat the 2 and 7 as numbers.

```{r}
digits_27 <- digits %>% filter(label %in% c(2,7)) %>%
  mutate(label =  as.character(label))
```

We note that to distinguish 2s from 7s it might be enough to look at the number of non-white pixels in the upper-left and lower-bottom quadrants:

```{r, echo=FALSE}
tmp <- lapply( c(40,45), function(i){
    expand.grid(Row=1:28, Column=1:28) %>%  
      mutate(id=i, label=digits_27$label[i],  
             value = unlist(digits_27[i,-1])) 
})
tmp <- Reduce(rbind, tmp)
tmp %>% ggplot(aes(Row, Column, fill=value)) + 
    geom_raster() + 
    scale_y_reverse() +
    scale_fill_gradient(low="white", high="black") +
    facet_grid(.~label) + 
    geom_vline(xintercept = 14.5) +
    geom_hline(yintercept = 14.5)
```


So we will define two features $X_1$ and $X_2$ as the percent of non-white pixels in these two quadrants respectively. We add these two features to the `digits_27` table

```{r}
row_column <- expand.grid(row=1:28, col=1:28)
ind1 <- which(row_column$col <= 14 & row_column$row <=14)
ind2 <- which(row_column$col > 14 & row_column$row > 14) 
ind <- c(ind1,ind2)
X <- as.matrix(digits_27[,-1])
X <- X > 200
X1 <- rowSums(X[,ind1])/rowSums(X)
X2 <- rowSums(X[,ind2])/rowSums(X)
digits_27 <- digits_27 %>% 
  mutate(y = ifelse(label == "7", 1, 0),
         X_1 = X1, X_2 = X2)
```

For illustrative purposes we consider this to be the population and use this data to define a function $p(X_1, X_2)$. Here is the conditional probability of being a 7 as a function of $(X_1, X_2)$.

```{r, echo=FALSE, cache=TRUE}
y <- as.factor(digits_27$label)
x <- cbind(X1, X2)
library(RColorBrewer)
library(caret)
fit <- knn3(x, y, 401)
GS <- 150
X1s <- seq(min(X1),max(X1),len=GS)
X2s <- seq(min(X2),max(X2),len=GS)
p_x <- expand.grid(X_1=X1s, X_2=X2s)
yhat <- predict(fit, newdata = p_x, type="prob")[,2]
p_x <- mutate(p_x, yhat=yhat)
fit_loess<- loess(yhat ~ X_1*X_2, data=p_x, 
           degree=1, span=1/5)$fitted
p_x <- p_x %>% mutate(p = fit_loess) 
p_x_plot <- p_x %>%
  ggplot(aes(X_1, X_2, fill=fit_loess))  +
  scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4"))+
  geom_raster() 
p_x_plot
```

Next we will take a smaller random sample to mimic our training data as well as our test data.

```{r}
set.seed(1971)
dat <- sample_n(digits_27, 1000)
```

We start by creating a train and test set using the `caret` package:

```{r}
library(caret)
index_train<- createDataPartition(y = dat$label, times =1, p=0.5, list = FALSE)
train_set <- slice(dat, index_train)
test_set <- slice(dat, -index_train)
```

We can visualize the training data now using color to denote the classes:

```{r}
train_set %>% 
  ggplot(aes(X_1, X_2, fill = label)) +
  geom_point(pch=21) 
```

Let's try logistic regression. The model is simply:

$$ g(\mbox{Pr}(Y=1 \mid X_1=x_1 , X_2 = x_2) = 
\beta_0 + \beta_1 x_1 + \beta_2 x_2$$

and we fit it like this:

```{r}
fit <-  glm(y ~ X_1 + X_2, data=train_set, family="binomial")
```

```{r}
p_hat <- predict(fit, newdata = test_set)
y_hat <- ifelse(p_hat > 0.5, 1, 0)
confusionMatrix(data = as.factor(y_hat), reference = as.factor(test_set$y))
```

Since we are using 0.5 as our cutoff, and the $\log\{0.5 / (1-0.5) \} = 0$ we know that the decisiotn rule is to call a 7 if
$\hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 > 0$ and 2 otherwise. This implies that the function 

$$
\hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 = 0 \implies x_2 = - \hat{\beta}_0/\hat{\beta}_2 - \hat{\beta}_1 x_1/ \hat{\beta}_2
$$

splits the $x_1, x_2$ plane in areas in which we call twos and areas in which we call sevens. 

```{r}
train_set %>% ggplot(aes(X_1, X_2, fill = label)) +
  geom_point(pch=21) +
  geom_abline(intercept = -fit$coef[1]/fit$coef[3],
              slope = -fit$coef[2]/fit$coef[3])
```

The estimate $\hat{p}(x_1, x_2)$ does not approximate the $p(x_1, x_2)$ very well:

```{r}
p_x %>% mutate(p = predict(fit, newdata = .)) %>%
  ggplot(aes(X_1, X_2, fill=p))  +
  scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4"))+ geom_raster() 
```

Given the shape of $p(x_1, x_2)$ it is impossible for a logistic regression model to provide a decent estimate because with logistic regression the estimate can only be a plane and the true conditional probability is not:

```{r}
p_x_plot
```

We will learn other machine learning algorithms that provide more flexibility. 

## Multiple predictors

What if we have two or more predictors? Here it is helpful to understand the concept of _distance_.


## Distance

The concept of distance is quite intuitive. For example, when we cluster animals into subgroups, we are implicitly defining a distance that permits us to say what animals are "close" to each other.

![Clustering of animals.](https://raw.githubusercontent.com/genomicsclass/labs/master/highdim/images/handmade/animals.png)

Many of the analyses we perform with high-dimensional data relate directly or indirectly to distance. Many clustering and machine learning techniques rely on being able to define distance, using features or predictors. 

## Euclidean Distance

As a review, let's define the distance between two points, $A$ and $B$, on a Cartesian plane.

```{r,echo=FALSE,fig.cap=""}
library(rafalib)
mypar()
plot(c(0,1,1),c(0,0,1),pch=16,cex=2,xaxt="n",yaxt="n",xlab="",ylab="",bty="n",xlim=c(-0.25,1.25),ylim=c(-0.25,1.25))
lines(c(0,1,1,0),c(0,0,1,0))
text(0,.2,expression(paste('(A'[x]*',A'[y]*')')),cex=1.5)
text(1,1.2,expression(paste('(B'[x]*',B'[y]*')')),cex=1.5)
text(-0.1,0,"A",cex=2)
text(1.1,1,"B",cex=2)
```

The euclidean distance between $A$ and $B$ is simply:

$$\sqrt{ (A_x-B_x)^2 + (A_y-B_y)^2}$$


## Distance in High Dimensions

Earlier we introduced a training dataset with feature matrix measurements for 784 features for 500 digits. 


```{r}
sample_n(train_set,10) %>% select(label, pixel351:pixel360) 
```

We are interested in describing the distance between observations, in this case digits. Later, for the purposes of selecting features, we might also be interested in finding pixels that _behave similarly_ across samples.

To define distance, we need to know what the points are since mathematical distance is computed between points. With high dimensional data, points are no longer on the Cartesian plane. Instead they are in higher dimensions. For example, observation $i$ is defined by a point in 784 dimensional space: $(Y_{i,1},\dots,Y_{i,784})^\top$. Feature $j$ is defined by a point in 500 dimensions $(Y_{1,j},\dots,Y_{500,j})^\top$

Once we define points, the Euclidean distance is defined in a very similar way as it is defined for two dimensions. For instance, the distance between two observations, say observations $i=1$ and $i=2$ is:

$$
\mbox{dist}(1,2) = \sqrt{ \sum_{j=1}^{784} (Y_{1,j}-Y_{2,j })^2 }
$$

and the distance between two features, say, $15$ and $273$ is:

$$
\mbox{dist}(15,273) = \sqrt{ \sum_{i=1}^{500} (Y_{i,15}-Y_{i,273})^2 }
$$


#### Example

The first thing we will do is create a _matrix_ with the predictors

```{r}
X <- select(train_set , pixel0:pixel783) %>% as.matrix()
```

Rows and columns of matrices can be accessed like this:

```{r}
third_row <- X[3,]
tenth_column <- X[,10]
```

So the first two observations are 7s and the 253rd is a 2. Let's see if their distances match this:
```{r}
X_1 <- X[1,]
X_2 <- X[2,]
X_253 <- X[253,]
sqrt(sum((X_1-X_2)^2))
sqrt(sum((X_1-X_253)^2))
```

As expected, the 7s are closer to each other. If you know matrix algebra, note that a faster way to compute this is using matrix algebra:

```{r}
sqrt( crossprod(X_1-X_2) )
sqrt( crossprod(X_1-X_253) )
```

Now to compute all the distances at once, we have the function `dist`.

```{r}
d <- dist(X)
class(d)
```


Note that this produces an object of class `dist` and, to access the entries using row and column indices, we need to coerce it into a matrix:

```{r}
as.matrix(d)[1,2]
as.matrix(d)[1,253]
```
We can quickly see an image of these distances

```{r}
image(as.matrix(d))
```

Note that for illustrative purposes we defined two predictors. Defining distances between observations based on these two covariates is much more intuitive since we can simply visualize the distance in a  two dimensional plot

```{r}
ggplot(train_set) + 
  geom_point(aes(X_1, X_2, fill=label), pch=21)
```

#### Distance between predictors

Perhaps a more interesting result comes from computing distance between predictors:

```{r}
image(as.matrix(dist(t(X))))
```


## k Nearest Neighbors

K-nearest neighbors (kNN) is easier to adapt to multiple dimensions. We first define the distance between all observations based on the features. Basically, for any point $\bf{x}$ for which we want an estimate of $p(\bf{x})$, we look for the $k$ nearest points and then take an average of these points. This gives us an estimate of $p(x_1,x_2)$. We can now control flexibility through $k$. 

Let's use our logistic regression as a baseline:

```{r}
library(caret)
glm_fit <- glm(y~.,data = select(train_set, y, X_1, X_2) )
f_hat <- predict(glm_fit, newdata = test_set, 
                 type = "response")
tab <- table(pred=round(f_hat), truth=test_set$y)
confusionMatrix(tab)$tab
confusionMatrix(tab)$overall["Accuracy"]
```

Now, lets compare to kNN. Let's start with the default $k = 5$

```{r}
knn_fit <- knn3(y~.,data = select(train_set, y, X_1, X_2) )
f_hat <- predict(knn_fit, newdata = test_set)[,2]
tab <- table(pred=round(f_hat), truth=test_set$y)
confusionMatrix(tab)$tab
confusionMatrix(tab)$overall["Accuracy"]
```

This already improves accuracy over the logistic model. Let's see why this is:

```{r, echo=FALSE, fig.width=10.5,fig.height=5.25, fig.align="center", message = FALSE }
f_hat <- predict(knn_fit, newdata = p_x)[,2]
g1 <- p_x %>% mutate(f_hat = f_hat) %>%
  ggplot(aes(X_1, X_2, fill=f_hat))  +
  scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) + geom_raster()  + guides(fill=FALSE) +  
  stat_contour(aes(x=X_1,y=X_2,z=f_hat),data=p_x, breaks=c(0.5),color="black",lwd=1.5)
g2 <- ggplot(p_x) +  
  geom_point(data=train_set, aes(x=X_1, y=X_2, fill=label), pch=21) + stat_contour(aes(X_1,X_2,z=f_hat), breaks=c(0.5),color="black",lwd=1.5) 
  
library(gridExtra)
grid.arrange(g1,g2, nrow=1)
``` 

When $k=5$, we see some islands of red in the blue area. This is due to what we call _over training_. Note that we have higher accuracy in the train set compared to the test set:

```{r}
f_hat <- predict(knn_fit, newdata = test_set)[,2]
tab <- table(pred=round(f_hat), truth=test_set$y)
confusionMatrix(tab)$overall["Accuracy"]
f_hat_train <- predict(knn_fit, newdata = train_set)[,2]
tab <- table(pred=round(f_hat_train), truth=train_set$y)
confusionMatrix(tab)$overall["Accuracy"]
```

## Over-training

Over-training is at its worse when we set $k = 1$. In this case we will obtain perfect accuracy in the training set because each point is used to predict itself. So perfect accuracy must happen by definition. However, the test set accuracy is actually worse than logistic regression.

```{r}
knn_fit_1 <- knn3(y~.,data = select(train_set, y, X_1, X_2), k=1)
f_hat <- predict(knn_fit_1, newdata = train_set)[,2]
tab <- table(pred=round(f_hat), truth=train_set$y)
confusionMatrix(tab)$overall["Accuracy"]
f_hat <- predict(knn_fit_1, newdata = test_set)[,2]
tab <- table(pred=round(f_hat), truth=test_set$y)
confusionMatrix(tab)$overall["Accuracy"]
```

We can see the over-fitting problem in this figure:
```{r, echo=FALSE, fig.width=10.5,fig.height=5.25, fig.align="center" }
f_hat <- predict(knn_fit_1, newdata = p_x)[,2]
g1 <- p_x %>% mutate(f_hat = f_hat) %>%
  ggplot(aes(X_1, X_2, fill=f_hat))  +
  scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) + geom_raster()  + guides(fill=FALSE) +  
  stat_contour(aes(x=X_1,y=X_2,z=f_hat),data=p_x, breaks=c(0.5),color="black",lwd=1.5)
g2 <- ggplot(p_x) +  
  geom_point(data=train_set, aes(x=X_1, y=X_2, fill=label),  pch=21) + stat_contour(aes(X_1,X_2,z=f_hat), breaks=c(0.5),color="black",lwd=1.5) 
  
library(gridExtra)
grid.arrange(g1,g2, nrow=1)
``` 

We can also go _over-smooth_. Look at what happens with 251 closest neighbors:

```{r}
knn_fit_251 <- knn3(y~.,data = select(train_set, y, X_1, X_2), k=251)
f_hat <- predict(knn_fit_251, newdata = test_set)[,2]
tab <- table(pred=round(f_hat), truth=test_set$y)
confusionMatrix(tab)$overall["Accuracy"]
```

This turns out to be similar to logistic regression:
```{r, echo=FALSE, fig.width=10.5,fig.height=5.25, fig.align="center" }
f_hat <- predict(knn_fit_251, newdata = p_x)[,2]
g1 <- p_x %>% mutate(f_hat = f_hat) %>%
  ggplot(aes(X_1, X_2, fill=f_hat))  +
  scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) + geom_raster()  + guides(fill=FALSE) +  
  stat_contour(aes(x=X_1,y=X_2,z=f_hat),data=p_x, breaks=c(0.5),color="black",lwd=1.5)
g2 <- ggplot(p_x) +  
  geom_point(data=train_set, aes(x=X_1, y=X_2, fill=label),  pch=21) + stat_contour(aes(X_1,X_2,z=f_hat), breaks=c(0.5),color="black",lwd=1.5) 
  
library(gridExtra)
grid.arrange(g1,g2, nrow=1)
``` 

Let's plot the accuracy for different numbers of closest neighbors.
```{r}
control <- trainControl(method='cv', number=2, p=.5)
dat2 <- mutate(dat, label=as.factor(label)) %>%
  select(label,X_1,X_2)
res <- train(label ~ .,
             data = dat2,
             method = "knn",
             trControl = control,
             tuneLength = 1, # How fine a mesh to go on grid
             tuneGrid=data.frame(k=seq(3,151,2)),
             metric="Accuracy")
plot(res)
```

With $k = 11$ we obtain what appears to be a decent estimate of the true $f$.

```{r, echo=FALSE, fig.width=10.5,fig.height=5.25, fig.align="center" }
knn_fit <- knn3(y ~ .,data = select(train_set, y, X_1, X_2),
                k=11)
f_hat <- predict(knn_fit, newdata = p_x)[,2]
g1 <- p_x %>%
  ggplot(aes(X_1, X_2, fill=p))  +
  scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) + geom_raster()  + guides(fill=FALSE) +  
  stat_contour(aes(x=X_1,y=X_2,z=p),data=p_x, breaks=c(0.5),color="black",lwd=1.5)
g2 <- p_x %>% mutate(f_hat = f_hat) %>%
  ggplot(aes(X_1, X_2, fill=f_hat))  +
  scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) + geom_raster()  + guides(fill=FALSE) +  
  stat_contour(aes(x=X_1,y=X_2,z=f_hat),data=p_x, breaks=c(0.5),color="black",lwd=1.5)
 
library(gridExtra)
grid.arrange(g1,g2, nrow=1)
``` 

An important part of data science is visualizing results to determine why we are succeeding and why we are failing.

```{r, echo=FALSE}
f_hat <- predict(knn_fit, newdata = test_set, k=11)[,2]
high_prob_and_correct_2 <- which(f_hat<0.02 &
                               test_set$label=="2")[1:5]
high_prob_and_incorrect_2 <- which(f_hat<0.2 &
                                   test_set$label=="7")[1:5]
low_prob <-  which(abs(f_hat-0.5)<0.05)[1:5] 
high_prob_and_incorrect_7 <- which(f_hat>0.75 &
                                   test_set$label=="2")[1:5]
high_prob_and_correct_7 <- which(f_hat>0.98 &
                                   test_set$label=="7")[1:5]
plot_it <- function(index){
  tmp <- lapply( index, function(i){
    expand.grid(Row=1:28, Column=1:28) %>%
      mutate(id=as.character(i),
             label=test_set$label[i],  
             value = unlist(test_set[i,2:785])) 
    })
  tmp <- Reduce(rbind,tmp)
  tmp  %>% ggplot(aes(Row, Column, fill=value)) + 
      geom_raster() + 
      scale_y_reverse() +
      scale_fill_gradient(low="white", high="black") +
      geom_vline(xintercept = 14.5) +
    geom_hline(yintercept = 14.5) +  
    facet_grid(.~id)
}
```

Here are some 2s that were correctly called with high probability:
```{r, echo=FALSE, fig.aling="center", fig.width=10}
plot_it(high_prob_and_correct_2)
```

Here are some 7s that were incorrectly called 2s with high probability:
```{r, echo=FALSE, fig.aling="center", fig.width=10}
plot_it(high_prob_and_incorrect_2)
```

Here are some for which the predictor was about 50-50:
```{r, echo=FALSE, fig.aling="center", fig.width=10}
plot_it(low_prob)
```

Here are some 7s that were correctly called with high probability:

```{r, echo=FALSE, fig.aling="center", fig.width=10}
plot_it(high_prob_and_correct_7)
```

Here are some 2s that were incorrectly called with high probability:

```{r, echo=FALSE, fig.aling="center", fig.width=10}
plot_it(high_prob_and_incorrect_7)
```


