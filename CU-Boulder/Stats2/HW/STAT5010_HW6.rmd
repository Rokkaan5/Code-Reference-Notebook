---
title: "STAT 5010: HW6"
author: "Jasmine Kobayashi"
format:
    html:
        code-fold: false
execute:
    output: true
    warning: false
toc: true
---

**See Canvas for this assignment's due date**. Complete all of the following problems. Ideally, the theoretical problems should be answered in a Markdown cell directly underneath the question. If you don't know LaTex/Markdown, you may submit separate handwritten solutions to the theoretical problems. Please do not turn in messy work. Computational problems should be completed in this notebook (using the R kernel is preferred). Computational questions may require code, plots, analysis, interpretation, etc. Working in small groups is allowed, but it is important that you make an effort to master the material and hand in your own work. 


# A. Theoretical Problems

## A.1 Adjusted $R^2$

**[8 points] Prove that the adjusted $R^2$ is always less than $R^2$.**

*YOUR ANSWER HERE*

R-squared:
$$R^2 = 1 - \frac{\text{RSS}}{\text{TSS}}$$

Adjusted R-squared:
$$R_a^2 = 1 - \frac{\text{RSS}/n-(p+1)}{\text{TSS}/(n-1)}$$

Trying to prove
$$R_a^2 < R^2$$
$$1 - \frac{\text{RSS}/n-(p+1)}{\text{TSS}/(n-1)} < 1 - \frac{\text{RSS}}{\text{TSS}} $$

\begin{align*}
1 - \frac{\text{RSS}}{\text{TSS}} \cdot \frac{(n-1)}{n-(p+1)} &< 1 - \frac{\text{RSS}}{\text{TSS}} \\
- \frac{\text{RSS}}{\text{TSS}} \cdot \frac{(n-1)}{n-(p+1)} &< - \frac{\text{RSS}}{\text{TSS}} \\
\frac{(n-1)}{n-(p+1)} &> 1 \\
n-1 &> n-(p+1) \\
-1 &> -(p+1) \\
1 &< p+1 \\
0 &< p
\end{align*}

And given that $p$ is the number of predictors, $p$ is always $>0$ (positive), thus adjusted $R^2$ is always less than $R^2$.  

## A.2 Exponential family of distributions

Let $Y$ be a random variable whose pdf/pmf can be written as

$$f(y; \theta, \phi) = \exp{\left\{\frac{y\theta - b(\theta)}{a(\phi)} + c(y, \phi)\right\} }, $$

where $b(\theta)$ and $a(\phi)$ are arbitrary functions. 

**[7 points] Show that $Y \sim \text{Poisson}(\lambda)$ meets this criterion.**

YOUR ANSWER HERE

# B. Computational Problems

## B.1 Comparing Model Selection Techniques

Recall again, the Amazon book data. The data consists of data on $n = 325$ books and includes measurements of:

- `aprice`: The price listed on Amazon (dollars)


- `lprice`: The book's list price (dollars)


- `weight`: The book's weight (ounces)


- `pages`: The number of pages in the book


- `height`: The book's height (inches)


- `width`: The book's width (inches)


- `thick`: The thickness of the book (inches)


- `cover`: Whether the book is a hard cover of paperback.


- And other variables...

We'll explore various models to predict `aprice`. But first, we'll repeat the data cleaning from our lesson on t-tests. We'll also split the data into a training set and a test/validation set.


```{R, warning=FALSE,message=FALSE}
library(ggplot2)
library(car) #for the vif() function
library(corrplot)

amazon = read.csv(url(paste0("https://raw.githubusercontent.com/bzaharatos/",
                    "-Statistical-Modeling-for-Data-Science-Applications/",
                    "master/Modern%20Regression%20Analysis%20/Datasets/amazon.txt")), sep = "\t")
names(amazon)
df = data.frame(aprice = amazon$Amazon.Price, lprice = as.numeric(amazon$List.Price),  
                pages = amazon$NumPages, width = amazon$Width, weight = amazon$Weight..oz,  
                height = amazon$Height, thick = amazon$Thick, cover = amazon$Hard..Paper)

#cleaning the data, as was done in our lesson on t-tests
df$weight[which(is.na(df$weight))] = mean(df$weight, na.rm = TRUE)
df$pages[which(is.na(df$pages))] = mean(df$pages, na.rm = TRUE)
df$height[which(is.na(df$height))] = mean(df$height, na.rm = TRUE)
df$width[which(is.na(df$width))] = mean(df$width, na.rm = TRUE)
df$thick[which(is.na(df$thick))] = mean(df$thick, na.rm = TRUE)
df = df[-205,]

#training and test set
set.seed(11111)
n = floor(0.8 * nrow(df)) #find the number corresponding to 80% of the data
index = sample(seq_len(nrow(df)), size = n) #randomly sample indicies to be included in the training set

train = df[index, ] #set the training set to be the randomly sampled rows of the dataframe
test = df[-index, ] #set the testing set to be the remaining rows
cat("There are", dim(train)[1], "rows and",dim(train)[2],"columns in the training set. ")  #check the dimensions
cat("There are", dim(test)[1], "rows and",dim(test)[2],"columns in the testing set.")  #check the dimensions

```

Also, here are some pairwise correlations:


```{R}
col4 = colorRampPalette(c("black", "darkgrey", "grey","#CFB87C"))
corrplot(cor(train[,-8]), method = "ellipse", col = col4(100),  addCoef.col = "black", tl.col = "black")
```

**B.1 (a) [16 points] Fit a full model on the training dataset. Then, use the `update()` function to perform backward selection (let $\alpha_{crit} = 0.15$). At each step of backward selection, calculate the mean squared prediction error (MSPE) on the test set.**


```{R}
#YOUR CODE HERE
backward <- lm(aprice~.,data = train)
summary(backward)

#mspe
yhat = predict(backward, test)
(mspe1 <- mean((test$aprice - yhat)^2))
```


```{R}
#YOUR CODE HERE
backward <- update(backward,.~. - pages)
summary(backward)

#mspe
yhat = predict(backward, test)
(mspe2 <- mean((test$aprice - yhat)^2))
```


```{R}
#YOUR CODE HERE
backward <- update(backward,.~.-width)
summary(backward)

#mspe
yhat = predict(backward, test)
(mspe3 <- mean((test$aprice - yhat)^2))
```


```{R}
#YOUR CODE HERE
backward <- update(backward,.~.-height)
summary(backward)

#mspe
yhat = predict(backward, test)
(mspe4 <- mean((test$aprice - yhat)^2))
```

```{r}
#YOUR CODE HERE
backward <- update(backward,.~.-thick)
summary(backward)

#mspe
yhat = predict(backward, test)
(mspe5 <- mean((test$aprice - yhat)^2))
```


**B.1 (b) [4 points] Check the standard diagnostic plots for this "best" model. Specifically, do you think this model satisfies the modeling assumptions?**


```{R}
#YOUR CODE HERE
options(repr.plot.width = 9, repr.plot.height = 9)
par(mfrow = c(2,2))
plot(backward)
```

*YOUR ANSWER HERE*

In short, no I don't think it satisfies our modeling assumptions. 

**B.1 (c) [5 points] Compare the MSPE for each of the models you fit along the way to the "best" model found through backward selection. If the goal of this model is prediction, which model is "best"?**


```{R}
#YOUR CODE HERE
plot(c(3:7),c(mspe5,mspe4,mspe3,mspe2,mspe1),xlab = "number of predictors",ylab = "MSPE")
```

*YOUR ANSWER HERE*

The MSPE suggest that the model that had 4 predictors was the "best" model (the one with the lowest prediction error). So not the last one, but the second to last one that still had one predictor that had a p-value above $\alpha = 0.15$.

**B.1 (d) [13 points] Now, compute the best model of size $1$, the best model of size $2$, etc. up through the best model of size $7$ (the full model). Then, among the remaining $7$ models, compute the best model according the AIC, BIC, and $R^2_a$. Do the criteria pick out different models? Which model do *you* think is best? Justify your answer.**


```{R, warning = FALSE,message=FALSE}
#YOUR CODE HERE
library(leaps)

reg1 = regsubsets(aprice ~ .,
  data = train)
rs = summary(reg1)
rs$which
```


```{R}
#YOUR CODE HERE
AIC = 2*(2:7) + n*log(rs$rss/n)
plot(AIC ~ I(1:7), xlab = "number of predictors",ylab = "AIC")
```


```{R}
#YOUR CODE HERE
plot(1:7, rs$adjr2, xlab = "number of predictors", ylab = "adjusted R Squared")
```

```{r}
BIC = log(n)*(2:7) + n*log(rs$rss/n) 
plot(BIC ~ I(1:7), xlab = "number of predictors", ylab = "BIC")
```


*YOUR ANSWER HERE*

Yes, the criteria pick out different models. If AIC and BIC look for the lowest of each criteria, interestingly then both suggest all 7 predictors are the best, unless something went wrong (which I suspect it did) in which case, AIC suggest around 4 predictors is the best model and about 2 or 3 predictors is best according to BIC. Then adjusted $R^2$ (which looks for the highest) suggest that around 5 predictors is the best model. 

In *my* opinion I want to select a model that most agrees with most if not all of these evaluation method which seems to be around 4 predictors (specifically with the predictors: `lprice`, `pages` (which is ironic because that was the first predictor out during backward selection), `weight`, and `coverP`). BIC didn't suggest that was the best model specifically but 2-3 predictors is still close to 4 predictors. 

**B.1 (e) [25 points] Compute the MSPE for each of the best models of size $1$, the best model of size $2$, etc. up through the best model of size $7$ (the full model). Which model is best according to this metric? Is this the same model that was selected by MSPE in part B.1 (c)?**

Rather than fit seven separate models, automate this process in a function with a loop:

1. The function should take in the training set, the test set, and the summary of your `regsubsets()` object (what we called `rs` in class).

2. The function should contain a loop. At step i = 1,...,p of the loop, you should:
    - select the training set model matrix corresponding the the best model of size i. You can do this using the logicals in the table given by `rs$which`.
    
    - fit the regression with the selected model matrix
    
    - select the test set matrix, xstar, of correct size i = 1,...,p. Again, you can do this using the logicals in the table given by `rs$which`.
    
    - compute the predicted value for the selected xstar
    
    - compute the MSPE 


```{R}
#YOUR CODE HERE
```


```{R}
#YOUR CODE HERE
```

YOUR ANSWER HERE

**B.1 (f) [5 points] Compute the variance inflation factor for the models selected by AIC, BIC, MSPE, and $R_a^2$. Do any of these models show evidence of collinearity?**


```{R}
#YOUR CODE HERE
```

YOUR ANSWER HERE

## B.2 Multicollinearity diagnostics and solutions

`longley` is a macroeconomic data set with the following variables:

1. `GNP.deflator`: GNP implicit price deflator (1954=100)

2. `GNP`: Gross National Product.

3. `Unemployed`: number of unemployed.

4. `Armed.Forces`: number of people in the armed forces.

5. `Population`: 'noninstitutionalized' population $\ge 14$ years of age.

6. `Year`: the year (time).

7. `Employed`: number of people employed.

Let's consider models mean to predict `Employed`.


```{R}
library(MASS)
head(longley)
```

**B.2 (a) [6 points] First, center and scale the data so that all variables have mean zero and variance one. Then, fit a full model, with `Employed` as the response and all other variables as predictors. Since *all* of the data are centered and scaled, including the responce, you should fit this model without an intercept (the intercept is known to be zero). Does this model appear to violate any of our linear regression assummptions?**


```{R}
#YOUR CODE HERE
df <- as.data.frame(scale(longley, center = TRUE, scale = TRUE))
summary(df)
```

```{r}
full_mod <- lm(Employed ~ 0 + ., data = df)
summary(full_mod)
```
```{r}
options(repr.plot.width = 9, repr.plot.height = 9)
par(mfrow = c(2,2))
plot(full_mod)
```


YOUR ANSWER HERE

**B.2 (b) [4 points] Calculate the variance inflation factors for this model. Comment on the output.**

(You can ignore any "No intercept" warnings; the VIFs are sensible because we know the intercept to be zero.)


```{R,warning=FALSE,message=FALSE}
#YOUR CODE HERE
vif(full_mod)
```

YOUR ANSWER HERE

**B.2 (c) [7 points] Compute the AIC *using the built-in `AIC()` function in `R`* for the full model above and a reduced model containing only GNP. Consider any violations of assumptions and comment on which model is best.**


```{R}
#YOUR CODE HERE
AIC(full_mod)
```

```{r}
red_mod <- lm(Employed ~ GNP, data = df)
AIC(red_mod)
```


YOUR ANSWER HERE

Suppose that there are theoretical reasons to keep all of the predictors in the model. In such cases, we might want to "shrink" the effect of each predictor, i.e., shrink the magnitude of each slope parameter. Ridge regression is one such "shrinkage" method. Instead of minimizing the RSS, ridge regression minimizes

$$Q(\beta) = \sum^n_{i=1}(Y_i - [\beta_0 + \beta_1 z_i + ... + \beta_p z_p])^2 + \lambda \sum^p_{j = 1}\beta_j^2,$$

where $z_j$ are the standardized predictors. The first term controls the the fit of the model, and the second term controls the size of the coefficients. $\lambda$ is a *penalty term* and controls the balance between these two terms. Smaller values of $\lambda$ correspond to larger values of $\widehat\beta$. In the extreme case where $\lambda = 0$, $\widehat\beta$ will be the least squares solution.

**B.2 (d) [5 points] Fit a ridge regression with `Employed` as the response and all others as predictors. Ridge regression can be performed with the `lm.ridge()` function in the `MASS` package. This function requires an argument, `lambda = `, for the tuning parameter. Use `lambda = seq(0,0.1,0.001)`. Plot the coefficients as a function of these $\lambda$ values.**


```{R}
#YOUR CODE HERE
ridge_mod <- lm.ridge(Employed ~ ., data = df,lambda = seq(0,0.1,0.001))
summary(ridge_mod)
                      
```

Which value of $\lambda$ should we choose? Some suggest that we choose the value where the estimates start to stablize. Others suggest using methods for *estimating* $\lambda$, such as generalized cross validation (GCV). Here are some [details](http://sfb649.wiwi.hu-berlin.de/fedc_homepage/xplore/ebooks/html/csa/node123.html) about GCV. GCV can be implemented by wrapping your ridge regression in the `select()` function.

**B.2 (e) [5 points] Find the GCV value of $\lambda$. Then fit the ridge regression with this value of $\lambda$.**


```{R}
#YOUR CODE HERE
```


```{R}
#YOUR CODE HERE
```
