---
title: "Regression Modeling in R - Part 1"
author: "Dr. Osita Onyejekwe"
---
# Homework #1 - Solutions

**See Canvas for the due date of this assignment**. Complete all of the following problems. Ideally, the theoretical problems should be answered in a Markdown cell directly underneath the question. If you don't know LaTex/Markdown, you may submit separate handwritten solutions to the theoretical problems, but please see the class [scanning policy](https://docs.google.com/document/d/1d_7PJvTGtMGtHWq8BezaTzYI5aYZkFgmchV_hTId6Ks/edit?usp=sharing). Please do not turn in messy work. Computational problems should be completed in this notebook (using the R kernel). Computational questions may require code, plots, analysis, interpretation, etc. Working in small groups is allowed, but it is important that you make an effort to master the material and hand in your own work.

## A. Theoretical Problems

### A.1: Connections between linear algebra and statistics

Let $\mathbf{a} = (a_1,...,a_n)^T$ and $\mathbf{y} = (y_1,...,y_n)^T$ be two vectors. Define the *inner product* of $\mathbf{a}$ and $\mathbf{y}$ to be $\mathbf{a}^T\mathbf{y} = a_1y_1 + a_2y_2 + ... + a_ny_n$.

**A.1 (a) [5 points] Interpret** $\mathbf{y}$ as a set of observations of a variable. Find $\mathbf{a}$ such that $\mathbf{a}^T\mathbf{y}$ is equal to the sample mean of $\mathbf{y}$, i.e., $\bar{y}$.

Let $\mathbf{a} = (1/n, ..., 1/n)^T$. Then $\mathbf{a}^T\mathbf{y} = (1/n)y_1 + (1/n)y_2 + ... + (1/n)y_n = \frac{1}{n}\sum^n_{i=1}y_i$.

**A.1 (b) [7 points] Define** $\mathbf{\bar{y}} = (\bar{y},..., \bar{y})^T$ (that is, $\mathbf{\bar{y}}$ is a vector where every entry is the sample mean of $\mathbf{y}$). Write $\displaystyle \sum^n_{i=1}(y_i - \bar{y})^2$ as an inner product of two vectors.

First, note that $\mathbf{y} - \mathbf{\bar{y}} = (y_1 - \bar{y} , \, y_2 - \bar{y} \, ,..., \, y_n - \bar{y})^T$. So,

$$(\mathbf{y} - \mathbf{\bar{y}})^T(\mathbf{y} - \mathbf{\bar{y}}) = (y_1 - \bar{y} , \, y_2 - \bar{y} \, ,..., \, y_n - \bar{y})\begin{pmatrix}
           y_1 - \bar{y} \\
           y_2 - \bar{y} \\
           \vdots \\
           y_n - \bar{y}
         \end{pmatrix} = (y_1 - \bar{y})^2 + (y_2 - \bar{y})^2 + ... + (y_2 - \bar{y})^2 = \sum^n_{i=1}\left(y_i - \bar{y} \right)^2.$$

### A.2 Linear regression with matrices and vectors

**This problem might require some outside-of-class research if you haven't taken a linear algebra/matrix methods course.**

Matrices and vectors will play an important role for us in linear regression. Let's review some matrix theory as it might relate to linear regression.

Consider the system of linear equations

$$Y_i = \beta_0 + \sum^p_{j=1}\beta_j x_{i,j} + \varepsilon_i,$$

for $i = 1,...,n$, where $n$ is the number of data points (measurements in the sample), and $j = 1,...,p$, where

1.  $p+1$ is the number of parameters in the model.
2.  $Y_i$ is the $i^{th}$ measurement of the *response variable*.
3.  $x_{i,j}$ is the $i^{th}$ measurement of the $j^{th}$ *predictor variable*.
4.  $\varepsilon_i$ is the $i^{th}$ *error term* and is a random variable, often assumed to be $N(0, \sigma^2)$.
5.  $\beta_j$, $j = 0,...,p$ are *unknown parameters* of the model. We hope to estimate these, which would help us characterize the relationship between the predictors and response.

**A.2.(a) [5 points] Write the equation above in matrix vector form. Call the matrix including the predictors** $X$, the vector of $Y_i$s $\mathbf{Y}$, the vector of parameters $\mathbf{\beta}$, and the vector of error terms $\mathbf{\varepsilon}$. (This is more LaTeX practice than anything else...)

$$\mathbf{Y} = X\mathbf{\beta} + \mathbf{\varepsilon},$$

where $\mathbf{Y} = (Y_1,...,Y_n)^T$, $X =$

$$
\begin{bmatrix}
1 & X_{1,1} & X_{1,2} & ... & X_{1,p}\\
1 & X_{2,1} & X_{2,2} & ... & X_{2,p} \\
\vdots & \vdots & \vdots & ... & \vdots \\
1 & X_{n,1} & X_{n,2} & ... & X_{n,p} 
\end{bmatrix}
$$  $\boldsymbol{\beta} = (\beta_1,...,\beta_n)^T$ and $\boldsymbol{\varepsilon} = (\varepsilon_1,...,\varepsilon_n)^T$

**A.2.(b) [8 points] In class, we will find that the OLS estimator for** $\mathbf{\beta}$ in MLR is $\widehat{\boldsymbol\beta} = (X^TX)^{-1}X^T\mathbf{Y}$.

1.  What condition must be true about the columns of $X$ for the "Gram" matrix $X^TX$ to be invertible?

2.  What does this condition mean in practical terms?

3.  Suppose that the number of measurements ($n$) is less than the number of model parameters ($p+1$). What does this say about the invertibility of $X^TX$? What does this mean on a practical level?

4.  What is true about about $\widehat{\boldsymbol\beta}$ if $X^TX$ is not invertible?

5.  For $X^TX$ to be invertible, the columns of $X$ must be linearly independent. That means that no column of $X$--i.e., no measured predictor--can be written as a linear combination of other columns. This implies that $n > (p+1)$.

6.  If we've measured a predictor that is simply a linear combination of others, that means that that predictor is not adding any new information that's not already contained in the other predictors. Imagine a simple case: $X_1$ is a predictor of measured weights in pounds, and $X_2$ is a predictor of measured weights in kilograms. Thus, $X_1 = 2.2046 X_2$. Measuring $X_1$ doesn't give any new information.

7.  This implies that we have more parameters than data/measurements, and thus $X^TX$ will not be invertible.

8.  The formula for $\widehat{\boldsymbol\beta} = (X^TX)^{-1}X^T\mathbf{Y}$ is derived from the "normal equations": $$ (X^TX)\mathbf{\beta} = X^T\mathbf{Y}.$$ The normal equations have a unique solutions if and only if $X^TX$ is invertible. If it's not invertible, then either the normal equations have no solutions or infinitely many solutions.

## B. Computational Problems

### B.1 Exploratory data analysis and basic regression

**B.1.(a) Load the "gala" dataset, and describe the variables.**

```{R}
gala = read.csv("https://raw.githubusercontent.com/CUBuffs/Statistical-Learning-/main/Modern%20Regression%20Analysis/Datasets/gala.csv")

gala = gala[,-1]
summary(gala)
```

**B.1.(b) [8 points] Use ggplot() to explore the relationship between the Species variable (response) and Endemics, Elevation, Nearest, and Adjacent (predictor variables). You might do so by creating four separate scatter plots. Do these relationships look linear? Does the variability in Species change as a function of any of the predictors? Are there any outliers in any of the plots?**

```{R}
#install.packages("gridExtra")
library(gridExtra)
library(ggplot2)

p = ggplot(data = gala) 
p1 = p + geom_point(mapping = aes(x = Endemics, y = Species)) 
p2 = p + geom_point(mapping = aes(x = Elevation, y = Species))
p3 = p + geom_point(mapping = aes(x = Nearest, y = Species))
p4 = p + geom_point(mapping = aes(x = Adjacent, y = Species))
grid.arrange(p1,p2,p3,p4, nrow = 2)
cor(gala)
```

1.  The Species vs Endemics plot appears to be approximately linear, but perhaps with some curvature.
2.  The Species vs Elevation plot appears roughly linear, but there appears to be more variance in Species as Elevation gets higher.
3.  The Species vs Nearest plot doesn't appear to have a linear relationship.
4.  The Species vs Adjacent plot doesn't appear to have a linear relationship, and there seem to be a few very large outliers.

**B.1.(c) [18 points] Perform a linear regression with Species as the response and Endemics, Elevation, Nearest, and Adjacent as predictors. Store your `lm()` object in the variable `lm_species`. Interpret the parameter estimate associated with Endemics in the markdown cell below (assume, for the moment, that the model is correct, and so the interpretation holds).**

```{R}
lm_species = lm(Species ~ Endemics + Elevation + Nearest + Adjacent, data = gala)
summary(lm_species)
```

After adjusting for the highest elevation of the island (m), the distance from the nearest island (km), and the area of the adjacent island (square km), if the number of endemic species on that island were increased by one, *on average*, we would see a $\approx 4.19$ increase in the number of plant species found on the island.

(This conclusion sounds causal; but it shouldn't necessarily be interpreted as such!)

### B.2 Wildfire data

The wildfire data has the following variables measured for $517$ wildfires:

1.  `X`: $x$-axis spatial coordinate within the Montesinho park map: $1$ to $9$
2.  `Y`: $y$-axis spatial coordinate within the Montesinho park map: $2$ to $9$
3.  `month`: month of the year: 'jan' to 'dec'
4.  `day`: day of the week: 'mon' to 'sun'
5.  `FFMC`: FFMC index from the FWI system: $18.7$ to $96.20$
6.  `DMC`: DMC index from the FWI system: $1.1$ to $291.3$
7.  `DC`: DC index from the FWI system: $7.9$ to $860.6$
8.  `ISI`: ISI index from the FWI system: $0.0$ to $56.10$
9.  `temp`: temperature in Celsius degrees: $2.2$ to $33.30$
10. `RH`: relative humidity in $\%$: $15.0$ to $100$
11. \`wind': wind speed in km/h: $0.40$ to $9.40$
12. `rain`: outside rain in mm/m2 : $0.0$ to $6.4$
13. `area`: the burned area of the forest (in ha): $0.00$ to $1090.84$

```{R}
fire = read.csv("https://raw.githubusercontent.com/CUBuffs/Statistical-Learning-/main/Modern%20Regression%20Analysis/Datasets/forestfires.csv")

dim(fire)
```

**B.2 (a) The area variable is very skewed towards** $0.0$. Thus it may make sense to model with the logarithm transform. Transform the \$y = \$ `area` variable using $\log(y + 1)$. Store this new variable in a $14^{th}$ column and name that column `log_area`. Also print a summary to detect

```{R}
#fire2 = fire[fire$area != 0,]
#dim(fire2)

#convert to log area
fire[,14] = log(fire[,13] + 1) 
colnames(fire)[14] ="log_area"

#convert rain to factor
fire[,12] = as.factor(ifelse(fire[,12] == 0,0,1))

head(fire)
summary(fire)
```

```{R}
#factor(fire$month, levels = month.abb)
```

```{R}
library(plyr)
library(ggplot2)
x = data.frame(count(fire$month)); x
ggplot(x,aes(x=x,y=freq)) + geom_bar(stat='identity') + coord_flip() 
```

```{R}
pairs(fire[,-c(1,2,3,4,12,13)])
```

```{R}
lmod_fire = lm(log_area ~ temp + RH + wind + DC + DMC + ISI + rain, data = fire)
summary(lmod_fire)
```
