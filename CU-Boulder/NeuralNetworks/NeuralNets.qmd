---
title: "CSCI 5922: Neural Networks & Deep Learning"
---

[Fall 2023] ***Neural Networks*** with Dr. Ami Gates at [CU Boulder](../../CUB.qmd)

# Sub pages

- [Professor Gates' Code Examples](Prof-Code-Ex.qmd)
- [Jasmine's Homework Submissions](JK-HW.qmd)

# Module 1: Intro to NN and Logistic Regression

## Resources
[Two code examples by professor:](Prof-Code-Ex.qmd#module-1) 

- Simple NN with Sklearn
- Logistic Regression

[One Assignment:](JK-HW.qmd#module-1) 

- Assignment 1

## Topics
- Introduction to Neural Networks
    1. History
    2. Challenges
    3. Applications
    4. Current Focus
    5. Discussion and Ethics

- Brief Review of Machine Learning Concepts, Data Formats, and Modeling Preparation Requirements for Data and Labels
    1. Review of concepts in modeling data for prediction/classification
    2. Gathering data and related challenges
    3. Formatting, cleaning, and preparing data for supervised learning
        - About Keras and Getting TF/Keras working in Python ([Keras about page](https://keras.io/about/))

- Logistic Regression in Python and by hand
    - Loss function (Cross Entropy)
    - Activation function (Sigmoid)
    - Gradient Descent - Chain Rule - Partial Derivatives
        - Updating parameters
    - Visualizing changes in the Loss

# Module 2: Neural Networks

## Resources
[Three code examples by professor (+ one extra):](Prof-Code-Ex.qmd#module-2) 

- NN to predict XOR 
- NN with 3D Data - Binary Label
- Using FF, BP, Softmax, CCE, One-Hot Encoding, etc.
- ***Extra:*** Softmax, CCE, and One-Hot Encoding - 3 Outputs

[One Assignment - Two Parts:](JK-HW.qmd#module-2) 

- Assignment 2 - Part 1: One Layer NN with One Output
- Assignment 2 - Part 2: Multinomial NN with Softmax, CCE, and One-Hot Encoding

## Topics

1. Architectures
2. Derivatives and Gradient Descent
3. Activation Functions and their Derivatives
    - Sigmoid
    - Softmax
    - reLU
4. Loss Functions and their Derivatives
    - MSE
    - Categorical Cross Entropy (CCE)
5. Label Encoding - One-Hot Encoding (OHE)
6. Back Prop (BP) for one hidden layer - one output - ANN
7. Back Prop for NN with multinomial output, one-hot encoded labels, multiple activation functions, CCE Loss Functions
    - Relative derivatives and proofs
8. Coding the XOR Problem using NN (one hidden layer perceptron)
9. Coding a multinomial NN with one-hot encoding, CCE, softmax, FF, BP
10. Related Math and Proofs, Jacobian Matrices, etc.
11. All coding will be "by hand" in Python

# Module 3

(Not there yet)

## Resources
[(...) code examples by professor:](Prof-Code-Ex.qmd#module-3) 


[One Assignment:](JK-HW.qmd#module-3) 