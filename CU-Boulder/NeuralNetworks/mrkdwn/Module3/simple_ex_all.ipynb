{
  "cells": [
    {
      "cell_type": "raw",
      "id": "133cf357",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Simple Examples of ANNs, RNNs (LSTM, BRNN), and CNNs in Python/Keras\"\n",
        "author: Professor Ami Gates\n",
        "format:\n",
        "  html:\n",
        "    code-fold: false\n",
        "execute:\n",
        "  output: true\n",
        "toc: true\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c98d6c2",
      "metadata": {},
      "source": [
        "Reference: Professor Ami Gates, Dept. Applied Math, Data Science, University of Colorado\n",
        "\n",
        "[Dr. Gates' Website](https://gatesboltonanalytics.com/?page_id=903)\n",
        "\n",
        "---\n",
        "\n",
        "<https://www.tensorflow.org/api_docs/python/tf/keras/Input>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bfdee768",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Activation\n",
        "import numpy as np\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import re  \n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.keras\n",
        "#from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, LSTM\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "## For Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import os\n",
        "\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import string\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import Counter\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "#nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "\n",
        "## These settings show/print dfs in diff ways\n",
        "pd.set_option('display.max_columns', 50)\n",
        "pd.set_option('display.width', 50)\n",
        "#pd.options.display.max_seq_items = None\n",
        "#pd.pandas.set_option('display.max_columns', None)\n",
        "pd.options.display.max_colwidth = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "795f021b",
      "metadata": {},
      "source": [
        "# Movies Dataset\n",
        "<https://www.kaggle.com/datasets/columbine/imdb-dataset-sentiment-analysis-in-csv-format>\n",
        "\n",
        "(Update the path for YOUR computer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e6221c89",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(40000, 2)\n",
            "                                                text  \\\n",
            "0  I grew up (b. 1965) watching and loving the Th...   \n",
            "1  When I put this movie in my DVD player, and sa...   \n",
            "2  Why do people who do not know what a particula...   \n",
            "3  Even though I have great interest in Biblical ...   \n",
            "4  Im a die hard Dads Army fan and nothing will e...   \n",
            "5  A terrible movie as everyone has said. What ma...   \n",
            "6  Finally watched this shocking movie last night...   \n",
            "7  I caught this film on AZN on cable. It sounded...   \n",
            "8  It may be the remake of 1987 Autumn's Tale aft...   \n",
            "9  My Super Ex Girlfriend turned out to be a plea...   \n",
            "\n",
            "   label  \n",
            "0      0  \n",
            "1      0  \n",
            "2      0  \n",
            "3      0  \n",
            "4      1  \n",
            "5      0  \n",
            "6      1  \n",
            "7      0  \n",
            "8      1  \n",
            "9      1  \n",
            "<class 'pandas.core.frame.DataFrame'>\n"
          ]
        }
      ],
      "source": [
        "path = \"IMBD_data/\"\n",
        "\n",
        "TrainData = pd.read_csv(str(path+\"Train.csv\"))\n",
        "print(TrainData.shape)\n",
        "print(TrainData.head(10))\n",
        "print(type(TrainData))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "85bfa0d0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5000, 2)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I always wrote this series off as being a comp...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1st watched 12/7/2002 - 3 out of 10(Dir-Steve ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>This movie was so poorly written and directed ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The most interesting thing about Miryang (Secr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>when i first read about \"berlin am meer\" i did...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>I saw this film on September 1st, 2005 in Indi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>I saw a screening of this movie last night. I ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>William Hurt may not be an American matinee id...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>IT IS A PIECE OF CRAP! not funny at all. durin...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>I'M BOUT IT(1997)&lt;br /&gt;&lt;br /&gt;Developed &amp; publi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  \\\n",
              "0  I always wrote this series off as being a comp...   \n",
              "1  1st watched 12/7/2002 - 3 out of 10(Dir-Steve ...   \n",
              "2  This movie was so poorly written and directed ...   \n",
              "3  The most interesting thing about Miryang (Secr...   \n",
              "4  when i first read about \"berlin am meer\" i did...   \n",
              "5  I saw this film on September 1st, 2005 in Indi...   \n",
              "6  I saw a screening of this movie last night. I ...   \n",
              "7  William Hurt may not be an American matinee id...   \n",
              "8  IT IS A PIECE OF CRAP! not funny at all. durin...   \n",
              "9  I'M BOUT IT(1997)<br /><br />Developed & publi...   \n",
              "\n",
              "   label  \n",
              "0      0  \n",
              "1      0  \n",
              "2      0  \n",
              "3      1  \n",
              "4      0  \n",
              "5      1  \n",
              "6      0  \n",
              "7      1  \n",
              "8      0  \n",
              "9      0  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "TestData = pd.read_csv(str(path+\"Test.csv\"))\n",
        "print(TestData.shape)\n",
        "TestData.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d45cb3c2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5000, 2)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>It's been about 14 years since Sharon Stone aw...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>someone needed to make a car payment... this i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The Guidelines state that a comment must conta...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This movie is a muddled mish-mash of clichés f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Before Stan Laurel became the smaller half of ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>This is the best movie I've ever seen! &lt;br /&gt;&lt;...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>The morbid Catholic writer Gerard Reve (Jeroen...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>\"Semana Santa\" or \"Angel Of Death\" is a very w...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Somebody mastered the difficult task of mergin...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Why did I waste 1.5 hours of my life watching ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  \\\n",
              "0  It's been about 14 years since Sharon Stone aw...   \n",
              "1  someone needed to make a car payment... this i...   \n",
              "2  The Guidelines state that a comment must conta...   \n",
              "3  This movie is a muddled mish-mash of clichés f...   \n",
              "4  Before Stan Laurel became the smaller half of ...   \n",
              "5  This is the best movie I've ever seen! <br /><...   \n",
              "6  The morbid Catholic writer Gerard Reve (Jeroen...   \n",
              "7  \"Semana Santa\" or \"Angel Of Death\" is a very w...   \n",
              "8  Somebody mastered the difficult task of mergin...   \n",
              "9  Why did I waste 1.5 hours of my life watching ...   \n",
              "\n",
              "   label  \n",
              "0      0  \n",
              "1      0  \n",
              "2      0  \n",
              "3      0  \n",
              "4      0  \n",
              "5      1  \n",
              "6      1  \n",
              "7      0  \n",
              "8      1  \n",
              "9      0  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ValidData = pd.read_csv(str(path+\"Valid.csv\"))\n",
        "print(ValidData.shape)\n",
        "ValidData.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1db1c1cc",
      "metadata": {},
      "source": [
        "Concat requires a list\n",
        "\n",
        "Place all data from above into one dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "50bc386c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(50000, 2)\n"
          ]
        }
      ],
      "source": [
        "FullDataset=pd.concat([TrainData,TestData, ValidData])\n",
        "print(FullDataset.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d8e94db",
      "metadata": {},
      "source": [
        "## Clean Up TrainData\n",
        "\n",
        "Get the vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a9a98a9a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                text  \\\n",
            "0  I grew up (b. 1965) watching and loving the Th...   \n",
            "1  When I put this movie in my DVD player, and sa...   \n",
            "2  Why do people who do not know what a particula...   \n",
            "3  Even though I have great interest in Biblical ...   \n",
            "4  Im a die hard Dads Army fan and nothing will e...   \n",
            "\n",
            "   label  \n",
            "0      0  \n",
            "1      0  \n",
            "2      0  \n",
            "3      0  \n",
            "4      1  \n"
          ]
        }
      ],
      "source": [
        "print(TrainData.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d6ad35f6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text\n",
            "label\n"
          ]
        }
      ],
      "source": [
        "# Testing iterating the columns \n",
        "for col in TrainData.columns: \n",
        "    print(col) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58aa8eb1",
      "metadata": {},
      "source": [
        "## Check Content   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d85135aa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0        I grew up (b. 1965) watching and loving the Th...\n",
            "1        When I put this movie in my DVD player, and sa...\n",
            "2        Why do people who do not know what a particula...\n",
            "3        Even though I have great interest in Biblical ...\n",
            "4        Im a die hard Dads Army fan and nothing will e...\n",
            "                               ...                        \n",
            "39995    \"Western Union\" is something of a forgotten cl...\n",
            "39996    This movie is an incredible piece of work. It ...\n",
            "39997    My wife and I watched this movie because we pl...\n",
            "39998    When I first watched Flatliners, I was amazed....\n",
            "39999    Why would this film be so good, but only gross...\n",
            "Name: text, Length: 40000, dtype: object\n",
            "0        0\n",
            "1        0\n",
            "2        0\n",
            "3        0\n",
            "4        1\n",
            "        ..\n",
            "39995    1\n",
            "39996    1\n",
            "39997    0\n",
            "39998    1\n",
            "39999    1\n",
            "Name: label, Length: 40000, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(TrainData[\"text\"])\n",
        "print(TrainData[\"label\"]) ##0 is negative, 1 is positive"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f480dac7",
      "metadata": {},
      "source": [
        "### Tokenize and Vectorize \n",
        "\n",
        "- Create the list \n",
        "- Keep the labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "41062023",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A Look at some of the reviews list is:\n",
            "\n",
            "['I grew up (b. 1965) watching and loving the Thunderbirds. All my mates at school watched. We played \"Thunderbirds\" before school, during lunch and after school. We all wanted to be Virgil or Scott. No one wanted to be Alan. Counting down from 5 became an art form. I took my children to see the movie hoping they would get a glimpse of what I loved as a child. How bitterly disappointing. The only high point was the snappy theme tune. Not that it could compare with the original score of the Thunderbirds. Thankfully early Saturday mornings one television channel still plays reruns of the series Gerry Anderson and his wife created. Jonatha Frakes should hand in his directors chair, his version was completely hopeless. A waste of film. Utter rubbish. A CGI remake may be acceptable but replacing marionettes with Homo sapiens subsp. sapiens was a huge error of judgment.', \"When I put this movie in my DVD player, and sat down with a coke and some chips, I had some expectations. I was hoping that this movie would contain some of the strong-points of the first movie: Awsome animation, good flowing story, excellent voice cast, funny comedy and a kick-ass soundtrack. But, to my disappointment, not any of this is to be found in Atlantis: Milo's Return. Had I read some reviews first, I might not have been so let down. The following paragraph will be directed to those who have seen the first movie, and who enjoyed it primarily for the points mentioned.<br /><br />When the first scene appears, your in for a shock if you just picked Atlantis: Milo's Return from the display-case at your local videoshop (or whatever), and had the expectations I had. The music feels as a bad imitation of the first movie, and the voice cast has been replaced by a not so fitting one. (With the exception of a few characters, like the voice of Sweet). The actual drawings isnt that bad, but the animation in particular is a sad sight. The storyline is also pretty weak, as its more like three episodes of Schooby-Doo than the single adventurous story we got the last time. But dont misunderstand, it's not very good Schooby-Doo episodes. I didnt laugh a single time, although I might have sniggered once or twice.<br /><br />To the audience who haven't seen the first movie, or don't especially care for a similar sequel, here is a fast review of this movie as a stand-alone product: If you liked schooby-doo, you might like this movie. If you didn't, you could still enjoy this movie if you have nothing else to do. And I suspect it might be a good kids movie, but I wouldn't know. It might have been better if Milo's Return had been a three-episode series on a cartoon channel, or on breakfast TV.\", 'Why do people who do not know what a particular time in the past was like feel the need to try to define that time for others? Replace Woodstock with the Civil War and the Apollo moon-landing with the Titanic sinking and you\\'ve got as realistic a flick as this formulaic soap opera populated entirely by low-life trash. Is this what kids who were too young to be allowed to go to Woodstock and who failed grade school composition do? \"I\\'ll show those old meanies, I\\'ll put out my own movie and prove that you don\\'t have to know nuttin about your topic to still make money!\" Yeah, we already know that. The one thing watching this film did for me was to give me a little insight into underclass thinking. The next time I see a slut in a bar who looks like Diane Lane, I\\'m running the other way. It\\'s child abuse to let parents that worthless raise kids. It\\'s audience abuse to simply stick Woodstock and the moonlanding into a flick as if that ipso facto means the film portrays 1969.', 'Even though I have great interest in Biblical movies, I was bored to death every minute of the movie. Everything is bad. The movie is too long, the acting is most of the time a Joke and the script is horrible. I did not get the point in mixing the story about Abraham and Noah together. So if you value your time and sanity stay away from this horror.', 'Im a die hard Dads Army fan and nothing will ever change that. I got all the tapes, DVD\\'s and audiobooks and every time i watch/listen to them its brand new. <br /><br />The film. The film is a re run of certain episodes, Man and the hour, Enemy within the gates, Battle School and numerous others with a different edge. Introduction of a new General instead of Captain Square was a brilliant move - especially when he wouldn\\'t cash the cheque (something that is rarely done now).<br /><br />It follows through the early years of getting equipment and uniforms, starting up and training. All in all, its a great film for a boring Sunday afternoon. <br /><br />Two draw backs. One is the Germans bogus dodgy accents (come one, Germans cant pronounced the letter \"W\" like us) and Two The casting of Liz Frazer instead of the familiar Janet Davis. I like Liz in other films like the carry ons but she doesn\\'t carry it correctly in this and Janet Davis would have been the better choice.', \"A terrible movie as everyone has said. What made me laugh was the cameo appearance by Scott McNealy, giving an award to one of the murdered programmers in front of a wall of SUN logos. McNealy is the CEO of SUN Microsystem, a company that practically defines itself by its hatred of Microsoft. They have been instrumental in filing antitrust complaints against Microsoft. So, were they silly enough to think this bad movie would add fuel to that fire?<br /><br />There's no public record I see of SUN's involvement, but clearly the makers of this movie know Scott McNealy. An interesting mystery.\", 'Finally watched this shocking movie last night, and what a disturbing mindf**ker it is, and unbelievably bloody and some unforgettable scenes, and a total assault on the senses. Looks like a movie from the minds of Lynch (specifically ERASERHEAD), Buttgereit, and even a little of \"Begotten\". What this guy does to his pregnant sister is beyond belief, but then again, did it really happen or is it his brain\\'s left and right sides doing battle. That\\'s the main theme of this piece of art, to draw a fine line between fantasy and reality, and what would happen if the right side of the brain that dreams and fantasizes overtakes the reasoning and logical left side. And the music in this movie is unbelievable, a kind of electronic score that is absolutely perfect. Even though this movie is totally shocking and pretty disgusting in some of the most extreme scenes (including hard core sex) you will ever see in any movie, I viewed it as a work of art, and loved it. And that music still amazes me, I have to try and find the soundtrack if is available. Watching \"Subconscious Cruelty\" is a real event, and not something the viewer will easily forget. And a note to gorehounds, this is a must-have.<br /><br />Warning... Be careful buying this movie, because some prints have fogging on the graphic sex scenes and extreme gore, especially the copies from the Japanese release.', 'I caught this film on AZN on cable. It sounded like it would be a good film, a Japanese \"Green Card\". I can\\'t say I\\'ve ever disliked an Asian film, quite the contrary. Some of the most incredible horror films of all time are Japanese and Korean, and I am a HUGE fan of John Woo\\'s Hong Kong films. I an not adverse to a light hearted films, like Tampopo or Chung King Express (two of my favourites), so I thought I would like this. Well, I would rather slit my wrists and drink my own blood than watch this laborious, badly acted film ever again.<br /><br />I think the director Steven Okazaki must have spiked the water with Quaalude, because no one in this film had a personality. And when any of the characters DID try to act, as opposed to mumbling a line or two, their performance came across as forced and incredibly fake. I honestly did not think that anyone had ever acted before...the ONLY person who sounded genuine was Brenda Aoki.. I find it amazing that this is promoted as a comedy, because I didn\\'t laugh once. Even MORE surprising is that CBS morning news called this \"a refreshing breath of comedy\". It was neither refreshing, nor a breath of comedy. And the ending was very predictable, the previous reviewer must be an idiot to think such things.<br /><br />AVOID this film unless you want to see a boring predictable plot line and wooden acting. I actually think that \"Spike of Bensonhurst\" is a better acted film than this...and I walked out half way through that film!', \"It may be the remake of 1987 Autumn's Tale after eleven years, as the director Mabel Cheung claimed. Mabel employs rock music as the medium in this movie to express her personal attitude to life, in which love, desire and the consequential frustration play significantly crucial roles. Rock music may not be the best vehicle to convey the profound sentiment, and yet it is not too inappropriate to utilize it as the life of underground rock musicians is bitterly more intense than an ordinary one. The director focuses on the depiction of subtle affection and ultimate vanity of life rather than mere rock music. The love between father and son, lovers, and friends is delicately and touchingly delivered through the fine performance. Mabel does not attempt to beautify rock musicians as artists at all, instead, she tries to reproduce a true life on screen, making huge efforts of years' working on this project and gathering information in Beijing underground pubs.<br /><br />Daniel has given probably the best performance in all his movies made so far. His innate dispiritedness and reticence fit the blue mood of the film perfectly.\", \"My Super Ex Girlfriend turned out to be a pleasant surprise for me, I was really expecting a horrible movie that would probably be stupid and predictable, and you know what? It was! But this movie did have so many wonderful laughs and a fun plot that anyone could get a kick out of. I know that this was a very cheesy movie, but Uma and Anna were just so cool and Steve was such a great addition along with a great cast that looked like they had so much fun and that's what made the movie really work.<br /><br />Jenny Johnson(scary, that's my best friend's actual name) is not your typical average librarian looking woman, when Matt, your average male, asks her out, he's in for more than he expected, he's asked G-Girl out on a date, the super hero of the world! But when he finds out what a jealous and crazy girl she really is and decides that it may be a good idea that they spend some time apart, but Jenny won't have it since he's fallen for another girl, Hannah, and she will make his life a living hell, I mean, let's face it, he couldn't have chosen a better girl to break up with.<br /><br />The effect were corny, but you seriously move past them quickly, the story and cast made the story really work and I loved Uma in this movie, it was such a step up from Prime. My Super Ex Girlfriend is a fun movie that you shouldn't really take seriously, it's just a cute romantic comedy that I think if I could get a laugh out of it, anyone could.<br /><br />7/10\", \"I can't believe people are looking for a plot in this film. This is Laural and Hardy. Lighten up already. These two were a riot. Their comic genius is as funny today as it was 70 years ago. Not a filthy word out of either mouth and they were able to keep audiences in stitches. Their comedy wasn't sophisticated by any stretch. If a whoopee cushion can't make you grin, there's no reason to watch any of the stuff these guys did. It was a simpler time, and people laughed at stuff that was funny without a plot. I guess it takes a simple mind to enjoy this stuff, so I qualify. Two man comedy teams don't compute, We're just too sophisticated... Aren't we fortunate?\", \"If you haven't seen the gong show TV series then you won't like this movie much at all, not that knowing the series makes this a great movie. <br /><br />I give it a 5 out of 10 because a few things make it kind of amusing that help make up for its obvious problems.<br /><br />1) It's a funny snapshot of the era it was made in, the late 1970's and early 1980's. 2) You get a lot of funny cameos of people you've seen on the show. 3) It's interesting to see Chuck (the host) when he isn't doing his on air TV personality. 4) You get to see a lot of bizarre people doing all sorts of weirdness just like you see on the TV show.<br /><br />I won't list all the bad things because there's a lot of them, but here's a few of the most prominent.<br /><br />1) The Gong Show Movie has a lot of the actual TV show clips which gets tired at movie length. 2) The movie's story line outside of the clip segments is very weak and basically is made up of just one plot point. 3) Chuck is actually halfway decent as an actor, but most of the rest of the actors are doing typical way over the top 1970's flatness.<br /><br />It's a good movie to watch when you don't have an hour and a half you want to watch all at once. Watch 20 minutes at a time and it's not so bad. But even then it's not so good either. ;)\", 'I have always been a huge fan of \"Homicide: Life On The Street\" so when I heard there was a reunion movie coming up, I couldn\\'t wait.<br /><br />Let me just say, I was not disappointed at all. It was one of the most powerful 2 hours of television I\\'ve ever seen. It was great to see everyone back again, but the biggest pleasure of all was to have Andre Braugher back, because the relationship between Pembleton and Bayliss was always the strongest part of an all-together great show.', 'Greg Davis and Bryan Daly take some crazed statements by a terrorists, add some commentary by a bunch of uber-right reactionaries, ascribe the most extreme positions of the most fundamentalist Moslems on the planet to everyone who calls themselves a Moslem, and presents this as the theology of Islam. Maybe their next film will involve interviewing Fred Phelps and the congregation of the Westboro Baptist Church, adding commentary by some militant atheist \"scholars, and call their film \"What the World Needs to Know About Christianity.\" Ultimately, this film suffers from both poor production values and lack of attention to the most basic standards of journalism. Don\\'t waste your time and money; just turn on your AM radio and listen to Rush Limbaugh for a couple of days for free and you\\'ll get the same message with the same level of intellectual analysis.', 'A half-hearted attempt to bring Elvis Presley into the modern day, but despite a sexy little shower scene and a pseudo-Playboy magazine subplot, Presley is surrounded by the same old coy, winking clichés. A woman picks E.P. up on the beach and then proceeds to take over his life--and he doesn\\'t seem to care! Dick Sargent is grueling in another sidebar, but Don Porter and Rudy Vallee (!) try hard as Elvis\\' two bosses (he\\'s moonlighting, you see). Some of the songs are quite good, especially \"Almost in Love\", but if you want to see a looser, hipper, updated Elvis sex-comedy--look elsewhere. When Elvis and his Fatal Attraction get into bed together, there\\'s actually a wooden board in between them! Get real. ** from ****', 'If you want a fun romp with loads of subtle humor, then you will enjoy this flick.<br /><br />I don\\'t understand why anyone wouldn\\'t enjoy this one. Take it for what it is: a vehicle for Dennis Hopper to mess with your head and make you laugh. It ain\\'t Shakespeare, but it is well done. Ericka Eleniak is absolutely beautiful and holds her own in this one - Better than any episode of Baywatch - and shows a knack for subtle humor. Too bad she hasn\\'t had many opportunities to expand on that.<br /><br />Tom Berenger fits his role of \"real Navy\" perfectly and William McNamara does a solid job as a hustler.<br /><br />Throw in a walk-on by Hopper in the middle of the chase for \"the Cherry on this Sundae\" and you\\'ve got a movie that kept my attention and kept me laughing. I bought this one as soon as it was available.<br /><br />Brain-candy.', 'I really wanted to be able to give this film a 10. I\\'ve long thought it was my favorite of the four modern live-action Batman films to date (and maybe it still will be--I have yet to watch the Schumacher films again). I\\'m also starting to become concerned about whether I\\'m somehow subconsciously being contrarian. You see, I always liked the Schumacher films. As far as I can remember, they were either 9s or 10s to me. But the conventional wisdom is that the two Tim Burton directed films are far superior. I had serious problems with the first Burton Batman this time around--I ended up giving it a 7--and apologize as I might, I just couldn\\'t help feel that Batman Returns just has too many small direction, plot and script problems scattered throughout to justify a 10.<br /><br />But Burton _almost_ trumps the problems with sheer force of style, and even though there are a lot of small flaws, Batman Returns is still a great film, especially if you\\'re a Burton fan, as Batman Returns has just as much in common with The Nightmare Before Christmas (1993) and Edward Scissorhands (1990) as it does with anything else in the Batman universe.<br /><br />The film begins strongly, with the Cobblepots having a baby. We see their dismay--people walk out of the birthing room with horror on their faces, ready to vomit. Later, they have the baby in a small cage. Finally they take it out for an evening stroll and dump it in the Gotham City River. The baby ends up becoming Batman villain The Penguin (Danny DeVito).<br /><br />Meanwhile, Max Shreck (Christopher Walken) is the film\\'s \"evil capitalist\", comparable to Grissom (Jack Palance) in Batman. He is planning on duping Gotham City in various ways, and we see him emotionally abusing his secretary, the timid Selina Kyle (Michelle Pfeiffer). When Kyle discovers one of the nefarious plots, Grissom tries to get rid of her, but she is rescued by cats, becoming Catwoman.<br /><br />While all of this is going on, The Penguin, who has long been only rumored to exist and who is thought to be dangerous, begins a scheme to be presented to the public as a good guy, despite having less than benevolent, ulterior motives.<br /><br />Before re-watching Burton\\'s Batman films this time, I didn\\'t remember just how little the films are about Batman (Michael Keaton). It\\'s almost as if Burton didn\\'t feel the character was interesting enough to focus on. The focus here is much more on the villains, especially The Penguin. Batman doesn\\'t appear very often, especially in the beginning of the film, and surprisingly often, we\\'re watching him watching The Penguin.<br /><br />Although some viewers necessarily count the above as a flaw, I can\\'t say that I do, even if I\\'d like to know more about Batman and follow his story more. The villains\\' stories are interesting, too, and as an \"origin story\" for two major Batman villains, Batman Returns is already more than complex in terms of plot.<br /><br />However, there are some character problems that I do count as a flaw. The Penguin has a cadre of circus performers who do his bidding, but even though they\\'re frequently on screen, we never get to learn anything about them. Burton has a core of characters as intriguing as those in Tod Browning\\'s Freaks (1932) available, with actors as interesting as Vincent Schiavelli, but he just doesn\\'t have the space to use them.<br /><br />For that matter, he hardly has space to explore Catwoman. The film plays as if Catwoman may have been as developed and featured in as many scenes as The Penguin, but that cut of the film would have been 4 hours long. So the bulk of the Catwoman scenes had to be excised. Of course, all of this barely leaves any room for Batman. Burton has Batman turn very dark in the public\\'s eye in this film, and unusually, he never bothers to resolve this. As far as we know, at the end, Gothamites still think that Batman is a murdering lunatic. That\\'s an interesting development, but unfortunately it ended up being dropped between this film and the next.<br /><br />As for the script, although there are minor problems including some non-sequiturs and bizarre decisions (in terms of logic) made by characters, it\\'s clear that Burton and writers Sam Hamm and Daniel Waters are not exactly trying to tell a traditional story. A lot of the dialogue is pun-oriented, but often this is fairly subtle and/or complex (of course, sometimes it is very blatant or transparent, too). It helps to look at Batman Returns as a more \"poetic\" film, as I believe was the intention. This also carries over into more general plot and directorial decisions--plenty of odd character actions, including from minor characters, are done in service of a general mood or style, and that style works very well.<br /><br />\"Dark\" is the easiest way to sum up Batman Returns in a word, and whether that\\'s a positive or negative depends on your disposition. Anyone who knows me knows that I love dark. So for me, Burton\\'s style largely transcends the flaws in the plot and the script. In many ways, Batman Returns is like an insane, campy horror film, with beautifully eerie production design. Like Batman, Burton is still making many references to other films, but instead of Vertigo (1958) and Star Wars (1977) (well, there\\'s still a slight Star Wars reference), he invokes films like Nosferatu (1922) (including that \"Max Schreck\" was the name of the actor who played the Dracula-like character there), Motel Hell (1980), the aforementioned Freaks, Willy Wonka and the Chocolate Factory (1971) (which has a surreal, dark edge to it) and zombie films--made most explicit in The Penguin\\'s final scene.<br /><br />In terms of visuals and general atmosphere--and that includes the general \"feel\" of the story, the characters and so on--this couldn\\'t be a stronger 10.', 'The main problem with \"Power\" is that it features way too may pointless characters and subplots that add absolutely nothing to the movie whatsoever. It gets boring after awhile, sitting around waiting through scenes that don\\'t connect to find something that drives the movie forward. You could probably pass it all off as character development, but all of them are either recycled from earlier scenes in the movie, or are just simply to flat and uninteresting. Lumet never gives enough time to let any of the supporting cast blossom. He should have cut a few of the characters (hackman, the wife) and concentrated harder on others (Billings). It could have been a great, hard political thriller instead of a jumbled mess that loses any message in a sea of bad writing and acting, a fact that amazed me considering the cast. Even Gene Hackman performance wasn\\'t up to par. Denzel Washington is the only real actor of note here. Gere and the others have all done much better performances elsewhere. <br /><br />Sidney Lumet needs to go back to the fierce one man shows he did in the seventies (i.e, Serpico) and stop trying to recapture his success with \"12 Angry Men\" and \"Fail Safe\". It hasn\\'t worked yet Sidney, and it most likely never will. leave the ensemble dramas to Altman. <br /><br />3/10<br /><br />* / * * * *', 'The folks at Disney have a lot to explain. First and foremost, why anyone thought this lesser-sitcom material would ever make even a half-decent motion picture. In the kooky 60\\'s teleplay, the unique idea of Martians among us had not yet been given the sophisticated X-Files treatment. Quaint visions of little green men have long since been dispelled by the likes of E.T., CLOSE ENCOUNTERS and ALIENS 1-3.<br /><br />Any charm the property had was mainly due to the endearing relationship created between the late Bill Bixby as Tim and Ray Walston as the unworldly visitor. The conceit that Martians have antenna seemed dopey back then. Now it seems positively idiotic. Yet, Christopher Lloyd\\'s Uncle Martin sports the metallic appendages. In an early shot, the antenna on a sign for the TV station Tim works at is supposed to make us think \"martian\"! When\\'s the last time you saw a TV with rabbit ears, eh?<br /><br />Disney doesn\\'t trust quaint or relationships and crams this flick with youth-wooing special effects that include a talking space suit named Zoot! Yes, you read that correctly - Uncle Martin\\'s silver space suit speaks. He is supposed to be a real hilarious cut-up! Figure again. I got stretch socks that are funnier than Zoot. Whenever the action lags (and it lags constantly), computer graphics are put into play to liven things up. Tim is here played by the amiable Jeff Daniels, who can\\'t (or won\\'t) do anything to save this floudering mess. Zesty Christine Ebersole brings some comic zeal to her neighbor lady role. Even Ray Walston himself is dragged painfully into the procedings - all to no avail. This alien visitor is dead on arrival.<br /><br />Constant talk of sitcoms turning to screen makes me only hope that the I DREAM OF JEANNIE movie won\\'t feature a talking harem outfit. I pray that Samantha\\'s cat in the movie BEWITCHED doesn\\'t have lines. I live in fear that I LOVE LUCY - THE MOVIE will proudly feature a CG Conga Drum named Bongo.<br /><br />Paging Michael Eisner! Mickey Mouse - take me to your leader.', \"A friend told me of John Fante last summer after we got into a conversation about Charles Bukowski. I did not know that Fante was a favorite writer of Bukowski's - an author with similar edge and humor except from one generation earlier. 'Ask the Dust' was the first Fante book I read, and it remains one of my favorite novels. The novel was a brilliant piece of writing about a sad, frightened young writer posing to himself and the outside world as an overconfident, masterfully talented author who had no idea how to write about the real world experiences he had none of. In the novel the protagonist is a virgin, with no idea how to win the graces of the women he desperately wants to write about in magazines. The story of his bizarre relationship with Camilla, how he settles for his first sexual experience with a 'wounded' admirer, and how he eventually is left with nothing but the story of his failed attempts at love is biting and real, with no touching Hollywood ending. The film adaptation stays true to the book for a while, but meanders into the cinematic trap of love persevering through racism, sickness and death. The heart of this story lies in the fact that Bandini is a jerk and Camilla is f-ing crazy, and their love never was and never would be the real thing, no matter how much either of them wanted to find it in each other. This movie tore out the real meaning of the story out and replaced it with schlock. I can't believe the man who wrote Chinatown could read this book and make a movie about it that got it so wrong.\"]\n",
            "A Look at some of the labels list is:\n",
            "\n",
            "[0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "ReviewsLIST=[]  ## from the text column\n",
        "LabelLIST=[]    \n",
        "\n",
        "for nextreview, nextlabel in zip(TrainData[\"text\"], TrainData[\"label\"]):\n",
        "    ReviewsLIST.append(nextreview)\n",
        "    LabelLIST.append(nextlabel)\n",
        "\n",
        "print(\"A Look at some of the reviews list is:\\n\")\n",
        "print(ReviewsLIST[0:20])\n",
        "\n",
        "print(\"A Look at some of the labels list is:\\n\")\n",
        "print(LabelLIST[0:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33b4afb8",
      "metadata": {},
      "source": [
        "## Optional - for Stemming the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "92e15aa0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fisher\n"
          ]
        }
      ],
      "source": [
        "## Instantiate it\n",
        "A_STEMMER=PorterStemmer()\n",
        "## test it\n",
        "print(A_STEMMER.stem(\"fishers\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d224080c",
      "metadata": {},
      "source": [
        "### Use NLTK's PorterStemmer in a function "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b9e3c983",
      "metadata": {},
      "outputs": [],
      "source": [
        "# DEFINE THE FUNCTION\n",
        "def MY_STEMMER(str_input):\n",
        "    ## Only use letters, no punct, no nums, make lowercase...\n",
        "    words = re.sub(r\"[^A-Za-z\\-]\", \" \", str_input).lower().split()\n",
        "    words = [A_STEMMER.stem(word) for word in words] ## Use the Stemmer...\n",
        "    return words"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ecf3bc6",
      "metadata": {},
      "source": [
        "Build the labeled dataframe\n",
        "\n",
        "Get the Vocab  - here keeping top 10,000\n",
        "\n",
        "### Vectorize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b3ca4c3d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'scipy.sparse._csr.csr_matrix'>\n"
          ]
        }
      ],
      "source": [
        "## Instantiate your CV\n",
        "MyCountV=CountVectorizer(\n",
        "        input=\"content\",  \n",
        "        lowercase=True, \n",
        "        #stop_words = \"english\", ## This is optional\n",
        "        #tokenizer=MY_STEMMER, ## Stemming is optional\n",
        "        max_features=11000  ## This can be updated\n",
        "        )\n",
        "\n",
        "## Use your CV \n",
        "MyDTM = MyCountV.fit_transform(ReviewsLIST)  # create a sparse matrix\n",
        "print(type(MyDTM))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "0e7d4c8e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['00' '000' '007' ... 'zoom' 'zorro' 'zucco']\n"
          ]
        }
      ],
      "source": [
        "ColumnNames=MyCountV.get_feature_names_out() ## This is the vocab\n",
        "print(ColumnNames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "dfac47b0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "print(type(ColumnNames))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df7533b0",
      "metadata": {},
      "source": [
        "Here we can clean up the columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "2cb7296d",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Build the data frame\n",
        "MyDTM_DF=pd.DataFrame(MyDTM.toarray(),columns=ColumnNames)\n",
        "\n",
        "## Convert the labels from list to df\n",
        "Labels_DF = pd.DataFrame(LabelLIST,columns=['LABEL'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17ed5a78",
      "metadata": {},
      "source": [
        "## Check your new DF and you new Labels df:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "79153dcd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Labels\n",
            "\n",
            "       LABEL\n",
            "0          0\n",
            "1          0\n",
            "2          0\n",
            "3          0\n",
            "4          1\n",
            "...      ...\n",
            "39995      1\n",
            "39996      1\n",
            "39997      0\n",
            "39998      1\n",
            "39999      1\n",
            "\n",
            "[40000 rows x 1 columns]\n"
          ]
        }
      ],
      "source": [
        "print(\"Labels\\n\")\n",
        "print(Labels_DF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "6b4bb049",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DF\n",
            "\n",
            "       00  000  007  10  100  1000  101  11  12  \\\n",
            "0       0    0    0   0    0     0    0   0   0   \n",
            "1       0    0    0   0    0     0    0   0   0   \n",
            "2       0    0    0   0    0     0    0   0   0   \n",
            "3       0    0    0   0    0     0    0   0   0   \n",
            "4       0    0    0   0    0     0    0   0   0   \n",
            "...    ..  ...  ...  ..  ...   ...  ...  ..  ..   \n",
            "39995   0    0    0   0    0     0    0   0   0   \n",
            "39996   0    0    0   0    0     0    0   0   0   \n",
            "39997   0    0    0   0    0     0    0   0   0   \n",
            "39998   0    0    0   1    0     0    0   0   0   \n",
            "39999   0    2    0   0    0     0    0   0   0   \n",
            "\n",
            "       13  13th  14  15  150  16  17  18  180  \\\n",
            "0       0     0   0   0    0   0   0   0    0   \n",
            "1       0     0   0   0    0   0   0   0    0   \n",
            "2       0     0   0   0    0   0   0   0    0   \n",
            "3       0     0   0   0    0   0   0   0    0   \n",
            "4       0     0   0   0    0   0   0   0    0   \n",
            "...    ..   ...  ..  ..  ...  ..  ..  ..  ...   \n",
            "39995   0     0   0   0    0   0   0   0    0   \n",
            "39996   0     0   0   0    0   0   0   0    0   \n",
            "39997   0     0   0   0    0   0   0   0    0   \n",
            "39998   0     0   0   0    0   0   0   0    0   \n",
            "39999   0     0   0   0    0   0   0   0    0   \n",
            "\n",
            "       18th  19  \n",
            "0         0   0  \n",
            "1         0   0  \n",
            "2         0   0  \n",
            "3         0   0  \n",
            "4         0   0  \n",
            "...     ...  ..  \n",
            "39995     0   0  \n",
            "39996     0   0  \n",
            "39997     0   0  \n",
            "39998     0   0  \n",
            "39999     0   0  \n",
            "\n",
            "[40000 rows x 20 columns]\n",
            "(40000, 11000)\n"
          ]
        }
      ],
      "source": [
        "print(\"DF\\n\")\n",
        "print(MyDTM_DF.iloc[:,0:20])\n",
        "print(MyDTM_DF.shape) ## 40,000 by 11000"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8573026d",
      "metadata": {},
      "source": [
        "Remove any columns that contain numbers\n",
        "\n",
        "Remove columns with words not the size you want. For example, words < 3 `chars`\n",
        "\n",
        "## DEFINE A FUNCTION that returns `True` if numbers are in a string \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "570d6fa3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 18min 49s\n",
            "Wall time: 22min 41s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "def Logical_Numbers_Present(anyString):\n",
        "    return any(char.isdigit() for char in anyString)\n",
        "##----------------------------------------------------\n",
        "\n",
        "for nextcol in MyDTM_DF.columns:\n",
        "    #print(nextcol)\n",
        "    ## Remove unwanted columns\n",
        "    #Result=str.isdigit(nextcol) ## Fast way to check numbers\n",
        "    #print(Result)\n",
        "    \n",
        "    ##-------------call the function -------\n",
        "    LogResult=Logical_Numbers_Present(nextcol)\n",
        "    #print(LogResult)\n",
        "    ## The above returns a logical of True or False\n",
        "    \n",
        "    ## The following will remove all columns that contains numbers\n",
        "    if(LogResult==True):\n",
        "        #print(LogResult)\n",
        "        #print(nextcol)\n",
        "        MyDTM_DF=MyDTM_DF.drop([nextcol], axis=1)\n",
        "\n",
        "    ## The following will remove any column with name\n",
        "    ## of 3 or smaller - like \"it\" or \"of\" or \"pre\".\n",
        "    ##print(len(nextcol))  ## check it first\n",
        "    ## NOTE: You can also use this code to CONTROL\n",
        "    ## the words in the columns. For example - you can\n",
        "    ## have only words between lengths 5 and 9. \n",
        "    ## In this case, we remove columns with words <= 3.\n",
        "    elif(len(str(nextcol))<3):\n",
        "        #print(nextcol)\n",
        "        MyDTM_DF=MyDTM_DF.drop([nextcol], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "092a8892",
      "metadata": {},
      "source": [
        "Save original DF - without the labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "dc9d305f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   aamir  aaron  abandon  abandoned  abbey  \\\n",
            "0      0      0        0          0      0   \n",
            "1      0      0        0          0      0   \n",
            "2      0      0        0          0      0   \n",
            "3      0      0        0          0      0   \n",
            "4      0      0        0          0      0   \n",
            "5      0      0        0          0      0   \n",
            "6      0      0        0          0      0   \n",
            "7      0      0        0          0      0   \n",
            "8      0      0        0          0      0   \n",
            "9      0      0        0          0      0   \n",
            "\n",
            "   abbott  abc  abduction  abilities  ability  \\\n",
            "0       0    0          0          0        0   \n",
            "1       0    0          0          0        0   \n",
            "2       0    0          0          0        0   \n",
            "3       0    0          0          0        0   \n",
            "4       0    0          0          0        0   \n",
            "5       0    0          0          0        0   \n",
            "6       0    0          0          0        0   \n",
            "7       0    0          0          0        0   \n",
            "8       0    0          0          0        0   \n",
            "9       0    0          0          0        0   \n",
            "\n",
            "   able  ably  aboard  abomination  abortion  \\\n",
            "0     0     0       0            0         0   \n",
            "1     0     0       0            0         0   \n",
            "2     0     0       0            0         0   \n",
            "3     0     0       0            0         0   \n",
            "4     0     0       0            0         0   \n",
            "5     0     0       0            0         0   \n",
            "6     0     0       0            0         0   \n",
            "7     0     0       0            0         0   \n",
            "8     0     0       0            0         0   \n",
            "9     0     0       0            0         0   \n",
            "\n",
            "   abound  about  above  abraham  abroad  \\\n",
            "0       0      0      0        0       0   \n",
            "1       0      0      0        0       0   \n",
            "2       0      1      0        0       0   \n",
            "3       0      1      0        1       0   \n",
            "4       0      0      0        0       0   \n",
            "5       0      0      0        0       0   \n",
            "6       0      0      0        0       0   \n",
            "7       0      0      0        0       0   \n",
            "8       0      0      0        0       0   \n",
            "9       0      0      0        0       0   \n",
            "\n",
            "   abrupt  abruptly  absence  absent  absolute  \\\n",
            "0       0         0        0       0         0   \n",
            "1       0         0        0       0         0   \n",
            "2       0         0        0       0         0   \n",
            "3       0         0        0       0         0   \n",
            "4       0         0        0       0         0   \n",
            "5       0         0        0       0         0   \n",
            "6       0         0        0       0         0   \n",
            "7       0         0        0       0         0   \n",
            "8       0         0        0       0         0   \n",
            "9       0         0        0       0         0   \n",
            "\n",
            "   ...  yourselves  youth  youthful  youtube  \\\n",
            "0  ...           0      0         0        0   \n",
            "1  ...           0      0         0        0   \n",
            "2  ...           0      0         0        0   \n",
            "3  ...           0      0         0        0   \n",
            "4  ...           0      0         0        0   \n",
            "5  ...           0      0         0        0   \n",
            "6  ...           0      0         0        0   \n",
            "7  ...           0      0         0        0   \n",
            "8  ...           0      0         0        0   \n",
            "9  ...           0      0         0        0   \n",
            "\n",
            "   yul  yup  zach  zane  zany  zealand  \\\n",
            "0    0    0     0     0     0        0   \n",
            "1    0    0     0     0     0        0   \n",
            "2    0    0     0     0     0        0   \n",
            "3    0    0     0     0     0        0   \n",
            "4    0    0     0     0     0        0   \n",
            "5    0    0     0     0     0        0   \n",
            "6    0    0     0     0     0        0   \n",
            "7    0    0     0     0     0        0   \n",
            "8    0    0     0     0     0        0   \n",
            "9    0    0     0     0     0        0   \n",
            "\n",
            "   zellweger  zero  zeta  zhang  zizek  zodiac  \\\n",
            "0          0     0     0      0      0       0   \n",
            "1          0     0     0      0      0       0   \n",
            "2          0     0     0      0      0       0   \n",
            "3          0     0     0      0      0       0   \n",
            "4          0     0     0      0      0       0   \n",
            "5          0     0     0      0      0       0   \n",
            "6          0     0     0      0      0       0   \n",
            "7          0     0     0      0      0       0   \n",
            "8          0     0     0      0      0       0   \n",
            "9          0     0     0      0      0       0   \n",
            "\n",
            "   zoey  zombi  zombie  zombies  zone  zoo  \\\n",
            "0     0      0       0        0     0    0   \n",
            "1     0      0       0        0     0    0   \n",
            "2     0      0       0        0     0    0   \n",
            "3     0      0       0        0     0    0   \n",
            "4     0      0       0        0     0    0   \n",
            "5     0      0       0        0     0    0   \n",
            "6     0      0       0        0     0    0   \n",
            "7     0      0       0        0     0    0   \n",
            "8     0      0       0        0     0    0   \n",
            "9     0      0       0        0     0    0   \n",
            "\n",
            "   zoom  zorro  zucco  \n",
            "0     0      0      0  \n",
            "1     0      0      0  \n",
            "2     0      0      0  \n",
            "3     0      0      0  \n",
            "4     0      0      0  \n",
            "5     0      0      0  \n",
            "6     0      0      0  \n",
            "7     0      0      0  \n",
            "8     0      0      0  \n",
            "9     0      0      0  \n",
            "\n",
            "[10 rows x 10711 columns]\n"
          ]
        }
      ],
      "source": [
        "My_Orig_DF=MyDTM_DF\n",
        "print(My_Orig_DF.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "281c1a75",
      "metadata": {},
      "source": [
        "Now - let's create a complete and labeled dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "9c4947cc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[       LABEL\n",
            "0          0\n",
            "1          0\n",
            "2          0\n",
            "3          0\n",
            "4          1\n",
            "...      ...\n",
            "39995      1\n",
            "39996      1\n",
            "39997      0\n",
            "39998      1\n",
            "39999      1\n",
            "\n",
            "[40000 rows x 1 columns],        aamir  aaron  abandon  abandoned  abbey  \\\n",
            "0          0      0        0          0      0   \n",
            "1          0      0        0          0      0   \n",
            "2          0      0        0          0      0   \n",
            "3          0      0        0          0      0   \n",
            "4          0      0        0          0      0   \n",
            "...      ...    ...      ...        ...    ...   \n",
            "39995      0      0        0          0      0   \n",
            "39996      0      0        0          0      0   \n",
            "39997      0      0        0          0      0   \n",
            "39998      0      0        0          0      0   \n",
            "39999      0      0        0          0      0   \n",
            "\n",
            "       abbott  abc  abduction  abilities  \\\n",
            "0           0    0          0          0   \n",
            "1           0    0          0          0   \n",
            "2           0    0          0          0   \n",
            "3           0    0          0          0   \n",
            "4           0    0          0          0   \n",
            "...       ...  ...        ...        ...   \n",
            "39995       0    0          0          0   \n",
            "39996       0    0          0          0   \n",
            "39997       0    0          0          0   \n",
            "39998       0    0          0          0   \n",
            "39999       0    0          0          0   \n",
            "\n",
            "       ability  able  ably  aboard  abomination  \\\n",
            "0            0     0     0       0            0   \n",
            "1            0     0     0       0            0   \n",
            "2            0     0     0       0            0   \n",
            "3            0     0     0       0            0   \n",
            "4            0     0     0       0            0   \n",
            "...        ...   ...   ...     ...          ...   \n",
            "39995        0     0     0       0            0   \n",
            "39996        0     0     0       0            0   \n",
            "39997        0     0     0       0            0   \n",
            "39998        0     0     0       0            0   \n",
            "39999        0     0     0       0            0   \n",
            "\n",
            "       abortion  abound  about  above  abraham  \\\n",
            "0             0       0      0      0        0   \n",
            "1             0       0      0      0        0   \n",
            "2             0       0      1      0        0   \n",
            "3             0       0      1      0        1   \n",
            "4             0       0      0      0        0   \n",
            "...         ...     ...    ...    ...      ...   \n",
            "39995         0       0      0      0        0   \n",
            "39996         0       0      1      0        0   \n",
            "39997         0       0      0      0        0   \n",
            "39998         0       0      0      0        0   \n",
            "39999         0       0      1      0        0   \n",
            "\n",
            "       abroad  abrupt  abruptly  absence  \\\n",
            "0           0       0         0        0   \n",
            "1           0       0         0        0   \n",
            "2           0       0         0        0   \n",
            "3           0       0         0        0   \n",
            "4           0       0         0        0   \n",
            "...       ...     ...       ...      ...   \n",
            "39995       0       0         0        0   \n",
            "39996       0       0         0        0   \n",
            "39997       0       0         0        0   \n",
            "39998       0       0         0        0   \n",
            "39999       0       0         0        0   \n",
            "\n",
            "       absent  absolute  ...  yourselves  youth  \\\n",
            "0           0         0  ...           0      0   \n",
            "1           0         0  ...           0      0   \n",
            "2           0         0  ...           0      0   \n",
            "3           0         0  ...           0      0   \n",
            "4           0         0  ...           0      0   \n",
            "...       ...       ...  ...         ...    ...   \n",
            "39995       0         0  ...           0      0   \n",
            "39996       0         0  ...           0      0   \n",
            "39997       0         0  ...           0      0   \n",
            "39998       0         0  ...           0      0   \n",
            "39999       0         0  ...           0      0   \n",
            "\n",
            "       youthful  youtube  yul  yup  zach  zane  \\\n",
            "0             0        0    0    0     0     0   \n",
            "1             0        0    0    0     0     0   \n",
            "2             0        0    0    0     0     0   \n",
            "3             0        0    0    0     0     0   \n",
            "4             0        0    0    0     0     0   \n",
            "...         ...      ...  ...  ...   ...   ...   \n",
            "39995         0        0    0    0     0     0   \n",
            "39996         0        0    0    0     0     0   \n",
            "39997         0        0    0    0     0     0   \n",
            "39998         0        0    0    0     0     0   \n",
            "39999         0        0    0    0     0     0   \n",
            "\n",
            "       zany  zealand  zellweger  zero  zeta  \\\n",
            "0         0        0          0     0     0   \n",
            "1         0        0          0     0     0   \n",
            "2         0        0          0     0     0   \n",
            "3         0        0          0     0     0   \n",
            "4         0        0          0     0     0   \n",
            "...     ...      ...        ...   ...   ...   \n",
            "39995     0        0          0     0     0   \n",
            "39996     0        0          0     0     0   \n",
            "39997     0        0          0     0     0   \n",
            "39998     0        0          0     0     0   \n",
            "39999     0        0          0     0     0   \n",
            "\n",
            "       zhang  zizek  zodiac  zoey  zombi  \\\n",
            "0          0      0       0     0      0   \n",
            "1          0      0       0     0      0   \n",
            "2          0      0       0     0      0   \n",
            "3          0      0       0     0      0   \n",
            "4          0      0       0     0      0   \n",
            "...      ...    ...     ...   ...    ...   \n",
            "39995      0      0       0     0      0   \n",
            "39996      0      0       0     0      0   \n",
            "39997      0      0       0     0      0   \n",
            "39998      0      0       0     0      0   \n",
            "39999      0      0       0     0      0   \n",
            "\n",
            "       zombie  zombies  zone  zoo  zoom  zorro  \\\n",
            "0           0        0     0    0     0      0   \n",
            "1           0        0     0    0     0      0   \n",
            "2           0        0     0    0     0      0   \n",
            "3           0        0     0    0     0      0   \n",
            "4           0        0     0    0     0      0   \n",
            "...       ...      ...   ...  ...   ...    ...   \n",
            "39995       0        0     0    0     0      0   \n",
            "39996       0        0     0    0     0      0   \n",
            "39997       0        0     0    0     0      0   \n",
            "39998       0        0     0    0     0      0   \n",
            "39999       0        0     0    0     0      0   \n",
            "\n",
            "       zucco  \n",
            "0          0  \n",
            "1          0  \n",
            "2          0  \n",
            "3          0  \n",
            "4          0  \n",
            "...      ...  \n",
            "39995      0  \n",
            "39996      0  \n",
            "39997      0  \n",
            "39998      0  \n",
            "39999      0  \n",
            "\n",
            "[40000 rows x 10711 columns]]\n",
            "shape of labels\n",
            " (40000, 1)\n",
            "shape of data\n",
            " (40000, 10711)\n"
          ]
        }
      ],
      "source": [
        "dfs = [Labels_DF, MyDTM_DF]\n",
        "print(dfs)\n",
        "print(\"shape of labels\\n\", Labels_DF.shape)\n",
        "print(\"shape of data\\n\", MyDTM_DF.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "34560483",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       LABEL  aamir\n",
            "0          0      0\n",
            "1          0      0\n",
            "2          0      0\n",
            "3          0      0\n",
            "4          1      0\n",
            "...      ...    ...\n",
            "39995      1      0\n",
            "39996      1      0\n",
            "39997      0      0\n",
            "39998      1      0\n",
            "39999      1      0\n",
            "\n",
            "[40000 rows x 2 columns]\n",
            "(40000, 10712)\n"
          ]
        }
      ],
      "source": [
        "Final_DF_Labeled = pd.concat(dfs,axis=1, join='inner')\n",
        "## DF with labels\n",
        "print(Final_DF_Labeled.iloc[:, 0:2])\n",
        "print(Final_DF_Labeled.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc055272",
      "metadata": {},
      "source": [
        "FYI\n",
        "\n",
        "An alternative option for most frequent 10,000 words \n",
        "\n",
        "Not needed here as we used `CountVectorizer` with option `max_features`\n",
        "\n",
        "```\n",
        "print (df.shape[0])\n",
        "print (df[:10000].value.sum()/df.value.sum())\n",
        "top_words = list(df[:10000].key.values)\n",
        "print(top_words)\n",
        "## Example using index\n",
        "index = top_words.index(\"humiliating\")\n",
        "print(index)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "e3a8f162",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LABEL\n",
            "10712\n"
          ]
        }
      ],
      "source": [
        "## Create list of all words\n",
        "print(Final_DF_Labeled.columns[0])\n",
        "NumCols=Final_DF_Labeled.shape[1]\n",
        "print(NumCols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "bcc1f357",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10712\n"
          ]
        }
      ],
      "source": [
        "print(len(list(Final_DF_Labeled.columns)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "d77cb2bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "top_words=list(Final_DF_Labeled.columns[1:NumCols+1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "a44bdd96",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "aamir\n",
            "zucco\n"
          ]
        }
      ],
      "source": [
        "## Exclude the Label\n",
        "print(top_words[0])\n",
        "print(top_words[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "9df12c67",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "0\n",
            "10710\n"
          ]
        }
      ],
      "source": [
        "print(type(top_words))\n",
        "print(top_words.index(\"aamir\")) ## index 0 in top_words\n",
        "print(top_words.index(\"zucco\")) #index NumCols - 2 in top_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "e71e9493",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Encoding the data\n",
        "def Encode(review):\n",
        "    words = review.split()\n",
        "   # print(words)\n",
        "    if len(words) > 500:\n",
        "        words = words[:500]\n",
        "        #print(words)\n",
        "    encoding = []\n",
        "    for word in words:\n",
        "        try:\n",
        "            index = top_words.index(word)\n",
        "        except:\n",
        "            index = (NumCols - 1)\n",
        "        encoding.append(index)\n",
        "    while len(encoding) < 500:\n",
        "        encoding.append(NumCols)\n",
        "    return encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f862d35",
      "metadata": {},
      "source": [
        "Test the code to assure that it is doing what you think it should "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "16e109b1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 0, 4, 5, 8, 10710, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712]\n",
            "[10677, 10679, 10680, 10681, 10682, 10687, 10688, 10689, 10692, 10693, 10694, 10695, 10696, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712, 10712]\n",
            "500\n"
          ]
        }
      ],
      "source": [
        "result1 = Encode(\"aaron aamir abbey abbott abilities zucco \")\n",
        "print(result1)\n",
        "result2 = Encode(\"york young younger youngest youngsters youth youthful youtube zach zane zany zealand zellweger\")\n",
        "print(result2)\n",
        "print(len(result2)) ## Will be 500 because we set it that way above"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f7bd469",
      "metadata": {},
      "source": [
        "Now we are ready to encode all of our reviews - which are called \"text\" in our dataset. \n",
        "\n",
        "Using vocab from above i -  convert reviews (text) into numerical form.\n",
        "Replacing each word with its corresponding integer index value from the \n",
        "vocabulary. Words not in the vocab will\n",
        "be assigned  as the max length of the vocab + 1 \n",
        "\n",
        "\n",
        "Encode our training and testing datasets\n",
        "with same vocab. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "8ed93b64",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                text  \\\n",
            "0  I always wrote this series off as being a comp...   \n",
            "1  1st watched 12/7/2002 - 3 out of 10(Dir-Steve ...   \n",
            "2  This movie was so poorly written and directed ...   \n",
            "3  The most interesting thing about Miryang (Secr...   \n",
            "4  when i first read about \"berlin am meer\" i did...   \n",
            "5  I saw this film on September 1st, 2005 in Indi...   \n",
            "6  I saw a screening of this movie last night. I ...   \n",
            "7  William Hurt may not be an American matinee id...   \n",
            "8  IT IS A PIECE OF CRAP! not funny at all. durin...   \n",
            "9  I'M BOUT IT(1997)<br /><br />Developed & publi...   \n",
            "\n",
            "   label  \n",
            "0      0  \n",
            "1      0  \n",
            "2      0  \n",
            "3      1  \n",
            "4      0  \n",
            "5      1  \n",
            "6      0  \n",
            "7      1  \n",
            "8      0  \n",
            "9      0  \n",
            "(5000, 2)\n",
            "(40000, 2)\n"
          ]
        }
      ],
      "source": [
        "print(TestData.head(10))\n",
        "print(TestData.shape)\n",
        "print(TrainData.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c77d6fe5",
      "metadata": {},
      "source": [
        "## Final Training and Testing data and labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "e1565b1a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[10711  8675 10711  4420 10711  9599 10711 10711 10204 10711 10711 10711\n",
            " 10711  9599 10711 10711 10711 10711  9599  4453 10711 10711 10711 10711\n",
            " 10711  3646  3822 10711 10711  8268  9645  6251    16  9666 10666   202\n",
            "  6491  4397   412  4710 10483 10711 10711 10711 10711 10711  5556 10711\n",
            " 10711 10711 10711  4116  9150 10711 10711  3291  4378 10711  5604  1522\n",
            "  2624 10473 10711  4189  3773 10711 10711 10711 10711 10711 10678  4026\n",
            "  1060 10558  2030 10711 10711  4026 10711 10711 10711  6491  8270 10711\n",
            "  5219 10711  6423 10711 10711 10711  7298  4116  6251 10711 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712]\n",
            "(40000, 500)\n",
            "CPU times: total: 59min\n",
            "Wall time: 1h 10min 44s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "training_data = np.array([Encode(review) for review in TrainData[\"text\"]])\n",
        "print(training_data[20])\n",
        "print(training_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "2a790f85",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[10711 10711  1714  7176 10679 10711  5993   343 10579 10711 10711 10711\n",
            " 10711  7657 10711 10711  3572  9597   259 10711  9599    94   343    96\n",
            " 10445  5768   343  7666  2649 10711  4189  5150 10711  1359  9599 10711\n",
            " 10711 10711   343 10711 10711 10599 10439  9741   343 10711  4601 10711\n",
            "  8388  6208 10598  3884  9599  9965 10711 10711 10711 10711  5917 10711\n",
            "  3459   259 10711  9599 10711  2649  9273 10711  3655  5150 10711 10622\n",
            " 10711  4189 10711  8388   393 10711  3884 10711 10711  6251  1320 10711\n",
            "  1883 10711  6223   412 10711  8101 10711 10711 10678  4395  6491  8400\n",
            "  9645  6251   285 10711  9194  7695  9597 10678 10711 10711  1320  2471\n",
            "  4060 10678   393  6952 10711  5535   343 10711 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712 10712\n",
            " 10712 10712 10712 10712 10712 10712 10712 10712]\n",
            "CPU times: total: 7min 6s\n",
            "Wall time: 8min 21s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "testing_data = np.array([Encode(review) for review in TestData['text']])\n",
        "print(testing_data[20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "7ae8683f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(40000, 500) (5000, 500)\n",
            "CPU times: total: 7min 1s\n",
            "Wall time: 8min 11s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "validation_data = np.array([Encode(review) for review in ValidData['text']])\n",
        "\n",
        "print (training_data.shape, testing_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fb0baf1",
      "metadata": {},
      "source": [
        "Prepare the labels if they are not already 0 and 1. In our case they are so these lines are commented out and just FYI\n",
        "```\n",
        "train_labels = [1 if label=='positive' else 0 for sentiment in TrainData['label']]\n",
        "test_labels = [1 if label=='positive' else 0 for sentiment in TestData['label']]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "3d91dd73",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(40000, 1)\n",
            "(5000, 1)\n"
          ]
        }
      ],
      "source": [
        "train_labels = np.array([TrainData['label']])\n",
        "train_labels=train_labels.T\n",
        "print(train_labels.shape)\n",
        "test_labels = np.array([TestData['label']])\n",
        "test_labels=test_labels.T\n",
        "print(test_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "588632b5",
      "metadata": {},
      "source": [
        "# ANN\n",
        "\n",
        "Simple Dense NN for sentiment analysis (classification 0 neg, 1 pos)\n",
        "\n",
        "- *First layer*: Embedding Layer (Keras Embedding Layer) that will learn embeddings for different words.\n",
        "    - **RE:** <https://keras.io/api/layers/core_layers/embedding/>\n",
        "- `input_dim`: Integer. Size of the vocabulary\n",
        "- `input_length`: Length of input sequences, when it is constant.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "584a51e7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10712\n"
          ]
        }
      ],
      "source": [
        "print(NumCols)   \n",
        "input_dim = NumCols + 1 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "3dc4ea30",
      "metadata": {},
      "outputs": [],
      "source": [
        "My_ANN_Model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Embedding(input_dim=input_dim, output_dim=32, input_length=500),\n",
        "  tf.keras.layers.Dense(32, activation='relu'), \n",
        "  tf.keras.layers.Dense(16, activation='relu'),\n",
        "  tf.keras.layers.Dropout(.5), \n",
        "  tf.keras.layers.Dense(8, activation='relu'),\n",
        "  tf.keras.layers.Dropout(.5), \n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "  \n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "1be6d9ea",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 500, 32)           342816    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 500, 32)           1056      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 500, 16)           528       \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 500, 16)           0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 500, 8)            136       \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 500, 8)            0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 500, 1)            9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 344,545\n",
            "Trainable params: 344,545\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "My_ANN_Model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "a630173e",
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_function = keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "My_ANN_Model.compile(\n",
        "                 loss=loss_function,\n",
        "                 metrics=[\"accuracy\"],\n",
        "                 optimizer='adam'\n",
        "                 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "bb85c017",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[10711  4207 10711]\n",
            " [10711 10711  7496]\n",
            " [10711 10711  6912]]\n",
            "(40000, 500)\n",
            "[1]\n"
          ]
        }
      ],
      "source": [
        "print(training_data[0:3, 0:3])\n",
        "print(training_data.shape)\n",
        "print(train_labels[10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "7a274d19",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1250/1250 [==============================] - 44s 33ms/step - loss: 0.6921 - accuracy: 0.5078 - val_loss: 0.6909 - val_accuracy: 0.5157\n",
            "Epoch 2/5\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6913 - accuracy: 0.5099 - val_loss: 0.6903 - val_accuracy: 0.5169\n",
            "Epoch 3/5\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6910 - accuracy: 0.5157 - val_loss: 0.6903 - val_accuracy: 0.5166\n",
            "Epoch 4/5\n",
            "1250/1250 [==============================] - 43s 34ms/step - loss: 0.6910 - accuracy: 0.5121 - val_loss: 0.6905 - val_accuracy: 0.5097\n",
            "Epoch 5/5\n",
            "1250/1250 [==============================] - 42s 34ms/step - loss: 0.6910 - accuracy: 0.5112 - val_loss: 0.6902 - val_accuracy: 0.5171\n",
            "CPU times: total: 12min 49s\n",
            "Wall time: 3min 27s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "Hist=My_ANN_Model.fit(training_data, train_labels, epochs=5, validation_data=(testing_data, test_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd622e9b",
      "metadata": {},
      "source": [
        "# SimpleRNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "0dd1ed97",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10713\n",
            "(40000, 500)\n"
          ]
        }
      ],
      "source": [
        "##batch_size=256,\n",
        "output_dim=32\n",
        "print(input_dim)\n",
        "print(training_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "5d95e61c",
      "metadata": {},
      "outputs": [],
      "source": [
        "My_SimpleRNN_Model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Embedding(input_dim=input_dim, output_dim=output_dim, input_length=500),\n",
        "  tf.keras.layers.SimpleRNN(units =50, input_shape=(32,32,3)),\n",
        "  ## If not using Embedding, you would use SimpleRNN(units, input_shape=(x,y))\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "68e5af18",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 500, 32)           342816    \n",
            "                                                                 \n",
            " simple_rnn (SimpleRNN)      (None, 50)                4150      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 347,017\n",
            "Trainable params: 347,017\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "My_SimpleRNN_Model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "5bdc555b",
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_function = keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "My_SimpleRNN_Model.compile(\n",
        "                 loss=loss_function,\n",
        "                 metrics=[\"accuracy\"],\n",
        "                 optimizer='adam'\n",
        "                 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "191707fc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[10711  4207 10711]\n",
            " [10711 10711  7496]\n",
            " [10711 10711  6912]]\n"
          ]
        }
      ],
      "source": [
        "print(training_data[0:3, 0:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "57569dbd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1]\n"
          ]
        }
      ],
      "source": [
        "print(train_labels[10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "476d0927",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1250/1250 [==============================] - 322s 254ms/step - loss: 0.6957 - accuracy: 0.5042 - val_loss: 0.6944 - val_accuracy: 0.4996\n",
            "Epoch 2/5\n",
            "1250/1250 [==============================] - 315s 252ms/step - loss: 0.6939 - accuracy: 0.5048 - val_loss: 0.6935 - val_accuracy: 0.4986\n",
            "Epoch 3/5\n",
            "1250/1250 [==============================] - 319s 255ms/step - loss: 0.6926 - accuracy: 0.5087 - val_loss: 0.6959 - val_accuracy: 0.4992\n",
            "Epoch 4/5\n",
            "1250/1250 [==============================] - 315s 252ms/step - loss: 0.6898 - accuracy: 0.5146 - val_loss: 0.6943 - val_accuracy: 0.5046\n",
            "Epoch 5/5\n",
            "1250/1250 [==============================] - 332s 265ms/step - loss: 0.6846 - accuracy: 0.5196 - val_loss: 0.7058 - val_accuracy: 0.5044\n",
            "CPU times: total: 1h 42min 34s\n",
            "Wall time: 26min 43s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "Hist=My_SimpleRNN_Model.fit(training_data, train_labels, epochs=5, validation_data=(testing_data, test_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3718785d",
      "metadata": {},
      "source": [
        "## RNN with Bidirectional \n",
        "\n",
        "\n",
        "<https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "d173933c",
      "metadata": {},
      "outputs": [],
      "source": [
        "My_RNN_Model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Embedding(input_dim=input_dim, output_dim=32, input_length=500),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(50)),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "c7147b16",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 500, 32)           342816    \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 100)              8300      \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 351,217\n",
            "Trainable params: 351,217\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "My_RNN_Model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "a454fc7d",
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_function = keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "My_RNN_Model.compile(\n",
        "                 loss=loss_function,\n",
        "                 metrics=[\"accuracy\"],\n",
        "                 optimizer='adam'\n",
        "                 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "a8671ccc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[10711  4207 10711]\n",
            " [10711 10711  7496]\n",
            " [10711 10711  6912]]\n",
            "(40000, 500)\n",
            "[1]\n"
          ]
        }
      ],
      "source": [
        "print(training_data[0:3, 0:3])\n",
        "print(training_data.shape)\n",
        "print(train_labels[10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "4e54281c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1250/1250 [==============================] - 519s 409ms/step - loss: 0.6839 - accuracy: 0.5469 - val_loss: 0.6778 - val_accuracy: 0.5798\n",
            "Epoch 2/5\n",
            "1250/1250 [==============================] - 498s 398ms/step - loss: 0.6762 - accuracy: 0.5644 - val_loss: 0.6895 - val_accuracy: 0.5192\n",
            "Epoch 3/5\n",
            "1250/1250 [==============================] - 498s 398ms/step - loss: 0.6738 - accuracy: 0.5724 - val_loss: 0.6717 - val_accuracy: 0.5856\n",
            "Epoch 4/5\n",
            "1250/1250 [==============================] - 492s 394ms/step - loss: 0.6091 - accuracy: 0.6740 - val_loss: 0.5643 - val_accuracy: 0.7342\n",
            "Epoch 5/5\n",
            "1250/1250 [==============================] - 491s 392ms/step - loss: 0.5536 - accuracy: 0.7282 - val_loss: 0.5849 - val_accuracy: 0.6972\n",
            "CPU times: total: 2h 42min 33s\n",
            "Wall time: 41min 36s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "Hist=My_RNN_Model.fit(training_data, train_labels, epochs=5, validation_data=(testing_data, test_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9c66a9a",
      "metadata": {},
      "source": [
        "# LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "713262d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "My_LSTM_Model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Embedding(input_dim=input_dim, output_dim=32, input_length=500),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50)),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "f63483db",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 500, 32)           342816    \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 100)              33200     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 376,117\n",
            "Trainable params: 376,117\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "My_LSTM_Model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "2fa9dfe7",
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_function = keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "My_LSTM_Model.compile(\n",
        "                 loss=loss_function,\n",
        "                 metrics=[\"accuracy\"],\n",
        "                 optimizer='adam'\n",
        "                 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "1f8282f4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[10711  4207 10711]\n",
            " [10711 10711  7496]\n",
            " [10711 10711  6912]]\n",
            "(40000, 500)\n",
            "[1]\n"
          ]
        }
      ],
      "source": [
        "print(training_data[0:3, 0:3])\n",
        "print(training_data.shape)\n",
        "print(train_labels[10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "0ef7829f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "3334/3334 [==============================] - 2175s 648ms/step - loss: 0.5643 - accuracy: 0.7034 - val_loss: 0.5807 - val_accuracy: 0.6936\n",
            "Epoch 2/5\n",
            "3334/3334 [==============================] - 1951s 585ms/step - loss: 0.4498 - accuracy: 0.8035 - val_loss: 0.4740 - val_accuracy: 0.7794\n",
            "Epoch 3/5\n",
            "3334/3334 [==============================] - 1900s 570ms/step - loss: 0.3775 - accuracy: 0.8396 - val_loss: 0.3382 - val_accuracy: 0.8596\n",
            "Epoch 4/5\n",
            "3334/3334 [==============================] - 1883s 565ms/step - loss: 0.3046 - accuracy: 0.8789 - val_loss: 0.3539 - val_accuracy: 0.8608\n",
            "Epoch 5/5\n",
            "3334/3334 [==============================] - 1892s 567ms/step - loss: 0.2644 - accuracy: 0.8964 - val_loss: 0.3355 - val_accuracy: 0.8662\n",
            "CPU times: total: 9h 38min 1s\n",
            "Wall time: 2h 43min 21s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "Hist=My_LSTM_Model.fit(training_data, train_labels, batch_size=12, epochs=5, validation_data=(testing_data, test_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5beb9fdb",
      "metadata": {},
      "source": [
        "# CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "dfeaed05",
      "metadata": {},
      "outputs": [],
      "source": [
        "My_CNN_Model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Embedding(input_dim=input_dim, output_dim=32, input_length=500),\n",
        "  \n",
        "  tf.keras.layers.Conv1D(50, kernel_size=3, activation='relu'),\n",
        "  tf.keras.layers.MaxPool1D(pool_size=2),\n",
        "  \n",
        "  tf.keras.layers.Conv1D(40, kernel_size=3, activation='relu'),\n",
        "  tf.keras.layers.MaxPool1D(pool_size=2),\n",
        "  \n",
        "  tf.keras.layers.Conv1D(30, kernel_size=3, activation='relu'),\n",
        "  tf.keras.layers.MaxPool1D(pool_size=2),\n",
        "  \n",
        "  tf.keras.layers.Conv1D(30, kernel_size=3, activation='relu'),\n",
        "  tf.keras.layers.MaxPool1D(pool_size=2),\n",
        "  \n",
        "  tf.keras.layers.Flatten(),\n",
        " \n",
        "  tf.keras.layers.Dense(20),\n",
        "  tf.keras.layers.Dropout(0.5),\n",
        " \n",
        "  tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "\n",
        "  \n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "20b5ce97",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 500, 32)           342816    \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 498, 50)           4850      \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 249, 50)          0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 247, 40)           6040      \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 123, 40)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 121, 30)           3630      \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPooling  (None, 60, 30)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 58, 30)            2730      \n",
            "                                                                 \n",
            " max_pooling1d_3 (MaxPooling  (None, 29, 30)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 870)               0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 20)                17420     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 20)                0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1)                 21        \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1)                 2         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 377,509\n",
            "Trainable params: 377,509\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "My_CNN_Model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "02829f1a",
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_function = keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "My_CNN_Model.compile(\n",
        "                 loss=loss_function,\n",
        "                 metrics=[\"accuracy\"],\n",
        "                 optimizer='adam'\n",
        "                 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "4a412e0a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[10711  4207 10711]\n",
            " [10711 10711  7496]\n",
            " [10711 10711  6912]]\n",
            "(40000, 500)\n",
            "[1]\n"
          ]
        }
      ],
      "source": [
        "print(training_data[0:3, 0:3])\n",
        "print(training_data.shape)\n",
        "print(train_labels[10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "010bdd9b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "3334/3334 [==============================] - 555s 165ms/step - loss: 0.6934 - accuracy: 0.4997 - val_loss: 0.6932 - val_accuracy: 0.4990\n",
            "Epoch 2/5\n",
            "3334/3334 [==============================] - 598s 179ms/step - loss: 0.6932 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.5010\n",
            "Epoch 3/5\n",
            "3334/3334 [==============================] - 629s 189ms/step - loss: 0.6932 - accuracy: 0.5025 - val_loss: 0.6932 - val_accuracy: 0.4990\n",
            "Epoch 4/5\n",
            "3334/3334 [==============================] - 536s 161ms/step - loss: 0.6932 - accuracy: 0.5001 - val_loss: 0.6931 - val_accuracy: 0.4990\n",
            "Epoch 5/5\n",
            "3334/3334 [==============================] - 557s 167ms/step - loss: 0.6932 - accuracy: 0.4998 - val_loss: 0.6932 - val_accuracy: 0.5010\n",
            "CPU times: total: 3h 23min 17s\n",
            "Wall time: 47min 55s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "Hist=My_CNN_Model.fit(training_data, train_labels, batch_size=12, epochs=5, validation_data=(testing_data, test_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "a26d5d42",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluate model on test data\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20/20 [==============================] - 2s 94ms/step - loss: 0.6932 - accuracy: 0.5010\n",
            "test loss, test acc: [0.6931760907173157, 0.5009999871253967]\n"
          ]
        }
      ],
      "source": [
        "print(\"Evaluate model on test data\")\n",
        "results = My_CNN_Model.evaluate(testing_data, test_labels, batch_size=256)\n",
        "print(\"test loss, test acc:\", results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38c53067",
      "metadata": {},
      "source": [
        "Generate a prediction using `model.predict()` and calculate it's shape:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "9beb32bd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generate a prediction\n",
            "157/157 [==============================] - 7s 34ms/step\n",
            "[[0.504932]\n",
            " [0.504932]\n",
            " [0.504932]\n",
            " ...\n",
            " [0.504932]\n",
            " [0.504932]\n",
            " [0.504932]]\n",
            "prediction shape: (5000, 1)\n",
            "<class 'numpy.ndarray'>\n",
            "[[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " ...\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]]\n"
          ]
        }
      ],
      "source": [
        "print(\"Generate a prediction\")\n",
        "prediction = My_CNN_Model.predict(testing_data)\n",
        "print(prediction)\n",
        "print(\"prediction shape:\", prediction.shape)\n",
        "print(type(prediction))\n",
        "prediction[prediction > .5] = 1\n",
        "prediction[prediction <= .5] = 0\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "9af367b1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[   0    0]\n",
            " [2495 2505]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(prediction, test_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0709adb9",
      "metadata": {},
      "source": [
        "# One more example - An ANN used on image data that is Flattened\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "504d5be2",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84e31a6b",
      "metadata": {},
      "source": [
        "## Load the CIFAR-10 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "05299dc8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 4s 0us/step\n"
          ]
        }
      ],
      "source": [
        "(training_data2, train_labels2), (testing_data2, test_labels2) = cifar10.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ccda7c6",
      "metadata": {},
      "source": [
        "## Scale the pixel values to between 0 and 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "f9917e06",
      "metadata": {},
      "outputs": [],
      "source": [
        "training_data2 = training_data2 / 255.0\n",
        "testing_data2 = testing_data2 / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a2af7e1",
      "metadata": {},
      "source": [
        "## Convert the labels to one-hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "c33e66d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_labels2 = to_categorical(train_labels2)\n",
        "test_labels2 = to_categorical(test_labels2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "3d27266c",
      "metadata": {},
      "outputs": [],
      "source": [
        "ANN_Model_Images = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(32, 32, 3)),\n",
        "  tf.keras.layers.Dense(200, activation='relu'), \n",
        "  tf.keras.layers.Dense(150, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation=\"softmax\"),  \n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "5420facf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_1 (Flatten)         (None, 3072)              0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 200)               614600    \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 150)               30150     \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 10)                1510      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 646,260\n",
            "Trainable params: 646,260\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "ANN_Model_Images.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "7dbfc6b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "ANN_Model_Images.compile(\n",
        "                 loss=\"categorical_crossentropy\",\n",
        "                 metrics=[\"accuracy\"],\n",
        "                 optimizer='adam'\n",
        "                 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "312dacb2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 30s 17ms/step - loss: 1.8601 - accuracy: 0.3287 - val_loss: 1.7122 - val_accuracy: 0.3851\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 29s 18ms/step - loss: 1.6742 - accuracy: 0.3992 - val_loss: 1.6167 - val_accuracy: 0.4124\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 25s 16ms/step - loss: 1.5954 - accuracy: 0.4274 - val_loss: 1.5727 - val_accuracy: 0.4323\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 29s 18ms/step - loss: 1.5512 - accuracy: 0.4451 - val_loss: 1.5428 - val_accuracy: 0.4461\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 23s 15ms/step - loss: 1.5186 - accuracy: 0.4585 - val_loss: 1.5605 - val_accuracy: 0.4435\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 1.4969 - accuracy: 0.4639 - val_loss: 1.4981 - val_accuracy: 0.4638\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 1.4738 - accuracy: 0.4731 - val_loss: 1.5250 - val_accuracy: 0.4551\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 1.4578 - accuracy: 0.4779 - val_loss: 1.4636 - val_accuracy: 0.4785\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 41s 27ms/step - loss: 1.4427 - accuracy: 0.4845 - val_loss: 1.4763 - val_accuracy: 0.4742\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 51s 33ms/step - loss: 1.4292 - accuracy: 0.4875 - val_loss: 1.4734 - val_accuracy: 0.4753\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 1.4149 - accuracy: 0.4950 - val_loss: 1.4384 - val_accuracy: 0.4869\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 27s 18ms/step - loss: 1.4014 - accuracy: 0.4998 - val_loss: 1.4699 - val_accuracy: 0.4720\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.3958 - accuracy: 0.4997 - val_loss: 1.4845 - val_accuracy: 0.4649\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 24s 15ms/step - loss: 1.3798 - accuracy: 0.5066 - val_loss: 1.5352 - val_accuracy: 0.4449\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.3748 - accuracy: 0.5081 - val_loss: 1.4495 - val_accuracy: 0.4811\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.3666 - accuracy: 0.5102 - val_loss: 1.4380 - val_accuracy: 0.4921\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 1.3569 - accuracy: 0.5161 - val_loss: 1.4789 - val_accuracy: 0.4778\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 24s 16ms/step - loss: 1.3469 - accuracy: 0.5174 - val_loss: 1.4350 - val_accuracy: 0.4884\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 26s 16ms/step - loss: 1.3441 - accuracy: 0.5190 - val_loss: 1.4504 - val_accuracy: 0.4865\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 1.3344 - accuracy: 0.5233 - val_loss: 1.4724 - val_accuracy: 0.4732\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 1.3296 - accuracy: 0.5242 - val_loss: 1.4524 - val_accuracy: 0.4840\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3219 - accuracy: 0.5267 - val_loss: 1.4421 - val_accuracy: 0.4882\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3159 - accuracy: 0.5285 - val_loss: 1.4370 - val_accuracy: 0.4954\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 24s 16ms/step - loss: 1.3128 - accuracy: 0.5306 - val_loss: 1.4503 - val_accuracy: 0.4901\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 1.3066 - accuracy: 0.5329 - val_loss: 1.4566 - val_accuracy: 0.4820\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 1.3008 - accuracy: 0.5357 - val_loss: 1.4677 - val_accuracy: 0.4873\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 24s 15ms/step - loss: 1.2973 - accuracy: 0.5347 - val_loss: 1.4553 - val_accuracy: 0.4900\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 1.2921 - accuracy: 0.5377 - val_loss: 1.4420 - val_accuracy: 0.4916\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 25s 16ms/step - loss: 1.2861 - accuracy: 0.5391 - val_loss: 1.4309 - val_accuracy: 0.4967\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 22s 14ms/step - loss: 1.2842 - accuracy: 0.5398 - val_loss: 1.4378 - val_accuracy: 0.4938\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.2783 - accuracy: 0.5429 - val_loss: 1.4515 - val_accuracy: 0.4921\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 37s 24ms/step - loss: 1.2773 - accuracy: 0.5404 - val_loss: 1.4527 - val_accuracy: 0.4945\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 1.2727 - accuracy: 0.5432 - val_loss: 1.4349 - val_accuracy: 0.4970\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 52s 33ms/step - loss: 1.2735 - accuracy: 0.5435 - val_loss: 1.4382 - val_accuracy: 0.4970\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 1.2631 - accuracy: 0.5469 - val_loss: 1.4326 - val_accuracy: 0.4986\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 1.2629 - accuracy: 0.5467 - val_loss: 1.4831 - val_accuracy: 0.4810\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 59s 38ms/step - loss: 1.2589 - accuracy: 0.5493 - val_loss: 1.4521 - val_accuracy: 0.4972\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 63s 40ms/step - loss: 1.2541 - accuracy: 0.5509 - val_loss: 1.4785 - val_accuracy: 0.4909\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 1.2498 - accuracy: 0.5515 - val_loss: 1.4610 - val_accuracy: 0.4901\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 43s 27ms/step - loss: 1.2492 - accuracy: 0.5513 - val_loss: 1.4695 - val_accuracy: 0.4911\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.2410 - accuracy: 0.5554 - val_loss: 1.4745 - val_accuracy: 0.4902\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 1.2436 - accuracy: 0.5534 - val_loss: 1.4564 - val_accuracy: 0.4924\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 1.2363 - accuracy: 0.5551 - val_loss: 1.4671 - val_accuracy: 0.4834\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 1.2365 - accuracy: 0.5550 - val_loss: 1.4527 - val_accuracy: 0.4968\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.2346 - accuracy: 0.5567 - val_loss: 1.4663 - val_accuracy: 0.4925\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.2293 - accuracy: 0.5580 - val_loss: 1.4728 - val_accuracy: 0.4837\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.2313 - accuracy: 0.5597 - val_loss: 1.4722 - val_accuracy: 0.4935\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 41s 27ms/step - loss: 1.2214 - accuracy: 0.5605 - val_loss: 1.4735 - val_accuracy: 0.4899\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 1.2212 - accuracy: 0.5628 - val_loss: 1.5285 - val_accuracy: 0.4802\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.2267 - accuracy: 0.5612 - val_loss: 1.4773 - val_accuracy: 0.4844\n",
            "CPU times: total: 1h 12min 54s\n",
            "Wall time: 26min 57s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "Hist= ANN_Model_Images.fit(training_data2, train_labels2, epochs=50, validation_data=(testing_data2, test_labels2))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
