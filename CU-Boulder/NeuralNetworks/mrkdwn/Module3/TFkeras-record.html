<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Professor Ami Gates">

<title>TF/Keras with Record dataset</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="TFkeras-record_files/libs/clipboard/clipboard.min.js"></script>
<script src="TFkeras-record_files/libs/quarto-html/quarto.js"></script>
<script src="TFkeras-record_files/libs/quarto-html/popper.min.js"></script>
<script src="TFkeras-record_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="TFkeras-record_files/libs/quarto-html/anchor.min.js"></script>
<link href="TFkeras-record_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="TFkeras-record_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="TFkeras-record_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="TFkeras-record_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="TFkeras-record_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">


</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#data" id="toc-data" class="nav-link active" data-scroll-target="#data">Data</a>
  <ul class="collapse">
  <li><a href="#link-to-dataset" id="toc-link-to-dataset" class="nav-link" data-scroll-target="#link-to-dataset">Link to dataset</a></li>
  <li><a href="#prepare-the-data" id="toc-prepare-the-data" class="nav-link" data-scroll-target="#prepare-the-data">Prepare the data</a>
  <ul class="collapse">
  <li><a href="#step-1---use-min-max-to-normalize-the-data-but-not-the-label" id="toc-step-1---use-min-max-to-normalize-the-data-but-not-the-label" class="nav-link" data-scroll-target="#step-1---use-min-max-to-normalize-the-data-but-not-the-label">Step 1 - Use min-max to normalize the data (but NOT the label!)</a></li>
  <li><a href="#split-df-into-training-and-testing-sets" id="toc-split-df-into-training-and-testing-sets" class="nav-link" data-scroll-target="#split-df-into-training-and-testing-sets">Split DF into Training and Testing sets</a></li>
  <li><a href="#drop-and-save-the-labels-from-the-traindf-and-testdf" id="toc-drop-and-save-the-labels-from-the-traindf-and-testdf" class="nav-link" data-scroll-target="#drop-and-save-the-labels-from-the-traindf-and-testdf">Drop and SAVE the labels from the TrainDF and TestDF</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#using-keras" id="toc-using-keras" class="nav-link" data-scroll-target="#using-keras">Using Keras</a>
  <ul class="collapse">
  <li><a href="#step-1-create-a-tf---keras-nn-model" id="toc-step-1-create-a-tf---keras-nn-model" class="nav-link" data-scroll-target="#step-1-create-a-tf---keras-nn-model">Step 1: Create a TF - Keras NN Model</a></li>
  <li><a href="#compile-the-model" id="toc-compile-the-model" class="nav-link" data-scroll-target="#compile-the-model">Compile the Model</a></li>
  <li><a href="#fit-the-model" id="toc-fit-the-model" class="nav-link" data-scroll-target="#fit-the-model">Fit the Model</a></li>
  <li><a href="#test-and-model" id="toc-test-and-model" class="nav-link" data-scroll-target="#test-and-model">Test and Model</a></li>
  <li><a href="#save-the-model" id="toc-save-the-model" class="nav-link" data-scroll-target="#save-the-model">Save the Model</a></li>
  <li><a href="#predictions" id="toc-predictions" class="nav-link" data-scroll-target="#predictions">Predictions</a>
  <ul class="collapse">
  <li><a href="#confusion-matrix" id="toc-confusion-matrix" class="nav-link" data-scroll-target="#confusion-matrix">Confusion Matrix</a></li>
  <li><a href="#pretty-confusion-matrix" id="toc-pretty-confusion-matrix" class="nav-link" data-scroll-target="#pretty-confusion-matrix">Pretty Confusion Matrix</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">TF/Keras with Record dataset</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Professor Ami Gates </p>
          </div>
  </div>
    
  
    
  </div>
  

</header>

<p>Gates’ full, raw (unedited) code Reference: Professor Ami Gates, Dept. Applied Math, Data Science, University of Colorado</p>
<p><a href="https://gatesboltonanalytics.com/?page_id=892">Dr.&nbsp;Gates’ Website</a></p>
<hr>
<p>This code uses a simple record dataset with numeric data.</p>
<ul>
<li>The dataset has 3 columns of data (3 features)</li>
<li>The dataset has labels of 0 and 1.</li>
</ul>
<p>There is a link to the data below.</p>
<p>This code uses Keras to build a simple NN to predict the label (called “Decision”) for this dataset.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># libraries</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> keras</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> layers</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow.keras</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">#from tensorflow.keras.datasets import mnist</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.models <span class="im">import</span> Sequential</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.layers <span class="im">import</span> LSTM, Dense, Dropout, LSTM</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.optimizers <span class="im">import</span> Adam</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> layers</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="data" class="level1">
<h1>Data</h1>
<section id="link-to-dataset" class="level2">
<h2 class="anchored" data-anchor-id="link-to-dataset">Link to dataset</h2>
<p><a href="https://drive.google.com/file/d/1JjkJ4q0MMGJP8jht2ZI9qDEvV5Jjfu0r/view?usp=drive_link" class="uri">https://drive.google.com/file/d/1JjkJ4q0MMGJP8jht2ZI9qDEvV5Jjfu0r/view?usp=drive_link</a></p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">## Path to dataset on my computer</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co">## YOU will update this path for YOUR computer</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>filepath<span class="op">=</span><span class="st">"StudentSummerProgramData_Numeric_2NumLabeled_3Cols.csv"</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>DF<span class="op">=</span>pd.read_csv(filepath)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(DF.head())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   Decision   GPA  WorkExp  TestScore
0         0  3.90      6.7        962
1         0  3.80      1.4        969
2         0  3.80      2.3        970
3         0  3.60      0.9        969
4         0  3.92      1.2        969</code></pre>
</div>
</div>
</section>
<section id="prepare-the-data" class="level2">
<h2 class="anchored" data-anchor-id="prepare-the-data">Prepare the data</h2>
<p>This means you need to:</p>
<ul>
<li>Normalize the data - but NOT the label!</li>
<li>Separate the data into a Training set and a Testing set</li>
<li>Remove and retain the labels for both the Training and Testing datasets.</li>
</ul>
<section id="step-1---use-min-max-to-normalize-the-data-but-not-the-label" class="level3">
<h3 class="anchored" data-anchor-id="step-1---use-min-max-to-normalize-the-data-but-not-the-label">Step 1 - Use min-max to normalize the data (but NOT the label!)</h3>
<p>What are the column names? What column is the label?</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(DF.columns)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Index(['Decision', 'GPA', 'WorkExp', 'TestScore'], dtype='object')</code></pre>
</div>
</div>
<p>“Decision” is the label of this dataset.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col <span class="kw">in</span> DF.columns:</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">#print(col)</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> col <span class="op">!=</span> <span class="st">"Decision"</span>:</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>        DF[col]<span class="op">=</span>(DF[col]<span class="op">-</span>DF[col].<span class="bu">min</span>())<span class="op">/</span>(DF[col].<span class="bu">max</span>()<span class="op">-</span>DF[col].<span class="bu">min</span>())</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>        <span class="co">#print(DF[col])</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(DF.head())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   Decision       GPA   WorkExp  TestScore
0         0  0.981132  0.728261   0.963470
1         0  0.918239  0.152174   0.995434
2         0  0.918239  0.250000   1.000000
3         0  0.792453  0.097826   0.995434
4         0  0.993711  0.130435   0.995434</code></pre>
</div>
</div>
</section>
<section id="split-df-into-training-and-testing-sets" class="level3">
<h3 class="anchored" data-anchor-id="split-df-into-training-and-testing-sets">Split DF into Training and Testing sets</h3>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Random sampling *without replacement*</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>TrainDF, TestDF <span class="op">=</span> train_test_split(DF, test_size<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(TrainDF.shape)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(TestDF.shape)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(TestDF)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(44, 4)
(19, 4)
    Decision       GPA   WorkExp  TestScore
47         1  0.415094  0.130435   0.077626
35         1  0.767296  0.402174   0.675799
52         1  0.402516  0.076087   0.054795
6          0  0.968553  0.163043   0.986301
7          0  0.855346  0.130435   0.995434
37         1  0.861635  0.076087   0.173516
26         0  0.981132  0.000000   0.986301
50         1  0.754717  0.119565   0.073059
56         1  0.616352  0.184783   0.077626
14         0  0.855346  0.402174   0.995434
8          0  0.981132  0.510870   0.958904
27         0  0.761006  0.000000   0.963470
42         1  0.295597  1.000000   0.059361
29         0  0.830189  0.097826   0.936073
57         1  0.421384  0.152174   0.082192
62         1  0.547170  0.184783   0.068493
41         1  0.943396  0.293478   0.662100
59         1  0.358491  0.673913   0.009132
45         1  0.622642  0.173913   0.068493</code></pre>
</div>
</div>
</section>
<section id="drop-and-save-the-labels-from-the-traindf-and-testdf" class="level3">
<h3 class="anchored" data-anchor-id="drop-and-save-the-labels-from-the-traindf-and-testdf">Drop and SAVE the labels from the TrainDF and TestDF</h3>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>TrainLabels<span class="op">=</span>TrainDF[<span class="st">"Decision"</span>]</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(TrainLabels.head())</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>TestLabels<span class="op">=</span>TestDF[<span class="st">"Decision"</span>]</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(TestLabels.head())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>30    0
18    0
25    0
0     0
13    0
Name: Decision, dtype: int64
47    1
35    1
52    1
6     0
7     0
Name: Decision, dtype: int64</code></pre>
</div>
</div>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>TrainDF <span class="op">=</span> TrainDF.drop(columns<span class="op">=</span><span class="st">"Decision"</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(TrainDF.head())</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(TrainDF.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         GPA   WorkExp  TestScore
30  0.905660  0.130435   0.990868
18  0.729560  0.076087   0.977169
25  0.830189  0.239130   0.986301
0   0.981132  0.728261   0.963470
13  0.849057  0.347826   0.986301
(44, 3)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>TestDF <span class="op">=</span> TestDF.drop(columns<span class="op">=</span><span class="st">"Decision"</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(TestDF.head())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         GPA   WorkExp  TestScore
47  0.415094  0.130435   0.077626
35  0.767296  0.402174   0.675799
52  0.402516  0.076087   0.054795
6   0.968553  0.163043   0.986301
7   0.855346  0.130435   0.995434</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="using-keras" class="level1">
<h1>Using Keras</h1>
<section id="step-1-create-a-tf---keras-nn-model" class="level2">
<h2 class="anchored" data-anchor-id="step-1-create-a-tf---keras-nn-model">Step 1: Create a TF - Keras NN Model</h2>
<p><a href="https://keras.io/guides/sequential_model/" class="uri">https://keras.io/guides/sequential_model/</a></p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>My_NN_Model <span class="op">=</span> tf.keras.models.Sequential([</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>  tf.keras.layers.Dense(<span class="dv">4</span>, input_shape<span class="op">=</span>(<span class="dv">3</span>,), activation<span class="op">=</span><span class="st">'relu'</span>), <span class="co">## Our data is flat and 3D</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># (*) See note below</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>  <span class="co">## second hidden layer -  with 2 units </span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>  tf.keras.layers.Dense(<span class="dv">2</span>, activation<span class="op">=</span><span class="st">'relu'</span>),   <span class="co"># (**) referenced link below</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>  <span class="co">## The first value, 2 here, are the units in the hidden layer. </span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>  <span class="co">## (***) referenced link below</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>  <span class="co">##tf.keras.layers.Dropout(0.2),                ## We do not need this here. </span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>  tf.keras.layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">'sigmoid'</span>) <span class="co">## for 0 or 1</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>(*) <strong>Note:</strong> Here, we are sending our data to a hidden layer that has 4 hidden units.</p>
<ul>
<li><code>Dense</code> implements the operation: <code>output = activation(dot(input, kernel) + bias)</code>
<ul>
<li>The “<code>kernel</code>” are the weights matrix.</li>
</ul></li>
<li>In Keras, the input layer itself is not a layer, but a tensor. It’s the starting tensor you send to the first hidden layer. This tensor must have the same shape as your training data</li>
</ul>
<p>Other relevant links referenced in the above code: - <code>tf.keras.layers.Dense</code> - (**) <a href="https://keras.io/api/layers/core_layers/dense/" class="uri">https://keras.io/api/layers/core_layers/dense/</a> - (***) <a href="https://www.tutorialspoint.com/keras/keras_dense_layer.htm" class="uri">https://www.tutorialspoint.com/keras/keras_dense_layer.htm</a></p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>My_NN_Model.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 4)                 16        
                                                                 
 dense_1 (Dense)             (None, 2)                 10        
                                                                 
 dense_2 (Dense)             (None, 1)                 3         
                                                                 
=================================================================
Total params: 29
Trainable params: 29
Non-trainable params: 0
_________________________________________________________________</code></pre>
</div>
</div>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co">## Print the weights</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>first_layer_weights <span class="op">=</span> My_NN_Model.layers[<span class="dv">0</span>].get_weights()[<span class="dv">0</span>]</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="co">#first_layer_biases  = My_NN_Model.layers[0].get_weights()[1]</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>second_layer_weights <span class="op">=</span> My_NN_Model.layers[<span class="dv">1</span>].get_weights()[<span class="dv">0</span>]</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>third_layer_weights <span class="op">=</span> My_NN_Model.layers[<span class="dv">2</span>].get_weights()[<span class="dv">0</span>]</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The first layer weights are: </span><span class="ch">\n</span><span class="st">"</span>, first_layer_weights)</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The first layer weights shape is</span><span class="ch">\n</span><span class="st">"</span>,first_layer_weights.shape )</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The second layer weights are: </span><span class="ch">\n</span><span class="st">"</span>, second_layer_weights)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The third layer weights are: </span><span class="ch">\n</span><span class="st">"</span>, third_layer_weights)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The first layer weights are: 
 [[-0.5025393  -0.05162388  0.8350564   0.5958649 ]
 [-0.7560888   0.07614303  0.78565943 -0.0158782 ]
 [ 0.14354777 -0.4847178  -0.13484716 -0.35753334]]
The first layer weights shape is
 (3, 4)
The second layer weights are: 
 [[ 0.60221076  0.8026402 ]
 [-0.5638485   0.41089344]
 [ 0.49096894 -0.429451  ]
 [ 0.1907084  -0.6241598 ]]
The third layer weights are: 
 [[-1.2455409]
 [-1.3225081]]</code></pre>
</div>
</div>
</section>
<section id="compile-the-model" class="level2">
<h2 class="anchored" data-anchor-id="compile-the-model">Compile the Model</h2>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>loss_function <span class="op">=</span> keras.losses.BinaryCrossentropy(from_logits<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>My_NN_Model.<span class="bu">compile</span>(</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>                 loss<span class="op">=</span>loss_function,</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>                 metrics<span class="op">=</span>[<span class="st">"accuracy"</span>],</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>                 optimizer<span class="op">=</span><span class="st">'adam'</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>                 )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="fit-the-model" class="level2">
<h2 class="anchored" data-anchor-id="fit-the-model">Fit the Model</h2>
<p>Fit the model to the data (train the model).</p>
<p>(Making the epochs larger can improve the model and prediction accuracy)</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>Hist<span class="op">=</span>My_NN_Model.fit(TrainDF,TrainLabels, epochs<span class="op">=</span><span class="dv">300</span>,  validation_data<span class="op">=</span>(TestDF, TestLabels))</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="co">## batch_size=   is also an option here for batch training</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/300
2/2 [==============================] - 1s 458ms/step - loss: 0.6407 - accuracy: 0.6364 - val_loss: 0.7859 - val_accuracy: 0.3684
Epoch 2/300
2/2 [==============================] - 0s 48ms/step - loss: 0.6400 - accuracy: 0.6364 - val_loss: 0.7859 - val_accuracy: 0.3684
Epoch 3/300
2/2 [==============================] - 0s 48ms/step - loss: 0.6395 - accuracy: 0.6364 - val_loss: 0.7863 - val_accuracy: 0.3684
Epoch 4/300
2/2 [==============================] - 0s 45ms/step - loss: 0.6388 - accuracy: 0.6364 - val_loss: 0.7867 - val_accuracy: 0.3684
Epoch 5/300
2/2 [==============================] - 0s 44ms/step - loss: 0.6383 - accuracy: 0.6364 - val_loss: 0.7873 - val_accuracy: 0.3684
Epoch 6/300
2/2 [==============================] - 0s 49ms/step - loss: 0.6376 - accuracy: 0.6364 - val_loss: 0.7878 - val_accuracy: 0.3684
Epoch 7/300
2/2 [==============================] - 0s 45ms/step - loss: 0.6370 - accuracy: 0.6364 - val_loss: 0.7885 - val_accuracy: 0.3684
Epoch 8/300
2/2 [==============================] - 0s 45ms/step - loss: 0.6363 - accuracy: 0.6364 - val_loss: 0.7893 - val_accuracy: 0.3684
Epoch 9/300
2/2 [==============================] - 0s 46ms/step - loss: 0.6357 - accuracy: 0.6364 - val_loss: 0.7903 - val_accuracy: 0.3684
Epoch 10/300
2/2 [==============================] - 0s 43ms/step - loss: 0.6352 - accuracy: 0.6364 - val_loss: 0.7914 - val_accuracy: 0.3684
Epoch 11/300
2/2 [==============================] - 0s 42ms/step - loss: 0.6345 - accuracy: 0.6364 - val_loss: 0.7924 - val_accuracy: 0.3684
Epoch 12/300
2/2 [==============================] - 0s 43ms/step - loss: 0.6339 - accuracy: 0.6364 - val_loss: 0.7935 - val_accuracy: 0.3684
Epoch 13/300
2/2 [==============================] - 0s 46ms/step - loss: 0.6335 - accuracy: 0.6364 - val_loss: 0.7946 - val_accuracy: 0.3684
Epoch 14/300
2/2 [==============================] - 0s 47ms/step - loss: 0.6329 - accuracy: 0.6364 - val_loss: 0.7952 - val_accuracy: 0.3684
Epoch 15/300
2/2 [==============================] - 0s 47ms/step - loss: 0.6323 - accuracy: 0.6364 - val_loss: 0.7954 - val_accuracy: 0.3684
Epoch 16/300
2/2 [==============================] - 0s 44ms/step - loss: 0.6317 - accuracy: 0.6364 - val_loss: 0.7952 - val_accuracy: 0.3684
Epoch 17/300
2/2 [==============================] - 0s 45ms/step - loss: 0.6313 - accuracy: 0.6364 - val_loss: 0.7952 - val_accuracy: 0.3684
Epoch 18/300
2/2 [==============================] - 0s 46ms/step - loss: 0.6308 - accuracy: 0.6364 - val_loss: 0.7950 - val_accuracy: 0.3684
Epoch 19/300
2/2 [==============================] - 0s 47ms/step - loss: 0.6303 - accuracy: 0.6364 - val_loss: 0.7948 - val_accuracy: 0.3684
Epoch 20/300
2/2 [==============================] - 0s 42ms/step - loss: 0.6298 - accuracy: 0.6364 - val_loss: 0.7948 - val_accuracy: 0.3684
Epoch 21/300
2/2 [==============================] - 0s 44ms/step - loss: 0.6293 - accuracy: 0.6364 - val_loss: 0.7950 - val_accuracy: 0.3684
Epoch 22/300
2/2 [==============================] - 0s 47ms/step - loss: 0.6288 - accuracy: 0.6364 - val_loss: 0.7951 - val_accuracy: 0.3684
Epoch 23/300
2/2 [==============================] - 0s 47ms/step - loss: 0.6283 - accuracy: 0.6364 - val_loss: 0.7950 - val_accuracy: 0.3684
Epoch 24/300
2/2 [==============================] - 0s 45ms/step - loss: 0.6278 - accuracy: 0.6364 - val_loss: 0.7950 - val_accuracy: 0.3684
Epoch 25/300
2/2 [==============================] - 0s 47ms/step - loss: 0.6273 - accuracy: 0.6364 - val_loss: 0.7950 - val_accuracy: 0.3684
Epoch 26/300
2/2 [==============================] - 0s 48ms/step - loss: 0.6267 - accuracy: 0.6364 - val_loss: 0.7956 - val_accuracy: 0.3684
Epoch 27/300
2/2 [==============================] - 0s 43ms/step - loss: 0.6263 - accuracy: 0.6364 - val_loss: 0.7962 - val_accuracy: 0.3684
Epoch 28/300
2/2 [==============================] - 0s 46ms/step - loss: 0.6258 - accuracy: 0.6364 - val_loss: 0.7966 - val_accuracy: 0.3684
Epoch 29/300
2/2 [==============================] - 0s 46ms/step - loss: 0.6253 - accuracy: 0.6364 - val_loss: 0.7969 - val_accuracy: 0.3684
Epoch 30/300
2/2 [==============================] - 0s 46ms/step - loss: 0.6248 - accuracy: 0.6364 - val_loss: 0.7969 - val_accuracy: 0.3684
Epoch 31/300
2/2 [==============================] - 0s 53ms/step - loss: 0.6243 - accuracy: 0.6364 - val_loss: 0.7972 - val_accuracy: 0.3684
Epoch 32/300
2/2 [==============================] - 0s 54ms/step - loss: 0.6238 - accuracy: 0.6364 - val_loss: 0.7976 - val_accuracy: 0.3684
Epoch 33/300
2/2 [==============================] - 0s 44ms/step - loss: 0.6233 - accuracy: 0.6364 - val_loss: 0.7981 - val_accuracy: 0.3684
Epoch 34/300
2/2 [==============================] - 0s 49ms/step - loss: 0.6229 - accuracy: 0.6364 - val_loss: 0.7987 - val_accuracy: 0.3684
Epoch 35/300
2/2 [==============================] - 0s 48ms/step - loss: 0.6224 - accuracy: 0.6364 - val_loss: 0.7990 - val_accuracy: 0.3684
Epoch 36/300
2/2 [==============================] - 0s 52ms/step - loss: 0.6219 - accuracy: 0.6364 - val_loss: 0.7989 - val_accuracy: 0.3684
Epoch 37/300
2/2 [==============================] - 0s 57ms/step - loss: 0.6214 - accuracy: 0.6364 - val_loss: 0.7987 - val_accuracy: 0.3684
Epoch 38/300
2/2 [==============================] - 0s 52ms/step - loss: 0.6209 - accuracy: 0.6364 - val_loss: 0.7983 - val_accuracy: 0.3684
Epoch 39/300
2/2 [==============================] - 0s 51ms/step - loss: 0.6205 - accuracy: 0.6364 - val_loss: 0.7982 - val_accuracy: 0.3684
Epoch 40/300
2/2 [==============================] - 0s 50ms/step - loss: 0.6200 - accuracy: 0.6364 - val_loss: 0.7982 - val_accuracy: 0.3684
Epoch 41/300
2/2 [==============================] - 0s 46ms/step - loss: 0.6196 - accuracy: 0.6364 - val_loss: 0.7983 - val_accuracy: 0.3684
Epoch 42/300
2/2 [==============================] - 0s 48ms/step - loss: 0.6191 - accuracy: 0.6364 - val_loss: 0.7982 - val_accuracy: 0.3684
Epoch 43/300
2/2 [==============================] - 0s 57ms/step - loss: 0.6186 - accuracy: 0.6364 - val_loss: 0.7982 - val_accuracy: 0.3684
Epoch 44/300
2/2 [==============================] - 0s 52ms/step - loss: 0.6182 - accuracy: 0.6364 - val_loss: 0.7981 - val_accuracy: 0.3684
Epoch 45/300
2/2 [==============================] - 0s 53ms/step - loss: 0.6177 - accuracy: 0.6364 - val_loss: 0.7981 - val_accuracy: 0.3684
Epoch 46/300
2/2 [==============================] - 0s 45ms/step - loss: 0.6172 - accuracy: 0.6364 - val_loss: 0.7980 - val_accuracy: 0.3684
Epoch 47/300
2/2 [==============================] - 0s 50ms/step - loss: 0.6167 - accuracy: 0.6364 - val_loss: 0.7983 - val_accuracy: 0.3684
Epoch 48/300
2/2 [==============================] - 0s 49ms/step - loss: 0.6163 - accuracy: 0.6364 - val_loss: 0.7989 - val_accuracy: 0.3684
Epoch 49/300
2/2 [==============================] - 0s 48ms/step - loss: 0.6159 - accuracy: 0.6364 - val_loss: 0.7991 - val_accuracy: 0.3684
Epoch 50/300
2/2 [==============================] - 0s 48ms/step - loss: 0.6153 - accuracy: 0.6364 - val_loss: 0.7991 - val_accuracy: 0.3684
Epoch 51/300
2/2 [==============================] - 0s 46ms/step - loss: 0.6149 - accuracy: 0.6364 - val_loss: 0.7994 - val_accuracy: 0.3684
Epoch 52/300
2/2 [==============================] - 0s 44ms/step - loss: 0.6144 - accuracy: 0.6364 - val_loss: 0.7997 - val_accuracy: 0.3684
Epoch 53/300
2/2 [==============================] - 0s 46ms/step - loss: 0.6139 - accuracy: 0.6364 - val_loss: 0.8003 - val_accuracy: 0.3684
Epoch 54/300
2/2 [==============================] - 0s 50ms/step - loss: 0.6135 - accuracy: 0.6364 - val_loss: 0.8008 - val_accuracy: 0.3684
Epoch 55/300
2/2 [==============================] - 0s 47ms/step - loss: 0.6130 - accuracy: 0.6364 - val_loss: 0.8011 - val_accuracy: 0.3684
Epoch 56/300
2/2 [==============================] - 0s 49ms/step - loss: 0.6126 - accuracy: 0.6364 - val_loss: 0.8013 - val_accuracy: 0.3684
Epoch 57/300
2/2 [==============================] - 0s 55ms/step - loss: 0.6121 - accuracy: 0.6364 - val_loss: 0.8010 - val_accuracy: 0.3684
Epoch 58/300
2/2 [==============================] - 0s 52ms/step - loss: 0.6116 - accuracy: 0.6364 - val_loss: 0.8007 - val_accuracy: 0.3684
Epoch 59/300
2/2 [==============================] - 0s 51ms/step - loss: 0.6111 - accuracy: 0.6364 - val_loss: 0.8004 - val_accuracy: 0.3684
Epoch 60/300
2/2 [==============================] - 0s 56ms/step - loss: 0.6107 - accuracy: 0.6364 - val_loss: 0.7999 - val_accuracy: 0.3684
Epoch 61/300
2/2 [==============================] - 0s 51ms/step - loss: 0.6102 - accuracy: 0.6364 - val_loss: 0.7994 - val_accuracy: 0.3684
Epoch 62/300
2/2 [==============================] - 0s 54ms/step - loss: 0.6097 - accuracy: 0.6364 - val_loss: 0.7986 - val_accuracy: 0.3684
Epoch 63/300
2/2 [==============================] - 0s 49ms/step - loss: 0.6093 - accuracy: 0.6364 - val_loss: 0.7976 - val_accuracy: 0.3684
Epoch 64/300
2/2 [==============================] - 0s 47ms/step - loss: 0.6088 - accuracy: 0.6364 - val_loss: 0.7966 - val_accuracy: 0.3684
Epoch 65/300
2/2 [==============================] - 0s 44ms/step - loss: 0.6084 - accuracy: 0.6364 - val_loss: 0.7958 - val_accuracy: 0.3684
Epoch 66/300
2/2 [==============================] - 0s 47ms/step - loss: 0.6079 - accuracy: 0.6364 - val_loss: 0.7954 - val_accuracy: 0.3684
Epoch 67/300
2/2 [==============================] - 0s 44ms/step - loss: 0.6074 - accuracy: 0.6364 - val_loss: 0.7952 - val_accuracy: 0.3684
Epoch 68/300
2/2 [==============================] - 0s 46ms/step - loss: 0.6070 - accuracy: 0.6364 - val_loss: 0.7951 - val_accuracy: 0.3684
Epoch 69/300
2/2 [==============================] - 0s 43ms/step - loss: 0.6065 - accuracy: 0.6364 - val_loss: 0.7949 - val_accuracy: 0.3684
Epoch 70/300
2/2 [==============================] - 0s 44ms/step - loss: 0.6060 - accuracy: 0.6364 - val_loss: 0.7949 - val_accuracy: 0.3684
Epoch 71/300
2/2 [==============================] - 0s 48ms/step - loss: 0.6055 - accuracy: 0.6364 - val_loss: 0.7949 - val_accuracy: 0.3684
Epoch 72/300
2/2 [==============================] - 0s 44ms/step - loss: 0.6051 - accuracy: 0.6364 - val_loss: 0.7948 - val_accuracy: 0.3684
Epoch 73/300
2/2 [==============================] - 0s 47ms/step - loss: 0.6046 - accuracy: 0.6364 - val_loss: 0.7945 - val_accuracy: 0.3684
Epoch 74/300
2/2 [==============================] - 0s 51ms/step - loss: 0.6041 - accuracy: 0.6364 - val_loss: 0.7941 - val_accuracy: 0.3684
Epoch 75/300
2/2 [==============================] - 0s 55ms/step - loss: 0.6036 - accuracy: 0.6364 - val_loss: 0.7934 - val_accuracy: 0.3684
Epoch 76/300
2/2 [==============================] - 0s 56ms/step - loss: 0.6031 - accuracy: 0.6591 - val_loss: 0.7925 - val_accuracy: 0.3684
Epoch 77/300
2/2 [==============================] - 0s 86ms/step - loss: 0.6026 - accuracy: 0.6591 - val_loss: 0.7916 - val_accuracy: 0.3684
Epoch 78/300
2/2 [==============================] - 0s 59ms/step - loss: 0.6022 - accuracy: 0.6591 - val_loss: 0.7908 - val_accuracy: 0.3684
Epoch 79/300
2/2 [==============================] - 0s 54ms/step - loss: 0.6017 - accuracy: 0.6591 - val_loss: 0.7903 - val_accuracy: 0.3684
Epoch 80/300
2/2 [==============================] - 0s 57ms/step - loss: 0.6012 - accuracy: 0.6591 - val_loss: 0.7895 - val_accuracy: 0.3684
Epoch 81/300
2/2 [==============================] - 0s 101ms/step - loss: 0.6007 - accuracy: 0.6591 - val_loss: 0.7888 - val_accuracy: 0.3684
Epoch 82/300
2/2 [==============================] - 0s 44ms/step - loss: 0.6002 - accuracy: 0.6591 - val_loss: 0.7883 - val_accuracy: 0.3684
Epoch 83/300
2/2 [==============================] - 0s 48ms/step - loss: 0.5997 - accuracy: 0.6591 - val_loss: 0.7882 - val_accuracy: 0.3684
Epoch 84/300
2/2 [==============================] - 0s 47ms/step - loss: 0.5993 - accuracy: 0.6591 - val_loss: 0.7882 - val_accuracy: 0.3684
Epoch 85/300
2/2 [==============================] - 0s 46ms/step - loss: 0.5987 - accuracy: 0.6591 - val_loss: 0.7881 - val_accuracy: 0.3684
Epoch 86/300
2/2 [==============================] - 0s 48ms/step - loss: 0.5983 - accuracy: 0.6591 - val_loss: 0.7879 - val_accuracy: 0.3684
Epoch 87/300
2/2 [==============================] - 0s 47ms/step - loss: 0.5978 - accuracy: 0.6591 - val_loss: 0.7874 - val_accuracy: 0.3684
Epoch 88/300
2/2 [==============================] - 0s 50ms/step - loss: 0.5973 - accuracy: 0.6591 - val_loss: 0.7868 - val_accuracy: 0.3684
Epoch 89/300
2/2 [==============================] - 0s 52ms/step - loss: 0.5968 - accuracy: 0.6591 - val_loss: 0.7861 - val_accuracy: 0.3684
Epoch 90/300
2/2 [==============================] - 0s 51ms/step - loss: 0.5964 - accuracy: 0.6591 - val_loss: 0.7854 - val_accuracy: 0.3684
Epoch 91/300
2/2 [==============================] - 0s 51ms/step - loss: 0.5959 - accuracy: 0.6591 - val_loss: 0.7846 - val_accuracy: 0.3684
Epoch 92/300
2/2 [==============================] - 0s 54ms/step - loss: 0.5955 - accuracy: 0.6591 - val_loss: 0.7835 - val_accuracy: 0.3684
Epoch 93/300
2/2 [==============================] - 0s 46ms/step - loss: 0.5950 - accuracy: 0.6591 - val_loss: 0.7827 - val_accuracy: 0.3684
Epoch 94/300
2/2 [==============================] - 0s 54ms/step - loss: 0.5946 - accuracy: 0.6591 - val_loss: 0.7819 - val_accuracy: 0.3684
Epoch 95/300
2/2 [==============================] - 0s 51ms/step - loss: 0.5941 - accuracy: 0.6591 - val_loss: 0.7813 - val_accuracy: 0.3684
Epoch 96/300
2/2 [==============================] - 0s 44ms/step - loss: 0.5937 - accuracy: 0.6591 - val_loss: 0.7809 - val_accuracy: 0.3684
Epoch 97/300
2/2 [==============================] - 0s 48ms/step - loss: 0.5932 - accuracy: 0.6591 - val_loss: 0.7809 - val_accuracy: 0.3684
Epoch 98/300
2/2 [==============================] - 0s 53ms/step - loss: 0.5927 - accuracy: 0.6591 - val_loss: 0.7809 - val_accuracy: 0.3684
Epoch 99/300
2/2 [==============================] - 0s 50ms/step - loss: 0.5922 - accuracy: 0.6591 - val_loss: 0.7809 - val_accuracy: 0.3684
Epoch 100/300
2/2 [==============================] - 0s 53ms/step - loss: 0.5917 - accuracy: 0.6591 - val_loss: 0.7808 - val_accuracy: 0.3684
Epoch 101/300
2/2 [==============================] - 0s 50ms/step - loss: 0.5912 - accuracy: 0.6591 - val_loss: 0.7806 - val_accuracy: 0.3684
Epoch 102/300
2/2 [==============================] - 0s 43ms/step - loss: 0.5907 - accuracy: 0.6591 - val_loss: 0.7805 - val_accuracy: 0.3684
Epoch 103/300
2/2 [==============================] - 0s 79ms/step - loss: 0.5902 - accuracy: 0.6591 - val_loss: 0.7806 - val_accuracy: 0.3684
Epoch 104/300
2/2 [==============================] - 0s 51ms/step - loss: 0.5897 - accuracy: 0.6591 - val_loss: 0.7806 - val_accuracy: 0.3684
Epoch 105/300
2/2 [==============================] - 0s 52ms/step - loss: 0.5892 - accuracy: 0.6591 - val_loss: 0.7805 - val_accuracy: 0.3684
Epoch 106/300
2/2 [==============================] - 0s 50ms/step - loss: 0.5886 - accuracy: 0.6591 - val_loss: 0.7801 - val_accuracy: 0.3684
Epoch 107/300
2/2 [==============================] - 0s 48ms/step - loss: 0.5882 - accuracy: 0.6591 - val_loss: 0.7800 - val_accuracy: 0.3684
Epoch 108/300
2/2 [==============================] - 0s 51ms/step - loss: 0.5876 - accuracy: 0.6591 - val_loss: 0.7797 - val_accuracy: 0.3684
Epoch 109/300
2/2 [==============================] - 0s 50ms/step - loss: 0.5871 - accuracy: 0.6591 - val_loss: 0.7793 - val_accuracy: 0.3684
Epoch 110/300
2/2 [==============================] - 0s 51ms/step - loss: 0.5866 - accuracy: 0.6591 - val_loss: 0.7790 - val_accuracy: 0.3684
Epoch 111/300
2/2 [==============================] - 0s 49ms/step - loss: 0.5861 - accuracy: 0.6591 - val_loss: 0.7783 - val_accuracy: 0.3684
Epoch 112/300
2/2 [==============================] - 0s 52ms/step - loss: 0.5856 - accuracy: 0.6591 - val_loss: 0.7777 - val_accuracy: 0.3684
Epoch 113/300
2/2 [==============================] - 0s 48ms/step - loss: 0.5851 - accuracy: 0.6591 - val_loss: 0.7768 - val_accuracy: 0.3684
Epoch 114/300
2/2 [==============================] - 0s 50ms/step - loss: 0.5846 - accuracy: 0.6591 - val_loss: 0.7757 - val_accuracy: 0.3684
Epoch 115/300
2/2 [==============================] - 0s 49ms/step - loss: 0.5841 - accuracy: 0.6591 - val_loss: 0.7749 - val_accuracy: 0.3684
Epoch 116/300
2/2 [==============================] - 0s 52ms/step - loss: 0.5836 - accuracy: 0.6591 - val_loss: 0.7743 - val_accuracy: 0.3684
Epoch 117/300
2/2 [==============================] - 0s 67ms/step - loss: 0.5831 - accuracy: 0.6591 - val_loss: 0.7742 - val_accuracy: 0.3684
Epoch 118/300
2/2 [==============================] - 0s 57ms/step - loss: 0.5826 - accuracy: 0.6591 - val_loss: 0.7740 - val_accuracy: 0.3684
Epoch 119/300
2/2 [==============================] - 0s 58ms/step - loss: 0.5820 - accuracy: 0.6591 - val_loss: 0.7735 - val_accuracy: 0.3684
Epoch 120/300
2/2 [==============================] - 0s 69ms/step - loss: 0.5815 - accuracy: 0.6591 - val_loss: 0.7730 - val_accuracy: 0.3684
Epoch 121/300
2/2 [==============================] - 0s 54ms/step - loss: 0.5809 - accuracy: 0.6591 - val_loss: 0.7728 - val_accuracy: 0.3684
Epoch 122/300
2/2 [==============================] - 0s 56ms/step - loss: 0.5804 - accuracy: 0.6591 - val_loss: 0.7728 - val_accuracy: 0.3684
Epoch 123/300
2/2 [==============================] - 0s 54ms/step - loss: 0.5798 - accuracy: 0.6591 - val_loss: 0.7728 - val_accuracy: 0.3684
Epoch 124/300
2/2 [==============================] - 0s 49ms/step - loss: 0.5793 - accuracy: 0.6591 - val_loss: 0.7727 - val_accuracy: 0.3684
Epoch 125/300
2/2 [==============================] - 0s 49ms/step - loss: 0.5788 - accuracy: 0.6591 - val_loss: 0.7726 - val_accuracy: 0.3684
Epoch 126/300
2/2 [==============================] - 0s 86ms/step - loss: 0.5782 - accuracy: 0.6591 - val_loss: 0.7721 - val_accuracy: 0.3684
Epoch 127/300
2/2 [==============================] - 0s 48ms/step - loss: 0.5776 - accuracy: 0.6591 - val_loss: 0.7714 - val_accuracy: 0.3684
Epoch 128/300
2/2 [==============================] - 0s 49ms/step - loss: 0.5771 - accuracy: 0.6591 - val_loss: 0.7707 - val_accuracy: 0.3684
Epoch 129/300
2/2 [==============================] - 0s 48ms/step - loss: 0.5765 - accuracy: 0.6591 - val_loss: 0.7703 - val_accuracy: 0.3684
Epoch 130/300
2/2 [==============================] - 0s 46ms/step - loss: 0.5761 - accuracy: 0.6591 - val_loss: 0.7700 - val_accuracy: 0.3684
Epoch 131/300
2/2 [==============================] - 0s 49ms/step - loss: 0.5754 - accuracy: 0.6591 - val_loss: 0.7694 - val_accuracy: 0.3684
Epoch 132/300
2/2 [==============================] - 0s 50ms/step - loss: 0.5749 - accuracy: 0.6591 - val_loss: 0.7689 - val_accuracy: 0.3684
Epoch 133/300
2/2 [==============================] - 0s 51ms/step - loss: 0.5743 - accuracy: 0.6591 - val_loss: 0.7682 - val_accuracy: 0.3684
Epoch 134/300
2/2 [==============================] - 0s 54ms/step - loss: 0.5738 - accuracy: 0.6591 - val_loss: 0.7672 - val_accuracy: 0.3684
Epoch 135/300
2/2 [==============================] - 0s 46ms/step - loss: 0.5733 - accuracy: 0.6591 - val_loss: 0.7662 - val_accuracy: 0.3684
Epoch 136/300
2/2 [==============================] - 0s 47ms/step - loss: 0.5727 - accuracy: 0.6591 - val_loss: 0.7655 - val_accuracy: 0.3684
Epoch 137/300
2/2 [==============================] - 0s 49ms/step - loss: 0.5721 - accuracy: 0.6591 - val_loss: 0.7651 - val_accuracy: 0.3684
Epoch 138/300
2/2 [==============================] - 0s 49ms/step - loss: 0.5716 - accuracy: 0.6591 - val_loss: 0.7650 - val_accuracy: 0.3684
Epoch 139/300
2/2 [==============================] - 0s 50ms/step - loss: 0.5710 - accuracy: 0.6591 - val_loss: 0.7646 - val_accuracy: 0.3684
Epoch 140/300
2/2 [==============================] - 0s 55ms/step - loss: 0.5704 - accuracy: 0.6591 - val_loss: 0.7643 - val_accuracy: 0.3684
Epoch 141/300
2/2 [==============================] - 0s 50ms/step - loss: 0.5698 - accuracy: 0.6591 - val_loss: 0.7641 - val_accuracy: 0.3684
Epoch 142/300
2/2 [==============================] - 0s 50ms/step - loss: 0.5692 - accuracy: 0.6591 - val_loss: 0.7640 - val_accuracy: 0.3684
Epoch 143/300
2/2 [==============================] - 0s 45ms/step - loss: 0.5686 - accuracy: 0.6591 - val_loss: 0.7642 - val_accuracy: 0.3684
Epoch 144/300
2/2 [==============================] - 0s 49ms/step - loss: 0.5679 - accuracy: 0.6591 - val_loss: 0.7643 - val_accuracy: 0.3684
Epoch 145/300
2/2 [==============================] - 0s 52ms/step - loss: 0.5674 - accuracy: 0.6591 - val_loss: 0.7644 - val_accuracy: 0.3684
Epoch 146/300
2/2 [==============================] - 0s 49ms/step - loss: 0.5667 - accuracy: 0.6591 - val_loss: 0.7644 - val_accuracy: 0.3684
Epoch 147/300
2/2 [==============================] - 0s 50ms/step - loss: 0.5661 - accuracy: 0.6591 - val_loss: 0.7644 - val_accuracy: 0.3684
Epoch 148/300
2/2 [==============================] - 0s 49ms/step - loss: 0.5655 - accuracy: 0.6591 - val_loss: 0.7645 - val_accuracy: 0.3684
Epoch 149/300
2/2 [==============================] - 0s 47ms/step - loss: 0.5649 - accuracy: 0.6591 - val_loss: 0.7646 - val_accuracy: 0.3684
Epoch 150/300
2/2 [==============================] - 0s 50ms/step - loss: 0.5644 - accuracy: 0.6591 - val_loss: 0.7647 - val_accuracy: 0.3684
Epoch 151/300
2/2 [==============================] - 0s 50ms/step - loss: 0.5637 - accuracy: 0.6591 - val_loss: 0.7644 - val_accuracy: 0.3684
Epoch 152/300
2/2 [==============================] - 0s 98ms/step - loss: 0.5631 - accuracy: 0.6591 - val_loss: 0.7641 - val_accuracy: 0.3684
Epoch 153/300
2/2 [==============================] - 0s 49ms/step - loss: 0.5625 - accuracy: 0.6591 - val_loss: 0.7637 - val_accuracy: 0.3684
Epoch 154/300
2/2 [==============================] - 0s 48ms/step - loss: 0.5619 - accuracy: 0.6591 - val_loss: 0.7633 - val_accuracy: 0.3684
Epoch 155/300
2/2 [==============================] - 0s 47ms/step - loss: 0.5612 - accuracy: 0.6591 - val_loss: 0.7631 - val_accuracy: 0.3684
Epoch 156/300
2/2 [==============================] - 0s 46ms/step - loss: 0.5606 - accuracy: 0.6591 - val_loss: 0.7629 - val_accuracy: 0.3684
Epoch 157/300
2/2 [==============================] - 0s 50ms/step - loss: 0.5600 - accuracy: 0.6591 - val_loss: 0.7626 - val_accuracy: 0.3684
Epoch 158/300
2/2 [==============================] - 0s 48ms/step - loss: 0.5594 - accuracy: 0.6591 - val_loss: 0.7626 - val_accuracy: 0.3684
Epoch 159/300
2/2 [==============================] - 0s 52ms/step - loss: 0.5588 - accuracy: 0.6591 - val_loss: 0.7625 - val_accuracy: 0.3684
Epoch 160/300
2/2 [==============================] - 0s 53ms/step - loss: 0.5581 - accuracy: 0.6591 - val_loss: 0.7623 - val_accuracy: 0.3684
Epoch 161/300
2/2 [==============================] - 0s 54ms/step - loss: 0.5575 - accuracy: 0.6591 - val_loss: 0.7624 - val_accuracy: 0.3684
Epoch 162/300
2/2 [==============================] - 0s 59ms/step - loss: 0.5568 - accuracy: 0.6591 - val_loss: 0.7628 - val_accuracy: 0.3684
Epoch 163/300
2/2 [==============================] - 0s 52ms/step - loss: 0.5562 - accuracy: 0.6591 - val_loss: 0.7631 - val_accuracy: 0.3684
Epoch 164/300
2/2 [==============================] - 0s 79ms/step - loss: 0.5555 - accuracy: 0.6591 - val_loss: 0.7634 - val_accuracy: 0.3684
Epoch 165/300
2/2 [==============================] - 0s 51ms/step - loss: 0.5549 - accuracy: 0.6591 - val_loss: 0.7636 - val_accuracy: 0.3684
Epoch 166/300
2/2 [==============================] - 0s 52ms/step - loss: 0.5543 - accuracy: 0.6591 - val_loss: 0.7638 - val_accuracy: 0.3684
Epoch 167/300
2/2 [==============================] - 0s 46ms/step - loss: 0.5539 - accuracy: 0.6591 - val_loss: 0.7639 - val_accuracy: 0.3684
Epoch 168/300
2/2 [==============================] - 0s 48ms/step - loss: 0.5530 - accuracy: 0.6591 - val_loss: 0.7630 - val_accuracy: 0.3684
Epoch 169/300
2/2 [==============================] - 0s 43ms/step - loss: 0.5524 - accuracy: 0.6591 - val_loss: 0.7618 - val_accuracy: 0.3684
Epoch 170/300
2/2 [==============================] - 0s 49ms/step - loss: 0.5517 - accuracy: 0.6591 - val_loss: 0.7607 - val_accuracy: 0.3684
Epoch 171/300
2/2 [==============================] - 0s 48ms/step - loss: 0.5510 - accuracy: 0.6591 - val_loss: 0.7599 - val_accuracy: 0.3684
Epoch 172/300
2/2 [==============================] - 0s 47ms/step - loss: 0.5504 - accuracy: 0.6591 - val_loss: 0.7591 - val_accuracy: 0.3684
Epoch 173/300
2/2 [==============================] - 0s 47ms/step - loss: 0.5497 - accuracy: 0.6591 - val_loss: 0.7583 - val_accuracy: 0.3684
Epoch 174/300
2/2 [==============================] - 0s 49ms/step - loss: 0.5491 - accuracy: 0.6591 - val_loss: 0.7576 - val_accuracy: 0.3684
Epoch 175/300
2/2 [==============================] - 0s 46ms/step - loss: 0.5484 - accuracy: 0.6591 - val_loss: 0.7566 - val_accuracy: 0.3684
Epoch 176/300
2/2 [==============================] - 0s 45ms/step - loss: 0.5478 - accuracy: 0.6591 - val_loss: 0.7554 - val_accuracy: 0.3684
Epoch 177/300
2/2 [==============================] - 0s 49ms/step - loss: 0.5470 - accuracy: 0.6591 - val_loss: 0.7545 - val_accuracy: 0.3684
Epoch 178/300
2/2 [==============================] - 0s 42ms/step - loss: 0.5464 - accuracy: 0.6591 - val_loss: 0.7535 - val_accuracy: 0.3684
Epoch 179/300
2/2 [==============================] - 0s 98ms/step - loss: 0.5457 - accuracy: 0.6591 - val_loss: 0.7525 - val_accuracy: 0.3684
Epoch 180/300
2/2 [==============================] - 0s 47ms/step - loss: 0.5450 - accuracy: 0.6591 - val_loss: 0.7513 - val_accuracy: 0.3684
Epoch 181/300
2/2 [==============================] - 0s 43ms/step - loss: 0.5444 - accuracy: 0.6591 - val_loss: 0.7497 - val_accuracy: 0.3684
Epoch 182/300
2/2 [==============================] - 0s 46ms/step - loss: 0.5436 - accuracy: 0.6591 - val_loss: 0.7484 - val_accuracy: 0.3684
Epoch 183/300
2/2 [==============================] - 0s 43ms/step - loss: 0.5429 - accuracy: 0.6591 - val_loss: 0.7469 - val_accuracy: 0.3684
Epoch 184/300
2/2 [==============================] - 0s 43ms/step - loss: 0.5423 - accuracy: 0.6591 - val_loss: 0.7456 - val_accuracy: 0.3684
Epoch 185/300
2/2 [==============================] - 0s 44ms/step - loss: 0.5415 - accuracy: 0.6591 - val_loss: 0.7445 - val_accuracy: 0.3684
Epoch 186/300
2/2 [==============================] - 0s 46ms/step - loss: 0.5408 - accuracy: 0.6591 - val_loss: 0.7431 - val_accuracy: 0.3684
Epoch 187/300
2/2 [==============================] - 0s 42ms/step - loss: 0.5401 - accuracy: 0.6591 - val_loss: 0.7414 - val_accuracy: 0.3684
Epoch 188/300
2/2 [==============================] - 0s 47ms/step - loss: 0.5395 - accuracy: 0.6591 - val_loss: 0.7395 - val_accuracy: 0.3684
Epoch 189/300
2/2 [==============================] - 0s 54ms/step - loss: 0.5389 - accuracy: 0.6591 - val_loss: 0.7379 - val_accuracy: 0.3684
Epoch 190/300
2/2 [==============================] - 0s 48ms/step - loss: 0.5381 - accuracy: 0.6591 - val_loss: 0.7368 - val_accuracy: 0.3684
Epoch 191/300
2/2 [==============================] - 0s 47ms/step - loss: 0.5375 - accuracy: 0.6591 - val_loss: 0.7356 - val_accuracy: 0.3684
Epoch 192/300
2/2 [==============================] - 0s 45ms/step - loss: 0.5368 - accuracy: 0.6591 - val_loss: 0.7346 - val_accuracy: 0.3684
Epoch 193/300
2/2 [==============================] - 0s 50ms/step - loss: 0.5361 - accuracy: 0.6591 - val_loss: 0.7338 - val_accuracy: 0.3684
Epoch 194/300
2/2 [==============================] - 0s 49ms/step - loss: 0.5353 - accuracy: 0.6591 - val_loss: 0.7334 - val_accuracy: 0.3684
Epoch 195/300
2/2 [==============================] - 0s 50ms/step - loss: 0.5346 - accuracy: 0.6591 - val_loss: 0.7334 - val_accuracy: 0.3684
Epoch 196/300
2/2 [==============================] - 0s 47ms/step - loss: 0.5339 - accuracy: 0.6591 - val_loss: 0.7333 - val_accuracy: 0.3684
Epoch 197/300
2/2 [==============================] - 0s 48ms/step - loss: 0.5331 - accuracy: 0.6591 - val_loss: 0.7329 - val_accuracy: 0.3684
Epoch 198/300
2/2 [==============================] - 0s 92ms/step - loss: 0.5324 - accuracy: 0.6591 - val_loss: 0.7326 - val_accuracy: 0.3684
Epoch 199/300
2/2 [==============================] - 0s 51ms/step - loss: 0.5317 - accuracy: 0.6591 - val_loss: 0.7325 - val_accuracy: 0.3684
Epoch 200/300
2/2 [==============================] - 0s 48ms/step - loss: 0.5309 - accuracy: 0.6591 - val_loss: 0.7327 - val_accuracy: 0.3684
Epoch 201/300
2/2 [==============================] - 0s 50ms/step - loss: 0.5301 - accuracy: 0.6591 - val_loss: 0.7328 - val_accuracy: 0.3684
Epoch 202/300
2/2 [==============================] - 0s 54ms/step - loss: 0.5294 - accuracy: 0.6591 - val_loss: 0.7332 - val_accuracy: 0.3684
Epoch 203/300
2/2 [==============================] - 0s 53ms/step - loss: 0.5286 - accuracy: 0.6591 - val_loss: 0.7337 - val_accuracy: 0.3684
Epoch 204/300
2/2 [==============================] - 0s 59ms/step - loss: 0.5279 - accuracy: 0.6591 - val_loss: 0.7339 - val_accuracy: 0.3684
Epoch 205/300
2/2 [==============================] - 0s 50ms/step - loss: 0.5272 - accuracy: 0.6591 - val_loss: 0.7339 - val_accuracy: 0.3684
Epoch 206/300
2/2 [==============================] - 0s 51ms/step - loss: 0.5264 - accuracy: 0.6591 - val_loss: 0.7333 - val_accuracy: 0.3684
Epoch 207/300
2/2 [==============================] - 0s 48ms/step - loss: 0.5257 - accuracy: 0.6591 - val_loss: 0.7325 - val_accuracy: 0.3684
Epoch 208/300
2/2 [==============================] - 0s 49ms/step - loss: 0.5249 - accuracy: 0.6591 - val_loss: 0.7321 - val_accuracy: 0.3684
Epoch 209/300
2/2 [==============================] - 0s 46ms/step - loss: 0.5241 - accuracy: 0.6591 - val_loss: 0.7317 - val_accuracy: 0.3684
Epoch 210/300
2/2 [==============================] - 0s 47ms/step - loss: 0.5234 - accuracy: 0.6591 - val_loss: 0.7313 - val_accuracy: 0.3684
Epoch 211/300
2/2 [==============================] - 0s 49ms/step - loss: 0.5226 - accuracy: 0.6591 - val_loss: 0.7308 - val_accuracy: 0.3684
Epoch 212/300
2/2 [==============================] - 0s 48ms/step - loss: 0.5219 - accuracy: 0.6591 - val_loss: 0.7302 - val_accuracy: 0.3684
Epoch 213/300
2/2 [==============================] - 0s 45ms/step - loss: 0.5211 - accuracy: 0.6591 - val_loss: 0.7299 - val_accuracy: 0.3684
Epoch 214/300
2/2 [==============================] - 0s 49ms/step - loss: 0.5203 - accuracy: 0.6591 - val_loss: 0.7296 - val_accuracy: 0.3684
Epoch 215/300
2/2 [==============================] - 0s 48ms/step - loss: 0.5196 - accuracy: 0.6591 - val_loss: 0.7293 - val_accuracy: 0.3684
Epoch 216/300
2/2 [==============================] - 0s 51ms/step - loss: 0.5188 - accuracy: 0.6591 - val_loss: 0.7289 - val_accuracy: 0.3684
Epoch 217/300
2/2 [==============================] - 0s 48ms/step - loss: 0.5180 - accuracy: 0.6591 - val_loss: 0.7286 - val_accuracy: 0.3684
Epoch 218/300
2/2 [==============================] - 0s 44ms/step - loss: 0.5172 - accuracy: 0.6591 - val_loss: 0.7284 - val_accuracy: 0.3684
Epoch 219/300
2/2 [==============================] - 0s 49ms/step - loss: 0.5165 - accuracy: 0.6591 - val_loss: 0.7281 - val_accuracy: 0.3684
Epoch 220/300
2/2 [==============================] - 0s 83ms/step - loss: 0.5157 - accuracy: 0.6591 - val_loss: 0.7276 - val_accuracy: 0.3684
Epoch 221/300
2/2 [==============================] - 0s 50ms/step - loss: 0.5149 - accuracy: 0.6591 - val_loss: 0.7267 - val_accuracy: 0.3684
Epoch 222/300
2/2 [==============================] - 0s 45ms/step - loss: 0.5142 - accuracy: 0.6591 - val_loss: 0.7257 - val_accuracy: 0.3684
Epoch 223/300
2/2 [==============================] - 0s 45ms/step - loss: 0.5134 - accuracy: 0.6591 - val_loss: 0.7248 - val_accuracy: 0.3684
Epoch 224/300
2/2 [==============================] - 0s 49ms/step - loss: 0.5125 - accuracy: 0.6591 - val_loss: 0.7236 - val_accuracy: 0.3684
Epoch 225/300
2/2 [==============================] - 0s 43ms/step - loss: 0.5117 - accuracy: 0.6818 - val_loss: 0.7221 - val_accuracy: 0.3684
Epoch 226/300
2/2 [==============================] - 0s 47ms/step - loss: 0.5109 - accuracy: 0.6818 - val_loss: 0.7206 - val_accuracy: 0.3684
Epoch 227/300
2/2 [==============================] - 0s 43ms/step - loss: 0.5101 - accuracy: 0.6818 - val_loss: 0.7192 - val_accuracy: 0.3684
Epoch 228/300
2/2 [==============================] - 0s 47ms/step - loss: 0.5092 - accuracy: 0.6818 - val_loss: 0.7178 - val_accuracy: 0.3684
Epoch 229/300
2/2 [==============================] - 0s 43ms/step - loss: 0.5084 - accuracy: 0.6818 - val_loss: 0.7164 - val_accuracy: 0.3684
Epoch 230/300
2/2 [==============================] - 0s 47ms/step - loss: 0.5075 - accuracy: 0.6818 - val_loss: 0.7152 - val_accuracy: 0.3684
Epoch 231/300
2/2 [==============================] - 0s 44ms/step - loss: 0.5066 - accuracy: 0.6818 - val_loss: 0.7143 - val_accuracy: 0.3684
Epoch 232/300
2/2 [==============================] - 0s 48ms/step - loss: 0.5058 - accuracy: 0.6818 - val_loss: 0.7135 - val_accuracy: 0.3684
Epoch 233/300
2/2 [==============================] - 0s 49ms/step - loss: 0.5050 - accuracy: 0.6818 - val_loss: 0.7127 - val_accuracy: 0.3684
Epoch 234/300
2/2 [==============================] - 0s 47ms/step - loss: 0.5040 - accuracy: 0.6818 - val_loss: 0.7120 - val_accuracy: 0.3684
Epoch 235/300
2/2 [==============================] - 0s 47ms/step - loss: 0.5032 - accuracy: 0.6818 - val_loss: 0.7110 - val_accuracy: 0.4211
Epoch 236/300
2/2 [==============================] - 0s 47ms/step - loss: 0.5023 - accuracy: 0.6818 - val_loss: 0.7098 - val_accuracy: 0.4211
Epoch 237/300
2/2 [==============================] - 0s 51ms/step - loss: 0.5014 - accuracy: 0.6818 - val_loss: 0.7083 - val_accuracy: 0.4211
Epoch 238/300
2/2 [==============================] - 0s 44ms/step - loss: 0.5006 - accuracy: 0.6818 - val_loss: 0.7070 - val_accuracy: 0.4211
Epoch 239/300
2/2 [==============================] - 0s 98ms/step - loss: 0.4996 - accuracy: 0.6818 - val_loss: 0.7061 - val_accuracy: 0.4211
Epoch 240/300
2/2 [==============================] - 0s 45ms/step - loss: 0.4987 - accuracy: 0.6818 - val_loss: 0.7052 - val_accuracy: 0.4211
Epoch 241/300
2/2 [==============================] - 0s 47ms/step - loss: 0.4979 - accuracy: 0.6818 - val_loss: 0.7041 - val_accuracy: 0.4211
Epoch 242/300
2/2 [==============================] - 0s 48ms/step - loss: 0.4969 - accuracy: 0.6818 - val_loss: 0.7030 - val_accuracy: 0.4211
Epoch 243/300
2/2 [==============================] - 0s 49ms/step - loss: 0.4960 - accuracy: 0.6818 - val_loss: 0.7018 - val_accuracy: 0.4211
Epoch 244/300
2/2 [==============================] - 0s 53ms/step - loss: 0.4951 - accuracy: 0.6818 - val_loss: 0.7003 - val_accuracy: 0.4211
Epoch 245/300
2/2 [==============================] - 0s 51ms/step - loss: 0.4942 - accuracy: 0.7045 - val_loss: 0.6987 - val_accuracy: 0.4211
Epoch 246/300
2/2 [==============================] - 0s 49ms/step - loss: 0.4933 - accuracy: 0.7045 - val_loss: 0.6970 - val_accuracy: 0.4211
Epoch 247/300
2/2 [==============================] - 0s 61ms/step - loss: 0.4923 - accuracy: 0.7045 - val_loss: 0.6954 - val_accuracy: 0.4211
Epoch 248/300
2/2 [==============================] - 0s 50ms/step - loss: 0.4914 - accuracy: 0.7045 - val_loss: 0.6937 - val_accuracy: 0.4211
Epoch 249/300
2/2 [==============================] - 0s 51ms/step - loss: 0.4905 - accuracy: 0.7045 - val_loss: 0.6925 - val_accuracy: 0.4211
Epoch 250/300
2/2 [==============================] - 0s 58ms/step - loss: 0.4895 - accuracy: 0.7045 - val_loss: 0.6917 - val_accuracy: 0.4737
Epoch 251/300
2/2 [==============================] - 0s 54ms/step - loss: 0.4885 - accuracy: 0.7045 - val_loss: 0.6911 - val_accuracy: 0.4737
Epoch 252/300
2/2 [==============================] - 0s 59ms/step - loss: 0.4876 - accuracy: 0.7045 - val_loss: 0.6903 - val_accuracy: 0.4737
Epoch 253/300
2/2 [==============================] - 0s 49ms/step - loss: 0.4867 - accuracy: 0.7045 - val_loss: 0.6895 - val_accuracy: 0.4737
Epoch 254/300
2/2 [==============================] - 0s 48ms/step - loss: 0.4858 - accuracy: 0.7045 - val_loss: 0.6888 - val_accuracy: 0.4737
Epoch 255/300
2/2 [==============================] - 0s 57ms/step - loss: 0.4848 - accuracy: 0.7045 - val_loss: 0.6882 - val_accuracy: 0.4737
Epoch 256/300
2/2 [==============================] - 0s 52ms/step - loss: 0.4839 - accuracy: 0.7045 - val_loss: 0.6871 - val_accuracy: 0.4737
Epoch 257/300
2/2 [==============================] - 0s 49ms/step - loss: 0.4829 - accuracy: 0.7045 - val_loss: 0.6857 - val_accuracy: 0.4737
Epoch 258/300
2/2 [==============================] - 0s 108ms/step - loss: 0.4820 - accuracy: 0.7045 - val_loss: 0.6846 - val_accuracy: 0.5263
Epoch 259/300
2/2 [==============================] - 0s 52ms/step - loss: 0.4810 - accuracy: 0.7045 - val_loss: 0.6840 - val_accuracy: 0.5263
Epoch 260/300
2/2 [==============================] - 0s 49ms/step - loss: 0.4800 - accuracy: 0.7045 - val_loss: 0.6835 - val_accuracy: 0.5263
Epoch 261/300
2/2 [==============================] - 0s 47ms/step - loss: 0.4791 - accuracy: 0.7045 - val_loss: 0.6828 - val_accuracy: 0.5263
Epoch 262/300
2/2 [==============================] - 0s 47ms/step - loss: 0.4781 - accuracy: 0.7045 - val_loss: 0.6822 - val_accuracy: 0.5263
Epoch 263/300
2/2 [==============================] - 0s 52ms/step - loss: 0.4772 - accuracy: 0.7045 - val_loss: 0.6817 - val_accuracy: 0.5263
Epoch 264/300
2/2 [==============================] - 0s 48ms/step - loss: 0.4762 - accuracy: 0.7045 - val_loss: 0.6809 - val_accuracy: 0.5263
Epoch 265/300
2/2 [==============================] - 0s 48ms/step - loss: 0.4752 - accuracy: 0.7045 - val_loss: 0.6797 - val_accuracy: 0.5263
Epoch 266/300
2/2 [==============================] - 0s 49ms/step - loss: 0.4742 - accuracy: 0.7045 - val_loss: 0.6784 - val_accuracy: 0.5263
Epoch 267/300
2/2 [==============================] - 0s 48ms/step - loss: 0.4732 - accuracy: 0.7045 - val_loss: 0.6769 - val_accuracy: 0.5263
Epoch 268/300
2/2 [==============================] - 0s 48ms/step - loss: 0.4722 - accuracy: 0.7045 - val_loss: 0.6748 - val_accuracy: 0.5263
Epoch 269/300
2/2 [==============================] - 0s 48ms/step - loss: 0.4712 - accuracy: 0.7045 - val_loss: 0.6730 - val_accuracy: 0.5263
Epoch 270/300
2/2 [==============================] - 0s 47ms/step - loss: 0.4701 - accuracy: 0.7045 - val_loss: 0.6716 - val_accuracy: 0.5263
Epoch 271/300
2/2 [==============================] - 0s 45ms/step - loss: 0.4691 - accuracy: 0.7045 - val_loss: 0.6701 - val_accuracy: 0.5263
Epoch 272/300
2/2 [==============================] - 0s 48ms/step - loss: 0.4681 - accuracy: 0.7045 - val_loss: 0.6681 - val_accuracy: 0.5263
Epoch 273/300
2/2 [==============================] - 0s 49ms/step - loss: 0.4671 - accuracy: 0.7045 - val_loss: 0.6663 - val_accuracy: 0.5263
Epoch 274/300
2/2 [==============================] - 0s 44ms/step - loss: 0.4660 - accuracy: 0.7045 - val_loss: 0.6649 - val_accuracy: 0.5263
Epoch 275/300
2/2 [==============================] - 0s 49ms/step - loss: 0.4650 - accuracy: 0.7273 - val_loss: 0.6637 - val_accuracy: 0.5263
Epoch 276/300
2/2 [==============================] - 0s 47ms/step - loss: 0.4639 - accuracy: 0.7273 - val_loss: 0.6625 - val_accuracy: 0.5263
Epoch 277/300
2/2 [==============================] - 0s 45ms/step - loss: 0.4630 - accuracy: 0.7273 - val_loss: 0.6611 - val_accuracy: 0.5263
Epoch 278/300
2/2 [==============================] - 0s 43ms/step - loss: 0.4619 - accuracy: 0.7500 - val_loss: 0.6598 - val_accuracy: 0.5263
Epoch 279/300
2/2 [==============================] - 0s 44ms/step - loss: 0.4609 - accuracy: 0.7500 - val_loss: 0.6585 - val_accuracy: 0.5263
Epoch 280/300
2/2 [==============================] - 0s 92ms/step - loss: 0.4599 - accuracy: 0.7500 - val_loss: 0.6574 - val_accuracy: 0.5263
Epoch 281/300
2/2 [==============================] - 0s 47ms/step - loss: 0.4589 - accuracy: 0.7727 - val_loss: 0.6564 - val_accuracy: 0.5263
Epoch 282/300
2/2 [==============================] - 0s 53ms/step - loss: 0.4579 - accuracy: 0.7727 - val_loss: 0.6555 - val_accuracy: 0.5263
Epoch 283/300
2/2 [==============================] - 0s 56ms/step - loss: 0.4569 - accuracy: 0.7727 - val_loss: 0.6545 - val_accuracy: 0.5263
Epoch 284/300
2/2 [==============================] - 0s 52ms/step - loss: 0.4558 - accuracy: 0.7727 - val_loss: 0.6536 - val_accuracy: 0.5263
Epoch 285/300
2/2 [==============================] - 0s 79ms/step - loss: 0.4549 - accuracy: 0.7727 - val_loss: 0.6529 - val_accuracy: 0.5263
Epoch 286/300
2/2 [==============================] - 0s 53ms/step - loss: 0.4538 - accuracy: 0.7727 - val_loss: 0.6523 - val_accuracy: 0.5263
Epoch 287/300
2/2 [==============================] - 0s 55ms/step - loss: 0.4529 - accuracy: 0.7727 - val_loss: 0.6517 - val_accuracy: 0.5263
Epoch 288/300
2/2 [==============================] - 0s 55ms/step - loss: 0.4519 - accuracy: 0.7727 - val_loss: 0.6508 - val_accuracy: 0.5263
Epoch 289/300
2/2 [==============================] - 0s 46ms/step - loss: 0.4509 - accuracy: 0.7727 - val_loss: 0.6496 - val_accuracy: 0.5263
Epoch 290/300
2/2 [==============================] - 0s 50ms/step - loss: 0.4499 - accuracy: 0.7727 - val_loss: 0.6485 - val_accuracy: 0.5263
Epoch 291/300
2/2 [==============================] - 0s 53ms/step - loss: 0.4488 - accuracy: 0.7727 - val_loss: 0.6477 - val_accuracy: 0.5263
Epoch 292/300
2/2 [==============================] - 0s 48ms/step - loss: 0.4479 - accuracy: 0.7727 - val_loss: 0.6469 - val_accuracy: 0.5263
Epoch 293/300
2/2 [==============================] - 0s 54ms/step - loss: 0.4468 - accuracy: 0.7727 - val_loss: 0.6458 - val_accuracy: 0.5263
Epoch 294/300
2/2 [==============================] - 0s 50ms/step - loss: 0.4458 - accuracy: 0.7727 - val_loss: 0.6443 - val_accuracy: 0.5263
Epoch 295/300
2/2 [==============================] - 0s 45ms/step - loss: 0.4447 - accuracy: 0.7727 - val_loss: 0.6429 - val_accuracy: 0.5263
Epoch 296/300
2/2 [==============================] - 0s 46ms/step - loss: 0.4438 - accuracy: 0.7727 - val_loss: 0.6412 - val_accuracy: 0.5263
Epoch 297/300
2/2 [==============================] - 0s 44ms/step - loss: 0.4427 - accuracy: 0.7727 - val_loss: 0.6399 - val_accuracy: 0.5263
Epoch 298/300
2/2 [==============================] - 0s 51ms/step - loss: 0.4417 - accuracy: 0.7727 - val_loss: 0.6391 - val_accuracy: 0.5263
Epoch 299/300
2/2 [==============================] - 0s 117ms/step - loss: 0.4407 - accuracy: 0.7727 - val_loss: 0.6384 - val_accuracy: 0.5263
Epoch 300/300
2/2 [==============================] - 0s 59ms/step - loss: 0.4397 - accuracy: 0.7727 - val_loss: 0.6374 - val_accuracy: 0.5263</code></pre>
</div>
</div>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>plt.plot(Hist.history[<span class="st">'accuracy'</span>], label<span class="op">=</span><span class="st">'accuracy'</span>)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>plt.plot(Hist.history[<span class="st">'val_accuracy'</span>], label <span class="op">=</span> <span class="st">'val_accuracy'</span>)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epoch'</span>)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy'</span>)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>plt.ylim([<span class="fl">0.5</span>, <span class="dv">1</span>])</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'lower right'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>&lt;matplotlib.legend.Legend at 0x208bbec0eb0&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="TFkeras-record_files/figure-html/cell-15-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="test-and-model" class="level2">
<h2 class="anchored" data-anchor-id="test-and-model">Test and Model</h2>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>Test_Loss, Test_Accuracy <span class="op">=</span> My_NN_Model.evaluate(TestDF, TestLabels)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1/1 [==============================] - 0s 40ms/step - loss: 0.6374 - accuracy: 0.5263</code></pre>
</div>
</div>
</section>
<section id="save-the-model" class="level2">
<h2 class="anchored" data-anchor-id="save-the-model">Save the Model</h2>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>My_NN_Model.save(<span class="st">"Example2_NN_Model"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>INFO:tensorflow:Assets written to: Example2_NN_Model\assets</code></pre>
</div>
</div>
</section>
<section id="predictions" class="level2">
<h2 class="anchored" data-anchor-id="predictions">Predictions</h2>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>predictions<span class="op">=</span>My_NN_Model.predict(TestDF)</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(predictions)</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">type</span>(predictions))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1/1 [==============================] - 0s 127ms/step
[[0.55399185]
 [0.24952714]
 [0.5670001 ]
 [0.1765249 ]
 [0.21249458]
 [0.3415211 ]
 [0.19097081]
 [0.39869913]
 [0.4496788 ]
 [0.18093817]
 [0.14197691]
 [0.26657587]
 [0.45018274]
 [0.23264357]
 [0.54624206]
 [0.483801  ]
 [0.20716172]
 [0.49750927]
 [0.4504492 ]]
&lt;class 'numpy.ndarray'&gt;</code></pre>
</div>
</div>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co">## For predictions &gt;=.5 --&gt; 1 ; else --&gt; 0</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>predictions[predictions <span class="op">&gt;=</span> <span class="fl">.5</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>predictions[predictions <span class="op">&lt;</span> <span class="fl">.5</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(predictions)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[1.]
 [0.]
 [1.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [1.]
 [0.]
 [0.]
 [0.]
 [0.]]</code></pre>
</div>
</div>
<section id="confusion-matrix" class="level3">
<h3 class="anchored" data-anchor-id="confusion-matrix">Confusion Matrix</h3>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The prediction accuracy via confusion matrix is:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(predictions, TestLabels))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The prediction accuracy via confusion matrix is:

[[7 9]
 [0 3]]</code></pre>
</div>
</div>
</section>
<section id="pretty-confusion-matrix" class="level3">
<h3 class="anchored" data-anchor-id="pretty-confusion-matrix">Pretty Confusion Matrix</h3>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co">#from sklearn.metrics import confusion_matrix</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>cm <span class="op">=</span> confusion_matrix(predictions, TestLabels, labels <span class="op">=</span> labels)</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(cm)</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt     </span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">10</span>)) </span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a><span class="co">#ax= plt.subplot()</span></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a><span class="co">#sns.set(font_scale=3)</span></span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a><span class="co">#sns.set (rc = {'figure.figsize':(40, 40)})</span></span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>sns.heatmap(cm, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">'g'</span>, ax<span class="op">=</span>ax, annot_kws<span class="op">=</span>{<span class="st">'size'</span>: <span class="dv">18</span>})</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a><span class="co">#annot=True to annotate cells, ftm='g' to disable scientific notation</span></span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a><span class="co"># annot_kws si size  of font in heatmap</span></span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a><span class="co"># labels, title and ticks</span></span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'True labels'</span>) </span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Predicted labels'</span>)</span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Confusion Matrix: NN'</span>) </span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>ax.xaxis.set_ticklabels([<span class="st">"0:Admit"</span>,<span class="st">"1:Decline"</span>],rotation<span class="op">=</span><span class="dv">90</span>, fontsize <span class="op">=</span> <span class="dv">18</span>)</span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a>ax.yaxis.set_ticklabels([<span class="st">"0:Admit"</span>,<span class="st">"1:Decline"</span>],rotation<span class="op">=</span><span class="dv">0</span>, fontsize <span class="op">=</span> <span class="dv">18</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[7 9]
 [0 3]]</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>[Text(0, 0.5, '0:Admit'), Text(0, 1.5, '1:Decline')]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="TFkeras-record_files/figure-html/cell-21-output-3.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>