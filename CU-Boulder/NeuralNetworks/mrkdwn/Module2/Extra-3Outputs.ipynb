{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: NN 3 Outputs (Extra Resource) - Raw code\n",
        "author: Professor Ami Gates\n",
        "format:\n",
        "  html:\n",
        "    code-fold: false\n",
        "execute:\n",
        "  output: true\n",
        "toc: true\n",
        "---"
      ],
      "id": "310a4e1f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here - there are many versions of $X$ for trying different things. You can make $X$ whatever YOU want it to be. \n",
        "\n",
        "If/when you update $X$, you must also update $Y$ sometimes the $W$'s, etc. \n",
        "\n",
        "Always think about what you are doing :)\n",
        "\n",
        "Compare this to what you do by hand for small examples.\n",
        "\n",
        "**IMPORTANT**\n",
        "\n",
        "If you use different $X$ and $y$ values you must also update the following below\n",
        "```\n",
        "self.InputNumColumns = ??  ## columns\n",
        "self.OutputSize = ??\n",
        "self.HiddenUnits = ??  ## one layer with h units\n",
        "self.n = ?? ## number of training examples, n\n",
        "```\n"
      ],
      "id": "11889cff"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "id": "c1da2d4d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DATAset\n",
        "\n",
        "Using a dataset with THREE label categories, 1, 2, and 3\n"
      ],
      "id": "619257a0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "filename=\"HeartRisk_JustNums_3_labels2.csv\"\n",
        "DF = pd.read_csv(filename)\n",
        "print(DF)"
      ],
      "id": "8b8ba3a1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "InputColumns = 3\n",
        "NumberOfLabels = 3\n",
        "n = len(DF) ## number of rows of entire X\n",
        "## Take the label off of X and make it a numpy array\n",
        "X = np.array(DF.iloc[:, [1, 2, 3]])\n",
        "## Set y to the label. Check the shape!\n",
        "y = np.array(DF.iloc[:,0]).T\n",
        "y = np.array([y]).T\n",
        "print(\"y is\\n\", y)"
      ],
      "id": "3051db69",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating one hot labels for y "
      ],
      "id": "3c1ff238"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "temp = y\n",
        "print(temp)"
      ],
      "id": "d8caa6a8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "one_hot_labels = np.zeros((n, NumberOfLabels))\n",
        "print(one_hot_labels)"
      ],
      "id": "69407bed",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for i in range(n):\n",
        "    one_hot_labels[i, temp[i]-1] = 1    \n",
        "print(one_hot_labels)\n",
        "y = one_hot_labels\n",
        "print(y)"
      ],
      "id": "b709d4b1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NN class object code"
      ],
      "id": "79dc375f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "LR=.01\n",
        "LRB = .01"
      ],
      "id": "3d93e408",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class NeuralNetwork(object):\n",
        "    def __init__(self):\n",
        "        \n",
        "        self.InputNumColumns = InputColumns  ## columns\n",
        "        self.OutputSize = 3 ## Categories\n",
        "        self.HiddenUnits = 2  ## one layer with h units\n",
        "        self.n = n  ## number of training examples, n\n",
        "        \n",
        "        print(\"Initialize NN\\n\")\n",
        "        #Random W1\n",
        "        self.W1 = np.random.randn(self.InputNumColumns, self.HiddenUnits) # c by h  \n",
        "       \n",
        "        print(\"INIT W1 is\\n\", self.W1)\n",
        "        \n",
        "        ##-----------------------------------------\n",
        "        ## NOTE ##\n",
        "        ##\n",
        "        ## The following are all random. However, you can comment this out\n",
        "        ## and can set any weights and biases by hand , etc.\n",
        "        ##\n",
        "        ##---------------------------------------------\n",
        "        \n",
        "        self.W2 = np.random.randn(self.HiddenUnits, self.OutputSize) # h by o \n",
        "        print(\"W2 is:\\n\", self.W2)\n",
        "        \n",
        "        self.b = np.random.randn(1, self.HiddenUnits)\n",
        "        print(\"The b's are:\\n\", self.b)\n",
        "        ## biases for layer 1\n",
        "        \n",
        "        self.c = np.random.randn(1, self.OutputSize)\n",
        "        print(\"The c is\\n\", self.c)\n",
        "        ## bias for last layer\n",
        "        \n",
        "        \n",
        "    def FeedForward(self, X):\n",
        "        print(\"FeedForward\\n\\n\")\n",
        "        self.z = (np.dot(X, self.W1)) + self.b \n",
        "        #X is n by c   W1  is c by h -->  n by h\n",
        "        print(\"Z1 is:\\n\", self.z)\n",
        "        \n",
        "        self.h = self.Sigmoid(self.z) #activation function    shape: n by h\n",
        "        print(\"H is:\\n\", self.h)\n",
        "        \n",
        "        self.z2 = (np.dot(self.h, self.W2)) + self.c # n by h  @  h by o  -->  n by o  \n",
        "        print(\"Z2 is:\\n\", self.z2)\n",
        "        \n",
        "        ## Using Softmax for the output activation\n",
        "        output = self.Softmax(self.z2)  \n",
        "        print(\"output Y^ is:\\n\", output)\n",
        "        return output\n",
        "        \n",
        "    def Sigmoid(self, s, deriv=False):\n",
        "        if (deriv == True):\n",
        "            return s * (1 - s)\n",
        "        return 1/(1 + np.exp(-s))\n",
        "    \n",
        "    def Softmax(self, M):\n",
        "        #print(\"M is\\n\", M)\n",
        "        expM = np.exp(M)\n",
        "        #print(\"expM is\\n\", expM)\n",
        "        SM=expM/np.sum(expM, axis=1)[:,None]\n",
        "        #print(\"SM is\\n\",SM )\n",
        "        return SM \n",
        "    \n",
        "    def BackProp(self, X, y, output):\n",
        "        print(\"\\n\\nBackProp\\n\")\n",
        "        self.LR = LR\n",
        "        self.LRB=LRB  ## LR for biases\n",
        "        \n",
        "        # Y^ - Y\n",
        "        self.output_error = output - y    \n",
        "        print(\"Y^ - Y\\n\", self.output_error)\n",
        "        \n",
        "        ## NOTE TO READER........................\n",
        "        ## Here - we DO NOT multiply by derivative of Sig for y^ b/c we are using \n",
        "        ## cross entropy and softmax for the loss and last activation\n",
        "        # REMOVED # self.output_delta = self.output_error * self.Sigmoid(output, deriv=True) \n",
        "        ## So the above line is commented out...............\n",
        "        \n",
        "        self.output_delta = self.output_error \n",
        "          \n",
        "        ##(Y^ - Y)(W2)\n",
        "        self.D_Error_W2 = self.output_delta.dot(self.W2.T) #  D_Error times W2\n",
        "        #print(\"W2 is\\n\", self.W2)\n",
        "        #print(\" D_Error times W2\\n\", self.D_Error_W2)\n",
        "        \n",
        "        ## (H)(1 - H) (Y^ - Y)(Y^)(1-Y^)(W2)\n",
        "        ## We still use the Sigmoid on H\n",
        "        \n",
        "        self.H_D_Error_W2 = self.D_Error_W2 * self.Sigmoid(self.h, deriv=True) \n",
        "        \n",
        "        ## Note that * will multiply respective values together in each matrix\n",
        "        #print(\"Derivative sig H is:\\n\", self.Sigmoid(self.h, deriv=True))\n",
        "        #print(\"self.H_D_Error_W2 is\\n\", self.H_D_Error_W2)\n",
        "        \n",
        "        ################------UPDATE weights and biases ------------------\n",
        "        #print(\"Old W1: \\n\", self.W1)\n",
        "        #print(\"Old W2 is:\\n\", self.W2)\n",
        "        #print(\"X transpose is\\n\", X.T)\n",
        "        \n",
        "        ##  XT  (H)(1 - H) (Y^ - Y)(Y^)(1-Y^)(W2)\n",
        "        self.X_H_D_Error_W2 = X.T.dot(self.H_D_Error_W2) ## this is dW1\n",
        "        \n",
        "        ## (H)T (Y^ - Y) - \n",
        "        self.h_output_delta = self.h.T.dot(self.output_delta) ## this is for dW2\n",
        "        \n",
        "        #print(\"the gradient :\\n\", self.X_H_D_Error_W2)\n",
        "        #print(\"the gradient average:\\n\", self.X_H_D_Error_W2/self.n)\n",
        "        \n",
        "        print(\"Using sum gradient........\\n\")\n",
        "        self.W1 = self.W1 - self.LR*(self.X_H_D_Error_W2) # c by h  adjusting first set (input -> hidden) weights\n",
        "        self.W2 = self.W2 - self.LR*(self.h_output_delta) \n",
        "        \n",
        "        \n",
        "        print(\"The sum of the b update is\\n\", np.sum(self.H_D_Error_W2, axis=0))\n",
        "        print(\"The b biases before the update are:\\n\", self.b)\n",
        "        self.b = self.b  - self.LRB*np.sum(self.H_D_Error_W2, axis=0)\n",
        "        #print(\"The H_D_Error_W2 is...\\n\", self.H_D_Error_W2)\n",
        "        print(\"Updated bs are:\\n\", self.b)\n",
        "        \n",
        "        self.c = self.c - self.LR*np.sum(self.output_delta, axis=0)\n",
        "        #print(\"Updated c's are:\\n\", self.c)\n",
        "        \n",
        "        print(\"The W1 is: \\n\", self.W1)\n",
        "        print(\"The W1 gradient is: \\n\", self.X_H_D_Error_W2)\n",
        "        #print(\"The W1 gradient average is: \\n\", self.X_H_D_Error_W2/self.n)\n",
        "        print(\"The W2 gradient  is: \\n\", self.h_output_delta)\n",
        "        #print(\"The W2 gradient average is: \\n\", self.h_output_delta/self.n)\n",
        "        print(\"The biases b gradient is:\\n\",np.sum(self.H_D_Error_W2, axis=0 ))\n",
        "        print(\"The bias c gradient is: \\n\", np.sum(self.output_delta, axis=0))\n",
        "        ################################################################\n",
        "        \n",
        "    def TrainNetwork(self, X, y):\n",
        "        output = self.FeedForward(X)\n",
        "        print(\"Output in TNN\\n\", output)\n",
        "        self.BackProp(X, y, output)\n",
        "        return output"
      ],
      "id": "e262d11d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train network\n"
      ],
      "id": "198465e3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "MyNN = NeuralNetwork()\n",
        "\n",
        "TotalLoss=[]\n",
        "AvgLoss=[]\n",
        "Epochs=1000\n",
        "\n",
        "for i in range(Epochs): \n",
        "    print(\"\\nRUN:\\n \", i)\n",
        "    output=MyNN.TrainNetwork(X, y)\n",
        "   \n",
        "    #print(\"The y is ...\\n\", y)\n",
        "    print(\"The output is: \\n\", output)\n",
        "    MaxValueIndex=np.argmax(output, axis=1)\n",
        "    print('Prediction y^ is', MaxValueIndex+1)\n",
        "    ## Using Categorical Cross Entropy...........\n",
        "    loss = np.mean(-y * np.log(output))  ## We need y to place the \"1\" in the right place\n",
        "    print(\"The current average loss is\\n\", loss)\n",
        "    TotalLoss.append(loss)\n",
        "    AvgLoss.append(loss)\n",
        "    \n",
        "    ## OLD---------------------\n",
        "    # OLD #print(\"Total Loss:\", .5*(np.sum(np.square(output-y))))\n",
        "    # OLD #TotalLoss.append( .5*(np.sum(np.square(output-y))))\n",
        "    #print(\"Average Loss:\", .5*(np.mean(np.square((output-y)))))\n",
        "    #AvgLoss.append(.5*(np.mean(np.square((output-y)))))"
      ],
      "id": "bc0eec52",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Output and Vis"
      ],
      "id": "cfa96f77"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Total Loss List:\", TotalLoss) "
      ],
      "id": "585c2527",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig1 = plt.figure()\n",
        "ax = plt.axes()\n",
        "x = np.linspace(0, 10, Epochs)\n",
        "ax.plot(x, TotalLoss)    "
      ],
      "id": "55b58194",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(JK Note: The following entire block of code was commented out in the original raw code but I'm going to leave it in to see what happens)\n"
      ],
      "id": "f6196ad3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "AvgLoss_ = np.mean(AvgLoss)\n",
        "print(AvgLoss_)\n",
        "fig2 = plt.figure()\n",
        "ax = plt.axes()\n",
        "x = np.linspace(0, 10, Epochs)\n",
        "ax.plot(x, AvgLoss_)  \n",
        "\n",
        "print(y)"
      ],
      "id": "fbe45db3",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}