{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: 3D Binary Label with NN - Raw code\n",
        "author: Professor Ami Gates\n",
        "format:\n",
        "  html:\n",
        "    code-fold: false\n",
        "execute:\n",
        "  output: true\n",
        "toc: true\n",
        "---"
      ],
      "id": "0b357b72"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Neural Network - FeedForward (FF) and BackPropagation (BP)"
      ],
      "id": "ae300a1f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "id": "81edd015",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DATAset\n",
        "<https://drive.google.com/file/d/1RjyrKHTOXgfztLP5JGAMoRehn69LYuZb/view?usp=sharing>\n",
        "\n",
        "- Labels: 0 and 1 \n",
        "    - where 0 is not at risk for heart disease\n",
        "- Data is 4D\n"
      ],
      "id": "7d354b3e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "filename=\"StudentSummerProgramData_Numeric_2NumLabeled_2D.csv\"\n",
        "DF = pd.read_csv(filename)\n",
        "print(DF)"
      ],
      "id": "d3bfd13d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set y to the label. Check the shape!"
      ],
      "id": "5919a799"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y = np.array(DF.iloc[:,0]).T\n",
        "y = np.array([y]).T\n",
        "print(\"y is\\n\", y)"
      ],
      "id": "561b0540",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalize the data (not the label!)\n",
        "\n",
        "or min/max\n",
        "```\n",
        "normalized_df=(df-df.min())/(df.max()-df.min())\n",
        "```\n"
      ],
      "id": "e96da100"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "DF=DF.iloc[:, [1, 2, 3]]\n",
        "DF=(DF-DF.mean())/DF.std()\n",
        "print(DF)"
      ],
      "id": "6c5df844",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = np.array(DF)\n",
        "print(\"X is\\n\", X)"
      ],
      "id": "d230204d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "InputColumns = 3\n",
        "NumberOfLabels = 2\n",
        "n = len(DF) ## number of rows of entire X"
      ],
      "id": "77610b6e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Take the label off of X and make it a numpy array\n",
        "\n",
        "## Creating one hot labels for y\n",
        "```\n",
        "temp = y\n",
        "#print(temp)\n",
        "one_hot_labels = np.zeros((n, NumberOfLabels))\n",
        "print(one_hot_labels)\n",
        "for i in range(n):\n",
        "    one_hot_labels[i, temp[i]-1] = 1    \n",
        "#print(one_hot_labels)\n",
        "y = one_hot_labels\n",
        "print(y)\n",
        "```\n",
        "# NN class object code"
      ],
      "id": "b1f38682"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "LR=.01\n",
        "LRB = .01"
      ],
      "id": "b8055d95",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class NeuralNetwork(object):\n",
        "    def __init__(self):\n",
        "        \n",
        "        self.InputNumColumns = InputColumns  ## columns\n",
        "        self.OutputSize = 1 ## Categories\n",
        "        self.HiddenUnits = 2  ## one layer with h units\n",
        "        self.n = n  ## number of training examples, n\n",
        "        \n",
        "        print(\"Initialize NN\\n\")\n",
        "        #Random W1\n",
        "        self.W1 = np.random.randn(self.InputNumColumns, self.HiddenUnits) # c by h  \n",
        "       \n",
        "        print(\"INIT W1 is\\n\", self.W1)\n",
        "        \n",
        "        ##-----------------------------------------\n",
        "        ## NOTE ##\n",
        "        ##\n",
        "        ## The following are all random. However, you can comment this out\n",
        "        ## and can set any weights and biases by hand , etc.\n",
        "        ##\n",
        "        ##---------------------------------------------\n",
        "        \n",
        "        self.W2 = np.random.randn(self.HiddenUnits, self.OutputSize) # h by o \n",
        "        print(\"W2 is:\\n\", self.W2)\n",
        "        \n",
        "        self.b = np.random.randn(1, self.HiddenUnits)\n",
        "        print(\"The b's are:\\n\", self.b)\n",
        "        ## biases for layer 1\n",
        "        \n",
        "        self.c = np.random.randn(1, self.OutputSize)\n",
        "        print(\"The c is\\n\", self.c)\n",
        "        ## bias for last layer\n",
        "        \n",
        "        \n",
        "    def FeedForward(self, X):\n",
        "        print(\"FeedForward\\n\\n\")\n",
        "        self.z = (np.dot(X, self.W1)) + self.b \n",
        "        #X is n by c   W1  is c by h -->  n by h\n",
        "        print(\"Z1 is:\\n\", self.z)\n",
        "        \n",
        "        self.h = self.Sigmoid(self.z) #activation function    shape: n by h\n",
        "        print(\"H is:\\n\", self.h)\n",
        "        \n",
        "        self.z2 = (np.dot(self.h, self.W2)) + self.c # n by h  @  h by o  -->  n by o  \n",
        "        print(\"Z2 is:\\n\", self.z2)\n",
        "        \n",
        "        ## Using Softmax for the output activation\n",
        "        output = self.Sigmoid(self.z2)  \n",
        "        print(\"output Y^ is:\\n\", output)\n",
        "        return output\n",
        "        \n",
        "    def Sigmoid(self, s, deriv=False):\n",
        "        if (deriv == True):\n",
        "            return s * (1 - s)\n",
        "        return 1/(1 + np.exp(-s))\n",
        "    \n",
        "    # def Softmax(self, M):\n",
        "    #     #print(\"M is\\n\", M)\n",
        "    #     expM = np.exp(M)\n",
        "    #     #print(\"expM is\\n\", expM)\n",
        "    #     SM=expM/np.sum(expM, axis=1)[:,None]\n",
        "    #     #print(\"SM is\\n\",SM )\n",
        "    #     return SM \n",
        "    \n",
        "    def BackProp(self, X, y, output):\n",
        "        print(\"\\n\\nBackProp\\n\")\n",
        "        self.LR = LR\n",
        "        self.LRB=LRB  ## LR for biases\n",
        "        \n",
        "        # Y^ - Y\n",
        "        self.output_error = output - y    \n",
        "        print(\"Y^ - Y\\n\", self.output_error)\n",
        "        \n",
        "        ## NOTE TO READER........................\n",
        "        ## Here - we DO NOT multiply by derivative of Sig for y^ b/c we are using \n",
        "        ## cross entropy and softmax for the loss and last activation\n",
        "        # REMOVED # self.output_delta = self.output_error * self.Sigmoid(output, deriv=True) \n",
        "        ## So the above line is commented out...............\n",
        "        \n",
        "        self.output_delta = self.output_error \n",
        "          \n",
        "        ##(Y^ - Y)(W2)\n",
        "        self.D_Error_W2 = self.output_delta.dot(self.W2.T) #  D_Error times W2\n",
        "        #print(\"W2 is\\n\", self.W2)\n",
        "        #print(\" D_Error times W2\\n\", self.D_Error_W2)\n",
        "        \n",
        "        ## (H)(1 - H) (Y^ - Y)(Y^)(1-Y^)(W2)\n",
        "        ## We still use the Sigmoid on H\n",
        "        \n",
        "        self.H_D_Error_W2 = self.D_Error_W2 * self.Sigmoid(self.h, deriv=True) \n",
        "        \n",
        "        ## Note that * will multiply respective values together in each matrix\n",
        "        #print(\"Derivative sig H is:\\n\", self.Sigmoid(self.h, deriv=True))\n",
        "        #print(\"self.H_D_Error_W2 is\\n\", self.H_D_Error_W2)\n",
        "        \n",
        "        ################------UPDATE weights and biases ------------------\n",
        "        #print(\"Old W1: \\n\", self.W1)\n",
        "        #print(\"Old W2 is:\\n\", self.W2)\n",
        "        #print(\"X transpose is\\n\", X.T)\n",
        "        \n",
        "        ##  XT  (H)(1 - H) (Y^ - Y)(Y^)(1-Y^)(W2)\n",
        "        self.X_H_D_Error_W2 = X.T.dot(self.H_D_Error_W2) ## this is dW1\n",
        "        \n",
        "        ## (H)T (Y^ - Y) - \n",
        "        self.h_output_delta = self.h.T.dot(self.output_delta) ## this is for dW2\n",
        "        \n",
        "        #print(\"the gradient :\\n\", self.X_H_D_Error_W2)\n",
        "        #print(\"the gradient average:\\n\", self.X_H_D_Error_W2/self.n)\n",
        "        \n",
        "        print(\"Using sum gradient........\\n\")\n",
        "        self.W1 = self.W1 - self.LR*(self.X_H_D_Error_W2) # c by h  adjusting first set (input -> hidden) weights\n",
        "        self.W2 = self.W2 - self.LR*(self.h_output_delta) \n",
        "        \n",
        "        \n",
        "        print(\"The sum of the b update is\\n\", np.mean(self.H_D_Error_W2, axis=0))\n",
        "        print(\"The b biases before the update are:\\n\", self.b)\n",
        "        self.b = self.b  - self.LRB*np.mean(self.H_D_Error_W2, axis=0)\n",
        "        #print(\"The H_D_Error_W2 is...\\n\", self.H_D_Error_W2)\n",
        "        print(\"Updated bs are:\\n\", self.b)\n",
        "        \n",
        "        self.c = self.c - self.LR*np.mean(self.output_delta, axis=0)\n",
        "        #print(\"Updated c's are:\\n\", self.c)\n",
        "        \n",
        "        print(\"The W1 is: \\n\", self.W1)\n",
        "        print(\"The W1 gradient is: \\n\", self.X_H_D_Error_W2)\n",
        "        #print(\"The W1 gradient average is: \\n\", self.X_H_D_Error_W2/self.n)\n",
        "        print(\"The W2 gradient  is: \\n\", self.h_output_delta)\n",
        "        #print(\"The W2 gradient average is: \\n\", self.h_output_delta/self.n)\n",
        "        print(\"The biases b gradient is:\\n\",np.mean(self.H_D_Error_W2, axis=0 ))\n",
        "        print(\"The bias c gradient is: \\n\", np.mean(self.output_delta, axis=0))\n",
        "        ################################################################\n",
        "        \n",
        "    def TrainNetwork(self, X, y):\n",
        "        output = self.FeedForward(X)\n",
        "        print(\"Output in TNN\\n\", output)\n",
        "        self.BackProp(X, y, output)\n",
        "        return output"
      ],
      "id": "d27ba146",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train Network"
      ],
      "id": "70b3ec81"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "MyNN = NeuralNetwork()\n",
        "\n",
        "TotalLoss=[]\n",
        "AvgLoss=[]\n",
        "Epochs=150\n",
        "\n",
        "for i in range(Epochs): \n",
        "    print(\"\\nRUN:\\n \", i)\n",
        "    output=MyNN.TrainNetwork(X, y)\n",
        "   \n",
        "    #print(\"The y is ...\\n\", y)\n",
        "    print(\"The output is: \\n\", output)\n",
        "    output=np.where(output > 0.5, 1, 0)\n",
        "    print('Prediction y^ is', output)\n",
        "    ## Using Categorical Cross Entropy...........\n",
        "    #loss = np.mean(-y * np.log(output))  ## We need y to place the \"1\" in the right place\n",
        "    loss=np.sum(np.square(output-y))\n",
        "    avgLoss=np.mean(np.square(output-y))\n",
        "    print(\"The current average loss is\\n\", loss)\n",
        "    TotalLoss.append(loss)\n",
        "    AvgLoss.append(avgLoss)\n",
        "    \n",
        "    ## OLD---------------------\n",
        "    # OLD #print(\"Total Loss:\", .5*(np.sum(np.square(output-y))))\n",
        "    # OLD #TotalLoss.append( .5*(np.sum(np.square(output-y))))\n",
        "    #print(\"Average Loss:\", .5*(np.mean(np.square((output-y)))))\n",
        "    #AvgLoss.append(.5*(np.mean(np.square((output-y)))))"
      ],
      "id": "4a87f902",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Output and Vis   "
      ],
      "id": "22514bd1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Total Loss List:\", TotalLoss) "
      ],
      "id": "9b30156a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig1 = plt.figure()\n",
        "ax = plt.axes()\n",
        "x = np.linspace(0, 10, Epochs)\n",
        "ax.plot(x, TotalLoss)    \n",
        "\n",
        "print(confusion_matrix(output, y))"
      ],
      "id": "8698c556",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(JK Note: The following entire block of code was commented out in the original raw code but I'm going to leave it in to see what happens)\n"
      ],
      "id": "d617fe66"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "AvgLoss_ = np.mean(AvgLoss)\n",
        "print(AvgLoss_)\n",
        "fig2 = plt.figure()\n",
        "ax = plt.axes()\n",
        "x = np.linspace(0, 10, Epochs)\n",
        "ax.plot(x, AvgLoss_)  \n",
        "\n",
        "print(y)"
      ],
      "id": "7e5390fa",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}