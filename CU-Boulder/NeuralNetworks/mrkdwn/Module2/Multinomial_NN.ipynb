{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: NN Multinomial - Raw code\n",
        "author: Professor Ami Gates\n",
        "format:\n",
        "  html:\n",
        "    code-fold: false\n",
        "execute:\n",
        "  output: true\n",
        "toc: true\n",
        "---"
      ],
      "id": "0b8e5afe"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Topics\n",
        "- Multiple outputs - 3 labels\n",
        "- 4D data\n",
        "- One- hot Encoding\n",
        "- Categorical Cross Entropy\n",
        "- Softmax\n"
      ],
      "id": "b11e19db"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "id": "b3df5642",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DATAset\n",
        "Using a dataset with THREE label categories, 1, 2, and 3\n",
        "\n",
        "<https://drive.google.com/file/d/1bbYwSUBXufbupPoAfqbBzhdMMdZfnLcI/view?usp=sharing>\n"
      ],
      "id": "c2f4165c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "filename=\"StudentSummerProgramData_Numeric_3NumLabeled_3D.csv\"\n",
        "DF = pd.read_csv(filename)\n",
        "print(DF)"
      ],
      "id": "c0898781",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y = np.array(DF.iloc[:,0]).T\n",
        "y = np.array([y]).T\n",
        "print(\"y is\\n\", y)"
      ],
      "id": "582c5d15",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "original_y_values=y # save a copy"
      ],
      "id": "f9cf7913",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalize the data (not the label!)\n",
        "or use min/max \n",
        "```\n",
        "normalized_df=(df-df.min())/(df.max()-df.min())\n",
        "```"
      ],
      "id": "f919b113"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "DF=DF.iloc[:, [1, 2, 3, 4]]\n",
        "DF=(DF-DF.mean())/DF.std()\n",
        "print(DF)"
      ],
      "id": "53e87ae2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = np.array(DF)\n",
        "print(\"X is\\n\", X)"
      ],
      "id": "c58b291a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "InputColumns = 4\n",
        "NumberOfLabels = 3\n",
        "n = len(DF) ## number of rows of entire X"
      ],
      "id": "9446e48d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Take the label off of X and make it a numpy array\n",
        "\n",
        "## Creating one hot labels for y"
      ],
      "id": "bfb7ac1b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "temp = y\n",
        "print(temp)\n",
        "\n",
        "one_hot_labels = np.zeros((n, NumberOfLabels))\n",
        "print(one_hot_labels)\n",
        "for i in range(n):\n",
        "    one_hot_labels[i, temp[i]] = 1    \n",
        "print(one_hot_labels)\n",
        "y = one_hot_labels\n",
        "print(\" Y is\\n\", y)"
      ],
      "id": "a8961184",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NN Class Object Code"
      ],
      "id": "f9a5fce9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "LR=.01\n",
        "LRB = .01"
      ],
      "id": "811090f8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class NeuralNetwork(object):\n",
        "    def __init__(self):\n",
        "        \n",
        "        self.InputNumColumns = InputColumns  ## columns\n",
        "        self.OutputSize = 3 ## Categories\n",
        "        self.HiddenUnits = 2   ## one layer with h units\n",
        "        self.n = n  ## number of training examples, n\n",
        "        \n",
        "        print(\"Initialize NN\\n\")\n",
        "        #Random W1\n",
        "        self.W1 = np.random.randn(self.InputNumColumns, self.HiddenUnits) # c by h  \n",
        "       \n",
        "        print(\"INIT W1 is\\n\", self.W1)\n",
        "        \n",
        "        ##-----------------------------------------\n",
        "        ## NOTE ##\n",
        "        ##\n",
        "        ## The following are all random. However, you can comment this out\n",
        "        ## and can set any weights and biases by hand , etc.\n",
        "        ##\n",
        "        ##---------------------------------------------\n",
        "        \n",
        "        self.W2 = np.random.randn(self.HiddenUnits, self.OutputSize) # h by o \n",
        "        print(\"W2 is:\\n\", self.W2)\n",
        "        \n",
        "        self.b = np.random.randn(1, self.HiddenUnits)\n",
        "        print(\"The b's are:\\n\", self.b)\n",
        "        ## biases for layer 1\n",
        "        \n",
        "        self.c = np.random.randn(1, self.OutputSize)\n",
        "        print(\"The c is\\n\", self.c)\n",
        "        ## bias for last layer\n",
        "        \n",
        "        \n",
        "    def FeedForward(self, X):\n",
        "        print(\"FeedForward\\n\\n\")\n",
        "        self.z = (np.dot(X, self.W1)) + self.b \n",
        "        #X is n by c   W1  is c by h -->  n by h\n",
        "        print(\"Z1 is:\\n\", self.z)\n",
        "        \n",
        "        self.h = self.Sigmoid(self.z) #activation function    shape: n by h\n",
        "        print(\"H is:\\n\", self.h)\n",
        "        \n",
        "        self.z2 = (np.dot(self.h, self.W2)) + self.c # n by h  @  h by o  -->  n by o  \n",
        "        print(\"Z2 is:\\n\", self.z2)\n",
        "        \n",
        "        ## Using Softmax for the output activation\n",
        "        output = self.Softmax(self.z2)  \n",
        "        print(\"output Y^ (SM of Z2) is:\\n\", output)\n",
        "        return output\n",
        "        \n",
        "    def Sigmoid(self, s, deriv=False):\n",
        "        if (deriv == True):\n",
        "            return s * (1 - s)\n",
        "        return 1/(1 + np.exp(-s))\n",
        "    \n",
        "    def Softmax(self, M):\n",
        "        #print(\"M is\\n\", M)\n",
        "        expM = np.exp(M)\n",
        "        #print(\"expM is\\n\", expM)\n",
        "        SM=expM/np.sum(expM, axis=1)[:,None]\n",
        "        #print(\"SM is\\n\",SM )\n",
        "        return SM \n",
        "    \n",
        "    def BackProp(self, X, y, output):\n",
        "        print(\"\\n\\nBackProp\\n\")\n",
        "        self.LR = LR\n",
        "        self.LRB=LRB  ## LR for biases\n",
        "        \n",
        "        # Y^ - Y\n",
        "        self.output_error = output - y    \n",
        "        #print(\"Y^ - Y\\n\", self.output_error)\n",
        "        \n",
        "        ## NOTE TO READER........................\n",
        "        ## Here - we DO NOT multiply by derivative of Sig for y^ b/c we are using \n",
        "        ## cross entropy and softmax for the loss and last activation\n",
        "\n",
        "        \n",
        "        self.output_delta = self.output_error \n",
        "          \n",
        "        ##(Y^ - Y)(W2)\n",
        "        self.D_Error_W2 = self.output_delta.dot(self.W2.T) #  D_Error times W2\n",
        "        print(\"(Y^ - Y) is\\n\",self.output_delta)\n",
        "        print(\"W2.T is\\n\", self.W2.T)\n",
        "        print(\" (Y^ - Y) @ W2.T\\n\", self.D_Error_W2)\n",
        "        \n",
        "        ## (H)(1 - H) (Y^ - Y)(W2)\n",
        "        ## We still use the Sigmoid on H \n",
        "        self.H_D_Error_W2 = self.D_Error_W2 * self.Sigmoid(self.h, deriv=True) \n",
        "        \n",
        "        ## Note that * will multiply respective values together in each matrix\n",
        "        #print(\"Derivative sig H is:\\n\", self.Sigmoid(self.h, deriv=True))\n",
        "        #print(\"self.H_D_Error_W2 is\\n\", self.H_D_Error_W2)\n",
        "        \n",
        "        ################------UPDATE weights and biases ------------------\n",
        "        print(\"Old W1: \\n\", self.W1)\n",
        "        #print(\"Old W2 is:\\n\", self.W2)\n",
        "        #print(\"X transpose is\\n\", X.T)\n",
        "        \n",
        "        ##  X.T  (H)(1 - H) (Y^ - Y)(W2)\n",
        "        print(\"Using sum gradient........\\n\")\n",
        "        ## The sum occurs implicitly because we are multiplying X.T (transpose)\n",
        "        \n",
        "        ## dW1 ==>  (X.T)@(H)(1 - H) (Y^ - Y)(W2)\n",
        "        self.X_H_D_Error_W2 = X.T.dot(self.H_D_Error_W2) ## this is dW1\n",
        "        \n",
        "        ## dW2 ==> (H)T (Y^ - Y)  \n",
        "        self.h_output_delta = self.h.T.dot(self.output_delta) ## this is for dW2\n",
        "        \n",
        "        #print(\"the gradient :\\n\", self.X_H_D_Error_W2)\n",
        "        #print(\"the gradient average:\\n\", self.X_H_D_Error_W2/self.n)\n",
        "        \n",
        "        \n",
        "        self.W1 = self.W1 - self.LR*(self.X_H_D_Error_W2) # c by h  adjusting first set (input -> hidden) weights\n",
        "        self.W2 = self.W2 - self.LR*(self.h_output_delta) \n",
        "        \n",
        "        \n",
        "        print(\"The mean of the b update is\\n\", np.mean(self.H_D_Error_W2, axis=0))\n",
        "        print(\"The b biases before the update are:\\n\", self.b)\n",
        "        self.b = self.b  - self.LRB*np.mean(self.H_D_Error_W2, axis=0)\n",
        "        #print(\"The H_D_Error_W2 is...\\n\", self.H_D_Error_W2)\n",
        "        print(\"Updated bs are:\\n\", self.b)\n",
        "        \n",
        "        self.c = self.c - self.LR*np.mean(self.output_delta, axis=0)\n",
        "        print(\"Updated c's are:\\n\", self.c)\n",
        "        \n",
        "        print(\"The W1 is: \\n\", self.W1)\n",
        "        print(\"The W1 gradient is: \\n\", self.X_H_D_Error_W2)\n",
        "        #print(\"The W1 gradient average is: \\n\", self.X_H_D_Error_W2/self.n)\n",
        "        print(\"The W2 gradient  is: \\n\", self.h_output_delta)\n",
        "        #print(\"The W2 gradient average is: \\n\", self.h_output_delta/self.n)\n",
        "        print(\"The mean biases b gradient is:\\n\",np.mean(self.H_D_Error_W2, axis=0 ))\n",
        "        print(\"The mean bias c gradient is: \\n\", np.mean(self.output_delta, axis=0))\n",
        "        ################################################################\n",
        "        \n",
        "    def TrainNetwork(self, X, y):\n",
        "        output = self.FeedForward(X)\n",
        "        #print(\"Output in TNN\\n\", output)\n",
        "        self.BackProp(X, y, output)\n",
        "        return output"
      ],
      "id": "1a12d9a3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train Network\n"
      ],
      "id": "e3973d04"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "MyNN = NeuralNetwork()\n",
        "\n",
        "AvgLoss=[]\n",
        "Epochs=500\n",
        "\n",
        "for i in range(Epochs): \n",
        "    print(\"\\nRUN:\\n \", i)\n",
        "    output=MyNN.TrainNetwork(X, y)\n",
        "   ## LOSS\n",
        "    loss = np.mean(-y * np.log(output))  ## We need y to place the \"1\" in the right place\n",
        "    print(\"The current average loss is\\n\", loss)\n",
        "    \n",
        "    AvgLoss.append(loss)\n",
        "    \n",
        "   ## OUTPUT\n",
        "    #print(\"The raw output is: \\n\", output)\n",
        "    #print(\"Original y values:\\n\", original_y_values)\n",
        "    numeric_output=np.argmax(output, axis=1) \n",
        "    \n",
        "    #print('Prediction y^ is', numeric_output)  \n",
        "    #print(\"Original Labels y are\\n\",original_y_values )\n",
        "    ## Using Categorical Cross Entropy..........."
      ],
      "id": "f73de0f5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Output and Vis"
      ],
      "id": "228218d2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig1 = plt.figure()\n",
        "ax = plt.axes()\n",
        "x = np.linspace(0, Epochs, Epochs)\n",
        "ax.plot(x, AvgLoss)    \n",
        "\n",
        "## FIX THE SHAPES FIRST!!\n",
        "print(numeric_output.shape)\n",
        "numeric_output2 = np.array([numeric_output])\n",
        "numeric_output2=numeric_output2.T\n",
        "print(numeric_output2.shape)\n",
        "print(original_y_values.shape)\n",
        "\n",
        "print(\"The prediction accuracy via confusion matrix is:\\n\")\n",
        "print(confusion_matrix(numeric_output2, original_y_values))"
      ],
      "id": "5a283fad",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}