<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Professor Ami Gates">

<title>XOR Problem using NN</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="XOR_NN_files/libs/clipboard/clipboard.min.js"></script>
<script src="XOR_NN_files/libs/quarto-html/quarto.js"></script>
<script src="XOR_NN_files/libs/quarto-html/popper.min.js"></script>
<script src="XOR_NN_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="XOR_NN_files/libs/quarto-html/anchor.min.js"></script>
<link href="XOR_NN_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="XOR_NN_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="XOR_NN_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="XOR_NN_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="XOR_NN_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#setting-up-x-and-y" id="toc-setting-up-x-and-y" class="nav-link active" data-scroll-target="#setting-up-x-and-y">Setting up <span class="math inline">\(X\)</span> and <span class="math inline">\(y\)</span></a></li>
  <li><a href="#neural-network-class-object-code" id="toc-neural-network-class-object-code" class="nav-link" data-scroll-target="#neural-network-class-object-code">Neural Network Class object code</a></li>
  <li><a href="#use-run-for-____-epochs" id="toc-use-run-for-____-epochs" class="nav-link" data-scroll-target="#use-run-for-____-epochs">Use run for ____ epochs</a>
  <ul class="collapse">
  <li><a href="#apply-filter-to-output" id="toc-apply-filter-to-output" class="nav-link" data-scroll-target="#apply-filter-to-output">Apply filter to output</a></li>
  </ul></li>
  <li><a href="#output-and-vis" id="toc-output-and-vis" class="nav-link" data-scroll-target="#output-and-vis">Output and Vis</a></li>
  <li><a href="#extra-matrix-multiplication" id="toc-extra-matrix-multiplication" class="nav-link" data-scroll-target="#extra-matrix-multiplication">Extra (matrix multiplication)</a></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">XOR Problem using NN</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Professor Ami Gates </p>
          </div>
  </div>
    
  
    
  </div>
  

</header>

<p>XOR - BP</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="setting-up-x-and-y" class="level1">
<h1>Setting up <span class="math inline">\(X\)</span> and <span class="math inline">\(y\)</span></h1>
<p><span class="math inline">\(X\)</span> can be whatever YOU want it to be…..</p>
<p>For XOR</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"STARTING</span><span class="ch">\n</span><span class="st">......."</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array( [[<span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>] , [<span class="dv">1</span>, <span class="dv">1</span>] ])</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"X is:</span><span class="ch">\n</span><span class="st">"</span>, X)   </span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The shape of X is</span><span class="ch">\n</span><span class="st">"</span>, X.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>STARTING
.......
X is:
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
The shape of X is
 (4, 2)</code></pre>
</div>
</div>
<p>Set <span class="math inline">\(y\)</span> to match with your choice of <span class="math inline">\(X\)</span></p>
<p>These are just examples I was playing with….</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array(( [[<span class="dv">0</span>], [<span class="dv">1</span>], [<span class="dv">1</span>], [<span class="dv">0</span>]] ))</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"y is:</span><span class="ch">\n</span><span class="st">"</span>, y) </span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The shape of Y is</span><span class="ch">\n</span><span class="st">"</span>, y.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>y is:
 [[0]
 [1]
 [1]
 [0]]
The shape of Y is
 (4, 1)</code></pre>
</div>
</div>
</section>
<section id="neural-network-class-object-code" class="level1">
<h1>Neural Network Class object code</h1>
<p><strong>NOTE</strong></p>
<p>You can set the parameters randomly or by hand Both options are below.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NeuralNetwork(<span class="bu">object</span>):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.InputNumColumns <span class="op">=</span> <span class="dv">2</span>  <span class="co">## columns</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.OutputSize <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.HiddenUnits <span class="op">=</span> <span class="dv">2</span>  <span class="co">## one layer with h units</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n <span class="op">=</span> <span class="dv">4</span>  <span class="co">## number of training examples, n</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>        <span class="co">#Random W1</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W1 <span class="op">=</span> np.random.randn(<span class="va">self</span>.InputNumColumns, <span class="va">self</span>.HiddenUnits) <span class="co"># c by h  </span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>        <span class="co">##self.W1=[[1, 1], [1, 1]] # If YOU want to control these values</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"W1 is</span><span class="ch">\n</span><span class="st">"</span>, <span class="va">self</span>.W1)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W2 <span class="op">=</span> np.random.randn(<span class="va">self</span>.HiddenUnits, <span class="va">self</span>.OutputSize) <span class="co"># h by o </span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>        <span class="co">#self.W2=np.array(( [[1], [1]] )) # If YOU want to control these values</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"W2 is:</span><span class="ch">\n</span><span class="st">"</span>, <span class="va">self</span>.W2)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b <span class="op">=</span> np.random.randn(<span class="va">self</span>.OutputSize, <span class="va">self</span>.HiddenUnits)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>        <span class="co">#self.b = [[1,1]] # If YOU want to control these values</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"The b's are:</span><span class="ch">\n</span><span class="st">"</span>, <span class="va">self</span>.b)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>        <span class="co">## biases for layer 1</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.c <span class="op">=</span> np.random.randn(<span class="dv">1</span>, <span class="va">self</span>.OutputSize)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>        <span class="co">#self.c =1 # If YOU want to control this value</span></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"The c is</span><span class="ch">\n</span><span class="st">"</span>, <span class="va">self</span>.c)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>        <span class="co">## bias for last layer</span></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.GA<span class="op">=</span><span class="va">False</span> <span class="co">## set this to True if you want the </span></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>        <span class="co">## average gradient over all examples rather than</span></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>        <span class="co">## the sum</span></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> FeedForward(<span class="va">self</span>, X):</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"FeedForward</span><span class="ch">\n\n</span><span class="st">"</span>)</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.z <span class="op">=</span> (np.dot(X, <span class="va">self</span>.W1)) <span class="op">+</span> <span class="va">self</span>.b </span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>        <span class="co">#X is n by c   W1  is c by h --&gt;  n by h</span></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Z1 is:</span><span class="ch">\n</span><span class="st">"</span>, <span class="va">self</span>.z)        </span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> <span class="va">self</span>.Sigmoid(<span class="va">self</span>.z) <span class="co">#activation function, shape: n by h</span></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"H is:</span><span class="ch">\n</span><span class="st">"</span>, <span class="va">self</span>.h)</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"The c is</span><span class="ch">\n</span><span class="st">"</span>, <span class="va">self</span>.c)</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.z2 <span class="op">=</span> (np.dot(<span class="va">self</span>.h, <span class="va">self</span>.W2)) <span class="op">+</span> <span class="va">self</span>.c<span class="co"># n by h  @  h by o  --&gt;  n by o  </span></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Z2 is:</span><span class="ch">\n</span><span class="st">"</span>, <span class="va">self</span>.z2)</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.Sigmoid(<span class="va">self</span>.z2)  </span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Y^ -  the output is:</span><span class="ch">\n</span><span class="st">"</span>, output)</span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> Sigmoid(<span class="va">self</span>, s, deriv<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (deriv <span class="op">==</span> <span class="va">True</span>):</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> s <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> s)</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>s))</span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> BackProp(<span class="va">self</span>, X, y, output):</span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n\n</span><span class="st">BackProp</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"input X is</span><span class="ch">\n</span><span class="st">"</span>, X)</span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"input y is </span><span class="ch">\n</span><span class="st">"</span>, y)</span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.LR <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Y^ - Y</span></span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_error <span class="op">=</span> output <span class="op">-</span> y    </span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a>        <span class="co">#print("Y^ - Y\n", self.output_error)</span></span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a>        <span class="co">#print("SIG Y^\n", self.Sigmoid(output, deriv=True))</span></span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a>        <span class="co">##(Y^ - Y)(Y^)(1-Y^)</span></span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Y is</span><span class="ch">\n</span><span class="st">"</span>, y)</span>
<span id="cb6-64"><a href="#cb6-64" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Y^ is</span><span class="ch">\n</span><span class="st">"</span>, output)</span>
<span id="cb6-65"><a href="#cb6-65" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_delta <span class="op">=</span> <span class="va">self</span>.output_error <span class="op">*</span> <span class="va">self</span>.Sigmoid(output, deriv<span class="op">=</span><span class="va">True</span>) </span>
<span id="cb6-66"><a href="#cb6-66" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"D_Error (Y^)(1-Y^)(Y^-Y) is:</span><span class="ch">\n</span><span class="st">"</span>, <span class="va">self</span>.output_delta)</span>
<span id="cb6-67"><a href="#cb6-67" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-68"><a href="#cb6-68" aria-hidden="true" tabindex="-1"></a>        <span class="co">##(Y^ - Y)(Y^)(1-Y^)(W2)</span></span>
<span id="cb6-69"><a href="#cb6-69" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.D_Error_W2 <span class="op">=</span> <span class="va">self</span>.output_delta.dot(<span class="va">self</span>.W2.T) <span class="co">#  D_Error times W2</span></span>
<span id="cb6-70"><a href="#cb6-70" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-71"><a href="#cb6-71" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"W2.T is</span><span class="ch">\n</span><span class="st">"</span>, <span class="va">self</span>.W2.T)</span>
<span id="cb6-72"><a href="#cb6-72" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">" D_Error times W2.T</span><span class="ch">\n</span><span class="st">"</span>, <span class="va">self</span>.D_Error_W2)</span>
<span id="cb6-73"><a href="#cb6-73" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-74"><a href="#cb6-74" aria-hidden="true" tabindex="-1"></a>        <span class="co">## (H)(1 - H) (Y^ - Y)(Y^)(1-Y^)(W2)</span></span>
<span id="cb6-75"><a href="#cb6-75" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.H_D_Error_W2 <span class="op">=</span> <span class="va">self</span>.D_Error_W2 <span class="op">*</span> <span class="va">self</span>.Sigmoid(<span class="va">self</span>.h, deriv<span class="op">=</span><span class="va">True</span>) </span>
<span id="cb6-76"><a href="#cb6-76" aria-hidden="true" tabindex="-1"></a>        <span class="co">## Note that * will multiply respective values together in each matrix</span></span>
<span id="cb6-77"><a href="#cb6-77" aria-hidden="true" tabindex="-1"></a>        <span class="co">#print("Derivative sig H is:\n", self.Sigmoid(self.h, deriv=True))</span></span>
<span id="cb6-78"><a href="#cb6-78" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"self.H_D_Error_W2 is</span><span class="ch">\n</span><span class="st">"</span>, <span class="va">self</span>.H_D_Error_W2)</span>
<span id="cb6-79"><a href="#cb6-79" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.H_D_Error_W2_mean<span class="op">=</span>np.mean(<span class="va">self</span>.H_D_Error_W2, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb6-80"><a href="#cb6-80" aria-hidden="true" tabindex="-1"></a>        <span class="co">#print("self.H_D_Error_W2 mean is\n", self.H_D_Error_W2_mean)</span></span>
<span id="cb6-81"><a href="#cb6-81" aria-hidden="true" tabindex="-1"></a>        <span class="co">#print("shape",self.H_D_Error_W2_mean.shape )</span></span>
<span id="cb6-82"><a href="#cb6-82" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-83"><a href="#cb6-83" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-84"><a href="#cb6-84" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-85"><a href="#cb6-85" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-86"><a href="#cb6-86" aria-hidden="true" tabindex="-1"></a>        <span class="co">##  XT  (H)(1 - H) (Y^ - Y)(Y^)(1-Y^)(W2)</span></span>
<span id="cb6-87"><a href="#cb6-87" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"X transpose is</span><span class="ch">\n</span><span class="st">"</span>, X.T)</span>
<span id="cb6-88"><a href="#cb6-88" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"self.H_D_Error_W2 is</span><span class="ch">\n</span><span class="st">"</span>, <span class="va">self</span>.H_D_Error_W2)</span>
<span id="cb6-89"><a href="#cb6-89" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.X_H_D_Error_W2 <span class="op">=</span> X.T <span class="op">@</span> (<span class="va">self</span>.H_D_Error_W2) <span class="co">## this is dW1</span></span>
<span id="cb6-90"><a href="#cb6-90" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"X_H_D_Error_W2 is</span><span class="ch">\n</span><span class="st">"</span>, <span class="va">self</span>.X_H_D_Error_W2)</span>
<span id="cb6-91"><a href="#cb6-91" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-92"><a href="#cb6-92" aria-hidden="true" tabindex="-1"></a>        <span class="co">## (H)T (Y^ - Y)(Y^)(1-Y^)</span></span>
<span id="cb6-93"><a href="#cb6-93" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h_output_delta <span class="op">=</span> <span class="va">self</span>.h.T.dot(<span class="va">self</span>.output_delta) <span class="co">## this is dW2</span></span>
<span id="cb6-94"><a href="#cb6-94" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-95"><a href="#cb6-95" aria-hidden="true" tabindex="-1"></a>        <span class="co">#print("the gradient :\n", self.X_H_D_Error_W2)</span></span>
<span id="cb6-96"><a href="#cb6-96" aria-hidden="true" tabindex="-1"></a>        <span class="co">#print("the gradient average:\n", self.X_H_D_Error_W2/self.n)</span></span>
<span id="cb6-97"><a href="#cb6-97" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-98"><a href="#cb6-98" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span>(<span class="va">self</span>.GA<span class="op">==</span><span class="va">True</span>):</span>
<span id="cb6-99"><a href="#cb6-99" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Using average gradient........</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb6-100"><a href="#cb6-100" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.W1 <span class="op">=</span> <span class="va">self</span>.W1 <span class="op">-</span> <span class="va">self</span>.LR<span class="op">*</span>(<span class="va">self</span>.X_H_D_Error_W2<span class="op">/</span><span class="va">self</span>.n)</span>
<span id="cb6-101"><a href="#cb6-101" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.W2 <span class="op">=</span> <span class="va">self</span>.W2 <span class="op">-</span> <span class="va">self</span>.LR<span class="op">*</span>(<span class="va">self</span>.h_output_delta<span class="op">/</span><span class="va">self</span>.n) <span class="co">## average the gradients</span></span>
<span id="cb6-102"><a href="#cb6-102" aria-hidden="true" tabindex="-1"></a>        <span class="co"># #print("New W1: \n", self.W1)</span></span>
<span id="cb6-103"><a href="#cb6-103" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>: </span>
<span id="cb6-104"><a href="#cb6-104" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Using sum gradient........</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb6-105"><a href="#cb6-105" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"W1 was :</span><span class="ch">\n</span><span class="st">"</span>, <span class="va">self</span>.W1)</span>
<span id="cb6-106"><a href="#cb6-106" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.W1 <span class="op">=</span> <span class="va">self</span>.W1 <span class="op">-</span> <span class="va">self</span>.LR<span class="op">*</span>(<span class="va">self</span>.X_H_D_Error_W2) <span class="co"># c by h  adjusting first set (input -&gt; hidden) weights</span></span>
<span id="cb6-107"><a href="#cb6-107" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Updated W1 is: </span><span class="ch">\n</span><span class="st">"</span>, <span class="va">self</span>.W1)</span>
<span id="cb6-108"><a href="#cb6-108" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"W2 was :</span><span class="ch">\n</span><span class="st">"</span>, <span class="va">self</span>.W2)</span>
<span id="cb6-109"><a href="#cb6-109" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.W2 <span class="op">=</span> <span class="va">self</span>.W2 <span class="op">-</span> <span class="va">self</span>.LR<span class="op">*</span>(<span class="va">self</span>.h_output_delta) <span class="co"># adjusting second set (hidden -&gt; output) weights</span></span>
<span id="cb6-110"><a href="#cb6-110" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Updated W2 is: </span><span class="ch">\n</span><span class="st">"</span>, <span class="va">self</span>.W2)</span>
<span id="cb6-111"><a href="#cb6-111" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-112"><a href="#cb6-112" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-113"><a href="#cb6-113" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-114"><a href="#cb6-114" aria-hidden="true" tabindex="-1"></a>        <span class="co">#print("The W1 gradient is: \n", self.X_H_D_Error_W2)</span></span>
<span id="cb6-115"><a href="#cb6-115" aria-hidden="true" tabindex="-1"></a>        <span class="co">#print("The W1 gradient average is: \n", self.X_H_D_Error_W2/self.n)</span></span>
<span id="cb6-116"><a href="#cb6-116" aria-hidden="true" tabindex="-1"></a>        <span class="co">#print("The W2 gradient  is: \n", self.h_output_delta)</span></span>
<span id="cb6-117"><a href="#cb6-117" aria-hidden="true" tabindex="-1"></a>        <span class="co">#print("The W2 gradient average is: \n", self.h_output_delta/self.n)</span></span>
<span id="cb6-118"><a href="#cb6-118" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"The mean of the biases b gradient is:</span><span class="ch">\n</span><span class="st">"</span>,<span class="va">self</span>.H_D_Error_W2_mean )</span>
<span id="cb6-119"><a href="#cb6-119" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"The b biases before the update are:</span><span class="ch">\n</span><span class="st">"</span>, <span class="va">self</span>.b)</span>
<span id="cb6-120"><a href="#cb6-120" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b <span class="op">=</span> <span class="va">self</span>.b  <span class="op">-</span> <span class="va">self</span>.LR<span class="op">*</span><span class="va">self</span>.H_D_Error_W2_mean</span>
<span id="cb6-121"><a href="#cb6-121" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"The new updated bs are:</span><span class="ch">\n</span><span class="st">"</span>, <span class="va">self</span>.b)</span>
<span id="cb6-122"><a href="#cb6-122" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-123"><a href="#cb6-123" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"The bias c is: </span><span class="ch">\n</span><span class="st">"</span>, <span class="va">self</span>.output_delta)</span>
<span id="cb6-124"><a href="#cb6-124" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"c bias before:"</span>, <span class="va">self</span>.c)</span>
<span id="cb6-125"><a href="#cb6-125" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.c <span class="op">=</span> <span class="va">self</span>.c <span class="op">-</span> np.mean(<span class="va">self</span>.output_delta)</span>
<span id="cb6-126"><a href="#cb6-126" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"The mean c bias after update:"</span>, <span class="va">self</span>.c)</span>
<span id="cb6-127"><a href="#cb6-127" aria-hidden="true" tabindex="-1"></a>       </span>
<span id="cb6-128"><a href="#cb6-128" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-129"><a href="#cb6-129" aria-hidden="true" tabindex="-1"></a>        <span class="co">################################################################</span></span>
<span id="cb6-130"><a href="#cb6-130" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-131"><a href="#cb6-131" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> TrainNetwork(<span class="va">self</span>, X, y):</span>
<span id="cb6-132"><a href="#cb6-132" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.FeedForward(X)</span>
<span id="cb6-133"><a href="#cb6-133" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.BackProp(X, y, output)</span>
<span id="cb6-134"><a href="#cb6-134" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="use-run-for-____-epochs" class="level1">
<h1>Use run for ____ epochs</h1>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>MyNN <span class="op">=</span> NeuralNetwork()</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>TotalLoss<span class="op">=</span>[]</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>AvgLoss<span class="op">=</span>[]</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>Epochs<span class="op">=</span> <span class="dv">150</span>    <span class="co"># was originall 800, but changed to bypass the quarto call stack error</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(Epochs): </span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">RUN:</span><span class="ch">\n</span><span class="st"> "</span>, i)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    output<span class="op">=</span>MyNN.TrainNetwork(X, y)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">#print("The y is ...\n", y)</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"The output is: </span><span class="ch">\n</span><span class="st">"</span>, output)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Total Loss:"</span>, <span class="fl">.5</span><span class="op">*</span>(np.<span class="bu">sum</span>(np.square(output<span class="op">-</span>y))))</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    TotalLoss.append( <span class="fl">.5</span><span class="op">*</span>(np.<span class="bu">sum</span>(np.square(output<span class="op">-</span>y))))</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Average Loss:"</span>, <span class="fl">.5</span><span class="op">*</span>(np.mean(np.square((output<span class="op">-</span>y)))))</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    AvgLoss.append(<span class="fl">.5</span><span class="op">*</span>(np.mean(np.square((output<span class="op">-</span>y)))))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>W1 is
 [[-0.33300347 -0.01515184]
 [-1.6648656  -0.5215048 ]]
W2 is:
 [[0.26503926]
 [0.95316107]]
The b's are:
 [[-2.48473534 -1.58295707]]
The c is
 [[-0.3614243]]

RUN:
  0
FeedForward


Z1 is:
 [[-2.48473534 -1.58295707]
 [-4.14960094 -2.10446188]
 [-2.81773881 -1.59810892]
 [-4.48260441 -2.11961372]]
H is:
 [[0.07693524 0.1703771 ]
 [0.01552585 0.10866391]
 [0.0563731  0.16824609]
 [0.01117758 0.10720504]]
The c is
 [[-0.3614243]]
Z2 is:
 [[-0.17863663]
 [-0.25373514]
 [-0.1861176 ]
 [-0.25627814]]
Y^ -  the output is:
 [[0.45545923]
 [0.43690437]
 [0.45360445]
 [0.43627884]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.45545923]
 [0.43690437]
 [0.45360445]
 [0.43627884]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.11296123]
 [-0.13853219]
 [-0.13542275]
 [ 0.10729825]]
W2.T is
 [[0.26503926 0.95316107]]
 D_Error times W2.T
 [[ 0.02993916  0.10767025]
 [-0.03671647 -0.13204349]
 [-0.03589234 -0.12907969]
 [ 0.02843825  0.10227252]]
self.H_D_Error_W2 is
 [[ 0.00212617  0.01521905]
 [-0.0005612  -0.01278921]
 [-0.0019093  -0.01806333]
 [ 0.00031432  0.00978872]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00212617  0.01521905]
 [-0.0005612  -0.01278921]
 [-0.0019093  -0.01806333]
 [ 0.00031432  0.00978872]]
X_H_D_Error_W2 is
 [[-0.00159498 -0.00827461]
 [-0.00024689 -0.00300049]]
Using sum gradient........

W1 was :
 [[-0.33300347 -0.01515184]
 [-1.6648656  -0.5215048 ]]
Updated W1 is: 
 [[-0.33140849 -0.00687724]
 [-1.66461871 -0.51850431]]
W2 was :
 [[0.26503926]
 [0.95316107]]
Updated W2 is: 
 [[0.26493426]
 [0.96024995]]
The mean of the biases b gradient is:
 [-7.50494051e-06 -1.46119158e-03]
The b biases before the update are:
 [[-2.48473534 -1.58295707]]
The new updated bs are:
 [[-2.48472784 -1.58149588]]
The bias c is: 
 [[ 0.11296123]
 [-0.13853219]
 [-0.13542275]
 [ 0.10729825]]
c bias before: [[-0.3614243]]
The mean c bias after update: [[-0.34800044]]
The output is: 
 [[0.45545923]
 [0.43690437]
 [0.45360445]
 [0.43627884]]
Total Loss: 0.506703560618369
Average Loss: 0.12667589015459224

RUN:
  1
FeedForward


Z1 is:
 [[-2.48472784 -1.58149588]
 [-4.14934655 -2.10000019]
 [-2.81613633 -1.58837312]
 [-4.48075504 -2.10687743]]
H is:
 [[0.07693577 0.17058373]
 [0.01552974 0.1090968 ]
 [0.0564584  0.16961291]
 [0.01119804 0.10843017]]
The c is
 [[-0.34800044]]
Z2 is:
 [[-0.1638145 ]
 [-0.23912588]
 [-0.17017188]
 [-0.24091363]]
Y^ -  the output is:
 [[0.45913771]
 [0.44050178]
 [0.4575594 ]
 [0.44006121]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.45913771]
 [0.44050178]
 [0.4575594 ]
 [0.44006121]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.11401779]
 [-0.13789391]
 [-0.1346331 ]
 [ 0.10843431]]
W2.T is
 [[0.26493426 0.96024995]]
 D_Error times W2.T
 [[ 0.03020722  0.10948558]
 [-0.03653282 -0.13241262]
 [-0.03566892 -0.12928143]
 [ 0.02872796  0.10412404]]
self.H_D_Error_W2 is
 [[ 0.00214522  0.01549056]
 [-0.00055853 -0.0128698 ]
 [-0.00190011 -0.01820856]
 [ 0.00031809  0.01006599]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00214522  0.01549056]
 [-0.00055853 -0.0128698 ]
 [-0.00190011 -0.01820856]
 [ 0.00031809  0.01006599]]
X_H_D_Error_W2 is
 [[-0.00158202 -0.00814257]
 [-0.00024044 -0.00280381]]
Using sum gradient........

W1 was :
 [[-0.33140849 -0.00687724]
 [-1.66461871 -0.51850431]]
Updated W1 is: 
 [[-3.29826472e-01  1.26533536e-03]
 [-1.66437827e+00 -5.15700497e-01]]
W2 was :
 [[0.26493426]
 [0.96024995]]
Updated W2 is: 
 [[0.26469058]
 [0.96692211]]
The mean of the biases b gradient is:
 [ 1.16551616e-06 -1.38045400e-03]
The b biases before the update are:
 [[-2.48472784 -1.58149588]]
The new updated bs are:
 [[-2.484729   -1.58011543]]
The bias c is: 
 [[ 0.11401779]
 [-0.13789391]
 [-0.1346331 ]
 [ 0.10843431]]
c bias before: [[-0.34800044]]
The mean c bias after update: [[-0.33548171]]
The output is: 
 [[0.45913771]
 [0.44050178]
 [0.4575594 ]
 [0.44006121]]
Total Loss: 0.5058706911542734
Average Loss: 0.12646767278856835

RUN:
  2
FeedForward


Z1 is:
 [[-2.484729   -1.58011543]
 [-4.14910727 -2.09581593]
 [-2.81455547 -1.57885009]
 [-4.47893375 -2.09455059]]
H is:
 [[0.07693569 0.17077914]
 [0.0155334  0.10950416]
 [0.05654267 0.1709584 ]
 [0.01121823 0.1096276 ]]
The c is
 [[-0.33548171]]
Z2 is:
 [[-0.14998744]
 [-0.22548818]
 [-0.15521194]
 [-0.226511  ]]
Y^ -  the output is:
 [[0.46257328]
 [0.4438656 ]
 [0.46127473]
 [0.44361313]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.46257328]
 [0.4438656 ]
 [0.46127473]
 [0.44361313]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.11499537]
 [-0.13728118]
 [-0.13387342]
 [ 0.10949282]]
W2.T is
 [[0.26469058 0.96692211]]
 D_Error times W2.T
 [[ 0.03043819  0.11119156]
 [-0.03633704 -0.13274021]
 [-0.03543503 -0.12944517]
 [ 0.02898172  0.10587103]]
self.H_D_Error_W2 is
 [[ 0.00216162  0.01574624]
 [-0.00055567 -0.0129439 ]
 [-0.0018903  -0.01834647]
 [ 0.00032148  0.01033401]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00216162  0.01574624]
 [-0.00055567 -0.0129439 ]
 [-0.0018903  -0.01834647]
 [ 0.00032148  0.01033401]]
X_H_D_Error_W2 is
 [[-0.00156883 -0.00801247]
 [-0.00023419 -0.00260989]]
Using sum gradient........

W1 was :
 [[-3.29826472e-01  1.26533536e-03]
 [-1.66437827e+00 -5.15700497e-01]]
Updated W1 is: 
 [[-0.32825764  0.0092778 ]
 [-1.66414408 -0.51309061]]
W2 was :
 [[0.26469058]
 [0.96692211]]
Updated W2 is: 
 [[0.26431703]
 [0.97319952]]
The mean of the biases b gradient is:
 [ 9.27985350e-06 -1.30253071e-03]
The b biases before the update are:
 [[-2.484729   -1.58011543]]
The new updated bs are:
 [[-2.48473828 -1.5788129 ]]
The bias c is: 
 [[ 0.11499537]
 [-0.13728118]
 [-0.13387342]
 [ 0.10949282]]
c bias before: [[-0.33548171]]
The mean c bias after update: [[-0.32381511]]
The output is: 
 [[0.46257328]
 [0.4438656 ]
 [0.46127473]
 [0.44361313]]
Total Loss: 0.5051385196246267
Average Loss: 0.1262846299061567

RUN:
  3
FeedForward


Z1 is:
 [[-2.48473828 -1.5788129 ]
 [-4.14888236 -2.09190351]
 [-2.81299593 -1.5695351 ]
 [-4.47714    -2.0826257 ]]
H is:
 [[0.07693503 0.17096367]
 [0.01553684 0.10988625]
 [0.05662593 0.17228268]
 [0.01123814 0.11079702]]
The c is
 [[-0.32381511]]
Z2 is:
 [[-0.13709811]
 [-0.21276721]
 [-0.14118249]
 [-0.21301707]]
Y^ -  the output is:
 [[0.46577906]
 [0.44700796]
 [0.46476289]
 [0.4469462 ]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.46577906]
 [0.44700796]
 [0.46476289]
 [0.4469462 ]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.1158993 ]
 [-0.13669512]
 [-0.1331447 ]
 [ 0.11047853]]
W2.T is
 [[0.26431703 0.97319952]]
 D_Error times W2.T
 [[ 0.03063416  0.11279315]
 [-0.03613085 -0.13303163]
 [-0.03519241 -0.12957636]
 [ 0.02920136  0.10751765]]
self.H_D_Error_W2 is
 [[ 0.00217552  0.01598675]
 [-0.00055264 -0.01301199]
 [-0.00187996 -0.01847776]
 [ 0.00032448  0.01059275]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00217552  0.01598675]
 [-0.00055264 -0.01301199]
 [-0.00187996 -0.01847776]
 [ 0.00032448  0.01059275]]
X_H_D_Error_W2 is
 [[-0.00155548 -0.00788501]
 [-0.00022816 -0.00241924]]
Using sum gradient........

W1 was :
 [[-0.32825764  0.0092778 ]
 [-1.66414408 -0.51309061]]
Updated W1 is: 
 [[-0.32670217  0.01716282]
 [-1.66391592 -0.51067137]]
W2 was :
 [[0.26431703]
 [0.97319952]]
Updated W2 is: 
 [[0.26382199]
 [0.97910369]]
The mean of the biases b gradient is:
 [ 1.68503294e-05 -1.22756461e-03]
The b biases before the update are:
 [[-2.48473828 -1.5788129 ]]
The new updated bs are:
 [[-2.48475513 -1.57758533]]
The bias c is: 
 [[ 0.1158993 ]
 [-0.13669512]
 [-0.1331447 ]
 [ 0.11047853]]
c bias before: [[-0.32381511]]
The mean c bias after update: [[-0.31294961]]
The output is: 
 [[0.46577906]
 [0.44700796]
 [0.46476289]
 [0.4469462 ]]
Total Loss: 0.5044949977898465
Average Loss: 0.12612374944746163

RUN:
  4
FeedForward


Z1 is:
 [[-2.48475513 -1.57758533]
 [-4.14867105 -2.0882567 ]
 [-2.8114573  -1.56042252]
 [-4.47537322 -2.07109389]]
H is:
 [[0.07693384 0.17113773]
 [0.01554007 0.11024346]
 [0.05670818 0.17358603]
 [0.01125779 0.11193825]]
The c is
 [[-0.31294961]]
Z2 is:
 [[-0.12509119]
 [-0.20091002]
 [-0.12803003]
 [-0.2003804 ]]
Y^ -  the output is:
 [[0.46876792]
 [0.44994077]
 [0.46803614]
 [0.45007185]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.46876792]
 [0.44994077]
 [0.46803614]
 [0.45007185]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.11673472]
 [-0.1361364 ]
 [-0.13244746]
 [ 0.11139601]]
W2.T is
 [[0.26382199 0.97910369]]
 D_Error times W2.T
 [[ 0.03079719  0.1142954 ]
 [-0.03591578 -0.13329165]
 [-0.03494255 -0.1296798 ]
 [ 0.02938872  0.10906825]]
self.H_D_Error_W2 is
 [[ 0.00218706  0.01621276]
 [-0.00054946 -0.01307456]
 [-0.00186916 -0.01860308]
 [ 0.00032713  0.01084227]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00218706  0.01621276]
 [-0.00054946 -0.01307456]
 [-0.00186916 -0.01860308]
 [ 0.00032713  0.01084227]]
X_H_D_Error_W2 is
 [[-0.00154203 -0.00776081]
 [-0.00022233 -0.00223229]]
Using sum gradient........

W1 was :
 [[-0.32670217  0.01716282]
 [-1.66391592 -0.51067137]]
Updated W1 is: 
 [[-0.32516014  0.02492363]
 [-1.66369359 -0.50843908]]
W2 was :
 [[0.26382199]
 [0.97910369]]
Updated W2 is: 
 [[0.26321349]
 [0.98465568]]
The mean of the biases b gradient is:
 [ 2.38925684e-05 -1.15565238e-03]
The b biases before the update are:
 [[-2.48475513 -1.57758533]]
The new updated bs are:
 [[-2.48477902 -1.57642968]]
The bias c is: 
 [[ 0.11673472]
 [-0.1361364 ]
 [-0.13244746]
 [ 0.11139601]]
c bias before: [[-0.31294961]]
The mean c bias after update: [[-0.30283633]]
The output is: 
 [[0.46876792]
 [0.44994077]
 [0.46803614]
 [0.45007185]]
Total Loss: 0.5039293668651307
Average Loss: 0.12598234171628267

RUN:
  5
FeedForward


Z1 is:
 [[-2.48477902 -1.57642968]
 [-4.14847261 -2.08486876]
 [-2.80993916 -1.55150605]
 [-4.47363275 -2.05994513]]
H is:
 [[0.07693214 0.17130172]
 [0.01554311 0.11057622]
 [0.05678944 0.17486885]
 [0.01127718 0.11305133]]
The c is
 [[-0.30283633]]
Z2 is:
 [[-0.11391354]
 [-0.18986567]
 [-0.11570297]
 [-0.18855139]]
Y^ -  the output is:
 [[0.47155237]
 [0.45267566]
 [0.47110648]
 [0.45300131]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.47155237]
 [0.45267566]
 [0.47110648]
 [0.45300131]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.11750648]
 [-0.1356053 ]
 [-0.13178184]
 [ 0.1122497 ]]
W2.T is
 [[0.26321349 0.98465568]]
 D_Error times W2.T
 [[ 0.03092929  0.11570342]
 [-0.03569314 -0.13352453]
 [-0.03468676 -0.12975974]
 [ 0.02954564  0.11052731]]
self.H_D_Error_W2 is
 [[ 0.0021964   0.01642496]
 [-0.00054616 -0.01313202]
 [-0.00185798 -0.018723  ]
 [ 0.00032943  0.01108265]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.0021964   0.01642496]
 [-0.00054616 -0.01313202]
 [-0.00185798 -0.018723  ]
 [ 0.00032943  0.01108265]]
X_H_D_Error_W2 is
 [[-0.00152854 -0.00764034]
 [-0.00021673 -0.00204937]]
Using sum gradient........

W1 was :
 [[-0.32516014  0.02492363]
 [-1.66369359 -0.50843908]]
Updated W1 is: 
 [[-0.32363159  0.03256397]
 [-1.66347686 -0.50638971]]
W2 was :
 [[0.26321349]
 [0.98465568]]
Updated W2 is: 
 [[0.26249915]
 [0.9898759 ]]
The mean of the biases b gradient is:
 [ 3.04247639e-05 -1.08685069e-03]
The b biases before the update are:
 [[-2.48477902 -1.57642968]]
The new updated bs are:
 [[-2.48480945 -1.57534283]]
The bias c is: 
 [[ 0.11750648]
 [-0.1356053 ]
 [-0.13178184]
 [ 0.1122497 ]]
c bias before: [[-0.30283633]]
The mean c bias after update: [[-0.29342859]]
The output is: 
 [[0.47155237]
 [0.45267566]
 [0.47110648]
 [0.45300131]]
Total Loss: 0.503432053591885
Average Loss: 0.12585801339797126

RUN:
  6
FeedForward


Z1 is:
 [[-2.48480945 -1.57534283]
 [-4.14828631 -2.08173254]
 [-2.80844104 -1.54277886]
 [-4.47191791 -2.04916857]]
H is:
 [[0.07692998 0.17145606]
 [0.01554596 0.11088504]
 [0.05686974 0.17613167]
 [0.01129632 0.11413642]]
The c is
 [[-0.29342859]]
Z2 is:
 [[-0.10351431]
 [-0.17958536]
 [-0.10415183]
 [-0.17748243]]
Y^ -  the output is:
 [[0.47414451]
 [0.45522393]
 [0.47398555]
 [0.4557455 ]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.47414451]
 [0.45522393]
 [0.47398555]
 [0.4557455 ]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.11821916]
 [-0.1351018 ]
 [-0.13114763]
 [ 0.11304382]]
W2.T is
 [[0.26249915 0.9898759 ]]
 D_Error times W2.T
 [[ 0.03103243  0.11702229]
 [-0.03546411 -0.13373401]
 [-0.03442614 -0.12981988]
 [ 0.02967391  0.11189935]]
self.H_D_Error_W2 is
 [[ 0.00220367  0.01662406]
 [-0.00054275 -0.01318478]
 [-0.00184647 -0.01883807]
 [ 0.00033142  0.01131406]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00220367  0.01662406]
 [-0.00054275 -0.01318478]
 [-0.00184647 -0.01883807]
 [ 0.00033142  0.01131406]]
X_H_D_Error_W2 is
 [[-0.00151505 -0.00752401]
 [-0.00021133 -0.00187071]]
Using sum gradient........

W1 was :
 [[-0.32363159  0.03256397]
 [-1.66347686 -0.50638971]]
Updated W1 is: 
 [[-0.32211655  0.04008798]
 [-1.66326553 -0.504519  ]]
W2 was :
 [[0.26249915]
 [0.9898759 ]]
Updated W2 is: 
 [[0.26168619]
 [0.99478411]]
The mean of the biases b gradient is:
 [ 3.64669928e-05 -1.02118196e-03]
The b biases before the update are:
 [[-2.48480945 -1.57534283]]
The new updated bs are:
 [[-2.48484592 -1.57432165]]
The bias c is: 
 [[ 0.11821916]
 [-0.1351018 ]
 [-0.13114763]
 [ 0.11304382]]
c bias before: [[-0.29342859]]
The mean c bias after update: [[-0.28468198]]
The output is: 
 [[0.47414451]
 [0.45522393]
 [0.47398555]
 [0.4557455 ]]
Total Loss: 0.5029945661635381
Average Loss: 0.12574864154088453

RUN:
  7
FeedForward


Z1 is:
 [[-2.48484592 -1.57432165]
 [-4.14811145 -2.07884065]
 [-2.80696246 -1.53423367]
 [-4.47022799 -2.03875267]]
H is:
 [[0.07692739 0.17160118]
 [0.01554864 0.11117047]
 [0.0569491  0.17737509]
 [0.01131521 0.1151938 ]]
The c is
 [[-0.28468198]]
Z2 is:
 [[-0.09384501]
 [-0.17002249]
 [-0.09332926]
 [-0.16712798]]
Y^ -  the output is:
 [[0.47655595]
 [0.45759648]
 [0.47668461]
 [0.45831499]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.47655595]
 [0.45759648]
 [0.47668461]
 [0.45831499]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.11887706]
 [-0.13462561]
 [-0.13054437]
 [ 0.11378236]]
W2.T is
 [[0.26168619 0.99478411]]
 D_Error times W2.T
 [[ 0.03110849  0.11825701]
 [-0.03522966 -0.13392342]
 [-0.03416166 -0.12986347]
 [ 0.02977527  0.11318888]]
self.H_D_Error_W2 is
 [[ 0.002209    0.01681073]
 [-0.00053926 -0.01323319]
 [-0.00183468 -0.01894879]
 [ 0.0003331   0.01153669]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.002209    0.01681073]
 [-0.00053926 -0.01323319]
 [-0.00183468 -0.01894879]
 [ 0.0003331   0.01153669]]
X_H_D_Error_W2 is
 [[-0.00150158 -0.0074121 ]
 [-0.00020616 -0.0016965 ]]
Using sum gradient........

W1 was :
 [[-0.32211655  0.04008798]
 [-1.66326553 -0.504519  ]]
Updated W1 is: 
 [[-0.32061497  0.04750008]
 [-1.66305937 -0.5028225 ]]
W2 was :
 [[0.26168619]
 [0.99478411]]
Updated W2 is: 
 [[0.26078145]
 [0.99939936]]
The mean of the biases b gradient is:
 [ 4.20406361e-05 -9.58639619e-04]
The b biases before the update are:
 [[-2.48484592 -1.57432165]]
The new updated bs are:
 [[-2.48488796 -1.57336301]]
The bias c is: 
 [[ 0.11887706]
 [-0.13462561]
 [-0.13054437]
 [ 0.11378236]]
c bias before: [[-0.28468198]]
The mean c bias after update: [[-0.27655434]]
The output is: 
 [[0.47655595]
 [0.45759648]
 [0.47668461]
 [0.45831499]]
Total Loss: 0.5026093927316053
Average Loss: 0.12565234818290133

RUN:
  8
FeedForward


Z1 is:
 [[-2.48488796 -1.57336301]
 [-4.14794733 -2.07618551]
 [-2.80550292 -1.52586293]
 [-4.4685623  -2.02868542]]
H is:
 [[0.0769244  0.1717375 ]
 [0.01555115 0.1114331 ]
 [0.05702753 0.1785998 ]
 [0.01133386 0.11622388]]
The c is
 [[-0.27655434]]
Z2 is:
 [[-0.08485954]
 [-0.16113272]
 [-0.08319009]
 [-0.1574446 ]]
Y^ -  the output is:
 [[0.47879784]
 [0.45980375]
 [0.47921446]
 [0.46071996]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.47879784]
 [0.45980375]
 [0.47921446]
 [0.46071996]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.11948422]
 [-0.13417625]
 [-0.12997138]
 [ 0.11446913]]
W2.T is
 [[0.26078145 0.99939936]]
 D_Error times W2.T
 [[ 0.03115927  0.11941246]
 [-0.03499068 -0.13409565]
 [-0.03389413 -0.12989332]
 [ 0.02985143  0.11440038]]
self.H_D_Error_W2 is
 [[ 0.00221253  0.01698567]
 [-0.00053568 -0.01327758]
 [-0.00182267 -0.0190556 ]
 [ 0.0003345   0.01175074]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00221253  0.01698567]
 [-0.00053568 -0.01327758]
 [-0.00182267 -0.0190556 ]
 [ 0.0003345   0.01175074]]
X_H_D_Error_W2 is
 [[-0.00148817 -0.00730486]
 [-0.00020119 -0.00152685]]
Using sum gradient........

W1 was :
 [[-0.32061497  0.04750008]
 [-1.66305937 -0.5028225 ]]
Updated W1 is: 
 [[-0.31912679  0.05480494]
 [-1.66285819 -0.50129565]]
W2 was :
 [[0.26078145]
 [0.99939936]]
Updated W2 is: 
 [[0.25979136]
 [1.00373993]]
The mean of the biases b gradient is:
 [ 4.71678923e-05 -8.99192879e-04]
The b biases before the update are:
 [[-2.48488796 -1.57336301]]
The new updated bs are:
 [[-2.48493512 -1.57246382]]
The bias c is: 
 [[ 0.11948422]
 [-0.13417625]
 [-0.12997138]
 [ 0.11446913]]
c bias before: [[-0.27655434]]
The mean c bias after update: [[-0.26900577]]
The output is: 
 [[0.47879784]
 [0.45980375]
 [0.47921446]
 [0.46071996]]
Total Loss: 0.502269904386843
Average Loss: 0.12556747609671076

RUN:
  9
FeedForward


Z1 is:
 [[-2.48493512 -1.57246382]
 [-4.14779331 -2.07375947]
 [-2.80406192 -1.51765887]
 [-4.46692011 -2.01895452]]
H is:
 [[0.07692106 0.17186544]
 [0.01555351 0.11167354]
 [0.05710507 0.17980652]
 [0.01135227 0.11722714]]
The c is
 [[-0.26900577]]
Z2 is:
 [[-0.07651414]
 [-0.15287391]
 [-0.07369138]
 [-0.14839099]]
Y^ -  the output is:
 [[0.48088079]
 [0.46185578]
 [0.48158549]
 [0.46297018]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.48088079]
 [0.46185578]
 [0.48158549]
 [0.46297018]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12004441]
 [-0.13375306]
 [-0.12942784]
 [ 0.11510772]]
W2.T is
 [[0.25979136 1.00373993]]
 D_Error times W2.T
 [[ 0.0311865   0.12049337]
 [-0.03474789 -0.13425329]
 [-0.03362423 -0.12991189]
 [ 0.02990399  0.11553821]]
self.H_D_Error_W2 is
 [[ 0.00221437  0.01714955]
 [-0.00053205 -0.01331827]
 [-0.00181047 -0.0191589 ]
 [ 0.00033562  0.01195646]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00221437  0.01714955]
 [-0.00053205 -0.01331827]
 [-0.00181047 -0.0191589 ]
 [ 0.00033562  0.01195646]]
X_H_D_Error_W2 is
 [[-0.00147484 -0.00720244]
 [-0.00019642 -0.00136181]]
Using sum gradient........

W1 was :
 [[-0.31912679  0.05480494]
 [-1.66285819 -0.50129565]]
Updated W1 is: 
 [[-0.31765195  0.06200738]
 [-1.66266177 -0.49993384]]
W2 was :
 [[0.25979136]
 [1.00373993]]
Updated W2 is: 
 [[0.258722  ]
 [1.00782334]]
The mean of the biases b gradient is:
 [ 5.18713759e-05 -8.42790971e-04]
The b biases before the update are:
 [[-2.48493512 -1.57246382]]
The new updated bs are:
 [[-2.484987   -1.57162103]]
The bias c is: 
 [[ 0.12004441]
 [-0.13375306]
 [-0.12942784]
 [ 0.11510772]]
c bias before: [[-0.26900577]]
The mean c bias after update: [[-0.26199858]]
The output is: 
 [[0.48088079]
 [0.46185578]
 [0.48158549]
 [0.46297018]]
Total Loss: 0.5019702638516874
Average Loss: 0.12549256596292185

RUN:
  10
FeedForward


Z1 is:
 [[-2.484987   -1.57162103]
 [-4.14764876 -2.07155487]
 [-2.80263895 -1.50961364]
 [-4.46530071 -2.00954749]]
H is:
 [[0.07691737 0.17198542]
 [0.01555572 0.11189243]
 [0.05718174 0.18099606]
 [0.01137046 0.11820414]]
The c is
 [[-0.26199858]]
Z2 is:
 [[-0.06876744]
 [-0.14520616]
 [-0.06479235]
 [-0.1399279 ]]
Y^ -  the output is:
 [[0.48281491]
 [0.46376211]
 [0.48380758]
 [0.46507499]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.48281491]
 [0.46376211]
 [0.48380758]
 [0.46507499]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12056114]
 [-0.13335529]
 [-0.12891276]
 [ 0.11570147]]
W2.T is
 [[0.258722   1.00782334]]
 D_Error times W2.T
 [[ 0.03119182  0.12150433]
 [-0.03450195 -0.13439858]
 [-0.03335257 -0.12992129]
 [ 0.02993452  0.11660664]]
self.H_D_Error_W2 is
 [[ 0.00221465  0.017303  ]
 [-0.00052835 -0.01335552]
 [-0.0017981  -0.01925908]
 [ 0.0003365   0.01215413]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00221465  0.017303  ]
 [-0.00052835 -0.01335552]
 [-0.0017981  -0.01925908]
 [ 0.0003365   0.01215413]]
X_H_D_Error_W2 is
 [[-0.0014616  -0.00710494]
 [-0.00019185 -0.00120139]]
Using sum gradient........

W1 was :
 [[-0.31765195  0.06200738]
 [-1.66266177 -0.49993384]]
Updated W1 is: 
 [[-0.31619035  0.06911233]
 [-1.66246991 -0.49873245]]
W2 was :
 [[0.258722  ]
 [1.00782334]]
Updated W2 is: 
 [[0.25757907]
 [1.01166634]]
The mean of the biases b gradient is:
 [ 5.61737892e-05 -7.89366899e-04]
The b biases before the update are:
 [[-2.484987   -1.57162103]]
The new updated bs are:
 [[-2.48504317 -1.57083166]]
The bias c is: 
 [[ 0.12056114]
 [-0.13335529]
 [-0.12891276]
 [ 0.11570147]]
c bias before: [[-0.26199858]]
The mean c bias after update: [[-0.25549722]]
The output is: 
 [[0.48281491]
 [0.46376211]
 [0.48380758]
 [0.46507499]]
Total Loss: 0.5017053406100518
Average Loss: 0.12542633515251295

RUN:
  11
FeedForward


Z1 is:
 [[-2.48504317 -1.57083166]
 [-4.14751308 -2.06956411]
 [-2.80123352 -1.50171933]
 [-4.46370343 -2.00045179]]
H is:
 [[0.07691338 0.17209786]
 [0.0155578  0.11209041]
 [0.05725756 0.18216923]
 [0.01138843 0.1191555 ]]
The c is
 [[-0.25549722]]
Z2 is:
 [[-0.06158032]
 [-0.13809175]
 [-0.05645439]
 [-0.13201819]]
Y^ -  the output is:
 [[0.48460978]
 [0.46553182]
 [0.48589015]
 [0.46704331]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.48460978]
 [0.46553182]
 [0.48589015]
 [0.46704331]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12103766]
 [-0.13298207]
 [-0.12842511]
 [ 0.11625355]]
W2.T is
 [[0.25757907 1.01166634]]
 D_Error times W2.T
 [[ 0.03117677  0.12244973]
 [-0.0342534  -0.13453348]
 [-0.03307962 -0.12992336]
 [ 0.02994448  0.1176098 ]]
self.H_D_Error_W2 is
 [[ 0.00221348  0.01744666]
 [-0.00052462 -0.0133896 ]
 [-0.00178561 -0.01935645]
 [ 0.00033714  0.01234403]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00221348  0.01744666]
 [-0.00052462 -0.0133896 ]
 [-0.00178561 -0.01935645]
 [ 0.00033714  0.01234403]]
X_H_D_Error_W2 is
 [[-0.00144847 -0.00701242]
 [-0.00018748 -0.00104557]]
Using sum gradient........

W1 was :
 [[-0.31619035  0.06911233]
 [-1.66246991 -0.49873245]]
Updated W1 is: 
 [[-0.31474188  0.07612475]
 [-1.66228243 -0.49768688]]
W2 was :
 [[0.25757907]
 [1.01166634]]
Updated W2 is: 
 [[0.25636792]
 [1.01528489]]
The mean of the biases b gradient is:
 [ 6.00976573e-05 -7.38840746e-04]
The b biases before the update are:
 [[-2.48504317 -1.57083166]]
The new updated bs are:
 [[-2.48510327 -1.57009282]]
The bias c is: 
 [[ 0.12103766]
 [-0.13298207]
 [-0.12842511]
 [ 0.11625355]]
c bias before: [[-0.25549722]]
The mean c bias after update: [[-0.24946822]]
The output is: 
 [[0.48460978]
 [0.46553182]
 [0.48589015]
 [0.46704331]]
Total Loss: 0.5014706328148621
Average Loss: 0.12536765820371554

RUN:
  12
FeedForward


Z1 is:
 [[-2.48510327 -1.57009282]
 [-4.1473857  -2.0677797 ]
 [-2.79984514 -1.49396807]
 [-4.46212758 -1.99165495]]
H is:
 [[0.07690912 0.17220316]
 [0.01555975 0.11226813]
 [0.05733254 0.18332689]
 [0.01140619 0.12008189]]
The c is
 [[-0.24946822]]
Z2 is:
 [[-0.05491593]
 [-0.13149506]
 [-0.04864098]
 [-0.12462672]]
Y^ -  the output is:
 [[0.48627447]
 [0.46717352]
 [0.48784215]
 [0.46888358]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.48627447]
 [0.46717352]
 [0.48784215]
 [0.46888358]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12147701]
 [-0.13263246]
 [-0.12796376]
 [ 0.11676691]]
W2.T is
 [[0.25636792 1.01528489]]
 D_Error times W2.T
 [[ 0.03114281  0.12333377]
 [-0.03400271 -0.13465973]
 [-0.0328058  -0.12991967]
 [ 0.02993529  0.11855168]]
self.H_D_Error_W2 is
 [[ 0.00221096  0.01758113]
 [-0.00052084 -0.01342073]
 [-0.00177301 -0.01945133]
 [ 0.00033755  0.01252643]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00221096  0.01758113]
 [-0.00052084 -0.01342073]
 [-0.00177301 -0.01945133]
 [ 0.00033755  0.01252643]]
X_H_D_Error_W2 is
 [[-0.00143545 -0.0069249 ]
 [-0.00018329 -0.00089429]]
Using sum gradient........

W1 was :
 [[-0.31474188  0.07612475]
 [-1.66228243 -0.49768688]]
Updated W1 is: 
 [[-0.31330642  0.08304965]
 [-1.66209914 -0.49679259]]
W2 was :
 [[0.25636792]
 [1.01528489]]
Updated W2 is: 
 [[0.25509358]
 [1.01869417]]
The mean of the biases b gradient is:
 [ 6.36651194e-05 -6.91122537e-04]
The b biases before the update are:
 [[-2.48510327 -1.57009282]]
The new updated bs are:
 [[-2.48516693 -1.5694017 ]]
The bias c is: 
 [[ 0.12147701]
 [-0.13263246]
 [-0.12796376]
 [ 0.11676691]]
c bias before: [[-0.24946822]]
The mean c bias after update: [[-0.24388015]]
The output is: 
 [[0.48627447]
 [0.46717352]
 [0.48784215]
 [0.46888358]]
Total Loss: 0.5012621960297272
Average Loss: 0.1253155490074318

RUN:
  13
FeedForward


Z1 is:
 [[-2.48516693 -1.5694017 ]
 [-4.14726608 -2.06619428]
 [-2.79847335 -1.48635205]
 [-4.4605725  -1.98314464]]
H is:
 [[0.0769046  0.1723017 ]
 [0.01556158 0.11242624]
 [0.05740673 0.1844699 ]
 [0.01142374 0.12098402]]
The c is
 [[-0.24388015]]
Z2 is:
 [[-0.04873954]
 [-0.12538254]
 [-0.04131765]
 [-0.11772031]]
Y^ -  the output is:
 [[0.48781753]
 [0.46869537]
 [0.48967206]
 [0.47060386]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.48781753]
 [0.46869537]
 [0.48967206]
 [0.47060386]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12188198]
 [-0.13230549]
 [-0.12752755]
 [ 0.1172443 ]]
W2.T is
 [[0.25509358 1.01869417]]
 D_Error times W2.T
 [[ 0.03109131  0.12416047]
 [-0.03375028 -0.13477883]
 [-0.03253146 -0.12991157]
 [ 0.02990827  0.11943609]]
self.H_D_Error_W2 is
 [[ 0.00220718  0.017707  ]
 [-0.00051703 -0.01344912]
 [-0.00176032 -0.01954399]
 [ 0.00033776  0.01270166]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00220718  0.017707  ]
 [-0.00051703 -0.01344912]
 [-0.00176032 -0.01954399]
 [ 0.00033776  0.01270166]]
X_H_D_Error_W2 is
 [[-0.00142256 -0.00684234]
 [-0.00017927 -0.00074746]]
Using sum gradient........

W1 was :
 [[-0.31330642  0.08304965]
 [-1.66209914 -0.49679259]]
Updated W1 is: 
 [[-0.31188387  0.08989199]
 [-1.66191987 -0.49604512]]
W2 was :
 [[0.25509358]
 [1.01869417]]
Updated W2 is: 
 [[0.25376075]
 [1.02190861]]
The mean of the biases b gradient is:
 [ 6.68977644e-05 -6.46114728e-04]
The b biases before the update are:
 [[-2.48516693 -1.5694017 ]]
The new updated bs are:
 [[-2.48523383 -1.56875558]]
The bias c is: 
 [[ 0.12188198]
 [-0.13230549]
 [-0.12752755]
 [ 0.1172443 ]]
c bias before: [[-0.24388015]]
The mean c bias after update: [[-0.23870346]]
The output is: 
 [[0.48781753]
 [0.46869537]
 [0.48967206]
 [0.47060386]]
Total Loss: 0.5010765786586594
Average Loss: 0.12526914466466485

RUN:
  14
FeedForward


Z1 is:
 [[-2.48523383 -1.56875558]
 [-4.1471537  -2.0648007 ]
 [-2.7971177  -1.4788636 ]
 [-4.45903757 -1.97490872]]
H is:
 [[0.07689985 0.17239387]
 [0.0155633  0.11256537]
 [0.05748013 0.18559913]
 [0.01144108 0.12186262]]
The c is
 [[-0.23870346]]
Z2 is:
 [[-0.04301852]
 [-0.11972258]
 [-0.03445191]
 [-0.1112677 ]]
Y^ -  the output is:
 [[0.48924703]
 [0.47010506]
 [0.49138787]
 [0.47221174]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.48924703]
 [0.47010506]
 [0.49138787]
 [0.47221174]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12225519]
 [-0.13200017]
 [-0.12711531]
 [ 0.1176883 ]]
W2.T is
 [[0.25376075 1.02190861]]
 D_Error times W2.T
 [[ 0.03102357  0.12493363]
 [-0.03349646 -0.13489211]
 [-0.03225688 -0.12990023]
 [ 0.02986467  0.12026669]]
self.H_D_Error_W2 is
 [[ 0.00220225  0.01782481]
 [-0.0005132  -0.01347497]
 [-0.00174755 -0.01963469]
 [ 0.00033777  0.01286999]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00220225  0.01782481]
 [-0.0005132  -0.01347497]
 [-0.00174755 -0.01963469]
 [ 0.00033777  0.01286999]]
X_H_D_Error_W2 is
 [[-0.00140978 -0.0067647 ]
 [-0.00017543 -0.00060497]]
Using sum gradient........

W1 was :
 [[-0.31188387  0.08989199]
 [-1.66191987 -0.49604512]]
Updated W1 is: 
 [[-0.31047409  0.09665668]
 [-1.66174444 -0.49544015]]
W2 was :
 [[0.25376075]
 [1.02190861]]
Updated W2 is: 
 [[0.25237383]
 [1.0249419 ]]
The mean of the biases b gradient is:
 [ 6.98165080e-05 -6.03714333e-04]
The b biases before the update are:
 [[-2.48523383 -1.56875558]]
The new updated bs are:
 [[-2.48530365 -1.56815187]]
The bias c is: 
 [[ 0.12225519]
 [-0.13200017]
 [-0.12711531]
 [ 0.1176883 ]]
c bias before: [[-0.23870346]]
The mean c bias after update: [[-0.23391046]]
The output is: 
 [[0.48924703]
 [0.47010506]
 [0.49138787]
 [0.47221174]]
Total Loss: 0.5009107637792787
Average Loss: 0.12522769094481967

RUN:
  15
FeedForward


Z1 is:
 [[-2.48530365 -1.56815187]
 [-4.14704809 -2.06359202]
 [-2.79577773 -1.47149518]
 [-4.45752218 -1.96693533]]
H is:
 [[0.07689489 0.17248002]
 [0.01556492 0.11268617]
 [0.05755277 0.18671546]
 [0.01145824 0.12271844]]
The c is
 [[-0.23391046]]
Z2 is:
 [[-0.03772221]
 [-0.1144855 ]
 [-0.02801315]
 [-0.10523943]]
Y^ -  the output is:
 [[0.49057057]
 [0.47140984]
 [0.49299717]
 [0.4737144 ]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49057057]
 [0.47140984]
 [0.49299717]
 [0.4737144 ]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12259902]
 [-0.13171547]
 [-0.12672584]
 [ 0.11810129]]
W2.T is
 [[0.25237383 1.0249419 ]]
 D_Error times W2.T
 [[ 0.03094078  0.12565688]
 [-0.03324154 -0.1350007 ]
 [-0.03198229 -0.12988663]
 [ 0.02980568  0.12104697]]
self.H_D_Error_W2 is
 [[ 0.00219624  0.01793509]
 [-0.00050935 -0.01349845]
 [-0.00173473 -0.01972365]
 [ 0.00033761  0.01303175]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00219624  0.01793509]
 [-0.00050935 -0.01349845]
 [-0.00173473 -0.01972365]
 [ 0.00033761  0.01303175]]
X_H_D_Error_W2 is
 [[-0.00139713 -0.0066919 ]
 [-0.00017174 -0.0004667 ]]
Using sum gradient........

W1 was :
 [[-0.31047409  0.09665668]
 [-1.66174444 -0.49544015]]
Updated W1 is: 
 [[-0.30907696  0.10334858]
 [-1.6615727  -0.49497345]]
W2 was :
 [[0.25237383]
 [1.0249419 ]]
Updated W2 is: 
 [[0.25093692]
 [1.027807  ]]
The mean of the biases b gradient is:
 [ 7.24415011e-05 -5.63814739e-04]
The b biases before the update are:
 [[-2.48530365 -1.56815187]]
The new updated bs are:
 [[-2.48537609 -1.56758805]]
The bias c is: 
 [[ 0.12259902]
 [-0.13171547]
 [-0.12672584]
 [ 0.11810129]]
c bias before: [[-0.23391046]]
The mean c bias after update: [[-0.22947521]]
The output is: 
 [[0.49057057]
 [0.47140984]
 [0.49299717]
 [0.4737144 ]]
Total Loss: 0.5007621170058001
Average Loss: 0.12519052925145002

RUN:
  16
FeedForward


Z1 is:
 [[-2.48537609 -1.56758805]
 [-4.14694879 -2.0625615 ]
 [-2.79445305 -1.46423947]
 [-4.45602575 -1.95921292]]
H is:
 [[0.07688975 0.17256051]
 [0.01556644 0.11278925]
 [0.05762466 0.18781977]
 [0.0114752  0.12355225]]
The c is
 [[-0.22947521]]
Z2 is:
 [[-0.03282184]
 [-0.10964344]
 [-0.02197259]
 [-0.09960779]]
Y^ -  the output is:
 [[0.49179528]
 [0.47261657]
 [0.49450707]
 [0.47511862]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49179528]
 [0.47261657]
 [0.49450707]
 [0.47511862]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12291571]
 [-0.1314504 ]
 [-0.12635798]
 [ 0.11848552]]
W2.T is
 [[0.25093692 1.027807  ]]
 D_Error times W2.T
 [[ 0.03084409  0.12633363]
 [-0.03298576 -0.13510564]
 [-0.03170788 -0.12987162]
 [ 0.02973239  0.12178024]]
self.H_D_Error_W2 is
 [[ 0.00218924  0.01803834]
 [-0.00050548 -0.01351973]
 [-0.00172187 -0.01981107]
 [ 0.00033727  0.01318723]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00218924  0.01803834]
 [-0.00050548 -0.01351973]
 [-0.00172187 -0.01981107]
 [ 0.00033727  0.01318723]]
X_H_D_Error_W2 is
 [[-0.0013846  -0.00662384]
 [-0.00016821 -0.0003325 ]]
Using sum gradient........

W1 was :
 [[-0.30907696  0.10334858]
 [-1.6615727  -0.49497345]]
Updated W1 is: 
 [[-0.30769236  0.10997242]
 [-1.66140449 -0.49464095]]
W2 was :
 [[0.25093692]
 [1.027807  ]]
Updated W2 is: 
 [[0.24945387]
 [1.03051617]]
The mean of the biases b gradient is:
 [ 7.47920660e-05 -5.26307236e-04]
The b biases before the update are:
 [[-2.48537609 -1.56758805]]
The new updated bs are:
 [[-2.48545088 -1.56706174]]
The bias c is: 
 [[ 0.12291571]
 [-0.1314504 ]
 [-0.12635798]
 [ 0.11848552]]
c bias before: [[-0.22947521]]
The mean c bias after update: [[-0.22537343]]
The output is: 
 [[0.49179528]
 [0.47261657]
 [0.49450707]
 [0.47511862]]
Total Loss: 0.5006283399563055
Average Loss: 0.12515708498907638

RUN:
  17
FeedForward


Z1 is:
 [[-2.48545088 -1.56706174]
 [-4.14685537 -2.06170269]
 [-2.79314325 -1.45708932]
 [-4.45454774 -1.95173027]]
H is:
 [[0.07688444 0.17263567]
 [0.01556788 0.11287522]
 [0.05769583 0.18891291]
 [0.01149198 0.12436481]]
The c is
 [[-0.22537343]]
Z2 is:
 [[-0.02829046]
 [-0.10517022]
 [-0.01630317]
 [-0.09434676]]
Y^ -  the output is:
 [[0.49292786]
 [0.47373165]
 [0.4959243 ]
 [0.47643079]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49292786]
 [0.47373165]
 [0.4959243 ]
 [0.47643079]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12320731]
 [-0.13120395]
 [-0.12601055]
 [ 0.11884304]]
W2.T is
 [[0.24945387 1.03051617]]
 D_Error times W2.T
 [[ 0.03073454  0.12696712]
 [-0.03272933 -0.13520779]
 [-0.03143382 -0.12985591]
 [ 0.02964586  0.12246967]]
self.H_D_Error_W2 is
 [[ 0.00218133  0.01813504]
 [-0.00050159 -0.01353895]
 [-0.00170896 -0.01989715]
 [ 0.00033677  0.01333673]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00218133  0.01813504]
 [-0.00050159 -0.01353895]
 [-0.00170896 -0.01989715]
 [ 0.00033677  0.01333673]]
X_H_D_Error_W2 is
 [[-0.00137219 -0.00656042]
 [-0.00016482 -0.00020222]]
Using sum gradient........

W1 was :
 [[-0.30769236  0.10997242]
 [-1.66140449 -0.49464095]]
Updated W1 is: 
 [[-0.30632018  0.11653284]
 [-1.66123967 -0.49443873]]
W2 was :
 [[0.24945387]
 [1.03051617]]
Updated W2 is: 
 [[0.24792825]
 [1.03308099]]
The mean of the biases b gradient is:
 [ 7.68866550e-05 -4.91082304e-04]
The b biases before the update are:
 [[-2.48545088 -1.56706174]]
The new updated bs are:
 [[-2.48552777 -1.56657066]]
The bias c is: 
 [[ 0.12320731]
 [-0.13120395]
 [-0.12601055]
 [ 0.11884304]]
c bias before: [[-0.22537343]]
The mean c bias after update: [[-0.22158239]]
The output is: 
 [[0.49292786]
 [0.47373165]
 [0.4959243 ]
 [0.47643079]]
Total Loss: 0.5005074288746452
Average Loss: 0.1251268572186613

RUN:
  18
FeedForward


Z1 is:
 [[-2.48552777 -1.56657066]
 [-4.14676744 -2.06100939]
 [-2.79184794 -1.45003782]
 [-4.45308762 -1.94447654]]
H is:
 [[0.07687899 0.17270582]
 [0.01556922 0.11294466]
 [0.05776629 0.18999575]
 [0.01150857 0.12515689]]
The c is
 [[-0.22158239]]
Z2 is:
 [[-0.02410281]
 [-0.10104135]
 [-0.0109795 ]
 [-0.08943189]]
Y^ -  the output is:
 [[0.49397459]
 [0.47476113]
 [0.49725515]
 [0.47765692]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49397459]
 [0.47476113]
 [0.49725515]
 [0.47765692]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12347571]
 [-0.13097514]
 [-0.12568242]
 [ 0.11917578]]
W2.T is
 [[0.24792825 1.03308099]]
 D_Error times W2.T
 [[ 0.03061312  0.12756041]
 [-0.03247244 -0.13530793]
 [-0.03116022 -0.12984012]
 [ 0.02954704  0.12311823]]
self.H_D_Error_W2 is
 [[ 0.00217257  0.01822564]
 [-0.0004977  -0.01355625]
 [-0.00169603 -0.01998205]
 [ 0.00033613  0.01348054]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00217257  0.01822564]
 [-0.0004977  -0.01355625]
 [-0.00169603 -0.01998205]
 [ 0.00033613  0.01348054]]
X_H_D_Error_W2 is
 [[-1.35989971e-03 -6.50151262e-03]
 [-1.61568405e-04 -7.57131037e-05]]
Using sum gradient........

W1 was :
 [[-0.30632018  0.11653284]
 [-1.66123967 -0.49443873]]
Updated W1 is: 
 [[-0.30496028  0.12303436]
 [-1.66107811 -0.49436301]]
W2 was :
 [[0.24792825]
 [1.03308099]]
Updated W2 is: 
 [[0.24636341]
 [1.03551242]]
The mean of the biases b gradient is:
 [ 7.87428271e-05 -4.58030679e-04]
The b biases before the update are:
 [[-2.48552777 -1.56657066]]
The new updated bs are:
 [[-2.48560651 -1.56611263]]
The bias c is: 
 [[ 0.12347571]
 [-0.13097514]
 [-0.12568242]
 [ 0.11917578]]
c bias before: [[-0.22158239]]
The mean c bias after update: [[-0.21808087]]
The output is: 
 [[0.49397459]
 [0.47476113]
 [0.49725515]
 [0.47765692]]
Total Loss: 0.5003976379531488
Average Loss: 0.1250994094882872

RUN:
  19
FeedForward


Z1 is:
 [[-2.48560651 -1.56611263]
 [-4.14668462 -2.06047564]
 [-2.79056679 -1.44307827]
 [-4.45164489 -1.93744129]]
H is:
 [[0.0768734  0.17277127]
 [0.01557049 0.11299815]
 [0.05783606 0.19106911]
 [0.011525   0.12592923]]
The c is
 [[-0.21808087]]
Z2 is:
 [[-0.02023528]
 [-0.09723388]
 [-0.00597774]
 [-0.08484025]]
Y^ -  the output is:
 [[0.49494135]
 [0.47571066]
 [0.49850557]
 [0.47880265]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49494135]
 [0.47571066]
 [0.49850557]
 [0.47880265]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12372267]
 [-0.13076302]
 [-0.12537249]
 [ 0.11948552]]
W2.T is
 [[0.24636341 1.03551242]]
 D_Error times W2.T
 [[ 0.03048074  0.12811636]
 [-0.03221522 -0.13540673]
 [-0.03088719 -0.12982477]
 [ 0.02943686  0.12372874]]
self.H_D_Error_W2 is
 [[ 0.00216303  0.01831057]
 [-0.0004938  -0.01357176]
 [-0.00168308 -0.02006594]
 [ 0.00033535  0.01361895]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00216303  0.01831057]
 [-0.0004938  -0.01357176]
 [-0.00168308 -0.02006594]
 [ 0.00033535  0.01361895]]
X_H_D_Error_W2 is
 [[-1.34772588e-03 -6.44698417e-03]
 [-1.58446842e-04  4.71957798e-05]]
Using sum gradient........

W1 was :
 [[-0.30496028  0.12303436]
 [-1.66107811 -0.49436301]]
Updated W1 is: 
 [[-0.30361255  0.12948134]
 [-1.66091966 -0.49441021]]
W2 was :
 [[0.24636341]
 [1.03551242]]
Updated W2 is: 
 [[0.24476245]
 [1.03782076]]
The mean of the biases b gradient is:
 [ 8.03772393e-05 -4.27044227e-04]
The b biases before the update are:
 [[-2.48560651 -1.56611263]]
The new updated bs are:
 [[-2.48568689 -1.56568559]]
The bias c is: 
 [[ 0.12372267]
 [-0.13076302]
 [-0.12537249]
 [ 0.11948552]]
c bias before: [[-0.21808087]]
The mean c bias after update: [[-0.21484904]]
The output is: 
 [[0.49494135]
 [0.47571066]
 [0.49850557]
 [0.47880265]]
Total Loss: 0.5002974469121719
Average Loss: 0.12507436172804298

RUN:
  20
FeedForward


Z1 is:
 [[-2.48568689 -1.56568559]
 [-4.14660655 -2.06009579]
 [-2.78929944 -1.43620425]
 [-4.4502191  -1.93061445]]
H is:
 [[0.07686769 0.17283232]
 [0.01557169 0.11303623]
 [0.05790516 0.19213383]
 [0.01154125 0.12668259]]
The c is
 [[-0.21484904]]
Z2 is:
 [[-0.01666575]
 [-0.09372634]
 [-0.00127555]
 [-0.08055036]]
Y^ -  the output is:
 [[0.49583366]
 [0.47658555]
 [0.49968111]
 [0.47987329]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49583366]
 [0.47658555]
 [0.49968111]
 [0.47987329]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12394981]
 [-0.13056666]
 [-0.12507967]
 [ 0.11977393]]
W2.T is
 [[0.24476245 1.03782076]]
 D_Error times W2.T
 [[ 0.03033826  0.12863768]
 [-0.03195782 -0.13550479]
 [-0.03061481 -0.12981028]
 [ 0.02931616  0.12430388]]
self.H_D_Error_W2 is
 [[ 0.00215277  0.01839021]
 [-0.00048989 -0.01358558]
 [-0.0016701  -0.02014895]
 [ 0.00033444  0.01375225]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00215277  0.01839021]
 [-0.00048989 -0.01358558]
 [-0.0016701  -0.02014895]
 [ 0.00033444  0.01375225]]
X_H_D_Error_W2 is
 [[-0.00133566 -0.0063967 ]
 [-0.00015545  0.00016667]]
Using sum gradient........

W1 was :
 [[-0.30361255  0.12948134]
 [-1.66091966 -0.49441021]]
Updated W1 is: 
 [[-0.30227689  0.13587804]
 [-1.66076421 -0.49457688]]
W2 was :
 [[0.24476245]
 [1.03782076]]
Updated W2 is: 
 [[0.24312828]
 [1.04001576]]
The mean of the biases b gradient is:
 [ 8.18056499e-05 -3.98016659e-04]
The b biases before the update are:
 [[-2.48568689 -1.56568559]]
The new updated bs are:
 [[-2.48576869 -1.56528757]]
The bias c is: 
 [[ 0.12394981]
 [-0.13056666]
 [-0.12507967]
 [ 0.11977393]]
c bias before: [[-0.21484904]]
The mean c bias after update: [[-0.2118684]]
The output is: 
 [[0.49583366]
 [0.47658555]
 [0.49968111]
 [0.47987329]]
Total Loss: 0.5002055324117833
Average Loss: 0.12505138310294583

RUN:
  21
FeedForward


Z1 is:
 [[-2.48576869 -1.56528757]
 [-4.1465329  -2.05986445]
 [-2.78804558 -1.42940953]
 [-4.44880979 -1.92398641]]
H is:
 [[0.07686189 0.17288922]
 [0.01557282 0.11305942]
 [0.0579736  0.1931907 ]
 [0.01155734 0.12741769]]
The c is
 [[-0.2118684]]
Z2 is:
 [[-0.01337358]
 [-0.09049862]
 [ 0.003148  ]
 [-0.07654207]]
Y^ -  the output is:
 [[0.49665665]
 [0.47739077]
 [0.500787  ]
 [0.48087382]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49665665]
 [0.47739077]
 [0.500787  ]
 [0.48087382]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12415861]
 [-0.13038516]
 [-0.12480294]
 [ 0.12004255]]
W2.T is
 [[0.24312828 1.04001576]]
 D_Error times W2.T
 [[ 0.03018647  0.12912691]
 [-0.03170032 -0.13560262]
 [-0.03034312 -0.12979703]
 [ 0.02918574  0.12484614]]
self.H_D_Error_W2 is
 [[ 0.00214185  0.01846496]
 [-0.00048598 -0.01359782]
 [-0.00165712 -0.02023121]
 [ 0.00033341  0.0138807 ]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00214185  0.01846496]
 [-0.00048598 -0.01359782]
 [-0.00165712 -0.02023121]
 [ 0.00033341  0.0138807 ]]
X_H_D_Error_W2 is
 [[-0.00132371 -0.00635051]
 [-0.00015256  0.00028287]]
Using sum gradient........

W1 was :
 [[-0.30227689  0.13587804]
 [-1.66076421 -0.49457688]]
Updated W1 is: 
 [[-0.30095318  0.14222855]
 [-1.66061165 -0.49485975]]
W2 was :
 [[0.24312828]
 [1.04001576]]
Updated W2 is: 
 [[0.24146358]
 [1.04210657]]
The mean of the biases b gradient is:
 [ 8.30429314e-05 -3.70844092e-04]
The b biases before the update are:
 [[-2.48576869 -1.56528757]]
The new updated bs are:
 [[-2.48585174 -1.56491673]]
The bias c is: 
 [[ 0.12415861]
 [-0.13038516]
 [-0.12480294]
 [ 0.12004255]]
c bias before: [[-0.2118684]]
The mean c bias after update: [[-0.20912166]]
The output is: 
 [[0.49665665]
 [0.47739077]
 [0.500787  ]
 [0.48087382]]
Total Loss: 0.5001207428961465
Average Loss: 0.12503018572403662

RUN:
  22
FeedForward


Z1 is:
 [[-2.48585174 -1.56491673]
 [-4.14646338 -2.05977648]
 [-2.78680491 -1.42268817]
 [-4.44741656 -1.91754792]]
H is:
 [[0.076856   0.17294226]
 [0.01557388 0.11306824]
 [0.05804139 0.19424051]
 [0.01157327 0.12813525]]
The c is
 [[-0.20912166]]
Z2 is:
 [[-0.01033947]
 [-0.08753197]
 [ 0.00731253]
 [-0.07279655]]
Y^ -  the output is:
 [[0.49741516]
 [0.47813097]
 [0.50182813]
 [0.4818089 ]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49741516]
 [0.47813097]
 [0.50182813]
 [0.4818089 ]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12435047]
 [-0.13021767]
 [-0.1245413 ]
 [ 0.12029279]]
W2.T is
 [[0.24146358 1.04210657]]
 D_Error times W2.T
 [[ 0.03002611  0.12958644]
 [-0.03144283 -0.13570069]
 [-0.03007219 -0.12978531]
 [ 0.02904633  0.1253579 ]]
self.H_D_Error_W2 is
 [[ 0.00213033  0.01853517]
 [-0.00048206 -0.01360858]
 [-0.00164412 -0.02031285]
 [ 0.00033227  0.01400456]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00213033  0.01853517]
 [-0.00048206 -0.01360858]
 [-0.00164412 -0.02031285]
 [ 0.00033227  0.01400456]]
X_H_D_Error_W2 is
 [[-0.00131185 -0.00630829]
 [-0.00014979  0.00039598]]
Using sum gradient........

W1 was :
 [[-0.30095318  0.14222855]
 [-1.66061165 -0.49485975]]
Updated W1 is: 
 [[-0.29964133  0.14853684]
 [-1.66046186 -0.49525573]]
W2 was :
 [[0.24146358]
 [1.04210657]]
Updated W2 is: 
 [[0.23977087]
 [1.04410182]]
The mean of the biases b gradient is:
 [ 8.41030893e-05 -3.45425502e-04]
The b biases before the update are:
 [[-2.48585174 -1.56491673]]
The new updated bs are:
 [[-2.48593584 -1.5645713 ]]
The bias c is: 
 [[ 0.12435047]
 [-0.13021767]
 [-0.1245413 ]
 [ 0.12029279]]
c bias before: [[-0.20912166]]
The mean c bias after update: [[-0.20659273]]
The output is: 
 [[0.49741516]
 [0.47813097]
 [0.50182813]
 [0.4818089 ]]
Total Loss: 0.500042076499783
Average Loss: 0.12501051912494576

RUN:
  23
FeedForward


Z1 is:
 [[-2.48593584 -1.5645713 ]
 [-4.1463977  -2.05982703]
 [-2.78557716 -1.41603446]
 [-4.44603902 -1.91129019]]
H is:
 [[0.07685003 0.17299167]
 [0.01557489 0.11306317]
 [0.05810855 0.19528401]
 [0.01158904 0.12883598]]
The c is
 [[-0.20659273]]
Z2 is:
 [[-0.00754541]
 [-0.08480886]
 [ 0.0112364 ]
 [-0.06929614]]
Y^ -  the output is:
 [[0.49811366]
 [0.47881048]
 [0.50280907]
 [0.48268289]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49811366]
 [0.47881048]
 [0.50280907]
 [0.48268289]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12452664]
 [-0.13006337]
 [-0.12429381]
 [ 0.12052598]]
W2.T is
 [[0.23977087 1.04410182]]
 D_Error times W2.T
 [[ 0.02985786  0.13001849]
 [-0.03118541 -0.1357994 ]
 [-0.02980203 -0.12977539]
 [ 0.02889862  0.12584139]]
self.H_D_Error_W2 is
 [[ 0.00211824  0.01860117]
 [-0.00047814 -0.01361795]
 [-0.00163112 -0.02039396]
 [ 0.00033103  0.01412409]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00211824  0.01860117]
 [-0.00047814 -0.01361795]
 [-0.00163112 -0.02039396]
 [ 0.00033103  0.01412409]]
X_H_D_Error_W2 is
 [[-0.0013001  -0.00626987]
 [-0.00014712  0.00050614]]
Using sum gradient........

W1 was :
 [[-0.29964133  0.14853684]
 [-1.66046186 -0.49525573]]
Updated W1 is: 
 [[-0.29834123  0.15480671]
 [-1.66031474 -0.49576187]]
W2 was :
 [[0.23977087]
 [1.04410182]]
Updated W2 is: 
 [[0.23805247]
 [1.04600964]]
The mean of the biases b gradient is:
 [ 8.49992883e-05 -3.21663061e-04]
The b biases before the update are:
 [[-2.48593584 -1.5645713 ]]
The new updated bs are:
 [[-2.48602084 -1.56424964]]
The bias c is: 
 [[ 0.12452664]
 [-0.13006337]
 [-0.12429381]
 [ 0.12052598]]
c bias before: [[-0.20659273]]
The mean c bias after update: [[-0.20426659]]
The output is: 
 [[0.49811366]
 [0.47881048]
 [0.50280907]
 [0.48268289]]
Total Loss: 0.49996866167506504
Average Loss: 0.12499216541876626

RUN:
  24
FeedForward


Z1 is:
 [[-2.48602084 -1.56424964]
 [-4.14633558 -2.06001151]
 [-2.78436207 -1.40944293]
 [-4.4446768  -1.9052048 ]]
H is:
 [[0.076844   0.1730377 ]
 [0.01557584 0.11304468]
 [0.05817509 0.19632194]
 [0.01160465 0.12952053]]
The c is
 [[-0.20426659]]
Z2 is:
 [[-0.00497459]
 [-0.0823129 ]
 [ 0.01493677]
 [-0.06602435]]
Y^ -  the output is:
 [[0.49875636]
 [0.47943339]
 [0.50373412]
 [0.48349991]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49875636]
 [0.47943339]
 [0.50373412]
 [0.48349991]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12468832]
 [-0.12992146]
 [-0.12405955]
 [ 0.12074334]]
W2.T is
 [[0.23805247 1.04600964]]
 D_Error times W2.T
 [[ 0.02968236  0.13042518]
 [-0.03092812 -0.1358991 ]
 [-0.02953268 -0.12976748]
 [ 0.02874325  0.1262987 ]]
self.H_D_Error_W2 is
 [[ 0.00210564  0.01866328]
 [-0.00047423 -0.013626  ]
 [-0.00161812 -0.02047467]
 [ 0.00032968  0.01423954]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00210564  0.01866328]
 [-0.00047423 -0.013626  ]
 [-0.00161812 -0.02047467]
 [ 0.00032968  0.01423954]]
X_H_D_Error_W2 is
 [[-0.00128843 -0.00623512]
 [-0.00014454  0.00061354]]
Using sum gradient........

W1 was :
 [[-0.29834123  0.15480671]
 [-1.66031474 -0.49576187]]
Updated W1 is: 
 [[-0.29705279  0.16104184]
 [-1.66017019 -0.49637541]]
W2 was :
 [[0.23805247]
 [1.04600964]]
Updated W2 is: 
 [[0.23631055]
 [1.04783765]]
The mean of the biases b gradient is:
 [ 8.57438804e-05 -2.99462386e-04]
The b biases before the update are:
 [[-2.48602084 -1.56424964]]
The new updated bs are:
 [[-2.48610658 -1.56395018]]
The bias c is: 
 [[ 0.12468832]
 [-0.12992146]
 [-0.12405955]
 [ 0.12074334]]
c bias before: [[-0.20426659]]
The mean c bias after update: [[-0.20212925]]
The output is: 
 [[0.49875636]
 [0.47943339]
 [0.50373412]
 [0.48349991]]
Total Loss: 0.499899740230613
Average Loss: 0.12497493505765325

RUN:
  25
FeedForward


Z1 is:
 [[-2.48610658 -1.56395018]
 [-4.14627678 -2.06032559]
 [-2.78315938 -1.40290834]
 [-4.44332957 -1.89928375]]
H is:
 [[0.07683792 0.17308055]
 [0.01557675 0.11301319]
 [0.05824102 0.19735501]
 [0.01162011 0.13018956]]
The c is
 [[-0.20212925]]
Z2 is:
 [[-0.00261132]
 [-0.08002883]
 [ 0.01842973]
 [-0.06296577]]
Y^ -  the output is:
 [[0.49934717]
 [0.48000346]
 [0.5046073 ]
 [0.48426376]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49934717]
 [0.48000346]
 [0.5046073 ]
 [0.48426376]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12483658]
 [-0.12979121]
 [-0.12383766]
 [ 0.12094602]]
W2.T is
 [[0.23631055 1.04783765]]
 D_Error times W2.T
 [[ 0.0295002   0.13080847]
 [-0.03067103 -0.13600011]
 [-0.02926414 -0.12976176]
 [ 0.02858082  0.1267318 ]]
self.H_D_Error_W2 is
 [[ 0.00209256  0.01872179]
 [-0.00047031 -0.01363282]
 [-0.00160511 -0.02055504]
 [ 0.00032825  0.01435114]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00209256  0.01872179]
 [-0.00047031 -0.01363282]
 [-0.00160511 -0.02055504]
 [ 0.00032825  0.01435114]]
X_H_D_Error_W2 is
 [[-0.00127686 -0.0062039 ]
 [-0.00014206  0.00071832]]
Using sum gradient........

W1 was :
 [[-0.29705279  0.16104184]
 [-1.66017019 -0.49637541]]
Updated W1 is: 
 [[-0.29577594  0.16724574]
 [-1.66002813 -0.49709373]]
W2 was :
 [[0.23631055]
 [1.04783765]]
Updated W2 is: 
 [[0.23454711]
 [1.04959306]]
The mean of the biases b gradient is:
 [ 8.63484378e-05 -2.78732719e-04]
The b biases before the update are:
 [[-2.48610658 -1.56395018]]
The new updated bs are:
 [[-2.48619293 -1.56367144]]
The bias c is: 
 [[ 0.12483658]
 [-0.12979121]
 [-0.12383766]
 [ 0.12094602]]
c bias before: [[-0.20212925]]
The mean c bias after update: [[-0.20016768]]
The output is: 
 [[0.49934717]
 [0.48000346]
 [0.5046073 ]
 [0.48426376]]
Total Loss: 0.4998346524998631
Average Loss: 0.12495866312496577

RUN:
  26
FeedForward


Z1 is:
 [[-2.48619293 -1.56367144]
 [-4.14622106 -2.06076518]
 [-2.78196887 -1.3964257 ]
 [-4.441997   -1.89351944]]
H is:
 [[0.07683179 0.17312045]
 [0.0155776  0.11296913]
 [0.05830636 0.19838391]
 [0.01163543 0.13084371]]
The c is
 [[-0.20016768]]
Z2 is:
 [[-0.00044099]
 [-0.07794239]
 [ 0.02173028]
 [-0.06010598]]
Y^ -  the output is:
 [[0.49988975]
 [0.48052426]
 [0.50543236]
 [0.48497803]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49988975]
 [0.48052426]
 [0.50543236]
 [0.48497803]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12497243]
 [-0.1296719 ]
 [-0.12362732]
 [ 0.12113507]]
W2.T is
 [[0.23454711 1.04959306]]
 D_Error times W2.T
 [[ 0.02931192  0.1311702 ]
 [-0.03041417 -0.13610272]
 [-0.02899643 -0.12975837]
 [ 0.02841188  0.12714253]]
self.H_D_Error_W2 is
 [[ 0.00207906  0.01877698]
 [-0.0004664  -0.01363846]
 [-0.0015921  -0.02063518]
 [ 0.00032674  0.01445911]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00207906  0.01877698]
 [-0.0004664  -0.01363846]
 [-0.0015921  -0.02063518]
 [ 0.00032674  0.01445911]]
X_H_D_Error_W2 is
 [[-0.00126536 -0.00617607]
 [-0.00013966  0.00082065]]
Using sum gradient........

W1 was :
 [[-0.29577594  0.16724574]
 [-1.66002813 -0.49709373]]
Updated W1 is: 
 [[-0.29451058  0.17342181]
 [-1.65988847 -0.49791438]]
W2 was :
 [[0.23454711]
 [1.04959306]]
Updated W2 is: 
 [[0.23276403]
 [1.05128261]]
The mean of the biases b gradient is:
 [ 8.68237861e-05 -2.59387038e-04]
The b biases before the update are:
 [[-2.48619293 -1.56367144]]
The new updated bs are:
 [[-2.48627975 -1.56341206]]
The bias c is: 
 [[ 0.12497243]
 [-0.1296719 ]
 [-0.12362732]
 [ 0.12113507]]
c bias before: [[-0.20016768]]
The mean c bias after update: [[-0.19836976]]
The output is: 
 [[0.49988975]
 [0.48052426]
 [0.50543236]
 [0.48497803]]
Total Loss: 0.499772824387298
Average Loss: 0.1249432060968245

RUN:
  27
FeedForward


Z1 is:
 [[-2.48627975 -1.56341206]
 [-4.14616823 -2.06132644]
 [-2.78079033 -1.38999025]
 [-4.4406788  -1.88790463]]
H is:
 [[0.07682563 0.17315758]
 [0.01557841 0.1129129 ]
 [0.0583711  0.19940931]
 [0.0116506  0.13148357]]
The c is
 [[-0.19836976]]
Z2 is:
 [[ 0.00155005]
 [-0.07604029]
 [ 0.02485248]
 [-0.05743153]]
Y^ -  the output is:
 [[0.50038751]
 [0.48099908]
 [0.5062128 ]
 [0.48564606]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50038751]
 [0.48099908]
 [0.5062128 ]
 [0.48564606]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.1250968 ]
 [-0.12956285]
 [-0.12342774]
 [ 0.12131146]]
W2.T is
 [[0.23276403 1.05128261]]
 D_Error times W2.T
 [[ 0.02911804  0.13151209]
 [-0.03015757 -0.13620717]
 [-0.02872954 -0.12975744]
 [ 0.02823694  0.12753262]]
self.H_D_Error_W2 is
 [[ 0.00206515  0.01882912]
 [-0.00046249 -0.013643  ]
 [-0.00157909 -0.02071516]
 [ 0.00032514  0.01456367]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00206515  0.01882912]
 [-0.00046249 -0.013643  ]
 [-0.00157909 -0.02071516]
 [ 0.00032514  0.01456367]]
X_H_D_Error_W2 is
 [[-0.00125394 -0.00615149]
 [-0.00013734  0.00092067]]
Using sum gradient........

W1 was :
 [[-0.29451058  0.17342181]
 [-1.65988847 -0.49791438]]
Updated W1 is: 
 [[-0.29325663  0.1795733 ]
 [-1.65975113 -0.49883506]]
W2 was :
 [[0.23276403]
 [1.05128261]]
Updated W2 is: 
 [[0.23096304]
 [1.05291264]]
The mean of the biases b gradient is:
 [ 8.71800396e-05 -2.41342115e-04]
The b biases before the update are:
 [[-2.48627975 -1.56341206]]
The new updated bs are:
 [[-2.48636693 -1.56317071]]
The bias c is: 
 [[ 0.1250968 ]
 [-0.12956285]
 [-0.12342774]
 [ 0.12131146]]
c bias before: [[-0.19836976]]
The mean c bias after update: [[-0.19672417]]
The output is: 
 [[0.50038751]
 [0.48099908]
 [0.5062128 ]
 [0.48564606]]
Total Loss: 0.49971375606632906
Average Loss: 0.12492843901658227

RUN:
  28
FeedForward


Z1 is:
 [[-2.48636693 -1.56317071]
 [-4.14611806 -2.06200577]
 [-2.77962357 -1.38359742]
 [-4.4393747  -1.88243247]]
H is:
 [[0.07681945 0.17319214]
 [0.01557918 0.11284487]
 [0.05843526 0.20043186]
 [0.01166562 0.13210972]]
The c is
 [[-0.19672417]]
Z2 is:
 [[ 0.00337448]
 [-0.07431016]
 [ 0.02780945]
 [-0.05492985]]
Y^ -  the output is:
 [[0.50084362]
 [0.481431  ]
 [0.50695192]
 [0.48627099]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50084362]
 [0.481431  ]
 [0.50695192]
 [0.48627099]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12521055]
 [-0.12946344]
 [-0.12323819]
 [ 0.12147609]]
W2.T is
 [[0.23096304 1.05291264]]
 D_Error times W2.T
 [[ 0.02891901  0.13183577]
 [-0.02990127 -0.1363137 ]
 [-0.02846347 -0.12975905]
 [ 0.02805649  0.12790371]]
self.H_D_Error_W2 is
 [[ 0.00205088  0.01887844]
 [-0.00045858 -0.01364649]
 [-0.00156608 -0.02079505]
 [ 0.00032348  0.01466502]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00205088  0.01887844]
 [-0.00045858 -0.01364649]
 [-0.00156608 -0.02079505]
 [ 0.00032348  0.01466502]]
X_H_D_Error_W2 is
 [[-0.0012426  -0.00613002]
 [-0.0001351   0.00101854]]
Using sum gradient........

W1 was :
 [[-0.29325663  0.1795733 ]
 [-1.65975113 -0.49883506]]
Updated W1 is: 
 [[-0.29201404  0.18570332]
 [-1.65961603 -0.49985359]]
W2 was :
 [[0.23096304]
 [1.05291264]]
Updated W2 is: 
 [[0.22914573]
 [1.05448913]]
The mean of the biases b gradient is:
 [ 8.74266369e-05 -2.24518532e-04]
The b biases before the update are:
 [[-2.48636693 -1.56317071]]
The new updated bs are:
 [[-2.48645436 -1.5629462 ]]
The bias c is: 
 [[ 0.12521055]
 [-0.12946344]
 [-0.12323819]
 [ 0.12147609]]
c bias before: [[-0.19672417]]
The mean c bias after update: [[-0.19522042]]
The output is: 
 [[0.50084362]
 [0.481431  ]
 [0.50695192]
 [0.48627099]]
Total Loss: 0.49965701212736113
Average Loss: 0.12491425303184028

RUN:
  29
FeedForward


Z1 is:
 [[-2.48645436 -1.5629462 ]
 [-4.14607039 -2.06279979]
 [-2.7784684  -1.37724287]
 [-4.43808442 -1.87709647]]
H is:
 [[0.07681325 0.17322429]
 [0.01557991 0.11276541]
 [0.05849885 0.20145217]
 [0.01168051 0.13272274]]
The c is
 [[-0.19522042]]
Z2 is:
 [[ 0.00504414]
 [-0.07274046]
 [ 0.03061346]
 [-0.0525892 ]]
Y^ -  the output is:
 [[0.50126103]
 [0.4818229 ]
 [0.50765277]
 [0.48685573]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50126103]
 [0.4818229 ]
 [0.50765277]
 [0.48685573]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12531446]
 [-0.12937307]
 [-0.12305797]
 [ 0.12162982]]
W2.T is
 [[0.22914573 1.05448913]]
 D_Error times W2.T
 [[ 0.02871527  0.13214274]
 [-0.02964529 -0.13642249]
 [-0.02819821 -0.1297633 ]
 [ 0.02787095  0.12825732]]
self.H_D_Error_W2 is
 [[ 0.00203629  0.01892517]
 [-0.00045468 -0.01364898]
 [-0.00155307 -0.02087492]
 [ 0.00032174  0.01476337]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00203629  0.01892517]
 [-0.00045468 -0.01364898]
 [-0.00155307 -0.02087492]
 [ 0.00032174  0.01476337]]
X_H_D_Error_W2 is
 [[-0.00123132 -0.00611155]
 [-0.00013293  0.00111438]]
Using sum gradient........

W1 was :
 [[-0.29201404  0.18570332]
 [-1.65961603 -0.49985359]]
Updated W1 is: 
 [[-0.29078271  0.19181487]
 [-1.6594831  -0.50096797]]
W2 was :
 [[0.22914573]
 [1.05448913]]
Updated W2 is: 
 [[0.22731359]
 [1.05601769]]
The mean of the biases b gradient is:
 [ 8.75723763e-05 -2.08840661e-04]
The b biases before the update are:
 [[-2.48645436 -1.5629462 ]]
The new updated bs are:
 [[-2.48654193 -1.56273735]]
The bias c is: 
 [[ 0.12531446]
 [-0.12937307]
 [-0.12305797]
 [ 0.12162982]]
c bias before: [[-0.19522042]]
The mean c bias after update: [[-0.19384873]]
The output is: 
 [[0.50126103]
 [0.4818229 ]
 [0.50765277]
 [0.48685573]]
Total Loss: 0.4996022129970854
Average Loss: 0.12490055324927135

RUN:
  30
FeedForward


Z1 is:
 [[-2.48654193 -1.56273735]
 [-4.14602503 -2.06370533]
 [-2.77732465 -1.37092249]
 [-4.43680774 -1.87189046]]
H is:
 [[0.07680704 0.17325421]
 [0.01558061 0.11267484]
 [0.05856188 0.20247085]
 [0.01169526 0.13332313]]
The c is
 [[-0.19384873]]
Z2 is:
 [[ 0.00657006]
 [-0.07132042]
 [ 0.03327597]
 [-0.05039866]]
Y^ -  the output is:
 [[0.50164251]
 [0.48217745]
 [0.50831823]
 [0.487403  ]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50164251]
 [0.48217745]
 [0.50831823]
 [0.487403  ]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12540927]
 [-0.12929116]
 [-0.12288642]
 [ 0.12177341]]
W2.T is
 [[0.22731359 1.05601769]]
 D_Error times W2.T
 [[ 0.02850723  0.13243441]
 [-0.02938964 -0.13653375]
 [-0.02793375 -0.12977024]
 [ 0.02768075  0.12859487]]
self.H_D_Error_W2 is
 [[ 0.00202138  0.01896953]
 [-0.00045077 -0.01365054]
 [-0.00154005 -0.02095483]
 [ 0.00031995  0.01485889]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00202138  0.01896953]
 [-0.00045077 -0.01365054]
 [-0.00154005 -0.02095483]
 [ 0.00031995  0.01485889]]
X_H_D_Error_W2 is
 [[-0.00122011 -0.00609594]
 [-0.00013083  0.00120835]]
Using sum gradient........

W1 was :
 [[-0.29078271  0.19181487]
 [-1.6594831  -0.50096797]]
Updated W1 is: 
 [[-0.28956261  0.19791081]
 [-1.65935227 -0.50217633]]
W2 was :
 [[0.22731359]
 [1.05601769]]
Updated W2 is: 
 [[0.225468  ]
 [1.05750357]]
The mean of the biases b gradient is:
 [ 8.76254505e-05 -1.94236613e-04]
The b biases before the update are:
 [[-2.48654193 -1.56273735]]
The new updated bs are:
 [[-2.48662956 -1.56254312]]
The bias c is: 
 [[ 0.12540927]
 [-0.12929116]
 [-0.12288642]
 [ 0.12177341]]
c bias before: [[-0.19384873]]
The mean c bias after update: [[-0.19260001]]
The output is: 
 [[0.50164251]
 [0.48217745]
 [0.50831823]
 [0.487403  ]]
Total Loss: 0.4995490274705271
Average Loss: 0.12488725686763177

RUN:
  31
FeedForward


Z1 is:
 [[-2.48662956 -1.56254312]
 [-4.14598183 -2.06471944]
 [-2.77619217 -1.36463231]
 [-4.43554444 -1.86680863]]
H is:
 [[0.07680083 0.17328203]
 [0.01558127 0.11257349]
 [0.05862435 0.20348846]
 [0.01170987 0.13391142]]
The c is
 [[-0.19260001]]
Z2 is:
 [[ 0.00796248]
 [-0.07004006]
 [ 0.03580768]
 [-0.048348  ]]
Y^ -  the output is:
 [[0.50199061]
 [0.48249714]
 [0.50895096]
 [0.48791535]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50199061]
 [0.48249714]
 [0.50895096]
 [0.48791535]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12549566]
 [-0.12921718]
 [-0.12272292]
 [ 0.12190758]]
W2.T is
 [[0.225468   1.05750357]]
 D_Error times W2.T
 [[ 0.02829526  0.13271211]
 [-0.02913434 -0.13664763]
 [-0.02767009 -0.12977992]
 [ 0.02748626  0.1289177 ]]
self.H_D_Error_W2 is
 [[ 0.0020062   0.01901172]
 [-0.00044688 -0.01365119]
 [-0.00152704 -0.02103485]
 [ 0.00031809  0.01495177]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.0020062   0.01901172]
 [-0.00044688 -0.01365119]
 [-0.00152704 -0.02103485]
 [ 0.00031809  0.01495177]]
X_H_D_Error_W2 is
 [[-0.00120895 -0.00608308]
 [-0.00012879  0.00130057]]
Using sum gradient........

W1 was :
 [[-0.28956261  0.19791081]
 [-1.65935227 -0.50217633]]
Updated W1 is: 
 [[-0.28835365  0.20399389]
 [-1.65922348 -0.5034769 ]]
W2 was :
 [[0.225468  ]
 [1.05750357]]
Updated W2 is: 
 [[0.22361022]
 [1.05895173]]
The mean of the biases b gradient is:
 [ 8.75934818e-05 -1.80638167e-04]
The b biases before the update are:
 [[-2.48662956 -1.56254312]]
The new updated bs are:
 [[-2.48671715 -1.56236248]]
The bias c is: 
 [[ 0.12549566]
 [-0.12921718]
 [-0.12272292]
 [ 0.12190758]]
c bias before: [[-0.19260001]]
The mean c bias after update: [[-0.1914658]]
The output is: 
 [[0.50199061]
 [0.48249714]
 [0.50895096]
 [0.48791535]]
Total Loss: 0.4994971662158775
Average Loss: 0.12487429155396937

RUN:
  32
FeedForward


Z1 is:
 [[-2.48671715 -1.56236248]
 [-4.14594064 -2.06583938]
 [-2.77507081 -1.35836859]
 [-4.43429429 -1.86184549]]
H is:
 [[0.07679462 0.17330791]
 [0.0155819  0.11246166]
 [0.05868626 0.20450558]
 [0.01172434 0.13448809]]
The c is
 [[-0.1914658]]
Z2 is:
 [[ 0.00923097]
 [-0.06889006]
 [ 0.03821859]
 [-0.04642772]]
Y^ -  the output is:
 [[0.50230773]
 [0.48278429]
 [0.50955348]
 [0.48839515]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50230773]
 [0.48278429]
 [0.50955348]
 [0.48839515]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12557426]
 [-0.12915063]
 [-0.12256687]
 [ 0.12203302]]
W2.T is
 [[0.22361022 1.05895173]]
 D_Error times W2.T
 [[ 0.02807969  0.13297708]
 [-0.0288794  -0.13676429]
 [-0.0274072  -0.1297924 ]
 [ 0.02728783  0.12922707]]
self.H_D_Error_W2 is
 [[ 0.00199077  0.01905193]
 [-0.00044298 -0.013651  ]
 [-0.00151403 -0.02111502]
 [ 0.00031618  0.01504217]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00199077  0.01905193]
 [-0.00044298 -0.013651  ]
 [-0.00151403 -0.02111502]
 [ 0.00031618  0.01504217]]
X_H_D_Error_W2 is
 [[-0.00119785 -0.00607286]
 [-0.0001268   0.00139117]]
Using sum gradient........

W1 was :
 [[-0.28835365  0.20399389]
 [-1.65922348 -0.5034769 ]]
Updated W1 is: 
 [[-0.2871558   0.21006675]
 [-1.65909668 -0.50486807]]
W2 was :
 [[0.22361022]
 [1.05895173]]
Updated W2 is: 
 [[0.22174144]
 [1.06036684]]
The mean of the biases b gradient is:
 [ 8.74835546e-05 -1.67980671e-04]
The b biases before the update are:
 [[-2.48671715 -1.56236248]]
The new updated bs are:
 [[-2.48680464 -1.5621945 ]]
The bias c is: 
 [[ 0.12557426]
 [-0.12915063]
 [-0.12256687]
 [ 0.12203302]]
c bias before: [[-0.1914658]]
The mean c bias after update: [[-0.19043824]]
The output is: 
 [[0.50230773]
 [0.48278429]
 [0.50955348]
 [0.48839515]]
Total Loss: 0.4994463761287683
Average Loss: 0.12486159403219207

RUN:
  33
FeedForward


Z1 is:
 [[-2.48680464 -1.5621945 ]
 [-4.14590132 -2.06706257]
 [-2.77396044 -1.35212775]
 [-4.43305712 -1.85699582]]
H is:
 [[0.07678842 0.17333198]
 [0.0155825  0.11233962]
 [0.05874763 0.20552273]
 [0.01173869 0.1350536 ]]
The c is
 [[-0.19043824]]
Z2 is:
 [[ 0.01038441]
 [-0.06786174]
 [ 0.04051803]
 [-0.04462893]]
Y^ -  the output is:
 [[0.50259608]
 [0.48304107]
 [0.51012812]
 [0.48884462]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50259608]
 [0.48304107]
 [0.51012812]
 [0.48884462]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12564563]
 [-0.12909105]
 [-0.12241772]
 [ 0.12215032]]
W2.T is
 [[0.22174144 1.06036684]]
 D_Error times W2.T
 [[ 0.02786084  0.13323046]
 [-0.02862484 -0.13688387]
 [-0.02714508 -0.12980769]
 [ 0.02708579  0.12952415]]
self.H_D_Error_W2 is
 [[ 0.00197511  0.01909033]
 [-0.0004391  -0.01364998]
 [-0.00150102 -0.02119541]
 [ 0.00031422  0.01513025]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00197511  0.01909033]
 [-0.0004391  -0.01364998]
 [-0.00150102 -0.02119541]
 [ 0.00031422  0.01513025]]
X_H_D_Error_W2 is
 [[-0.0011868  -0.00606516]
 [-0.00012488  0.00148027]]
Using sum gradient........

W1 was :
 [[-0.2871558   0.21006675]
 [-1.65909668 -0.50486807]]
Updated W1 is: 
 [[-0.285969    0.2161319 ]
 [-1.6589718  -0.50634834]]
W2 was :
 [[0.22174144]
 [1.06036684]]
Updated W2 is: 
 [[0.21986274]
 [1.06175325]]
The mean of the biases b gradient is:
 [ 8.73022487e-05 -1.56202943e-04]
The b biases before the update are:
 [[-2.48680464 -1.5621945 ]]
The new updated bs are:
 [[-2.48689194 -1.5620383 ]]
The bias c is: 
 [[ 0.12564563]
 [-0.12909105]
 [-0.12241772]
 [ 0.12215032]]
c bias before: [[-0.19043824]]
The mean c bias after update: [[-0.18951004]]
The output is: 
 [[0.50259608]
 [0.48304107]
 [0.51012812]
 [0.48884462]]
Total Loss: 0.49939643542752143
Average Loss: 0.12484910885688036

RUN:
  34
FeedForward


Z1 is:
 [[-2.48689194 -1.5620383 ]
 [-4.14586374 -2.06838663]
 [-2.77286094 -1.34590639]
 [-4.43183274 -1.85225473]]
H is:
 [[0.07678223 0.17335436]
 [0.01558308 0.11220766]
 [0.05880846 0.20654043]
 [0.0117529  0.13560838]]
The c is
 [[-0.18951004]]
Z2 is:
 [[ 0.01143107]
 [-0.06694705]
 [ 0.04271473]
 [-0.04294337]]
Y^ -  the output is:
 [[0.50285774]
 [0.48326949]
 [0.51067706]
 [0.48926581]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50285774]
 [0.48326949]
 [0.51067706]
 [0.48926581]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12571033]
 [-0.12903799]
 [-0.12227495]
 [ 0.12226008]]
W2.T is
 [[0.21986274 1.06175325]]
 D_Error times W2.T
 [[ 0.02763902  0.13347335]
 [-0.02837065 -0.13700651]
 [-0.02688371 -0.12982583]
 [ 0.02688044  0.12981003]]
self.H_D_Error_W2 is
 [[ 0.00195924  0.01912708]
 [-0.00043521 -0.01364819]
 [-0.00148801 -0.02127605]
 [ 0.00031221  0.01521617]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00195924  0.01912708]
 [-0.00043521 -0.01364819]
 [-0.00148801 -0.02127605]
 [ 0.00031221  0.01521617]]
X_H_D_Error_W2 is
 [[-0.0011758  -0.00605988]
 [-0.000123    0.00156798]]
Using sum gradient........

W1 was :
 [[-0.285969    0.2161319 ]
 [-1.6589718  -0.50634834]]
Updated W1 is: 
 [[-0.28479319  0.22219178]
 [-1.6588488  -0.50791632]]
W2 was :
 [[0.21986274]
 [1.06175325]]
Updated W2 is: 
 [[0.21797513]
 [1.0631151 ]]
The mean of the biases b gradient is:
 [ 8.70556697e-05 -1.45247149e-04]
The b biases before the update are:
 [[-2.48689194 -1.5620383 ]]
The new updated bs are:
 [[-2.48697899 -1.56189305]]
The bias c is: 
 [[ 0.12571033]
 [-0.12903799]
 [-0.12227495]
 [ 0.12226008]]
c bias before: [[-0.18951004]]
The mean c bias after update: [[-0.1886744]]
The output is: 
 [[0.50285774]
 [0.48326949]
 [0.51067706]
 [0.48926581]]
Total Loss: 0.49934714939414604
Average Loss: 0.12483678734853651

RUN:
  35
FeedForward


Z1 is:
 [[-2.48697899 -1.56189305]
 [-4.1458278  -2.06980937]
 [-2.77177219 -1.33970127]
 [-4.43062099 -1.84761758]]
H is:
 [[0.07677606 0.17337517]
 [0.01558363 0.11206601]
 [0.05886875 0.20755919]
 [0.01176698 0.13615286]]
The c is
 [[-0.1886744]]
Z2 is:
 [[ 0.01237864]
 [-0.06613849]
 [ 0.04481683]
 [-0.04136333]]
Y^ -  the output is:
 [[0.50309462]
 [0.4834714 ]
 [0.51120233]
 [0.48966064]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50309462]
 [0.4834714 ]
 [0.51120233]
 [0.48966064]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12576884]
 [-0.12899104]
 [-0.12213808]
 [ 0.12236281]]
W2.T is
 [[0.21797513 1.0631151 ]]
 D_Error times W2.T
 [[ 0.02741448  0.13370675]
 [-0.02811684 -0.13713232]
 [-0.02662306 -0.12984683]
 [ 0.02667205  0.13008576]]
self.H_D_Error_W2 is
 [[ 0.00194318  0.01916235]
 [-0.00043133 -0.01364566]
 [-0.001475   -0.021357  ]
 [ 0.00031016  0.01530007]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00194318  0.01916235]
 [-0.00043133 -0.01364566]
 [-0.001475   -0.021357  ]
 [ 0.00031016  0.01530007]]
X_H_D_Error_W2 is
 [[-0.00116485 -0.00605693]
 [-0.00012118  0.00165441]]
Using sum gradient........

W1 was :
 [[-0.28479319  0.22219178]
 [-1.6588488  -0.50791632]]
Updated W1 is: 
 [[-0.28362835  0.22824871]
 [-1.65872762 -0.50957073]]
W2 was :
 [[0.21797513]
 [1.0631151 ]]
Updated W2 is: 
 [[0.21607951]
 [1.06445625]]
The mean of the biases b gradient is:
 [ 8.67494789e-05 -1.35058679e-04]
The b biases before the update are:
 [[-2.48697899 -1.56189305]]
The new updated bs are:
 [[-2.48706574 -1.56175799]]
The bias c is: 
 [[ 0.12576884]
 [-0.12899104]
 [-0.12213808]
 [ 0.12236281]]
c bias before: [[-0.1886744]]
The mean c bias after update: [[-0.18792504]]
The output is: 
 [[0.50309462]
 [0.4834714 ]
 [0.51120233]
 [0.48966064]]
Total Loss: 0.49929834667762374
Average Loss: 0.12482458666940593

RUN:
  36
FeedForward


Z1 is:
 [[-2.48706574 -1.56175799]
 [-4.14579337 -2.07132872]
 [-2.77069409 -1.33350928]
 [-4.42942171 -1.84308001]]
H is:
 [[0.07676991 0.17339453]
 [0.01558416 0.11191491]
 [0.05892851 0.20857948]
 [0.01178094 0.13668743]]
The c is
 [[-0.18792504]]
Z2 is:
 [[ 0.01323426]
 [-0.06542909]
 [ 0.04683194]
 [-0.03988163]]
Y^ -  the output is:
 [[0.50330852]
 [0.48364856]
 [0.51170585]
 [0.49003091]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50330852]
 [0.48364856]
 [0.51170585]
 [0.49003091]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12582162]
 [-0.1289498 ]
 [-0.12200663]
 [ 0.12245903]]
W2.T is
 [[0.21607951 1.06445625]]
 D_Error times W2.T
 [[ 0.02718747  0.13393161]
 [-0.02786341 -0.13726142]
 [-0.02636313 -0.12987072]
 [ 0.02646089  0.13035228]]
self.H_D_Error_W2 is
 [[ 0.00192695  0.01919627]
 [-0.00042746 -0.01364241]
 [-0.00146199 -0.02143829]
 [ 0.00030806  0.01538209]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00192695  0.01919627]
 [-0.00042746 -0.01364241]
 [-0.00146199 -0.02143829]
 [ 0.00030806  0.01538209]]
X_H_D_Error_W2 is
 [[-0.00115393 -0.0060562 ]
 [-0.0001194   0.00173968]]
Using sum gradient........

W1 was :
 [[-0.28362835  0.22824871]
 [-1.65872762 -0.50957073]]
Updated W1 is: 
 [[-0.28247442  0.23430491]
 [-1.65860823 -0.51131041]]
W2 was :
 [[0.21607951]
 [1.06445625]]
Updated W2 is: 
 [[0.21417676]
 [1.06578034]]
The mean of the biases b gradient is:
 [ 8.63889221e-05 -1.25586013e-04]
The b biases before the update are:
 [[-2.48706574 -1.56175799]]
The new updated bs are:
 [[-2.48715213 -1.5616324 ]]
The bias c is: 
 [[ 0.12582162]
 [-0.1289498 ]
 [-0.12200663]
 [ 0.12245903]]
c bias before: [[-0.18792504]]
The mean c bias after update: [[-0.18725609]]
The output is: 
 [[0.50330852]
 [0.48364856]
 [0.51170585]
 [0.49003091]]
Total Loss: 0.4992498760864252
Average Loss: 0.1248124690216063

RUN:
  37
FeedForward


Z1 is:
 [[-2.48715213 -1.5616324 ]
 [-4.14576036 -2.07294282]
 [-2.76962655 -1.32732749]
 [-4.42823477 -1.8386379 ]]
H is:
 [[0.07676379 0.17341253]
 [0.01558467 0.11175458]
 [0.05898774 0.20960177]
 [0.01179476 0.13721246]]
The c is
 [[-0.18725609]]
Z2 is:
 [[ 0.0140046 ]
 [-0.06481238]
 [ 0.04876716]
 [-0.03849158]]
Y^ -  the output is:
 [[0.50350109]
 [0.48380258]
 [0.51218937]
 [0.49037829]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50350109]
 [0.48380258]
 [0.51218937]
 [0.49037829]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.1258691 ]
 [-0.12891393]
 [-0.12188018]
 [ 0.12254918]]
W2.T is
 [[0.21417676 1.06578034]]
 D_Error times W2.T
 [[ 0.02695824  0.13414881]
 [-0.02761037 -0.13739393]
 [-0.0261039  -0.1298975 ]
 [ 0.02624719  0.1306105 ]]
self.H_D_Error_W2 is
 [[ 0.00191056  0.01922898]
 [-0.00042359 -0.01363848]
 [-0.00144898 -0.02151997]
 [ 0.00030593  0.01546235]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00191056  0.01922898]
 [-0.00042359 -0.01363848]
 [-0.00144898 -0.02151997]
 [ 0.00030593  0.01546235]]
X_H_D_Error_W2 is
 [[-0.00114305 -0.00605762]
 [-0.00011766  0.00182387]]
Using sum gradient........

W1 was :
 [[-0.28247442  0.23430491]
 [-1.65860823 -0.51131041]]
Updated W1 is: 
 [[-0.28133136  0.24036253]
 [-1.65849056 -0.51313429]]
W2 was :
 [[0.21417676]
 [1.06578034]]
Updated W2 is: 
 [[0.21226765]
 [1.06709081]]
The mean of the biases b gradient is:
 [ 8.59788563e-05 -1.16780592e-04]
The b biases before the update are:
 [[-2.48715213 -1.5616324 ]]
The new updated bs are:
 [[-2.48723811 -1.56151562]]
The bias c is: 
 [[ 0.1258691 ]
 [-0.12891393]
 [-0.12188018]
 [ 0.12254918]]
c bias before: [[-0.18725609]]
The mean c bias after update: [[-0.18666213]]
The output is: 
 [[0.50350109]
 [0.48380258]
 [0.51218937]
 [0.49037829]]
Total Loss: 0.499201603806395
Average Loss: 0.12480040095159875

RUN:
  38
FeedForward


Z1 is:
 [[-2.48723811 -1.56151562]
 [-4.14572867 -2.07464991]
 [-2.76856947 -1.32115309]
 [-4.42706003 -1.83428738]]
H is:
 [[0.07675769 0.17342927]
 [0.01558515 0.11158524]
 [0.05904644 0.21062651]
 [0.01180846 0.13772832]]
The c is
 [[-0.18666213]]
Z2 is:
 [[ 0.01469583]
 [-0.06428232]
 [ 0.05062913]
 [-0.03718696]]
Y^ -  the output is:
 [[0.50367389]
 [0.48393495]
 [0.51265458]
 [0.49070433]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50367389]
 [0.48393495]
 [0.51265458]
 [0.49070433]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12591167]
 [-0.12888307]
 [-0.12175831]
 [ 0.12263368]]
W2.T is
 [[0.21226765 1.06709081]]
 D_Error times W2.T
 [[ 0.02672698  0.13435919]
 [-0.02735771 -0.13752994]
 [-0.02584535 -0.12992718]
 [ 0.02603116  0.13086128]]
self.H_D_Error_W2 is
 [[ 0.00189403  0.0192606 ]
 [-0.00041973 -0.01363389]
 [-0.00143597 -0.02160208]
 [ 0.00030376  0.01554098]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00189403  0.0192606 ]
 [-0.00041973 -0.01363389]
 [-0.00143597 -0.02160208]
 [ 0.00030376  0.01554098]]
X_H_D_Error_W2 is
 [[-0.00113221 -0.0060611 ]
 [-0.00011597  0.00190709]]
Using sum gradient........

W1 was :
 [[-0.28133136  0.24036253]
 [-1.65849056 -0.51313429]]
Updated W1 is: 
 [[-0.28019915  0.24642363]
 [-1.65837459 -0.51504138]]
W2 was :
 [[0.21226765]
 [1.06709081]]
Updated W2 is: 
 [[0.2103529 ]
 [1.06839089]]
The mean of the biases b gradient is:
 [ 8.55237757e-05 -1.08596678e-04]
The b biases before the update are:
 [[-2.48723811 -1.56151562]]
The new updated bs are:
 [[-2.48732363 -1.56140703]]
The bias c is: 
 [[ 0.12591167]
 [-0.12888307]
 [-0.12175831]
 [ 0.12263368]]
c bias before: [[-0.18666213]]
The mean c bias after update: [[-0.18613813]]
The output is: 
 [[0.50367389]
 [0.48393495]
 [0.51265458]
 [0.49070433]]
Total Loss: 0.499153410988237
Average Loss: 0.12478835274705925

RUN:
  39
FeedForward


Z1 is:
 [[-2.48732363 -1.56140703]
 [-4.14569822 -2.07644841]
 [-2.76752279 -1.3149834 ]
 [-4.42589738 -1.83002478]]
H is:
 [[0.07675163 0.17344484]
 [0.01558562 0.11140707]
 [0.05910462 0.21165414]
 [0.01182204 0.13823532]]
The c is
 [[-0.18613813]]
Z2 is:
 [[ 0.01531369]
 [-0.06383334]
 [ 0.05242405]
 [-0.03596197]]
Y^ -  the output is:
 [[0.50382835]
 [0.48404708]
 [0.51310301]
 [0.49101048]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50382835]
 [0.48404708]
 [0.51310301]
 [0.49101048]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.1259497 ]
 [-0.12885692]
 [-0.12164065]
 [ 0.12271294]]
W2.T is
 [[0.2103529  1.06839089]]
 D_Error times W2.T
 [[ 0.02649389  0.13456352]
 [-0.02710543 -0.13766956]
 [-0.02558746 -0.12995976]
 [ 0.02581302  0.13110539]]
self.H_D_Error_W2 is
 [[ 0.00187738  0.01929126]
 [-0.00041587 -0.01362867]
 [-0.00142295 -0.02168465]
 [ 0.00030155  0.0156181 ]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00187738  0.01929126]
 [-0.00041587 -0.01362867]
 [-0.00142295 -0.02168465]
 [ 0.00030155  0.0156181 ]]
X_H_D_Error_W2 is
 [[-0.0011214  -0.00606655]
 [-0.00011432  0.00198943]]
Using sum gradient........

W1 was :
 [[-0.28019915  0.24642363]
 [-1.65837459 -0.51504138]]
Updated W1 is: 
 [[-0.27907776  0.25249018]
 [-1.65826027 -0.51703081]]
W2 was :
 [[0.2103529 ]
 [1.06839089]]
Updated W2 is: 
 [[0.20843318]
 [1.06968362]]
The mean of the biases b gradient is:
 [ 8.50278355e-05 -1.00991219e-04]
The b biases before the update are:
 [[-2.48732363 -1.56140703]]
The new updated bs are:
 [[-2.48740866 -1.56130604]]
The bias c is: 
 [[ 0.1259497 ]
 [-0.12885692]
 [-0.12164065]
 [ 0.12271294]]
c bias before: [[-0.18613813]]
The mean c bias after update: [[-0.18567939]]
The output is: 
 [[0.50382835]
 [0.48404708]
 [0.51310301]
 [0.49101048]]
Total Loss: 0.49910519165595113
Average Loss: 0.12477629791398778

RUN:
  40
FeedForward


Z1 is:
 [[-2.48740866 -1.56130604]
 [-4.14566894 -2.07833685]
 [-2.76648642 -1.30881586]
 [-4.4247467  -1.82584667]]
H is:
 [[0.07674561 0.17345932]
 [0.01558607 0.11122026]
 [0.05916228 0.21268506]
 [0.01183549 0.1387338 ]]
The c is
 [[-0.18567939]]
Z2 is:
 [[ 0.01586353]
 [-0.06346024]
 [ 0.05415772]
 [-0.03481121]]
Y^ -  the output is:
 [[0.5039658 ]
 [0.48414026]
 [0.51353612]
 [0.49129808]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.5039658 ]
 [0.48414026]
 [0.51353612]
 [0.49129808]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12598352]
 [-0.12883518]
 [-0.12152684]
 [ 0.12278732]]
W2.T is
 [[0.20843318 1.06968362]]
 D_Error times W2.T
 [[ 0.02625915  0.13476251]
 [-0.02685353 -0.13781288]
 [-0.02533023 -0.12999527]
 [ 0.02559295  0.13134358]]
self.H_D_Error_W2 is
 [[ 0.00186061  0.01932106]
 [-0.00041202 -0.01362285]
 [-0.00140993 -0.02176772]
 [ 0.00029932  0.01569382]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00186061  0.01932106]
 [-0.00041202 -0.01362285]
 [-0.00140993 -0.02176772]
 [ 0.00029932  0.01569382]]
X_H_D_Error_W2 is
 [[-0.00111061 -0.00607391]
 [-0.0001127   0.00207097]]
Using sum gradient........

W1 was :
 [[-0.27907776  0.25249018]
 [-1.65826027 -0.51703081]]
Updated W1 is: 
 [[-0.27796714  0.25856409]
 [-1.65814758 -0.51910178]]
W2 was :
 [[0.20843318]
 [1.06968362]]
Updated W2 is: 
 [[0.20650909]
 [1.07097188]]
The mean of the biases b gradient is:
 [ 8.44948751e-05 -9.39237181e-05]
The b biases before the update are:
 [[-2.48740866 -1.56130604]]
The new updated bs are:
 [[-2.48749316 -1.56121211]]
The bias c is: 
 [[ 0.12598352]
 [-0.12883518]
 [-0.12152684]
 [ 0.12278732]]
c bias before: [[-0.18567939]]
The mean c bias after update: [[-0.1852816]]
The output is: 
 [[0.5039658 ]
 [0.48414026]
 [0.51353612]
 [0.49129808]]
Total Loss: 0.49905685089382074
Average Loss: 0.12476421272345518

RUN:
  41
FeedForward


Z1 is:
 [[-2.48749316 -1.56121211]
 [-4.14564073 -2.08031389]
 [-2.7654603  -1.30264802]
 [-4.42360788 -1.8217498 ]]
H is:
 [[0.07673962 0.17347279]
 [0.0155865  0.11102498]
 [0.05921943 0.2137197 ]
 [0.01184881 0.13922404]]
The c is
 [[-0.1852816]]
Z2 is:
 [[ 0.01635031]
 [-0.06315821]
 [ 0.05583554]
 [-0.03372968]]
Y^ -  the output is:
 [[0.50408749]
 [0.48421569]
 [0.51395526]
 [0.49156838]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50408749]
 [0.48421569]
 [0.51395526]
 [0.49156838]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12601345]
 [-0.12881757]
 [-0.12141653]
 [ 0.12285715]]
W2.T is
 [[0.20650909 1.07097188]]
 D_Error times W2.T
 [[ 0.02602292  0.13495686]
 [-0.026602   -0.13796   ]
 [-0.02507362 -0.13003369]
 [ 0.02537112  0.13157655]]
self.H_D_Error_W2 is
 [[ 0.00184374  0.01935011]
 [-0.00040817 -0.01361644]
 [-0.00139691 -0.02185133]
 [ 0.00029706  0.01576823]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00184374  0.01935011]
 [-0.00040817 -0.01361644]
 [-0.00139691 -0.02185133]
 [ 0.00029706  0.01576823]]
X_H_D_Error_W2 is
 [[-0.00109986 -0.0060831 ]
 [-0.00011111  0.00215179]]
Using sum gradient........

W1 was :
 [[-0.27796714  0.25856409]
 [-1.65814758 -0.51910178]]
Updated W1 is: 
 [[-0.27686729  0.26464719]
 [-1.65803646 -0.52125357]]
W2 was :
 [[0.20650909]
 [1.07097188]]
Updated W2 is: 
 [[0.20458119]
 [1.07225838]]
The mean of the biases b gradient is:
 [ 8.39284397e-05 -8.73560948e-05]
The b biases before the update are:
 [[-2.48749316 -1.56121211]]
The new updated bs are:
 [[-2.48757709 -1.56112476]]
The bias c is: 
 [[ 0.12601345]
 [-0.12881757]
 [-0.12141653]
 [ 0.12285715]]
c bias before: [[-0.1852816]]
The mean c bias after update: [[-0.18494072]]
The output is: 
 [[0.50408749]
 [0.48421569]
 [0.51395526]
 [0.49156838]]
Total Loss: 0.4990083032750232
Average Loss: 0.1247520758187558

RUN:
  42
FeedForward


Z1 is:
 [[-2.48757709 -1.56112476]
 [-4.14561355 -2.08237833]
 [-2.76444437 -1.29647757]
 [-4.42248084 -1.81773114]]
H is:
 [[0.07673367 0.17348531]
 [0.01558692 0.11082139]
 [0.05927605 0.21475843]
 [0.01186202 0.13970634]]
The c is
 [[-0.18494072]]
Z2 is:
 [[ 0.01677862]
 [-0.06292277]
 [ 0.05746257]
 [-0.03271268]]
Y^ -  the output is:
 [[0.50419456]
 [0.4842745 ]
 [0.51436169]
 [0.49182256]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50419456]
 [0.4842745 ]
 [0.51436169]
 [0.49182256]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12603977]
 [-0.12880384]
 [-0.12130941]
 [ 0.12292275]]
W2.T is
 [[0.20458119 1.07225838]]
 D_Error times W2.T
 [[ 0.02578537  0.1351472 ]
 [-0.02635084 -0.138111  ]
 [-0.02481762 -0.13007503]
 [ 0.02514768  0.13180495]]
self.H_D_Error_W2 is
 [[ 0.00182678  0.01937851]
 [-0.00040433 -0.01360946]
 [-0.00138389 -0.0219355 ]
 [ 0.00029476  0.01584144]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00182678  0.01937851]
 [-0.00040433 -0.01360946]
 [-0.00138389 -0.0219355 ]
 [ 0.00029476  0.01584144]]
X_H_D_Error_W2 is
 [[-0.00108913 -0.00609406]
 [-0.00010956  0.00223198]]
Using sum gradient........

W1 was :
 [[-0.27686729  0.26464719]
 [-1.65803646 -0.52125357]]
Updated W1 is: 
 [[-0.27577816  0.27074125]
 [-1.6579269  -0.52348555]]
W2 was :
 [[0.20458119]
 [1.07225838]]
Updated W2 is: 
 [[0.20264998]
 [1.07354568]]
The mean of the biases b gradient is:
 [ 8.33318006e-05 -8.12525599e-05]
The b biases before the update are:
 [[-2.48757709 -1.56112476]]
The new updated bs are:
 [[-2.48766042 -1.5610435 ]]
The bias c is: 
 [[ 0.12603977]
 [-0.12880384]
 [-0.12130941]
 [ 0.12292275]]
c bias before: [[-0.18494072]]
The mean c bias after update: [[-0.18465304]]
The output is: 
 [[0.50419456]
 [0.4842745 ]
 [0.51436169]
 [0.49182256]]
Total Loss: 0.4989594714997327
Average Loss: 0.12473986787493317

RUN:
  43
FeedForward


Z1 is:
 [[-2.48766042 -1.5610435 ]
 [-4.14558732 -2.08452905]
 [-2.76343858 -1.29030226]
 [-4.42136548 -1.8137878 ]]
H is:
 [[0.07672777 0.17349696]
 [0.01558732 0.11060963]
 [0.05933216 0.21580166]
 [0.0118751  0.14018096]]
The c is
 [[-0.18465304]]
Z2 is:
 [[ 0.01715276]
 [-0.06274977]
 [ 0.05904356]
 [-0.03175589]]
Y^ -  the output is:
 [[0.50428808]
 [0.4843177 ]
 [0.5147566 ]
 [0.49206169]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50428808]
 [0.4843177 ]
 [0.5147566 ]
 [0.49206169]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12606275]
 [-0.12879375]
 [-0.12120518]
 [ 0.12298442]]
W2.T is
 [[0.20264998 1.07354568]]
 D_Error times W2.T
 [[ 0.02554661  0.13533412]
 [-0.02610005 -0.13826597]
 [-0.02456223 -0.1301193 ]
 [ 0.02492279  0.13202939]]
self.H_D_Error_W2 is
 [[ 0.00180974  0.01940634]
 [-0.00040049 -0.01360194]
 [-0.00137086 -0.02202026]
 [ 0.00029245  0.01591354]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00180974  0.01940634]
 [-0.00040049 -0.01360194]
 [-0.00137086 -0.02202026]
 [ 0.00029245  0.01591354]]
X_H_D_Error_W2 is
 [[-0.00107842 -0.00610672]
 [-0.00010804  0.0023116 ]]
Using sum gradient........

W1 was :
 [[-0.27577816  0.27074125]
 [-1.6579269  -0.52348555]]
Updated W1 is: 
 [[-0.27469974  0.27684797]
 [-1.65781886 -0.52579715]]
W2 was :
 [[0.20264998]
 [1.07354568]]
Updated W2 is: 
 [[0.20071593]
 [1.07483622]]
The mean of the biases b gradient is:
 [ 8.27079739e-05 -7.55794876e-05]
The b biases before the update are:
 [[-2.48766042 -1.5610435 ]]
The new updated bs are:
 [[-2.48774313 -1.56096792]]
The bias c is: 
 [[ 0.12606275]
 [-0.12879375]
 [-0.12120518]
 [ 0.12298442]]
c bias before: [[-0.18465304]]
The mean c bias after update: [[-0.1844151]]
The output is: 
 [[0.50428808]
 [0.4843177 ]
 [0.5147566 ]
 [0.49206169]]
Total Loss: 0.498910285214771
Average Loss: 0.12472757130369275

RUN:
  44
FeedForward


Z1 is:
 [[-2.48774313 -1.56096792]
 [-4.14556198 -2.08676507]
 [-2.76244287 -1.28411995]
 [-4.42026173 -1.8099171 ]]
H is:
 [[0.07672191 0.1735078 ]
 [0.01558771 0.11038986]
 [0.05938776 0.21684973]
 [0.01188806 0.14064815]]
The c is
 [[-0.1844151]]
Z2 is:
 [[ 0.01747668]
 [-0.06263538]
 [ 0.06058292]
 [-0.03085526]]
Y^ -  the output is:
 [[0.50436906]
 [0.48434627]
 [0.5151411 ]
 [0.4922868 ]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50436906]
 [0.48434627]
 [0.5151411 ]
 [0.4922868 ]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12608264]
 [-0.12878708]
 [-0.12110357]
 [ 0.12304241]]
W2.T is
 [[0.20071593 1.07483622]]
 D_Error times W2.T
 [[ 0.02530679  0.13551818]
 [-0.02584962 -0.13842501]
 [-0.02430742 -0.1301665 ]
 [ 0.02469657  0.13225044]]
self.H_D_Error_W2 is
 [[ 0.00179262  0.01943369]
 [-0.00039666 -0.01359388]
 [-0.00135783 -0.02210565]
 [ 0.0002901   0.01598461]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00179262  0.01943369]
 [-0.00039666 -0.01359388]
 [-0.00135783 -0.02210565]
 [ 0.0002901   0.01598461]]
X_H_D_Error_W2 is
 [[-0.00106773 -0.00612103]
 [-0.00010655  0.00239073]]
Using sum gradient........

W1 was :
 [[-0.27469974  0.27684797]
 [-1.65781886 -0.52579715]]
Updated W1 is: 
 [[-0.27363201  0.282969  ]
 [-1.65771231 -0.52818788]]
W2 was :
 [[0.20071593]
 [1.07483622]]
Updated W2 is: 
 [[0.19877946]
 [1.07613227]]
The mean of the biases b gradient is:
 [ 8.20597391e-05 -7.03052928e-05]
The b biases before the update are:
 [[-2.48774313 -1.56096792]]
The new updated bs are:
 [[-2.48782519 -1.56089762]]
The bias c is: 
 [[ 0.12608264]
 [-0.12878708]
 [-0.12110357]
 [ 0.12304241]]
c bias before: [[-0.1844151]]
The mean c bias after update: [[-0.1842237]]
The output is: 
 [[0.50436906]
 [0.48434627]
 [0.5151411 ]
 [0.4922868 ]]
Total Loss: 0.49886067999051975
Average Loss: 0.12471516999762994

RUN:
  45
FeedForward


Z1 is:
 [[-2.48782519 -1.56089762]
 [-4.14553749 -2.0890855 ]
 [-2.7614572  -1.27792862]
 [-4.41916951 -1.8061165 ]]
H is:
 [[0.0767161  0.17351788]
 [0.01558809 0.11016219]
 [0.05944284 0.21790303]
 [0.01190089 0.14110814]]
The c is
 [[-0.1842237]]
Z2 is:
 [[ 0.01775408]
 [-0.06257602]
 [ 0.0620848 ]
 [-0.03000702]]
Y^ -  the output is:
 [[0.5044384 ]
 [0.4843611 ]
 [0.51551622]
 [0.49249881]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.5044384 ]
 [0.4843611 ]
 [0.51551622]
 [0.49249881]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12609966]
 [-0.12878361]
 [-0.12100431]
 [ 0.12309699]]
W2.T is
 [[0.19877946 1.07613227]]
 D_Error times W2.T
 [[ 0.02506602  0.13569992]
 [-0.02559954 -0.1385882 ]
 [-0.02405317 -0.13021664]
 [ 0.02446915  0.13246864]]
self.H_D_Error_W2 is
 [[ 0.00177544  0.01946065]
 [-0.00039283 -0.01358531]
 [-0.0013448  -0.02219169]
 [ 0.00028774  0.01605475]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00177544  0.01946065]
 [-0.00039283 -0.01358531]
 [-0.0013448  -0.02219169]
 [ 0.00028774  0.01605475]]
X_H_D_Error_W2 is
 [[-0.00105706 -0.00613694]
 [-0.00010509  0.00246944]]
Using sum gradient........

W1 was :
 [[-0.27363201  0.282969  ]
 [-1.65771231 -0.52818788]]
Updated W1 is: 
 [[-0.27257495  0.28910594]
 [-1.65760722 -0.53065732]]
W2 was :
 [[0.19877946]
 [1.07613227]]
Updated W2 is: 
 [[0.19684095]
 [1.07743603]]
The mean of the biases b gradient is:
 [ 8.13896549e-05 -6.54003131e-05]
The b biases before the update are:
 [[-2.48782519 -1.56089762]]
The new updated bs are:
 [[-2.48790658 -1.56083222]]
The bias c is: 
 [[ 0.12609966]
 [-0.12878361]
 [-0.12100431]
 [ 0.12309699]]
c bias before: [[-0.1842237]]
The mean c bias after update: [[-0.18407588]]
The output is: 
 [[0.5044384 ]
 [0.4843611 ]
 [0.51551622]
 [0.49249881]]
Total Loss: 0.49881059643400155
Average Loss: 0.12470264910850039

RUN:
  46
FeedForward


Z1 is:
 [[-2.48790658 -1.56083222]
 [-4.14551379 -2.09148954]
 [-2.76048153 -1.27172628]
 [-4.41808875 -1.8023836 ]]
H is:
 [[0.07671033 0.17352726]
 [0.01558845 0.10992675]
 [0.05949742 0.21896188]
 [0.01191361 0.14156116]]
The c is
 [[-0.18407588]]
Z2 is:
 [[ 0.01798838]
 [-0.0625684 ]
 [ 0.06355307]
 [-0.0292077 ]]
Y^ -  the output is:
 [[0.50449697]
 [0.484363  ]
 [0.51588292]
 [0.49269859]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50449697]
 [0.484363  ]
 [0.51588292]
 [0.49269859]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12611404]
 [-0.12878317]
 [-0.12090714]
 [ 0.12314838]]
W2.T is
 [[0.19684095 1.07743603]]
 D_Error times W2.T
 [[ 0.02482441  0.13587981]
 [-0.0253498  -0.13875563]
 [-0.02379948 -0.13026971]
 [ 0.02424064  0.1326845 ]]
self.H_D_Error_W2 is
 [[ 0.00175821  0.01948728]
 [-0.000389   -0.01357625]
 [-0.00133176 -0.02227841]
 [ 0.00028535  0.01612403]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00175821  0.01948728]
 [-0.000389   -0.01357625]
 [-0.00133176 -0.02227841]
 [ 0.00028535  0.01612403]]
X_H_D_Error_W2 is
 [[-0.00104641 -0.00615438]
 [-0.00010365  0.00254779]]
Using sum gradient........

W1 was :
 [[-0.27257495  0.28910594]
 [-1.65760722 -0.53065732]]
Updated W1 is: 
 [[-0.27152855  0.29526032]
 [-1.65750357 -0.53320511]]
W2 was :
 [[0.19684095]
 [1.07743603]]
Updated W2 is: 
 [[0.19490075]
 [1.07874955]]
The mean of the biases b gradient is:
 [ 8.07000758e-05 -6.08366945e-05]
The b biases before the update are:
 [[-2.48790658 -1.56083222]]
The new updated bs are:
 [[-2.48798728 -1.56077138]]
The bias c is: 
 [[ 0.12611404]
 [-0.12878317]
 [-0.12090714]
 [ 0.12314838]]
c bias before: [[-0.18407588]]
The mean c bias after update: [[-0.18396891]]
The output is: 
 [[0.50449697]
 [0.484363  ]
 [0.51588292]
 [0.49269859]]
Total Loss: 0.49875997941981604
Average Loss: 0.12468999485495401

RUN:
  47
FeedForward


Z1 is:
 [[-2.48798728 -1.56077138]
 [-4.14549084 -2.09397649]
 [-2.75951582 -1.26551107]
 [-4.41701939 -1.79871617]]
H is:
 [[0.07670462 0.17353599]
 [0.0155888  0.10968365]
 [0.05955148 0.22002665]
 [0.0119262  0.14200742]]
The c is
 [[-0.18396891]]
Z2 is:
 [[ 0.01818274]
 [-0.06260945]
 [ 0.06499137]
 [-0.02845405]]
Y^ -  the output is:
 [[0.50454556]
 [0.48435275]
 [0.51624213]
 [0.49288697]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50454556]
 [0.48435275]
 [0.51624213]
 [0.49288697]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12612597]
 [-0.12878556]
 [-0.12081185]
 [ 0.1231968 ]]
W2.T is
 [[0.19490075 1.07874955]]
 D_Error times W2.T
 [[ 0.02458205  0.13605833]
 [-0.0251004  -0.13892737]
 [-0.02354632 -0.13032573]
 [ 0.02401115  0.1328985 ]]
self.H_D_Error_W2 is
 [[ 0.00174093  0.01951366]
 [-0.00038519 -0.0135667 ]
 [-0.00131871 -0.02236584]
 [ 0.00028295  0.01619253]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00174093  0.01951366]
 [-0.00038519 -0.0135667 ]
 [-0.00131871 -0.02236584]
 [ 0.00028295  0.01619253]]
X_H_D_Error_W2 is
 [[-0.00103577 -0.00617331]
 [-0.00010224  0.00262583]]
Using sum gradient........

W1 was :
 [[-0.27152855  0.29526032]
 [-1.65750357 -0.53320511]]
Updated W1 is: 
 [[-0.27049278  0.30143363]
 [-1.65740133 -0.53583094]]
W2 was :
 [[0.19490075]
 [1.07874955]]
Updated W2 is: 
 [[0.19295917]
 [1.08007479]]
The mean of the biases b gradient is:
 [ 7.99931659e-05 -5.65882826e-05]
The b biases before the update are:
 [[-2.48798728 -1.56077138]]
The new updated bs are:
 [[-2.48806727 -1.56071479]]
The bias c is: 
 [[ 0.12612597]
 [-0.12878556]
 [-0.12081185]
 [ 0.1231968 ]]
c bias before: [[-0.18396891]]
The mean c bias after update: [[-0.18390025]]
The output is: 
 [[0.50454556]
 [0.48435275]
 [0.51624213]
 [0.49288697]]
Total Loss: 0.49870877742304287
Average Loss: 0.12467719435576072

RUN:
  48
FeedForward


Z1 is:
 [[-2.48806727 -1.56071479]
 [-4.1454686  -2.09654573]
 [-2.75856005 -1.25928116]
 [-4.41596138 -1.7951121 ]]
H is:
 [[0.07669895 0.1735441 ]
 [0.01558914 0.10943301]
 [0.05960503 0.22109766]
 [0.01193868 0.14244711]]
The c is
 [[-0.18390025]]
Z2 is:
 [[ 0.01834013]
 [-0.06269634]
 [ 0.0664031 ]
 [-0.02774304]]
Y^ -  the output is:
 [[0.5045849 ]
 [0.48433105]
 [0.51659468]
 [0.49306468]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.5045849 ]
 [0.48433105]
 [0.51659468]
 [0.49306468]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12613562]
 [-0.12879063]
 [-0.12071821]
 [ 0.12324246]]
W2.T is
 [[0.19295917 1.08007479]]
 D_Error times W2.T
 [[ 0.02433902  0.1362359 ]
 [-0.02485133 -0.13910352]
 [-0.02329369 -0.13038469]
 [ 0.02378076  0.13311107]]
self.H_D_Error_W2 is
 [[ 0.0017236   0.01953985]
 [-0.00038137 -0.01355667]
 [-0.00130566 -0.022454  ]
 [ 0.00028052  0.01626031]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.0017236   0.01953985]
 [-0.00038137 -0.01355667]
 [-0.00130566 -0.022454  ]
 [ 0.00028052  0.01626031]]
X_H_D_Error_W2 is
 [[-0.00102514 -0.0061937 ]
 [-0.00010085  0.00270364]]
Using sum gradient........

W1 was :
 [[-0.27049278  0.30143363]
 [-1.65740133 -0.53583094]]
Updated W1 is: 
 [[-0.26946764  0.30762732]
 [-1.65730048 -0.53853457]]
W2 was :
 [[0.19295917]
 [1.08007479]]
Updated W2 is: 
 [[0.1910165 ]
 [1.08141363]]
The mean of the biases b gradient is:
 [ 7.92709132e-05 -5.26305170e-05]
The b biases before the update are:
 [[-2.48806727 -1.56071479]]
The new updated bs are:
 [[-2.48814654 -1.56066216]]
The bias c is: 
 [[ 0.12613562]
 [-0.12879063]
 [-0.12071821]
 [ 0.12324246]]
c bias before: [[-0.18390025]]
The mean c bias after update: [[-0.18386756]]
The output is: 
 [[0.5045849 ]
 [0.48433105]
 [0.51659468]
 [0.49306468]]
Total Loss: 0.4986569419403276
Average Loss: 0.1246642354850819

RUN:
  49
FeedForward


Z1 is:
 [[-2.48814654 -1.56066216]
 [-4.14544702 -2.09919674]
 [-2.75761418 -1.25303484]
 [-4.41491466 -1.79156941]]
H is:
 [[0.07669334 0.17355165]
 [0.01558947 0.10917492]
 [0.05965807 0.22217524]
 [0.01195103 0.14288042]]
The c is
 [[-0.18386756]]
Z2 is:
 [[ 0.01846326]
 [-0.06282646]
 [ 0.06779145]
 [-0.02707188]]
Y^ -  the output is:
 [[0.50461568]
 [0.48429855]
 [0.51694137]
 [0.49323244]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50461568]
 [0.48429855]
 [0.51694137]
 [0.49323244]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12614317]
 [-0.12879822]
 [-0.12062601]
 [ 0.12328552]]
W2.T is
 [[0.1910165  1.08141363]]
 D_Error times W2.T
 [[ 0.02409543  0.13641294]
 [-0.02460259 -0.13928415]
 [-0.02304156 -0.13044662]
 [ 0.02354957  0.13332264]]
self.H_D_Error_W2 is
 [[ 0.00170623  0.01956591]
 [-0.00037756 -0.01354619]
 [-0.00129261 -0.02254292]
 [ 0.00027808  0.01632744]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00170623  0.01956591]
 [-0.00037756 -0.01354619]
 [-0.00129261 -0.02254292]
 [ 0.00027808  0.01632744]]
X_H_D_Error_W2 is
 [[-1.01452988e-03 -6.21548533e-03]
 [-9.94840616e-05  2.78125205e-03]]
Using sum gradient........

W1 was :
 [[-0.26946764  0.30762732]
 [-1.65730048 -0.53853457]]
Updated W1 is: 
 [[-0.26845311  0.31384281]
 [-1.65720099 -0.54131583]]
W2 was :
 [[0.1910165 ]
 [1.08141363]]
Updated W2 is: 
 [[0.18907298]
 [1.08276783]]
The mean of the biases b gradient is:
 [ 7.85351418e-05 -4.89403319e-05]
The b biases before the update are:
 [[-2.48814654 -1.56066216]]
The new updated bs are:
 [[-2.48822507 -1.56061322]]
The bias c is: 
 [[ 0.12614317]
 [-0.12879822]
 [-0.12062601]
 [ 0.12328552]]
c bias before: [[-0.18386756]]
The mean c bias after update: [[-0.18386867]]
The output is: 
 [[0.50461568]
 [0.48429855]
 [0.51694137]
 [0.49323244]]
Total Loss: 0.4986044269872053
Average Loss: 0.12465110674680133

RUN:
  50
FeedForward


Z1 is:
 [[-2.48822507 -1.56061322]
 [-4.14542607 -2.10192905]
 [-2.75667818 -1.24677041]
 [-4.41387918 -1.78808624]]
H is:
 [[0.07668778 0.17355867]
 [0.0155898  0.10890947]
 [0.0597106  0.2232597 ]
 [0.01196326 0.14330752]]
The c is
 [[-0.18386867]]
Z2 is:
 [[ 0.01855466]
 [-0.06299739]
 [ 0.06915941]
 [-0.02643797]]
Y^ -  the output is:
 [[0.50463853]
 [0.48425586]
 [0.51728296]
 [0.49339089]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50463853]
 [0.48425586]
 [0.51728296]
 [0.49339089]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12614878]
 [-0.12880819]
 [-0.12053507]
 [ 0.12332617]]
W2.T is
 [[0.18907298 1.08276783]]
 D_Error times W2.T
 [[ 0.02385132  0.13658984]
 [-0.02435415 -0.13946937]
 [-0.02278992 -0.1305115 ]
 [ 0.02331765  0.13353361]]
self.H_D_Error_W2 is
 [[ 0.00168884  0.01959191]
 [-0.00037376 -0.01353525]
 [-0.00127955 -0.02263263]
 [ 0.00027562  0.01639398]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00168884  0.01959191]
 [-0.00037376 -0.01353525]
 [-0.00127955 -0.02263263]
 [ 0.00027562  0.01639398]]
X_H_D_Error_W2 is
 [[-1.00392788e-03 -6.23864128e-03]
 [-9.81391742e-05  2.85873382e-03]]
Using sum gradient........

W1 was :
 [[-0.26845311  0.31384281]
 [-1.65720099 -0.54131583]]
Updated W1 is: 
 [[-0.26744918  0.32008145]
 [-1.65710285 -0.54417456]]
W2 was :
 [[0.18907298]
 [1.08276783]]
Updated W2 is: 
 [[0.18712884]
 [1.08413911]]
The mean of the biases b gradient is:
 [ 7.77875240e-05 -4.54960603e-05]
The b biases before the update are:
 [[-2.48822507 -1.56061322]]
The new updated bs are:
 [[-2.48830286 -1.56056773]]
The bias c is: 
 [[ 0.12614878]
 [-0.12880819]
 [-0.12053507]
 [ 0.12332617]]
c bias before: [[-0.18386867]]
The mean c bias after update: [[-0.18390159]]
The output is: 
 [[0.50463853]
 [0.48425586]
 [0.51728296]
 [0.49339089]]
Total Loss: 0.4985511886613039
Average Loss: 0.12463779716532597

RUN:
  51
FeedForward


Z1 is:
 [[-2.48830286 -1.56056773]
 [-4.14540572 -2.10474229]
 [-2.75575204 -1.24048628]
 [-4.4128549  -1.78466083]]
H is:
 [[0.07668227 0.1735652 ]
 [0.01559011 0.10863675]
 [0.05976262 0.22435135]
 [0.01197538 0.14372857]]
The c is
 [[-0.18390159]]
Z2 is:
 [[ 0.01861669]
 [-0.06320688]
 [ 0.0705098 ]
 [-0.02583889]]
Y^ -  the output is:
 [[0.50465404]
 [0.48420354]
 [0.51762015]
 [0.49354064]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50465404]
 [0.48420354]
 [0.51762015]
 [0.49354064]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12615258]
 [-0.12882041]
 [-0.1204452 ]
 [ 0.12336457]]
W2.T is
 [[0.18712884 1.08413911]]
 D_Error times W2.T
 [[ 0.02360679  0.13676694]
 [-0.02410601 -0.13965924]
 [-0.02253877 -0.13057935]
 [ 0.02308507  0.13374435]]
self.H_D_Error_W2 is
 [[ 0.00167141  0.01961789]
 [-0.00036996 -0.01352388]
 [-0.00126648 -0.02272313]
 [ 0.00027314  0.01646001]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00167141  0.01961789]
 [-0.00036996 -0.01352388]
 [-0.00126648 -0.02272313]
 [ 0.00027314  0.01646001]]
X_H_D_Error_W2 is
 [[-9.93335291e-04 -6.26312767e-03]
 [-9.68145709e-05  2.93613077e-03]]
Using sum gradient........

W1 was :
 [[-0.26744918  0.32008145]
 [-1.65710285 -0.54417456]]
Updated W1 is: 
 [[-0.26645585  0.32634458]
 [-1.65700604 -0.54711069]]
W2 was :
 [[0.18712884]
 [1.08413911]]
Updated W2 is: 
 [[0.18518428]
 [1.08552907]]
The mean of the biases b gradient is:
 [ 7.70295916e-05 -4.22773430e-05]
The b biases before the update are:
 [[-2.48830286 -1.56056773]]
The new updated bs are:
 [[-2.48837989 -1.56052545]]
The bias c is: 
 [[ 0.12615258]
 [-0.12882041]
 [-0.1204452 ]
 [ 0.12336457]]
c bias before: [[-0.18390159]]
The mean c bias after update: [[-0.18396447]]
The output is: 
 [[0.50465404]
 [0.48420354]
 [0.51762015]
 [0.49354064]]
Total Loss: 0.49849718476246274
Average Loss: 0.12462429619061569

RUN:
  52
FeedForward


Z1 is:
 [[-2.48837989 -1.56052545]
 [-4.14538593 -2.10763614]
 [-2.75483574 -1.23418087]
 [-4.41184178 -1.78129156]]
H is:
 [[0.07667682 0.17357126]
 [0.01559041 0.10835684]
 [0.05981413 0.22545051]
 [0.01198737 0.14414373]]
The c is
 [[-0.18396447]]
Z2 is:
 [[ 0.01865152]
 [-0.06345287]
 [ 0.07184525]
 [-0.0252724 ]]
Y^ -  the output is:
 [[0.50466274]
 [0.4841421 ]
 [0.51795359]
 [0.49368224]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50466274]
 [0.4841421 ]
 [0.51795359]
 [0.49368224]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12615471]
 [-0.12883475]
 [-0.12035622]
 [ 0.12340085]]
W2.T is
 [[0.18518428 1.08552907]]
 D_Error times W2.T
 [[ 0.02336187  0.13694461]
 [-0.02385817 -0.13985387]
 [-0.02228808 -0.13065018]
 [ 0.0228519   0.13395521]]
self.H_D_Error_W2 is
 [[ 0.00165396  0.01964392]
 [-0.00036616 -0.01351207]
 [-0.0012534  -0.02281447]
 [ 0.00027065  0.01652556]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00165396  0.01964392]
 [-0.00036616 -0.01351207]
 [-0.0012534  -0.02281447]
 [ 0.00027065  0.01652556]]
X_H_D_Error_W2 is
 [[-9.82750902e-04 -6.28891057e-03]
 [-9.55092847e-05  3.01349051e-03]]
Using sum gradient........

W1 was :
 [[-0.26645585  0.32634458]
 [-1.65700604 -0.54711069]]
Updated W1 is: 
 [[-0.2654731   0.33263349]
 [-1.65691053 -0.55012418]]
W2 was :
 [[0.18518428]
 [1.08552907]]
Updated W2 is: 
 [[0.18323948]
 [1.08693928]]
The mean of the biases b gradient is:
 [ 7.62627456e-05 -3.92650424e-05]
The b biases before the update are:
 [[-2.48837989 -1.56052545]]
The new updated bs are:
 [[-2.48845615 -1.56048618]]
The bias c is: 
 [[ 0.12615471]
 [-0.12883475]
 [-0.12035622]
 [ 0.12340085]]
c bias before: [[-0.18396447]]
The mean c bias after update: [[-0.18405562]]
The output is: 
 [[0.50466274]
 [0.4841421 ]
 [0.51795359]
 [0.49368224]]
Total Loss: 0.49844237446199446
Average Loss: 0.12461059361549862

RUN:
  53
FeedForward


Z1 is:
 [[-2.48845615 -1.56048618]
 [-4.14536669 -2.11061036]
 [-2.75392925 -1.22785269]
 [-4.41083978 -1.77797687]]
H is:
 [[0.07667142 0.17357689]
 [0.01559071 0.10806982]
 [0.05986512 0.22655748]
 [0.01199924 0.14455313]]
The c is
 [[-0.18405562]]
Z2 is:
 [[ 0.01866115]
 [-0.06373346]
 [ 0.07316825]
 [-0.02473641]]
Y^ -  the output is:
 [[0.50466515]
 [0.48407203]
 [0.51828391]
 [0.49381621]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50466515]
 [0.48407203]
 [0.51828391]
 [0.49381621]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.1261553 ]
 [-0.1288511 ]
 [-0.12026799]
 [ 0.12343517]]
W2.T is
 [[0.18323948 1.08693928]]
 D_Error times W2.T
 [[ 0.02311663  0.13712316]
 [-0.02361061 -0.14005332]
 [-0.02203784 -0.130724  ]
 [ 0.0226182   0.13416653]]
self.H_D_Error_W2 is
 [[ 0.00163649  0.01967004]
 [-0.00036237 -0.01349984]
 [-0.00124032 -0.02290666]
 [ 0.00026814  0.0165907 ]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00163649  0.01967004]
 [-0.00036237 -0.01349984]
 [-0.00124032 -0.02290666]
 [ 0.00026814  0.0165907 ]]
X_H_D_Error_W2 is
 [[-9.72173580e-04 -6.31595842e-03]
 [-9.42224094e-05  3.09085852e-03]]
Using sum gradient........

W1 was :
 [[-0.2654731   0.33263349]
 [-1.65691053 -0.55012418]]
Updated W1 is: 
 [[-0.26450092  0.33894945]
 [-1.65681631 -0.55321504]]
W2 was :
 [[0.18323948]
 [1.08693928]]
Updated W2 is: 
 [[0.18129458]
 [1.08837122]]
The mean of the biases b gradient is:
 [ 7.54882663e-05 -3.64411602e-05]
The b biases before the update are:
 [[-2.48845615 -1.56048618]]
The new updated bs are:
 [[-2.48853164 -1.56044974]]
The bias c is: 
 [[ 0.1261553 ]
 [-0.1288511 ]
 [-0.12026799]
 [ 0.12343517]]
c bias before: [[-0.18405562]]
The mean c bias after update: [[-0.18417347]]
The output is: 
 [[0.50466515]
 [0.48407203]
 [0.51828391]
 [0.49381621]]
Total Loss: 0.4983867180143682
Average Loss: 0.12459667950359204

RUN:
  54
FeedForward


Z1 is:
 [[-2.48853164 -1.56044974]
 [-4.14534795 -2.11366478]
 [-2.75303256 -1.22150029]
 [-4.40984887 -1.77471533]]
H is:
 [[0.07666608 0.17358212]
 [0.015591   0.10777575]
 [0.05991561 0.22767253]
 [0.012011   0.14495691]]
The c is
 [[-0.18417347]]
Z2 is:
 [[ 0.01864746]
 [-0.06404688]
 [ 0.07448114]
 [-0.02422901]]
Y^ -  the output is:
 [[0.50466173]
 [0.48399375]
 [0.51861168]
 [0.49394304]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50466173]
 [0.48399375]
 [0.51861168]
 [0.49394304]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12615447]
 [-0.12886936]
 [-0.12018033]
 [ 0.12346764]]
W2.T is
 [[0.18129458 1.08837122]]
 D_Error times W2.T
 [[ 0.02287112  0.13730289]
 [-0.02336332 -0.1402577 ]
 [-0.02178804 -0.13080081]
 [ 0.02238401  0.13437863]]
self.H_D_Error_W2 is
 [[ 0.00161901  0.01969629]
 [-0.00035858 -0.0134872 ]
 [-0.00122723 -0.02299972]
 [ 0.00026563  0.01665548]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00161901  0.01969629]
 [-0.00035858 -0.0134872 ]
 [-0.00122723 -0.02299972]
 [ 0.00026563  0.01665548]]
X_H_D_Error_W2 is
 [[-9.61602257e-04 -6.34424182e-03]
 [-9.29530955e-05  3.16827819e-03]]
Using sum gradient........

W1 was :
 [[-0.26450092  0.33894945]
 [-1.65681631 -0.55321504]]
Updated W1 is: 
 [[-0.26353932  0.34529369]
 [-1.65672336 -0.55638332]]
W2 was :
 [[0.18129458]
 [1.08837122]]
Updated W2 is: 
 [[0.17934972]
 [1.08982632]]
The mean of the biases b gradient is:
 [ 7.47073216e-05 -3.37887599e-05]
The b biases before the update are:
 [[-2.48853164 -1.56044974]]
The new updated bs are:
 [[-2.48860635 -1.56041595]]
The bias c is: 
 [[ 0.12615447]
 [-0.12886936]
 [-0.12018033]
 [ 0.12346764]]
c bias before: [[-0.18417347]]
The mean c bias after update: [[-0.18431657]]
The output is: 
 [[0.50466173]
 [0.48399375]
 [0.51861168]
 [0.49394304]]
Total Loss: 0.49833017650549527
Average Loss: 0.12458254412637382

RUN:
  55
FeedForward


Z1 is:
 [[-2.48860635 -1.56041595]
 [-4.14532971 -2.11679927]
 [-2.75214567 -1.21512226]
 [-4.40886902 -1.77150558]]
H is:
 [[0.07666079 0.17358697]
 [0.01559128 0.10747471]
 [0.05996559 0.22879598]
 [0.01202263 0.1453552 ]]
The c is
 [[-0.18431657]]
Z2 is:
 [[ 0.01861216]
 [-0.06439151]
 [ 0.07578612]
 [-0.0237484 ]]
Y^ -  the output is:
 [[0.50465291]
 [0.48390768]
 [0.51893747]
 [0.49406318]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50465291]
 [0.48390768]
 [0.51893747]
 [0.49406318]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.1261523 ]
 [-0.12888943]
 [-0.12009311]
 [ 0.12349838]]
W2.T is
 [[0.17934972 1.08982632]]
 D_Error times W2.T
 [[ 0.02262538  0.1374841 ]
 [-0.02311628 -0.14046709]
 [-0.02153867 -0.13088063]
 [ 0.0221494   0.13459179]]
self.H_D_Error_W2 is
 [[ 0.00160151  0.01972272]
 [-0.00035479 -0.01347415]
 [-0.00121413 -0.02309368]
 [ 0.00026309  0.01671994]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00160151  0.01972272]
 [-0.00035479 -0.01347415]
 [-0.00121413 -0.02309368]
 [ 0.00026309  0.01671994]]
X_H_D_Error_W2 is
 [[-9.51035927e-04 -6.37373348e-03]
 [-9.17005454e-05  3.24579104e-03]]
Using sum gradient........

W1 was :
 [[-0.26353932  0.34529369]
 [-1.65672336 -0.55638332]]
Updated W1 is: 
 [[-0.26258828  0.35166742]
 [-1.65663165 -0.55962911]]
W2 was :
 [[0.17934972]
 [1.08982632]]
Updated W2 is: 
 [[0.17740501]
 [1.09130597]]
The mean of the biases b gradient is:
 [ 7.39209758e-05 -3.12918925e-05]
The b biases before the update are:
 [[-2.48860635 -1.56041595]]
The new updated bs are:
 [[-2.48868027 -1.56038466]]
The bias c is: 
 [[ 0.1261523 ]
 [-0.12888943]
 [-0.12009311]
 [ 0.12349838]]
c bias before: [[-0.18431657]]
The mean c bias after update: [[-0.18448361]]
The output is: 
 [[0.50465291]
 [0.48390768]
 [0.51893747]
 [0.49406318]]
Total Loss: 0.49827271163258374
Average Loss: 0.12456817790814594

RUN:
  56
FeedForward


Z1 is:
 [[-2.48868027 -1.56038466]
 [-4.14531193 -2.12001377]
 [-2.75126855 -1.20871724]
 [-4.40790021 -1.76834635]]
H is:
 [[0.07665555 0.17359146]
 [0.01559155 0.10716675]
 [0.06001505 0.2299281 ]
 [0.01203414 0.1457481 ]]
The c is
 [[-0.18448361]]
Z2 is:
 [[ 0.01855687]
 [-0.06476587]
 [ 0.07708527]
 [-0.02329292]]
Y^ -  the output is:
 [[0.50463908]
 [0.48381419]
 [0.51926178]
 [0.49417703]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50463908]
 [0.48381419]
 [0.51926178]
 [0.49417703]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12614891]
 [-0.12891122]
 [-0.12000619]
 [ 0.1235275 ]]
W2.T is
 [[0.17740501 1.09130597]]
 D_Error times W2.T
 [[ 0.02237945  0.13766706]
 [-0.0228695  -0.14068159]
 [-0.0212897  -0.13096348]
 [ 0.0219144   0.1348063 ]]
self.H_D_Error_W2 is
 [[ 0.00158401  0.01974937]
 [-0.00035101 -0.0134607 ]
 [-0.00120102 -0.02318855]
 [ 0.00026055  0.01678414]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00158401  0.01974937]
 [-0.00035101 -0.0134607 ]
 [-0.00120102 -0.02318855]
 [ 0.00026055  0.01678414]]
X_H_D_Error_W2 is
 [[-9.40473640e-04 -6.40440804e-03]
 [-9.04640095e-05  3.32343677e-03]]
Using sum gradient........

W1 was :
 [[-0.26258828  0.35166742]
 [-1.65663165 -0.55962911]]
Updated W1 is: 
 [[-0.26164781  0.35807183]
 [-1.65654119 -0.56295254]]
W2 was :
 [[0.17740501]
 [1.09130597]]
Updated W2 is: 
 [[0.17546055]
 [1.09281149]]
The mean of the biases b gradient is:
 [ 7.31301969e-05 -2.89355272e-05]
The b biases before the update are:
 [[-2.48868027 -1.56038466]]
The new updated bs are:
 [[-2.4887534  -1.56035573]]
The bias c is: 
 [[ 0.12614891]
 [-0.12891122]
 [-0.12000619]
 [ 0.1235275 ]]
c bias before: [[-0.18448361]]
The mean c bias after update: [[-0.18467336]]
The output is: 
 [[0.50463908]
 [0.48381419]
 [0.51926178]
 [0.49417703]]
Total Loss: 0.49821428551120756
Average Loss: 0.12455357137780189

RUN:
  57
FeedForward


Z1 is:
 [[-2.4887534  -1.56035573]
 [-4.14529459 -2.12330827]
 [-2.75040121 -1.2022839 ]
 [-4.4069424  -1.76523644]]
H is:
 [[0.07665038 0.17359561]
 [0.01559181 0.10685194]
 [0.060064   0.23106917]
 [0.01204554 0.14613572]]
The c is
 [[-0.18467336]]
Z2 is:
 [[ 0.01848304]
 [-0.06516859]
 [ 0.07838055]
 [-0.02286104]]
Y^ -  the output is:
 [[0.50462063]
 [0.48371362]
 [0.51958511]
 [0.49428499]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50462063]
 [0.48371362]
 [0.51958511]
 [0.49428499]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12614438]
 [-0.12893465]
 [-0.11991945]
 [ 0.1235551 ]]
W2.T is
 [[0.17546055 1.09281149]]
 D_Error times W2.T
 [[ 0.02213336  0.13785203]
 [-0.02262295 -0.14090127]
 [-0.02104113 -0.13104935]
 [ 0.02167905  0.13502244]]
self.H_D_Error_W2 is
 [[ 0.00156649  0.01977628]
 [-0.00034723 -0.01344686]
 [-0.0011879  -0.02328435]
 [ 0.00025799  0.01684811]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00156649  0.01977628]
 [-0.00034723 -0.01344686]
 [-0.0011879  -0.02328435]
 [ 0.00025799  0.01684811]]
X_H_D_Error_W2 is
 [[-9.29914500e-04 -6.43624194e-03]
 [-8.92427829e-05  3.40125341e-03]]
Using sum gradient........

W1 was :
 [[-0.26164781  0.35807183]
 [-1.65654119 -0.56295254]]
Updated W1 is: 
 [[-0.2607179   0.36450807]
 [-1.65645195 -0.5663538 ]]
W2 was :
 [[0.17546055]
 [1.09281149]]
Updated W2 is: 
 [[0.17351642]
 [1.09434417]]
The mean of the biases b gradient is:
 [ 7.23358639e-05 -2.67054852e-05]
The b biases before the update are:
 [[-2.4887534  -1.56035573]]
The new updated bs are:
 [[-2.48882574 -1.56032902]]
The bias c is: 
 [[ 0.12614438]
 [-0.12893465]
 [-0.11991945]
 [ 0.1235551 ]]
c bias before: [[-0.18467336]]
The mean c bias after update: [[-0.18488471]]
The output is: 
 [[0.50462063]
 [0.48371362]
 [0.51958511]
 [0.49428499]]
Total Loss: 0.4981548605058267
Average Loss: 0.12453871512645667

RUN:
  58
FeedForward


Z1 is:
 [[-2.48882574 -1.56032902]
 [-4.14527768 -2.12668282]
 [-2.74954363 -1.19582095]
 [-4.40599558 -1.76217475]]
H is:
 [[0.07664526 0.17359944]
 [0.01559207 0.10653031]
 [0.06011243 0.23221948]
 [0.01205681 0.14651818]]
The c is
 [[-0.18488471]]
Z2 is:
 [[ 0.01839204]
 [-0.0655984 ]
 [ 0.07967382]
 [-0.02245134]]
Y^ -  the output is:
 [[0.50459788]
 [0.48360628]
 [0.51990793]
 [0.4943874 ]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50459788]
 [0.48360628]
 [0.51990793]
 [0.4943874 ]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.1261388 ]
 [-0.12895965]
 [-0.11983275]
 [ 0.12358128]]
W2.T is
 [[0.17351642 1.09434417]]
 D_Error times W2.T
 [[ 0.02188715  0.13803926]
 [-0.02237662 -0.14112624]
 [-0.02079295 -0.13113827]
 [ 0.02144338  0.13524045]]
self.H_D_Error_W2 is
 [[ 0.00154897  0.01980348]
 [-0.00034346 -0.01343262]
 [-0.00117478 -0.02338111]
 [ 0.00025542  0.0169119 ]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00154897  0.01980348]
 [-0.00034346 -0.01343262]
 [-0.00117478 -0.02338111]
 [ 0.00025542  0.0169119 ]]
X_H_D_Error_W2 is
 [[-9.19357658e-04 -6.46921334e-03]
 [-8.80362015e-05  3.47927745e-03]]
Using sum gradient........

W1 was :
 [[-0.2607179   0.36450807]
 [-1.65645195 -0.5663538 ]]
Updated W1 is: 
 [[-0.25979854  0.37097729]
 [-1.65636391 -0.56983308]]
W2 was :
 [[0.17351642]
 [1.09434417]]
Updated W2 is: 
 [[0.17157267]
 [1.09590525]]
The mean of the biases b gradient is:
 [ 7.1538773e-05 -2.4588377e-05]
The b biases before the update are:
 [[-2.48882574 -1.56032902]]
The new updated bs are:
 [[-2.48889728 -1.56030443]]
The bias c is: 
 [[ 0.1261388 ]
 [-0.12895965]
 [-0.11983275]
 [ 0.12358128]]
c bias before: [[-0.18488471]]
The mean c bias after update: [[-0.18511663]]
The output is: 
 [[0.50459788]
 [0.48360628]
 [0.51990793]
 [0.4943874 ]]
Total Loss: 0.49809439908050823
Average Loss: 0.12452359977012706

RUN:
  59
FeedForward


Z1 is:
 [[-2.48889728 -1.56030443]
 [-4.14526119 -2.13013751]
 [-2.74869581 -1.18932715]
 [-4.40505973 -1.75916022]]
H is:
 [[0.0766402  0.17360297]
 [0.01559233 0.10620194]
 [0.06016035 0.2333793 ]
 [0.01206796 0.14689555]]
The c is
 [[-0.18511663]]
Z2 is:
 [[ 0.01828514]
 [-0.06605415]
 [ 0.08096684]
 [-0.02206249]]
Y^ -  the output is:
 [[0.50457116]
 [0.48349246]
 [0.52023066]
 [0.4944846 ]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50457116]
 [0.48349246]
 [0.52023066]
 [0.4944846 ]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12613225]
 [-0.12898614]
 [-0.11974598]
 [ 0.12360611]]
W2.T is
 [[0.17157267 1.09590525]]
 D_Error times W2.T
 [[ 0.02164085  0.13822899]
 [-0.0221305  -0.14135658]
 [-0.02054514 -0.13123024]
 [ 0.02120743  0.13546058]]
self.H_D_Error_W2 is
 [[ 0.00153145  0.01983102]
 [-0.00033969 -0.013418  ]
 [-0.00116164 -0.02347885]
 [ 0.00025284  0.01697555]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00153145  0.01983102]
 [-0.00033969 -0.013418  ]
 [-0.00116164 -0.02347885]
 [ 0.00025284  0.01697555]]
X_H_D_Error_W2 is
 [[-9.08802312e-04 -6.50330194e-03]
 [-8.68436391e-05  3.55754388e-03]]
Using sum gradient........

W1 was :
 [[-0.25979854  0.37097729]
 [-1.65636391 -0.56983308]]
Updated W1 is: 
 [[-0.25888974  0.37748059]
 [-1.65627707 -0.57339062]]
W2 was :
 [[0.17157267]
 [1.09590525]]
Updated W2 is: 
 [[0.16962935]
 [1.09749594]]
The mean of the biases b gradient is:
 [ 7.07396442e-05 -2.25715437e-05]
The b biases before the update are:
 [[-2.48889728 -1.56030443]]
The new updated bs are:
 [[-2.48896802 -1.56028186]]
The bias c is: 
 [[ 0.12613225]
 [-0.12898614]
 [-0.11974598]
 [ 0.12360611]]
c bias before: [[-0.18511663]]
The mean c bias after update: [[-0.18536819]]
The output is: 
 [[0.50457116]
 [0.48349246]
 [0.52023066]
 [0.4944846 ]]
Total Loss: 0.4980328636670349
Average Loss: 0.12450821591675873

RUN:
  60
FeedForward


Z1 is:
 [[-2.48896802 -1.56028186]
 [-4.14524508 -2.13367248]
 [-2.74785775 -1.18280127]
 [-4.40413482 -1.75619189]]
H is:
 [[0.07663519 0.17360621]
 [0.01559257 0.10586685]
 [0.06020775 0.23454889]
 [0.01207899 0.14726792]]
The c is
 [[-0.18536819]]
Z2 is:
 [[ 0.0181635 ]
 [-0.06653479]
 [ 0.08226127]
 [-0.02169329]]
Y^ -  the output is:
 [[0.50454075]
 [0.48337244]
 [0.52055373]
 [0.49457689]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50454075]
 [0.48337244]
 [0.52055373]
 [0.49457689]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12612478]
 [-0.12901406]
 [-0.11965902]
 [ 0.12362968]]
W2.T is
 [[0.16962935 1.09749594]]
 D_Error times W2.T
 [[ 0.02139446  0.13842144]
 [-0.02188457 -0.1415924 ]
 [-0.02029768 -0.13132529]
 [ 0.02097122  0.13568307]]
self.H_D_Error_W2 is
 [[ 0.00151392  0.01985892]
 [-0.00033592 -0.013403  ]
 [-0.0011485  -0.02357758]
 [ 0.00025025  0.01703909]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00151392  0.01985892]
 [-0.00033592 -0.013403  ]
 [-0.0011485  -0.02357758]
 [ 0.00025025  0.01703909]]
X_H_D_Error_W2 is
 [[-8.98247698e-04 -6.53848897e-03]
 [-8.56645041e-05  3.63608637e-03]]
Using sum gradient........

W1 was :
 [[-0.25888974  0.37748059]
 [-1.65627707 -0.57339062]]
Updated W1 is: 
 [[-0.25799149  0.38401908]
 [-1.6561914  -0.57702671]]
W2 was :
 [[0.16962935]
 [1.09749594]]
Updated W2 is: 
 [[0.16768649]
 [1.09911742]]
The mean of the biases b gradient is:
 [ 6.99391264e-05 -2.06430010e-05]
The b biases before the update are:
 [[-2.48896802 -1.56028186]]
The new updated bs are:
 [[-2.48903795 -1.56026122]]
The bias c is: 
 [[ 0.12612478]
 [-0.12901406]
 [-0.11965902]
 [ 0.12362968]]
c bias before: [[-0.18536819]]
The mean c bias after update: [[-0.18563853]]
The output is: 
 [[0.50454075]
 [0.48337244]
 [0.52055373]
 [0.49457689]]
Total Loss: 0.49797021654797674
Average Loss: 0.12449255413699419

RUN:
  61
FeedForward


Z1 is:
 [[-2.48903795 -1.56026122]
 [-4.14522936 -2.13728792]
 [-2.74702944 -1.17624214]
 [-4.40322085 -1.75326885]]
H is:
 [[0.07663024 0.17360917]
 [0.01559282 0.10552511]
 [0.06025464 0.23572854]
 [0.01208991 0.14763538]]
The c is
 [[-0.18563853]]
Z2 is:
 [[ 0.01802818]
 [-0.06703935]
 [ 0.0835587 ]
 [-0.02134261]]
Y^ -  the output is:
 [[0.50450692]
 [0.48324644]
 [0.52087753]
 [0.49466455]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50450692]
 [0.48324644]
 [0.52087753]
 [0.49466455]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12611648]
 [-0.12904335]
 [-0.11957178]
 [ 0.12365206]]
W2.T is
 [[0.16768649 1.09911742]]
 D_Error times W2.T
 [[ 0.02114803  0.13861682]
 [-0.02163883 -0.14183379]
 [-0.02005057 -0.13142343]
 [ 0.02073478  0.13590813]]
self.H_D_Error_W2 is
 [[ 0.00149639  0.01988722]
 [-0.00033215 -0.01338763]
 [-0.00113534 -0.02367732]
 [ 0.00024765  0.01710257]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00149639  0.01988722]
 [-0.00033215 -0.01338763]
 [-0.00113534 -0.02367732]
 [ 0.00024765  0.01710257]]
X_H_D_Error_W2 is
 [[-8.87693093e-04 -6.57475696e-03]
 [-8.44982373e-05  3.71493728e-03]]
Using sum gradient........

W1 was :
 [[-0.25799149  0.38401908]
 [-1.6561914  -0.57702671]]
Updated W1 is: 
 [[-0.25710379  0.39059383]
 [-1.65610691 -0.58074164]]
W2 was :
 [[0.16768649]
 [1.09911742]]
Updated W2 is: 
 [[0.16574411]
 [1.10077082]]
The mean of the biases b gradient is:
 [ 6.91378030e-05 -1.87913868e-05]
The b biases before the update are:
 [[-2.48903795 -1.56026122]]
The new updated bs are:
 [[-2.48910709 -1.56024243]]
The bias c is: 
 [[ 0.12611648]
 [-0.12904335]
 [-0.11957178]
 [ 0.12365206]]
c bias before: [[-0.18563853]]
The mean c bias after update: [[-0.18592689]]
The output is: 
 [[0.50450692]
 [0.48324644]
 [0.52087753]
 [0.49466455]]
Total Loss: 0.4979064197526319
Average Loss: 0.12447660493815797

RUN:
  62
FeedForward


Z1 is:
 [[-2.48910709 -1.56024243]
 [-4.145214   -2.14098407]
 [-2.74621089 -1.16964859]
 [-4.40231779 -1.75039024]]
H is:
 [[0.07662535 0.17361186]
 [0.01559305 0.10517674]
 [0.060301   0.23691851]
 [0.0121007  0.14799798]]
The c is
 [[-0.18592689]]
Z2 is:
 [[ 0.01788019]
 [-0.06756695]
 [ 0.08486063]
 [-0.0210094 ]]
Y^ -  the output is:
 [[0.50446993]
 [0.48311469]
 [0.52120244]
 [0.49474784]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50446993]
 [0.48311469]
 [0.52120244]
 [0.49474784]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.1261074 ]
 [-0.12907396]
 [-0.11948415]
 [ 0.12367331]]
W2.T is
 [[0.16574411 1.10077082]]
 D_Error times W2.T
 [[ 0.02090156  0.13881535]
 [-0.02139325 -0.14208085]
 [-0.01980379 -0.13152467]
 [ 0.02049812  0.13613597]]
self.H_D_Error_W2 is
 [[ 0.00147887  0.01991595]
 [-0.00032838 -0.01337188]
 [-0.00112218 -0.0237781 ]
 [ 0.00024504  0.01716601]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00147887  0.01991595]
 [-0.00032838 -0.01337188]
 [-0.00112218 -0.0237781 ]
 [ 0.00024504  0.01716601]]
X_H_D_Error_W2 is
 [[-8.77137807e-04 -6.61208977e-03]
 [-8.33443085e-05  3.79412781e-03]]
Using sum gradient........

W1 was :
 [[-0.25710379  0.39059383]
 [-1.65610691 -0.58074164]]
Updated W1 is: 
 [[-0.25622666  0.39720592]
 [-1.65602356 -0.58453577]]
W2 was :
 [[0.16574411]
 [1.10077082]]
Updated W2 is: 
 [[0.16380223]
 [1.10245726]]
The mean of the biases b gradient is:
 [ 6.83361967e-05 -1.70059110e-05]
The b biases before the update are:
 [[-2.48910709 -1.56024243]]
The new updated bs are:
 [[-2.48917543 -1.56022542]]
The bias c is: 
 [[ 0.1261074 ]
 [-0.12907396]
 [-0.11948415]
 [ 0.12367331]]
c bias before: [[-0.18592689]]
The mean c bias after update: [[-0.18623254]]
The output is: 
 [[0.50446993]
 [0.48311469]
 [0.52120244]
 [0.49474784]]
Total Loss: 0.4978414349640269
Average Loss: 0.12446035874100672

RUN:
  63
FeedForward


Z1 is:
 [[-2.48917543 -1.56022542]
 [-4.14519899 -2.14476119]
 [-2.74540209 -1.1630195 ]
 [-4.40142565 -1.74755527]]
H is:
 [[0.07662052 0.1736143 ]
 [0.01559328 0.10482179]
 [0.06034685 0.23811906]
 [0.01211137 0.14835582]]
The c is
 [[-0.18623254]]
Z2 is:
 [[ 0.01772042]
 [-0.06811678]
 [ 0.0861685 ]
 [-0.02069272]]
Y^ -  the output is:
 [[0.50442999]
 [0.48297739]
 [0.5215288 ]
 [0.494827  ]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50442999]
 [0.48297739]
 [0.5215288 ]
 [0.494827  ]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.1260976 ]
 [-0.12910584]
 [-0.11939603]
 [ 0.12369351]]
W2.T is
 [[0.16380223 1.10245726]]
 D_Error times W2.T
 [[ 0.02065507  0.13901721]
 [-0.02114782 -0.14233367]
 [-0.01955734 -0.13162902]
 [ 0.02026127  0.13636681]]
self.H_D_Error_W2 is
 [[ 0.00146134  0.01994513]
 [-0.00032462 -0.01335576]
 [-0.001109   -0.02387992]
 [ 0.00024242  0.01722945]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00146134  0.01994513]
 [-0.00032462 -0.01335576]
 [-0.001109   -0.02387992]
 [ 0.00024242  0.01722945]]
X_H_D_Error_W2 is
 [[-8.66581183e-04 -6.65047238e-03]
 [-8.22022149e-05  3.87368805e-03]]
Using sum gradient........

W1 was :
 [[-0.25622666  0.39720592]
 [-1.65602356 -0.58453577]]
Updated W1 is: 
 [[-0.25536008  0.4038564 ]
 [-1.65594136 -0.58840946]]
W2 was :
 [[0.16380223]
 [1.10245726]]
Updated W2 is: 
 [[0.16186083]
 [1.10417784]]
The mean of the biases b gradient is:
 [ 6.75347735e-05 -1.52763092e-05]
The b biases before the update are:
 [[-2.48917543 -1.56022542]]
The new updated bs are:
 [[-2.48924296 -1.56021014]]
The bias c is: 
 [[ 0.1260976 ]
 [-0.12910584]
 [-0.11939603]
 [ 0.12369351]]
c bias before: [[-0.18623254]]
The mean c bias after update: [[-0.18655485]]
The output is: 
 [[0.50442999]
 [0.48297739]
 [0.5215288 ]
 [0.494827  ]]
Total Loss: 0.4977752234354218
Average Loss: 0.12444380585885545

RUN:
  64
FeedForward


Z1 is:
 [[-2.48924296 -1.56021014]
 [-4.14518432 -2.1486196 ]
 [-2.74460304 -1.15635375]
 [-4.4005444  -1.74476321]]
H is:
 [[0.07661574 0.17361649]
 [0.01559351 0.10446029]
 [0.06039217 0.23933046]
 [0.01212191 0.14870893]]
The c is
 [[-0.18655485]]
Z2 is:
 [[ 0.01754972]
 [-0.06868814]
 [ 0.08748367]
 [-0.02039168]]
Y^ -  the output is:
 [[0.50438732]
 [0.48283471]
 [0.52185698]
 [0.49490226]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50438732]
 [0.48283471]
 [0.52185698]
 [0.49490226]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12608712]
 [-0.12913894]
 [-0.11930733]
 [ 0.1237127 ]]
W2.T is
 [[0.16186083 1.10417784]]
 D_Error times W2.T
 [[ 0.02040857  0.1392226 ]
 [-0.02090254 -0.14259236]
 [-0.01931118 -0.13173651]
 [ 0.02002424  0.13660082]]
self.H_D_Error_W2 is
 [[ 0.00144382  0.0199748 ]
 [-0.00032086 -0.01333928]
 [-0.00109581 -0.02398282]
 [ 0.00023979  0.01729292]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00144382  0.0199748 ]
 [-0.00032086 -0.01333928]
 [-0.00109581 -0.02398282]
 [ 0.00023979  0.01729292]]
X_H_D_Error_W2 is
 [[-8.56022594e-04 -6.68989089e-03]
 [-8.10714782e-05  3.95364705e-03]]
Using sum gradient........

W1 was :
 [[-0.25536008  0.4038564 ]
 [-1.65594136 -0.58840946]]
Updated W1 is: 
 [[-0.25450405  0.41054629]
 [-1.65586029 -0.59236311]]
W2 was :
 [[0.16186083]
 [1.10417784]]
Updated W2 is: 
 [[0.15991989]
 [1.10593362]]
The mean of the biases b gradient is:
 [ 6.67339475e-05 -1.35927978e-05]
The b biases before the update are:
 [[-2.48924296 -1.56021014]]
The new updated bs are:
 [[-2.4893097  -1.56019655]]
The bias c is: 
 [[ 0.12608712]
 [-0.12913894]
 [-0.11930733]
 [ 0.1237127 ]]
c bias before: [[-0.18655485]]
The mean c bias after update: [[-0.18689323]]
The output is: 
 [[0.50438732]
 [0.48283471]
 [0.52185698]
 [0.49490226]]
Total Loss: 0.4977077459149743
Average Loss: 0.12442693647874357

RUN:
  65
FeedForward


Z1 is:
 [[-2.4893097  -1.56019655]
 [-4.14516998 -2.15255966]
 [-2.74381375 -1.14965026]
 [-4.39967404 -1.74201337]]
H is:
 [[0.07661102 0.17361844]
 [0.01559373 0.10409227]
 [0.06043698 0.24055297]
 [0.01213234 0.14905738]]
The c is
 [[-0.18689323]]
Z2 is:
 [[ 0.01736887]
 [-0.06928034]
 [ 0.08880746]
 [-0.02010546]]
Y^ -  the output is:
 [[0.50434211]
 [0.48268684]
 [0.52218728]
 [0.4949738 ]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50434211]
 [0.48268684]
 [0.52218728]
 [0.4949738 ]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12607602]
 [-0.12917323]
 [-0.11921796]
 [ 0.12373095]]
W2.T is
 [[0.15991989 1.10593362]]
 D_Error times W2.T
 [[ 0.02016206  0.13943171]
 [-0.02065737 -0.14285702]
 [-0.01906532 -0.13184715]
 [ 0.01978704  0.13683821]]
self.H_D_Error_W2 is
 [[ 0.0014263   0.02000498]
 [-0.0003171  -0.01332243]
 [-0.00108261 -0.02408679]
 [ 0.00023715  0.01735646]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.0014263   0.02000498]
 [-0.0003171  -0.01332243]
 [-0.00108261 -0.02408679]
 [ 0.00023715  0.01735646]]
X_H_D_Error_W2 is
 [[-8.45461440e-04 -6.73033235e-03]
 [-7.99516433e-05  4.03403292e-03]]
Using sum gradient........

W1 was :
 [[-0.25450405  0.41054629]
 [-1.65586029 -0.59236311]]
Updated W1 is: 
 [[-0.25365859  0.41727662]
 [-1.65578034 -0.59639714]]
W2 was :
 [[0.15991989]
 [1.10593362]]
Updated W2 is: 
 [[0.1579794 ]
 [1.10772566]]
The mean of the biases b gradient is:
 [ 6.59340841e-05 -1.19460322e-05]
The b biases before the update are:
 [[-2.4893097  -1.56019655]]
The new updated bs are:
 [[-2.48937563 -1.56018461]]
The bias c is: 
 [[ 0.12607602]
 [-0.12917323]
 [-0.11921796]
 [ 0.12373095]]
c bias before: [[-0.18689323]]
The mean c bias after update: [[-0.18724718]]
The output is: 
 [[0.50434211]
 [0.48268684]
 [0.52218728]
 [0.4949738 ]]
Total Loss: 0.49763896257740864
Average Loss: 0.12440974064435216

RUN:
  66
FeedForward


Z1 is:
 [[-2.48937563 -1.56018461]
 [-4.14515597 -2.15658174]
 [-2.74303422 -1.14290799]
 [-4.39881456 -1.73930513]]
H is:
 [[0.07660635 0.17362016]
 [0.01559394 0.10371778]
 [0.06048126 0.24178685]
 [0.01214265 0.14940122]]
The c is
 [[-0.18724718]]
Z2 is:
 [[ 0.01717855]
 [-0.06989281]
 [ 0.09014111]
 [-0.01983333]]
Y^ -  the output is:
 [[0.50429453]
 [0.48253391]
 [0.52252003]
 [0.49504183]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50429453]
 [0.48253391]
 [0.52252003]
 [0.49504183]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12606433]
 [-0.12920866]
 [-0.11912784]
 [ 0.12374829]]
W2.T is
 [[0.1579794  1.10772566]]
 D_Error times W2.T
 [[ 0.01991557  0.1396447 ]
 [-0.02041231 -0.14312775]
 [-0.01881974 -0.13196096]
 [ 0.01954968  0.13707915]]
self.H_D_Error_W2 is
 [[ 0.00140878  0.02003569]
 [-0.00031334 -0.01330521]
 [-0.0010694  -0.02419187]
 [ 0.0002345   0.01742009]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00140878  0.02003569]
 [-0.00031334 -0.01330521]
 [-0.0010694  -0.02419187]
 [ 0.0002345   0.01742009]]
X_H_D_Error_W2 is
 [[-8.34897146e-04 -6.77178475e-03]
 [-7.88422755e-05  4.11487288e-03]]
Using sum gradient........

W1 was :
 [[-0.25365859  0.41727662]
 [-1.65578034 -0.59639714]]
Updated W1 is: 
 [[-0.25282369  0.4240484 ]
 [-1.65570149 -0.60051201]]
W2 was :
 [[0.1579794 ]
 [1.10772566]]
Updated W2 is: 
 [[0.15603931]
 [1.10955498]]
The mean of the biases b gradient is:
 [ 6.51355039e-05 -1.03270674e-05]
The b biases before the update are:
 [[-2.48937563 -1.56018461]]
The new updated bs are:
 [[-2.48944077 -1.56017428]]
The bias c is: 
 [[ 0.12606433]
 [-0.12920866]
 [-0.11912784]
 [ 0.12374829]]
c bias before: [[-0.18724718]]
The mean c bias after update: [[-0.18761621]]
The output is: 
 [[0.50429453]
 [0.48253391]
 [0.52252003]
 [0.49504183]]
Total Loss: 0.49756883296169513
Average Loss: 0.12439220824042378

RUN:
  67
FeedForward


Z1 is:
 [[-2.48944077 -1.56017428]
 [-4.14514226 -2.16068629]
 [-2.74226446 -1.13612587]
 [-4.39796595 -1.73663789]]
H is:
 [[0.07660174 0.17362164]
 [0.01559415 0.10333684]
 [0.06052501 0.24303237]
 [0.01215283 0.14974049]]
The c is
 [[-0.18761621]]
Z2 is:
 [[ 0.01697943]
 [-0.070525  ]
 [ 0.09148585]
 [-0.01957458]]
Y^ -  the output is:
 [[0.50424476]
 [0.48237605]
 [0.52285552]
 [0.49510651]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50424476]
 [0.48237605]
 [0.52285552]
 [0.49510651]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.1260521 ]
 [-0.12924521]
 [-0.11903687]
 [ 0.12376477]]
W2.T is
 [[0.15603931 1.10955498]]
 D_Error times W2.T
 [[ 0.01966908  0.13986174]
 [-0.02016733 -0.14340467]
 [-0.01857443 -0.13207795]
 [ 0.01931217  0.13732382]]
self.H_D_Error_W2 is
 [[ 0.00139127  0.02006697]
 [-0.00030959 -0.01328764]
 [-0.00105617 -0.02429807]
 [ 0.00023185  0.01748383]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00139127  0.02006697]
 [-0.00030959 -0.01328764]
 [-0.00105617 -0.02429807]
 [ 0.00023185  0.01748383]]
X_H_D_Error_W2 is
 [[-8.24329162e-04 -6.81423690e-03]
 [-7.77429596e-05  4.19619330e-03]]
Using sum gradient........

W1 was :
 [[-0.25282369  0.4240484 ]
 [-1.65570149 -0.60051201]]
Updated W1 is: 
 [[-0.25199937  0.43086264]
 [-1.65562375 -0.60470821]]
W2 was :
 [[0.15603931]
 [1.10955498]]
Updated W2 is: 
 [[0.15409959]
 [1.11142262]]
The mean of the biases b gradient is:
 [ 6.43384857e-05 -8.72732046e-06]
The b biases before the update are:
 [[-2.48944077 -1.56017428]]
The new updated bs are:
 [[-2.48950511 -1.56016555]]
The bias c is: 
 [[ 0.1260521 ]
 [-0.12924521]
 [-0.11903687]
 [ 0.12376477]]
c bias before: [[-0.18761621]]
The mean c bias after update: [[-0.18799991]]
The output is: 
 [[0.50424476]
 [0.48237605]
 [0.52285552]
 [0.49510651]]
Total Loss: 0.49749731591388596
Average Loss: 0.12437432897847149

RUN:
  68
FeedForward


Z1 is:
 [[-2.48950511 -1.56016555]
 [-4.14512886 -2.16487376]
 [-2.74150447 -1.12930291]
 [-4.39712822 -1.73401112]]
H is:
 [[0.07659719 0.17362289]
 [0.01559436 0.10294948]
 [0.06056824 0.24428977]
 [0.01216289 0.15007523]]
The c is
 [[-0.18799991]]
Z2 is:
 [[ 0.0167721 ]
 [-0.07117644]
 [ 0.09284281]
 [-0.0193286 ]]
Y^ -  the output is:
 [[0.50419293]
 [0.4822134 ]
 [0.52319404]
 [0.495168  ]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50419293]
 [0.4822134 ]
 [0.52319404]
 [0.495168  ]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12603937]
 [-0.12928284]
 [-0.11894498]
 [ 0.12378044]]
W2.T is
 [[0.15409959 1.11142262]]
 D_Error times W2.T
 [[ 0.01942261  0.140083  ]
 [-0.01992243 -0.14368787]
 [-0.01832937 -0.13219815]
 [ 0.01907451  0.13757238]]
self.H_D_Error_W2 is
 [[ 0.00137376  0.02009883]
 [-0.00030583 -0.0132697 ]
 [-0.00104294 -0.0244054 ]
 [ 0.00022918  0.01754772]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00137376  0.02009883]
 [-0.00030583 -0.0132697 ]
 [-0.00104294 -0.0244054 ]
 [ 0.00022918  0.01754772]]
X_H_D_Error_W2 is
 [[-8.13756957e-04 -6.85767837e-03]
 [-7.66532973e-05  4.27801980e-03]]
Using sum gradient........

W1 was :
 [[-0.25199937  0.43086264]
 [-1.65562375 -0.60470821]]
Updated W1 is: 
 [[-0.25118561  0.43772032]
 [-1.6555471  -0.60898623]]
W2 was :
 [[0.15409959]
 [1.11142262]]
Updated W2 is: 
 [[0.15216017]
 [1.11332956]]
The mean of the biases b gradient is:
 [ 6.35432696e-05 -7.13853530e-06]
The b biases before the update are:
 [[-2.48950511 -1.56016555]]
The new updated bs are:
 [[-2.48956865 -1.56015841]]
The bias c is: 
 [[ 0.12603937]
 [-0.12928284]
 [-0.11894498]
 [ 0.12378044]]
c bias before: [[-0.18799991]]
The mean c bias after update: [[-0.1883979]]
The output is: 
 [[0.50419293]
 [0.4822134 ]
 [0.52319404]
 [0.495168  ]]
Total Loss: 0.4974243695343738
Average Loss: 0.12435609238359345

RUN:
  69
FeedForward


Z1 is:
 [[-2.48956865 -1.56015841]
 [-4.14511575 -2.16914464]
 [-2.74075426 -1.12243809]
 [-4.39630135 -1.73142432]]
H is:
 [[0.0765927  0.17362392]
 [0.01559456 0.10255573]
 [0.06061094 0.24555932]
 [0.01217283 0.15040548]]
The c is
 [[-0.1883979]]
Z2 is:
 [[ 0.0165571 ]
 [-0.0718467 ]
 [ 0.09421312]
 [-0.01909481]]
Y^ -  the output is:
 [[0.50413918]
 [0.48204605]
 [0.52353587]
 [0.49522644]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50413918]
 [0.48204605]
 [0.52353587]
 [0.49522644]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12602616]
 [-0.12932153]
 [-0.1188521 ]
 [ 0.12379533]]
W2.T is
 [[0.15216017 1.11332956]]
 D_Error times W2.T
 [[ 0.01917616  0.14030865]
 [-0.01967759 -0.14397748]
 [-0.01808456 -0.13232156]
 [ 0.01883672  0.137825  ]]
self.H_D_Error_W2 is
 [[ 0.00135626  0.0201313 ]
 [-0.00030208 -0.01325141]
 [-0.00102968 -0.02451388]
 [ 0.0002265   0.01761178]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00135626  0.0201313 ]
 [-0.00030208 -0.01325141]
 [-0.00102968 -0.02451388]
 [ 0.0002265   0.01761178]]
X_H_D_Error_W2 is
 [[-8.03180023e-04 -6.90209937e-03]
 [-7.55729067e-05  4.36037728e-03]]
Using sum gradient........

W1 was :
 [[-0.25118561  0.43772032]
 [-1.6555471  -0.60898623]]
Updated W1 is: 
 [[-0.25038243  0.44462242]
 [-1.65547152 -0.6133466 ]]
W2 was :
 [[0.15216017]
 [1.11332956]]
Updated W2 is: 
 [[0.150221  ]
 [1.11527682]]
The mean of the biases b gradient is:
 [ 6.27500597e-05 -5.55274897e-06]
The b biases before the update are:
 [[-2.48956865 -1.56015841]]
The new updated bs are:
 [[-2.4896314  -1.56015286]]
The bias c is: 
 [[ 0.12602616]
 [-0.12932153]
 [-0.1188521 ]
 [ 0.12379533]]
c bias before: [[-0.1883979]]
The mean c bias after update: [[-0.18880987]]
The output is: 
 [[0.50413918]
 [0.48204605]
 [0.52353587]
 [0.49522644]]
Total Loss: 0.4973499511289465
Average Loss: 0.12433748778223662

RUN:
  70
FeedForward


Z1 is:
 [[-2.4896314  -1.56015286]
 [-4.14510292 -2.17349946]
 [-2.74001383 -1.11553044]
 [-4.39548535 -1.72887704]]
H is:
 [[0.07658826 0.17362471]
 [0.01559476 0.10215562]
 [0.06065312 0.24684128]
 [0.01218265 0.15073127]]
The c is
 [[-0.18880987]]
Z2 is:
 [[ 0.01633492]
 [-0.07253541]
 [ 0.09559787]
 [-0.01887268]]
Y^ -  the output is:
 [[0.50408364]
 [0.48187409]
 [0.52388128]
 [0.49528197]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50408364]
 [0.48187409]
 [0.52388128]
 [0.49528197]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.1260125 ]
 [-0.12936125]
 [-0.11875814]
 [ 0.12380947]]
W2.T is
 [[0.150221   1.11527682]]
 D_Error times W2.T
 [[ 0.01892972  0.14053882]
 [-0.01943278 -0.1442736 ]
 [-0.01783997 -0.1324482 ]
 [ 0.01859878  0.13808183]]
self.H_D_Error_W2 is
 [[ 0.00133876  0.02016439]
 [-0.00029832 -0.01323275]
 [-0.00101642 -0.02462353]
 [ 0.00022382  0.01767604]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00133876  0.02016439]
 [-0.00029832 -0.01323275]
 [-0.00101642 -0.02462353]
 [ 0.00022382  0.01767604]]
X_H_D_Error_W2 is
 [[-7.92597870e-04 -6.94749077e-03]
 [-7.45014199e-05  4.44328995e-03]]
Using sum gradient........

W1 was :
 [[-0.25038243  0.44462242]
 [-1.65547152 -0.6133466 ]]
Updated W1 is: 
 [[-0.24958983  0.45156991]
 [-1.65539702 -0.61778989]]
W2 was :
 [[0.150221  ]
 [1.11527682]]
Updated W2 is: 
 [[0.148282  ]
 [1.11726536]]
The mean of the biases b gradient is:
 [ 6.19590266e-05 -3.96226020e-06]
The b biases before the update are:
 [[-2.4896314  -1.56015286]]
The new updated bs are:
 [[-2.48969336 -1.5601489 ]]
The bias c is: 
 [[ 0.1260125 ]
 [-0.12936125]
 [-0.11875814]
 [ 0.12380947]]
c bias before: [[-0.18880987]]
The mean c bias after update: [[-0.18923551]]
The output is: 
 [[0.50408364]
 [0.48187409]
 [0.52388128]
 [0.49528197]]
Total Loss: 0.4972740171631032
Average Loss: 0.1243185042907758

RUN:
  71
FeedForward


Z1 is:
 [[-2.48969336 -1.5601489 ]
 [-4.14509038 -2.17793879]
 [-2.73928319 -1.10857899]
 [-4.39468021 -1.72636888]]
H is:
 [[0.07658388 0.17362528]
 [0.01559495 0.10174916]
 [0.06069476 0.2481359 ]
 [0.01219234 0.15105263]]
The c is
 [[-0.18923551]]
Z2 is:
 [[ 0.01610601]
 [-0.07324225]
 [ 0.09699808]
 [-0.01866174]]
Y^ -  the output is:
 [[0.50402642]
 [0.48169762]
 [0.52423053]
 [0.4953347 ]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50402642]
 [0.48169762]
 [0.52423053]
 [0.4953347 ]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12599843]
 [-0.12940198]
 [-0.11866304]
 [ 0.12382289]]
W2.T is
 [[0.148282   1.11726536]]
 D_Error times W2.T
 [[ 0.0186833   0.14077368]
 [-0.01918798 -0.14457635]
 [-0.01759559 -0.1325781 ]
 [ 0.01836071  0.13834303]]
self.H_D_Error_W2 is
 [[ 0.00132126  0.02019814]
 [-0.00029457 -0.01321374]
 [-0.00100314 -0.02473436]
 [ 0.00022113  0.01774052]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00132126  0.02019814]
 [-0.00029457 -0.01321374]
 [-0.00100314 -0.02473436]
 [ 0.00022113  0.01774052]]
X_H_D_Error_W2 is
 [[-7.82010024e-04 -6.99384394e-03]
 [-7.34384825e-05  4.52678142e-03]]
Using sum gradient........

W1 was :
 [[-0.24958983  0.45156991]
 [-1.65539702 -0.61778989]]
Updated W1 is: 
 [[-0.24880782  0.45856375]
 [-1.65532358 -0.62231667]]
W2 was :
 [[0.148282  ]
 [1.11726536]]
Updated W2 is: 
 [[0.1463431 ]
 [1.11929618]]
The mean of the biases b gradient is:
 [ 6.11703098e-05 -2.35959935e-06]
The b biases before the update are:
 [[-2.48969336 -1.5601489 ]]
The new updated bs are:
 [[-2.48975453 -1.56014654]]
The bias c is: 
 [[ 0.12599843]
 [-0.12940198]
 [-0.11866304]
 [ 0.12382289]]
c bias before: [[-0.18923551]]
The mean c bias after update: [[-0.18967459]]
The output is: 
 [[0.50402642]
 [0.48169762]
 [0.52423053]
 [0.4953347 ]]
Total Loss: 0.49719652321917285
Average Loss: 0.12429913080479321

RUN:
  72
FeedForward


Z1 is:
 [[-2.48975453 -1.56014654]
 [-4.14507811 -2.18246321]
 [-2.73856235 -1.10158278]
 [-4.39388593 -1.72389946]]
H is:
 [[0.07657955 0.17362562]
 [0.01559514 0.10133639]
 [0.06073587 0.24944345]
 [0.01220191 0.15136957]]
The c is
 [[-0.18967459]]
Z2 is:
 [[ 0.01587079]
 [-0.07396692]
 [ 0.09841478]
 [-0.01846154]]
Y^ -  the output is:
 [[0.50396762]
 [0.4815167 ]
 [0.52458386]
 [0.49538475]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50396762]
 [0.4815167 ]
 [0.52458386]
 [0.49538475]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12598397]
 [-0.12944369]
 [-0.11856671]
 [ 0.12383563]]
W2.T is
 [[0.1463431  1.11929618]]
 D_Error times W2.T
 [[ 0.01843688  0.14101338]
 [-0.01894319 -0.14488583]
 [-0.01735142 -0.13271127]
 [ 0.01812249  0.13860875]]
self.H_D_Error_W2 is
 [[ 0.00130377  0.02023257]
 [-0.00029081 -0.01319437]
 [-0.00098985 -0.02484639]
 [ 0.00021843  0.01780524]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00130377  0.02023257]
 [-0.00029081 -0.01319437]
 [-0.00098985 -0.02484639]
 [ 0.00021843  0.01780524]]
X_H_D_Error_W2 is
 [[-7.71416029e-04 -7.04115074e-03]
 [-7.23837516e-05  4.61087471e-03]]
Using sum gradient........

W1 was :
 [[-0.24880782  0.45856375]
 [-1.65532358 -0.62231667]]
Updated W1 is: 
 [[-0.2480364   0.4656049 ]
 [-1.6552512  -0.62692755]]
W2 was :
 [[0.1463431 ]
 [1.11929618]]
Updated W2 is: 
 [[0.14440422]
 [1.12137023]]
The mean of the biases b gradient is:
 [ 6.03840197e-05 -7.37499968e-07]
The b biases before the update are:
 [[-2.48975453 -1.56014654]]
The new updated bs are:
 [[-2.48981491 -1.5601458 ]]
The bias c is: 
 [[ 0.12598397]
 [-0.12944369]
 [-0.11856671]
 [ 0.12383563]]
c bias before: [[-0.18967459]]
The mean c bias after update: [[-0.19012689]]
The output is: 
 [[0.50396762]
 [0.4815167 ]
 [0.52458386]
 [0.49538475]]
Total Loss: 0.4971174239558502
Average Loss: 0.12427935598896254

RUN:
  73
FeedForward


Z1 is:
 [[-2.48981491 -1.5601458 ]
 [-4.14506611 -2.18707335]
 [-2.73785132 -1.0945409 ]
 [-4.39310252 -1.72146844]]
H is:
 [[0.07657528 0.17362573]
 [0.01559532 0.10091733]
 [0.06077644 0.25076416]
 [0.01221135 0.15168212]]
The c is
 [[-0.19012689]]
Z2 is:
 [[ 0.01562963]
 [-0.07470917]
 [ 0.09984895]
 [-0.01827171]]
Y^ -  the output is:
 [[0.50390733]
 [0.48133139]
 [0.52494152]
 [0.4954322 ]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50390733]
 [0.48133139]
 [0.52494152]
 [0.4954322 ]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12596914]
 [-0.12948639]
 [-0.1184691 ]
 [ 0.12384771]]
W2.T is
 [[0.14440422 1.12137023]]
 D_Error times W2.T
 [[ 0.01819047  0.14125804]
 [-0.01869838 -0.14520218]
 [-0.01710744 -0.13284772]
 [ 0.01788413  0.13887914]]
self.H_D_Error_W2 is
 [[ 0.00128628  0.02026768]
 [-0.00028706 -0.01317463]
 [-0.00097654 -0.02495963]
 [ 0.00021572  0.01787022]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00128628  0.02026768]
 [-0.00028706 -0.01317463]
 [-0.00097654 -0.02495963]
 [ 0.00021572  0.01787022]]
X_H_D_Error_W2 is
 [[-7.60815445e-04 -7.08940342e-03]
 [-7.13368954e-05  4.69559227e-03]]
Using sum gradient........

W1 was :
 [[-0.2480364   0.4656049 ]
 [-1.6552512  -0.62692755]]
Updated W1 is: 
 [[-0.24727559  0.47269431]
 [-1.65517986 -0.63162314]]
W2 was :
 [[0.14440422]
 [1.12137023]]
Updated W2 is: 
 [[0.14246526]
 [1.12348849]]
The mean of the biases b gradient is:
 [5.96002396e-05 9.11128166e-07]
The b biases before the update are:
 [[-2.48981491 -1.5601458 ]]
The new updated bs are:
 [[-2.48987451 -1.56014671]]
The bias c is: 
 [[ 0.12596914]
 [-0.12948639]
 [-0.1184691 ]
 [ 0.12384771]]
c bias before: [[-0.19012689]]
The mean c bias after update: [[-0.19059223]]
The output is: 
 [[0.50390733]
 [0.48133139]
 [0.52494152]
 [0.4954322 ]]
Total Loss: 0.4970366730698219
Average Loss: 0.12425916826745548

RUN:
  74
FeedForward


Z1 is:
 [[-2.48987451 -1.56014671]
 [-4.14505438 -2.19176985]
 [-2.7371501  -1.0874524 ]
 [-4.39232997 -1.71907554]]
H is:
 [[0.07657107 0.1736256 ]
 [0.0155955  0.100492  ]
 [0.06081648 0.25209831]
 [0.01222068 0.15199028]]
The c is
 [[-0.19059223]]
Z2 is:
 [[ 0.01538284]
 [-0.07546881]
 [ 0.10130155]
 [-0.01809188]]
Y^ -  the output is:
 [[0.50384564]
 [0.48114175]
 [0.52530375]
 [0.49547715]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50384564]
 [0.48114175]
 [0.52530375]
 [0.49547715]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12595396]
 [-0.12953004]
 [-0.11837012]
 [ 0.12385915]]
W2.T is
 [[0.14246526 1.12348849]]
 D_Error times W2.T
 [[ 0.01794406  0.14150782]
 [-0.01845353 -0.14552551]
 [-0.01686363 -0.13298747]
 [ 0.01764563  0.13915433]]
self.H_D_Error_W2 is
 [[ 0.00126879  0.02030351]
 [-0.0002833  -0.01315454]
 [-0.00096321 -0.02507409]
 [ 0.00021301  0.0179355 ]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00126879  0.02030351]
 [-0.0002833  -0.01315454]
 [-0.00096321 -0.02507409]
 [ 0.00021301  0.0179355 ]]
X_H_D_Error_W2 is
 [[-7.50207844e-04 -7.13859460e-03]
 [-7.02975914e-05  4.78095608e-03]]
Using sum gradient........

W1 was :
 [[-0.24727559  0.47269431]
 [-1.65517986 -0.63162314]]
Updated W1 is: 
 [[-0.24652538  0.4798329 ]
 [-1.65510957 -0.6364041 ]]
W2 was :
 [[0.14246526]
 [1.12348849]]
Updated W2 is: 
 [[0.14052613]
 [1.12565191]]
The mean of the biases b gradient is:
 [5.88190276e-05 2.59322464e-06]
The b biases before the update are:
 [[-2.48987451 -1.56014671]]
The new updated bs are:
 [[-2.48993333 -1.56014931]]
The bias c is: 
 [[ 0.12595396]
 [-0.12953004]
 [-0.11837012]
 [ 0.12385915]]
c bias before: [[-0.19059223]]
The mean c bias after update: [[-0.19107047]]
The output is: 
 [[0.50384564]
 [0.48114175]
 [0.52530375]
 [0.49547715]]
Total Loss: 0.4969542232592066
Average Loss: 0.12423855581480164

RUN:
  75
FeedForward


Z1 is:
 [[-2.48993333 -1.56014931]
 [-4.1450429  -2.1965534 ]
 [-2.73645871 -1.0803164 ]
 [-4.39156828 -1.7167205 ]]
H is:
 [[0.07656691 0.17362522]
 [0.01559568 0.10006042]
 [0.06085598 0.25344615]
 [0.01222987 0.15229407]]
The c is
 [[-0.19107047]]
Z2 is:
 [[ 0.01513075]
 [-0.07624566]
 [ 0.10277353]
 [-0.01792174]]
Y^ -  the output is:
 [[0.50378261]
 [0.48094781]
 [0.52567079]
 [0.49551968]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50378261]
 [0.48094781]
 [0.52567079]
 [0.49551968]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12593845]
 [-0.12957464]
 [-0.11826972]
 [ 0.12386997]]
W2.T is
 [[0.14052613 1.12565191]]
 D_Error times W2.T
 [[ 0.01769764  0.14176285]
 [-0.01820862 -0.14585594]
 [-0.01661999 -0.13313054]
 [ 0.01740697  0.13943447]]
self.H_D_Error_W2 is
 [[ 0.0012513   0.02034006]
 [-0.00027955 -0.01313408]
 [-0.00094987 -0.02518979]
 [ 0.00021028  0.01800107]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.0012513   0.02034006]
 [-0.00027955 -0.01313408]
 [-0.00094987 -0.02518979]
 [ 0.00021028  0.01800107]]
X_H_D_Error_W2 is
 [[-7.39592815e-04 -7.18871717e-03]
 [-6.92655261e-05  4.86698761e-03]]
Using sum gradient........

W1 was :
 [[-0.24652538  0.4798329 ]
 [-1.65510957 -0.6364041 ]]
Updated W1 is: 
 [[-0.24578579  0.48702162]
 [-1.6550403  -0.64127108]]
W2 was :
 [[0.14052613]
 [1.12565191]]
Updated W2 is: 
 [[0.13858672]
 [1.12786146]]
The mean of the biases b gradient is:
 [5.80404182e-05 4.31560276e-06]
The b biases before the update are:
 [[-2.48993333 -1.56014931]]
The new updated bs are:
 [[-2.48999137 -1.56015362]]
The bias c is: 
 [[ 0.12593845]
 [-0.12957464]
 [-0.11826972]
 [ 0.12386997]]
c bias before: [[-0.19107047]]
The mean c bias after update: [[-0.19156148]]
The output is: 
 [[0.50378261]
 [0.48094781]
 [0.52567079]
 [0.49551968]]
Total Loss: 0.496870026188583
Average Loss: 0.12421750654714575

RUN:
  76
FeedForward


Z1 is:
 [[-2.48999137 -1.56015362]
 [-4.14503167 -2.20142471]
 [-2.73577716 -1.073132  ]
 [-4.39081746 -1.71440309]]
H is:
 [[0.07656281 0.1736246 ]
 [0.01559585 0.09962262]
 [0.06089495 0.25480792]
 [0.01223895 0.15259349]]
The c is
 [[-0.19156148]]
Z2 is:
 [[ 0.01487361]
 [-0.07703959]
 [ 0.10426578]
 [-0.01776101]]
Y^ -  the output is:
 [[0.50371833]
 [0.48074962]
 [0.52604286]
 [0.49555986]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50371833]
 [0.48074962]
 [0.52604286]
 [0.49555986]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12592262]
 [-0.12962017]
 [-0.11816783]
 [ 0.1238802 ]]
W2.T is
 [[0.13858672 1.12786146]]
 D_Error times W2.T
 [[ 0.0174512   0.14202327]
 [-0.01796363 -0.1461936 ]
 [-0.01637649 -0.13327695]
 [ 0.01716815  0.1397197 ]]
self.H_D_Error_W2 is
 [[ 0.00123382  0.02037737]
 [-0.00027579 -0.01311327]
 [-0.00093652 -0.02530674]
 [ 0.00020755  0.01806697]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00123382  0.02037737]
 [-0.00027579 -0.01311327]
 [-0.00093652 -0.02530674]
 [ 0.00020755  0.01806697]]
X_H_D_Error_W2 is
 [[-7.28969958e-04 -7.23976426e-03]
 [-6.82403933e-05  4.95370788e-03]]
Using sum gradient........

W1 was :
 [[-0.24578579  0.48702162]
 [-1.6550403  -0.64127108]]
Updated W1 is: 
 [[-0.24505682  0.49426138]
 [-1.65497206 -0.64622479]]
W2 was :
 [[0.13858672]
 [1.12786146]]
Updated W2 is: 
 [[0.13664693]
 [1.13011808]]
The mean of the biases b gradient is:
 [5.72644236e-05 6.08497269e-06]
The b biases before the update are:
 [[-2.48999137 -1.56015362]]
The new updated bs are:
 [[-2.49004864 -1.56015971]]
The bias c is: 
 [[ 0.12592262]
 [-0.12962017]
 [-0.11816783]
 [ 0.1238802 ]]
c bias before: [[-0.19156148]]
The mean c bias after update: [[-0.19206518]]
The output is: 
 [[0.50371833]
 [0.48074962]
 [0.52604286]
 [0.49555986]]
Total Loss: 0.4967840324554173
Average Loss: 0.12419600811385433

RUN:
  77
FeedForward


Z1 is:
 [[-2.49004864 -1.56015971]
 [-4.1450207  -2.2063845 ]
 [-2.73510545 -1.06589832]
 [-4.39007751 -1.71212311]]
H is:
 [[0.07655876 0.17362373]
 [0.01559602 0.09917862]
 [0.06093337 0.25618389]
 [0.0122479  0.15288854]]
The c is
 [[-0.19206518]]
Z2 is:
 [[ 0.01461165]
 [-0.07785048]
 [ 0.10577922]
 [-0.01760944]]
Y^ -  the output is:
 [[0.50365285]
 [0.4805472 ]
 [0.52642018]
 [0.49559775]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50365285]
 [0.4805472 ]
 [0.52642018]
 [0.49559775]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12590649]
 [-0.12966663]
 [-0.11806439]
 [ 0.12388983]]
W2.T is
 [[0.13664693 1.13011808]]
 D_Error times W2.T
 [[ 0.01720474  0.1422892 ]
 [-0.01771855 -0.14653861]
 [-0.01613314 -0.1334267 ]
 [ 0.01692917  0.14001014]]
self.H_D_Error_W2 is
 [[ 0.00121633  0.02041545]
 [-0.00027203 -0.01309208]
 [-0.00092315 -0.02542495]
 [ 0.00020481  0.01813322]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00121633  0.02041545]
 [-0.00027203 -0.01309208]
 [-0.00092315 -0.02542495]
 [ 0.00020481  0.01813322]]
X_H_D_Error_W2 is
 [[-7.18338888e-04 -7.29172918e-03]
 [-6.72218941e-05  5.04113749e-03]]
Using sum gradient........

W1 was :
 [[-0.24505682  0.49426138]
 [-1.65497206 -0.64622479]]
Updated W1 is: 
 [[-0.24433848  0.50155311]
 [-1.65490484 -0.65126593]]
W2 was :
 [[0.13664693]
 [1.13011808]]
Updated W2 is: 
 [[0.13470664]
 [1.13242274]]
The mean of the biases b gradient is:
 [5.64910353e-05 7.90796351e-06]
The b biases before the update are:
 [[-2.49004864 -1.56015971]]
The new updated bs are:
 [[-2.49010513 -1.56016761]]
The bias c is: 
 [[ 0.12590649]
 [-0.12966663]
 [-0.11806439]
 [ 0.12388983]]
c bias before: [[-0.19206518]]
The mean c bias after update: [[-0.19258151]]
The output is: 
 [[0.50365285]
 [0.4805472 ]
 [0.52642018]
 [0.49559775]]
Total Loss: 0.49669619155773614
Average Loss: 0.12417404788943404

RUN:
  78
FeedForward


Z1 is:
 [[-2.49010513 -1.56016761]
 [-4.14500997 -2.21143354]
 [-2.73444361 -1.0586145 ]
 [-4.38934845 -1.70988043]]
H is:
 [[0.07655477 0.1736226 ]
 [0.01559618 0.09872844]
 [0.06097125 0.25757431]
 [0.01225672 0.15317923]]
The c is
 [[-0.19258151]]
Z2 is:
 [[ 0.0143451 ]
 [-0.07867827]
 [ 0.10731473]
 [-0.01746681]]
Y^ -  the output is:
 [[0.50358621]
 [0.48034057]
 [0.52680297]
 [0.49563341]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50358621]
 [0.48034057]
 [0.52680297]
 [0.49563341]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12589008]
 [-0.12971401]
 [-0.11795931]
 [ 0.1238989 ]]
W2.T is
 [[0.13470664 1.13242274]]
 D_Error times W2.T
 [[ 0.01695823  0.14256079]
 [-0.01747334 -0.1468911 ]
 [-0.0158899  -0.13357981]
 [ 0.01669     0.14030593]]
self.H_D_Error_W2 is
 [[ 0.00119885  0.02045431]
 [-0.00026827 -0.01307054]
 [-0.00090976 -0.02554444]
 [ 0.00020206  0.01819983]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00119885  0.02045431]
 [-0.00026827 -0.01307054]
 [-0.00090976 -0.02554444]
 [ 0.00020206  0.01819983]]
X_H_D_Error_W2 is
 [[-7.07699230e-04 -7.34460533e-03]
 [-6.62097352e-05  5.12929664e-03]]
Using sum gradient........

W1 was :
 [[-0.24433848  0.50155311]
 [-1.65490484 -0.65126593]]
Updated W1 is: 
 [[-0.24363078  0.50889772]
 [-1.65483863 -0.65639523]]
W2 was :
 [[0.13470664]
 [1.13242274]]
Updated W2 is: 
 [[0.13276573]
 [1.1347764 ]]
The mean of the biases b gradient is:
 [5.57202254e-05 9.79114422e-06]
The b biases before the update are:
 [[-2.49010513 -1.56016761]]
The new updated bs are:
 [[-2.49016085 -1.5601774 ]]
The bias c is: 
 [[ 0.12589008]
 [-0.12971401]
 [-0.11795931]
 [ 0.1238989 ]]
c bias before: [[-0.19258151]]
The mean c bias after update: [[-0.19311042]]
The output is: 
 [[0.50358621]
 [0.48034057]
 [0.52680297]
 [0.49563341]]
Total Loss: 0.4966064518629268
Average Loss: 0.1241516129657317

RUN:
  79
FeedForward


Z1 is:
 [[-2.49016085 -1.5601774 ]
 [-4.14499948 -2.21657263]
 [-2.73379163 -1.05127969]
 [-4.38863026 -1.70767491]]
H is:
 [[0.07655083 0.17362119]
 [0.01559634 0.0982721 ]
 [0.06100859 0.25897944]
 [0.01226542 0.15346553]]
The c is
 [[-0.19311042]]
Z2 is:
 [[ 0.01407413]
 [-0.0795229 ]
 [ 0.10887318]
 [-0.01733293]]
Y^ -  the output is:
 [[0.50351847]
 [0.48012974]
 [0.52719144]
 [0.49566688]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50351847]
 [0.48012974]
 [0.52719144]
 [0.49566688]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12587339]
 [-0.12976231]
 [-0.11785256]
 [ 0.12390741]]
W2.T is
 [[0.13276573 1.1347764 ]]
 D_Error times W2.T
 [[ 0.01671167  0.14283815]
 [-0.01722799 -0.1472512 ]
 [-0.01564678 -0.1337363 ]
 [ 0.01645066  0.14060721]]
self.H_D_Error_W2 is
 [[ 0.00118136  0.02049397]
 [-0.0002645  -0.01304862]
 [-0.00089635 -0.02566521]
 [ 0.0001993   0.01826683]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00118136  0.02049397]
 [-0.0002645  -0.01304862]
 [-0.00089635 -0.02566521]
 [ 0.0001993   0.01826683]]
X_H_D_Error_W2 is
 [[-6.97050622e-04 -7.39838620e-03]
 [-6.52036291e-05  5.21820512e-03]]
Using sum gradient........

W1 was :
 [[-0.24363078  0.50889772]
 [-1.65483863 -0.65639523]]
Updated W1 is: 
 [[-0.24293373  0.5162961 ]
 [-1.65477342 -0.66161343]]
W2 was :
 [[0.13276573]
 [1.1347764 ]]
Updated W2 is: 
 [[0.13082408]
 [1.13718   ]]
The mean of the biases b gradient is:
 [5.49519475e-05 1.17410438e-05]
The b biases before the update are:
 [[-2.49016085 -1.5601774 ]]
The new updated bs are:
 [[-2.4902158  -1.56018915]]
The bias c is: 
 [[ 0.12587339]
 [-0.12976231]
 [-0.11785256]
 [ 0.12390741]]
c bias before: [[-0.19311042]]
The mean c bias after update: [[-0.19365191]]
The output is: 
 [[0.50351847]
 [0.48012974]
 [0.52719144]
 [0.49566688]]
Total Loss: 0.49651476057756805
Average Loss: 0.12412869014439201

RUN:
  80
FeedForward


Z1 is:
 [[-2.4902158  -1.56018915]
 [-4.14498922 -2.22180258]
 [-2.73314953 -1.04389304]
 [-4.38792295 -1.70550647]]
H is:
 [[0.07654694 0.17361951]
 [0.0155965  0.09780962]
 [0.06104539 0.26039953]
 [0.01227399 0.15374746]]
The c is
 [[-0.19365191]]
Z2 is:
 [[ 0.01379891]
 [-0.08038436]
 [ 0.11045543]
 [-0.01720764]]
Y^ -  the output is:
 [[0.50344967]
 [0.47991472]
 [0.52758582]
 [0.4956982 ]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50344967]
 [0.47991472]
 [0.52758582]
 [0.4956982 ]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12585643]
 [-0.12981151]
 [-0.11774405]
 [ 0.12391538]]
W2.T is
 [[0.13082408 1.13718   ]]
 D_Error times W2.T
 [[ 0.01646505  0.14312141]
 [-0.01698247 -0.14761905]
 [-0.01540376 -0.13389618]
 [ 0.01621111  0.14091409]]
self.H_D_Error_W2 is
 [[ 0.00116387  0.02053446]
 [-0.00026074 -0.01302633]
 [-0.00088293 -0.02578728]
 [ 0.00019653  0.01833422]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00116387  0.02053446]
 [-0.00026074 -0.01302633]
 [-0.00088293 -0.02578728]
 [ 0.00019653  0.01833422]]
X_H_D_Error_W2 is
 [[-6.86392715e-04 -7.45306529e-03]
 [-6.42032923e-05  5.30788236e-03]]
Using sum gradient........

W1 was :
 [[-0.24293373  0.5162961 ]
 [-1.65477342 -0.66161343]]
Updated W1 is: 
 [[-0.24224734  0.52374917]
 [-1.65470922 -0.66692131]]
W2 was :
 [[0.13082408]
 [1.13718   ]]
Updated W2 is: 
 [[0.12888155]
 [1.1396345 ]]
The mean of the biases b gradient is:
 [5.41861379e-05 1.37641705e-05]
The b biases before the update are:
 [[-2.4902158  -1.56018915]]
The new updated bs are:
 [[-2.49026999 -1.56020291]]
The bias c is: 
 [[ 0.12585643]
 [-0.12981151]
 [-0.11774405]
 [ 0.12391538]]
c bias before: [[-0.19365191]]
The mean c bias after update: [[-0.19420597]]
The output is: 
 [[0.50344967]
 [0.47991472]
 [0.52758582]
 [0.4956982 ]]
Total Loss: 0.4964210637182274
Average Loss: 0.12410526592955685

RUN:
  81
FeedForward


Z1 is:
 [[-2.49026999 -1.56020291]
 [-4.14497921 -2.22712422]
 [-2.73251732 -1.03645374]
 [-4.38722654 -1.70337505]]
H is:
 [[0.07654311 0.17361753]
 [0.01559666 0.09734103]
 [0.06108163 0.26183483]
 [0.01228244 0.15402498]]
The c is
 [[-0.19420597]]
Z2 is:
 [[ 0.01351956]
 [-0.08126265]
 [ 0.11206233]
 [-0.01709081]]
Y^ -  the output is:
 [[0.50337984]
 [0.47969551]
 [0.5279863 ]
 [0.4957274 ]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50337984]
 [0.47969551]
 [0.5279863 ]
 [0.4957274 ]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12583921]
 [-0.12986162]
 [-0.11763373]
 [ 0.1239228 ]]
W2.T is
 [[0.12888155 1.1396345 ]]
 D_Error times W2.T
 [[ 0.01621835  0.1434107 ]
 [-0.01673677 -0.14799478]
 [-0.01516082 -0.13405946]
 [ 0.01597136  0.1412267 ]]
self.H_D_Error_W2 is
 [[ 0.00114638  0.02057578]
 [-0.00025697 -0.01300367]
 [-0.00086948 -0.02591066]
 [ 0.00019376  0.01840202]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00114638  0.02057578]
 [-0.00025697 -0.01300367]
 [-0.00086948 -0.02591066]
 [ 0.00019376  0.01840202]]
X_H_D_Error_W2 is
 [[-6.75725171e-04 -7.50863604e-03]
 [-6.32084456e-05  5.39834743e-03]]
Using sum gradient........

W1 was :
 [[-0.24224734  0.52374917]
 [-1.65470922 -0.66692131]]
Updated W1 is: 
 [[-0.24157161  0.53125781]
 [-1.65464601 -0.67231966]]
W2 was :
 [[0.12888155]
 [1.1396345 ]]
Updated W2 is: 
 [[0.12693802]
 [1.14214087]]
The mean of the biases b gradient is:
 [5.34227164e-05 1.58670298e-05]
The b biases before the update are:
 [[-2.49026999 -1.56020291]]
The new updated bs are:
 [[-2.49032341 -1.56021878]]
The bias c is: 
 [[ 0.12583921]
 [-0.12986162]
 [-0.11763373]
 [ 0.1239228 ]]
c bias before: [[-0.19420597]]
The mean c bias after update: [[-0.19477264]]
The output is: 
 [[0.50337984]
 [0.47969551]
 [0.5279863 ]
 [0.4957274 ]]
Total Loss: 0.49632530608317627
Average Loss: 0.12408132652079407

RUN:
  82
FeedForward


Z1 is:
 [[-2.49032341 -1.56021878]
 [-4.14496942 -2.23253844]
 [-2.73189502 -1.02896097]
 [-4.38654103 -1.70128063]]
H is:
 [[0.07653934 0.17361526]
 [0.01559681 0.09686634]
 [0.06111733 0.26328559]
 [0.01229075 0.15429808]]
The c is
 [[-0.19477264]]
Z2 is:
 [[ 0.0132362 ]
 [-0.0821578 ]
 [ 0.11369471]
 [-0.01698233]]
Y^ -  the output is:
 [[0.503309  ]
 [0.4794721 ]
 [0.5283931 ]
 [0.49575452]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.503309  ]
 [0.4794721 ]
 [0.5283931 ]
 [0.49575452]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12582174]
 [-0.12991263]
 [-0.11752153]
 [ 0.12392969]]
W2.T is
 [[0.12693802 1.14214087]]
 D_Error times W2.T
 [[ 0.01597156  0.14370615]
 [-0.01649085 -0.14837852]
 [-0.01491795 -0.13422614]
 [ 0.01573139  0.14154517]]
self.H_D_Error_W2 is
 [[ 0.00112889  0.02061795]
 [-0.00025319 -0.01298064]
 [-0.00085602 -0.02603535]
 [ 0.00019097  0.01847026]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00112889  0.02061795]
 [-0.00025319 -0.01298064]
 [-0.00085602 -0.02603535]
 [ 0.00019097  0.01847026]]
X_H_D_Error_W2 is
 [[-6.65047663e-04 -7.56509183e-03]
 [-6.22188128e-05  5.48961905e-03]]
Using sum gradient........

W1 was :
 [[-0.24157161  0.53125781]
 [-1.65464601 -0.67231966]]
Updated W1 is: 
 [[-0.24090656  0.5388229 ]
 [-1.65458379 -0.67780928]]
W2 was :
 [[0.12693802]
 [1.14214087]]
Updated W2 is: 
 [[0.12499334]
 [1.14470007]]
The mean of the biases b gradient is:
 [5.26615874e-05 1.80561425e-05]
The b biases before the update are:
 [[-2.49032341 -1.56021878]]
The new updated bs are:
 [[-2.49037607 -1.56023683]]
The bias c is: 
 [[ 0.12582174]
 [-0.12991263]
 [-0.11752153]
 [ 0.12392969]]
c bias before: [[-0.19477264]]
The mean c bias after update: [[-0.19535196]]
The output is: 
 [[0.503309  ]
 [0.4794721 ]
 [0.5283931 ]
 [0.49575452]]
Total Loss: 0.49622743122499635
Average Loss: 0.12405685780624909

RUN:
  83
FeedForward


Z1 is:
 [[-2.49037607 -1.56023683]
 [-4.14495986 -2.23804611]
 [-2.73128263 -1.02141394]
 [-4.38586643 -1.69922322]]
H is:
 [[0.07653561 0.17361267]
 [0.01559695 0.09638558]
 [0.06115248 0.26475207]
 [0.01229895 0.15456674]]
The c is
 [[-0.19535196]]
Z2 is:
 [[ 0.01294892]
 [-0.08306986]
 [ 0.11535342]
 [-0.0168821 ]]
Y^ -  the output is:
 [[0.50323718]
 [0.47924447]
 [0.52880642]
 [0.49577957]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50323718]
 [0.47924447]
 [0.52880642]
 [0.49577957]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12580402]
 [-0.12996455]
 [-0.11740739]
 [ 0.12393606]]
W2.T is
 [[0.12499334 1.14470007]]
 D_Error times W2.T
 [[ 0.01572467  0.14400787]
 [-0.0162447  -0.14877042]
 [-0.01467514 -0.13439625]
 [ 0.01549118  0.14186962]]
self.H_D_Error_W2 is
 [[ 0.00111139  0.020661  ]
 [-0.00024942 -0.01295722]
 [-0.00084254 -0.02616136]
 [ 0.00018818  0.01853894]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00111139  0.020661  ]
 [-0.00024942 -0.01295722]
 [-0.00084254 -0.02616136]
 [ 0.00018818  0.01853894]]
X_H_D_Error_W2 is
 [[-6.54359877e-04 -7.62242587e-03]
 [-6.12341206e-05  5.58171561e-03]]
Using sum gradient........

W1 was :
 [[-0.24090656  0.5388229 ]
 [-1.65458379 -0.67780928]]
Updated W1 is: 
 [[-0.2402522   0.54644532]
 [-1.65452256 -0.683391  ]]
W2 was :
 [[0.12499334]
 [1.14470007]]
Updated W2 is: 
 [[0.12304738]
 [1.14731307]]
The mean of the biases b gradient is:
 [5.19026404e-05 2.03380611e-05]
The b biases before the update are:
 [[-2.49037607 -1.56023683]]
The new updated bs are:
 [[-2.49042797 -1.56025717]]
The bias c is: 
 [[ 0.12580402]
 [-0.12996455]
 [-0.11740739]
 [ 0.12393606]]
c bias before: [[-0.19535196]]
The mean c bias after update: [[-0.19594399]]
The output is: 
 [[0.50323718]
 [0.47924447]
 [0.52880642]
 [0.49577957]]
Total Loss: 0.4961273814240729
Average Loss: 0.12403184535601823

RUN:
  84
FeedForward


Z1 is:
 [[-2.49042797 -1.56025717]
 [-4.14495053 -2.24364817]
 [-2.73068018 -1.01381185]
 [-4.38520274 -1.69720284]]
H is:
 [[0.07653194 0.17360975]
 [0.0155971  0.09589877]
 [0.06118708 0.26623453]
 [0.01230701 0.15483094]]
The c is
 [[-0.19594399]]
Z2 is:
 [[ 0.0126578 ]
 [-0.0839989 ]
 [ 0.11703927]
 [-0.01679008]]
Y^ -  the output is:
 [[0.50316441]
 [0.47901261]
 [0.52922646]
 [0.49580258]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50316441]
 [0.47901261]
 [0.52922646]
 [0.49580258]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12578606]
 [-0.13001737]
 [-0.11729126]
 [ 0.12394191]]
W2.T is
 [[0.12304738 1.14731307]]
 D_Error times W2.T
 [[ 0.01547765  0.14431599]
 [-0.0159983  -0.14917062]
 [-0.01443238 -0.13456979]
 [ 0.01525073  0.14220017]]
self.H_D_Error_W2 is
 [[ 0.00109388  0.02070493]
 [-0.00024564 -0.01293342]
 [-0.00082904 -0.02628871]
 [ 0.00018538  0.01860808]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00109388  0.02070493]
 [-0.00024564 -0.01293342]
 [-0.00082904 -0.02628871]
 [ 0.00018538  0.01860808]]
X_H_D_Error_W2 is
 [[-6.43661509e-04 -7.68063120e-03]
 [-6.02540976e-05  5.67465513e-03]]
Using sum gradient........

W1 was :
 [[-0.2402522   0.54644532]
 [-1.65452256 -0.683391  ]]
Updated W1 is: 
 [[-0.23960854  0.55412595]
 [-1.65446231 -0.68906565]]
W2 was :
 [[0.12304738]
 [1.14731307]]
Updated W2 is: 
 [[0.12109997]
 [1.14998082]]
The mean of the biases b gradient is:
 [5.11457508e-05 2.27193861e-05]
The b biases before the update are:
 [[-2.49042797 -1.56025717]]
The new updated bs are:
 [[-2.49047912 -1.56027989]]
The bias c is: 
 [[ 0.12578606]
 [-0.13001737]
 [-0.11729126]
 [ 0.12394191]]
c bias before: [[-0.19594399]]
The mean c bias after update: [[-0.19654883]]
The output is: 
 [[0.50316441]
 [0.47901261]
 [0.52922646]
 [0.49580258]]
Total Loss: 0.4960250976629814
Average Loss: 0.12400627441574535

RUN:
  85
FeedForward


Z1 is:
 [[-2.49047912 -1.56027989]
 [-4.14494142 -2.24934554]
 [-2.73008766 -1.00615394]
 [-4.38454997 -1.69521959]]
H is:
 [[0.07652833 0.17360649]
 [0.01559724 0.09540593]
 [0.06122112 0.26773321]
 [0.01231495 0.15509065]]
The c is
 [[-0.19654883]]
Z2 is:
 [[ 0.01236288]
 [-0.08494501]
 [ 0.1187531 ]
 [-0.01670622]]
Y^ -  the output is:
 [[0.50309068]
 [0.47877651]
 [0.52965343]
 [0.49582354]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50309068]
 [0.47877651]
 [0.52965343]
 [0.49582354]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12576786]
 [-0.1300711 ]
 [-0.11717305]
 [ 0.12394724]]
W2.T is
 [[0.12109997 1.14998082]]
 D_Error times W2.T
 [[ 0.01523049  0.14463063]
 [-0.01575161 -0.14957927]
 [-0.01418965 -0.13474676]
 [ 0.01501001  0.14253695]]
self.H_D_Error_W2 is
 [[ 0.00107636  0.02074976]
 [-0.00024185 -0.01290924]
 [-0.00081552 -0.02641739]
 [ 0.00018257  0.01867769]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00107636  0.02074976]
 [-0.00024185 -0.01290924]
 [-0.00081552 -0.02641739]
 [ 0.00018257  0.01867769]]
X_H_D_Error_W2 is
 [[-6.32952270e-04 -7.73970061e-03]
 [-5.92784741e-05  5.76845534e-03]]
Using sum gradient........

W1 was :
 [[-0.23960854  0.55412595]
 [-1.65446231 -0.68906565]]
Updated W1 is: 
 [[-0.23897559  0.56186566]
 [-1.65440303 -0.69483411]]
W2 was :
 [[0.12109997]
 [1.14998082]]
Updated W2 is: 
 [[0.11915098]
 [1.15270432]]
The mean of the biases b gradient is:
 [5.03907806e-05 2.52067814e-05]
The b biases before the update are:
 [[-2.49047912 -1.56027989]]
The new updated bs are:
 [[-2.49052951 -1.5603051 ]]
The bias c is: 
 [[ 0.12576786]
 [-0.1300711 ]
 [-0.11717305]
 [ 0.12394724]]
c bias before: [[-0.19654883]]
The mean c bias after update: [[-0.19716657]]
The output is: 
 [[0.50309068]
 [0.47877651]
 [0.52965343]
 [0.49582354]]
Total Loss: 0.495920519601792
Average Loss: 0.123980129900448

RUN:
  86
FeedForward


Z1 is:
 [[-2.49052951 -1.5603051 ]
 [-4.14493254 -2.2551392 ]
 [-2.7295051  -0.99843944]
 [-4.38390813 -1.69327355]]
H is:
 [[0.07652477 0.17360287]
 [0.01559737 0.09490709]
 [0.06125461 0.26924836]
 [0.01232276 0.15534582]]
The c is
 [[-0.19716657]]
Z2 is:
 [[ 0.01206421]
 [-0.08590831]
 [ 0.12049572]
 [-0.0166305 ]]
Y^ -  the output is:
 [[0.50301602]
 [0.47853612]
 [0.53008754]
 [0.49584247]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50301602]
 [0.47853612]
 [0.53008754]
 [0.49584247]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12574943]
 [-0.13012573]
 [-0.11705272]
 [ 0.12395205]]
W2.T is
 [[0.11915098 1.15270432]]
 D_Error times W2.T
 [[ 0.01498317  0.14495191]
 [-0.01550461 -0.14999649]
 [-0.01394695 -0.13492718]
 [ 0.01476901  0.14288006]]
self.H_D_Error_W2 is
 [[ 0.00105884  0.02079551]
 [-0.00023806 -0.01288466]
 [-0.00080198 -0.02654742]
 [ 0.00017975  0.01874779]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00105884  0.02079551]
 [-0.00023806 -0.01288466]
 [-0.00080198 -0.02654742]
 [ 0.00017975  0.01874779]]
X_H_D_Error_W2 is
 [[-6.22231879e-04 -7.79962660e-03]
 [-5.83069815e-05  5.86313360e-03]]
Using sum gradient........

W1 was :
 [[-0.23897559  0.56186566]
 [-1.65440303 -0.69483411]]
Updated W1 is: 
 [[-0.23835336  0.56966528]
 [-1.65434472 -0.70069724]]
W2 was :
 [[0.11915098]
 [1.15270432]]
Updated W2 is: 
 [[0.11720024]
 [1.15548453]]
The mean of the biases b gradient is:
 [4.96375786e-05 2.78069893e-05]
The b biases before the update are:
 [[-2.49052951 -1.5603051 ]]
The new updated bs are:
 [[-2.49057915 -1.5603329 ]]
The bias c is: 
 [[ 0.12574943]
 [-0.13012573]
 [-0.11705272]
 [ 0.12395205]]
c bias before: [[-0.19716657]]
The mean c bias after update: [[-0.19779732]]
The output is: 
 [[0.50301602]
 [0.47853612]
 [0.53008754]
 [0.49584247]]
Total Loss: 0.4958135855543325
Average Loss: 0.12395339638858313

RUN:
  87
FeedForward


Z1 is:
 [[-2.49057915 -1.5603329 ]
 [-4.14492387 -2.26103014]
 [-2.72893251 -0.99066762]
 [-4.38327723 -1.69136486]]
H is:
 [[0.07652126 0.17359888]
 [0.01559751 0.09440226]
 [0.06128755 0.27078023]
 [0.01233044 0.15559643]]
The c is
 [[-0.19779732]]
Z2 is:
 [[ 0.01176181]
 [-0.08688893]
 [ 0.12226796]
 [-0.01656292]]
Y^ -  the output is:
 [[0.50294042]
 [0.47829142]
 [0.53052897]
 [0.49585936]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50294042]
 [0.47829142]
 [0.53052897]
 [0.49585936]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12573076]
 [-0.13018128]
 [-0.1169302 ]
 [ 0.12395634]]
W2.T is
 [[0.11720024 1.15548453]]
 D_Error times W2.T
 [[ 0.01473568  0.14527994]
 [-0.01525728 -0.15042246]
 [-0.01370425 -0.13511104]
 [ 0.01452771  0.14322963]]
self.H_D_Error_W2 is
 [[ 0.00104131  0.0208422 ]
 [-0.00023426 -0.01285969]
 [-0.00078842 -0.0266788 ]
 [ 0.00017692  0.01881839]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00104131  0.0208422 ]
 [-0.00023426 -0.01285969]
 [-0.00078842 -0.0266788 ]
 [ 0.00017692  0.01881839]]
X_H_D_Error_W2 is
 [[-6.11500073e-04 -7.86040135e-03]
 [-5.73393516e-05  5.95870694e-03]]
Using sum gradient........

W1 was :
 [[-0.23835336  0.56966528]
 [-1.65434472 -0.70069724]]
Updated W1 is: 
 [[-0.23774186  0.57752568]
 [-1.65428738 -0.70665595]]
W2 was :
 [[0.11720024]
 [1.15548453]]
Updated W2 is: 
 [[0.1152476 ]
 [1.15832245]]
The mean of the biases b gradient is:
 [4.88859814e-05 3.05268445e-05]
The b biases before the update are:
 [[-2.49057915 -1.5603329 ]]
The new updated bs are:
 [[-2.49062803 -1.56036343]]
The bias c is: 
 [[ 0.12573076]
 [-0.13018128]
 [-0.1169302 ]
 [ 0.12395634]]
c bias before: [[-0.19779732]]
The mean c bias after update: [[-0.19844122]]
The output is: 
 [[0.50294042]
 [0.47829142]
 [0.53052897]
 [0.49585936]]
Total Loss: 0.49570423246545997
Average Loss: 0.12392605811636499

RUN:
  88
FeedForward


Z1 is:
 [[-2.49062803 -1.56036343]
 [-4.14491541 -2.26701938]
 [-2.72836989 -0.98283775]
 [-4.38265727 -1.68949369]]
H is:
 [[0.07651781 0.1735945 ]
 [0.01559764 0.09389148]
 [0.06131992 0.27232908]
 [0.01233799 0.15584244]]
The c is
 [[-0.19844122]]
Z2 is:
 [[ 0.01145568]
 [-0.08788702]
 [ 0.12407063]
 [-0.01650351]]
Y^ -  the output is:
 [[0.50286389]
 [0.47804238]
 [0.53097793]
 [0.49587422]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50286389]
 [0.47804238]
 [0.53097793]
 [0.49587422]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12571185]
 [-0.13023775]
 [-0.11680543]
 [ 0.12396011]]
W2.T is
 [[0.1152476  1.15832245]]
 D_Error times W2.T
 [[ 0.01448799  0.14561485]
 [-0.01500959 -0.15085731]
 [-0.01346155 -0.13529835]
 [ 0.01428611  0.14358578]]
self.H_D_Error_W2 is
 [[ 0.00102376  0.02088983]
 [-0.00023046 -0.01283432]
 [-0.00077484 -0.02681153]
 [ 0.00017409  0.01888951]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00102376  0.02088983]
 [-0.00023046 -0.01283432]
 [-0.00077484 -0.02681153]
 [ 0.00017409  0.01888951]]
X_H_D_Error_W2 is
 [[-6.00756597e-04 -7.92201665e-03]
 [-5.63753165e-05  6.05519205e-03]]
Using sum gradient........

W1 was :
 [[-0.23774186  0.57752568]
 [-1.65428738 -0.70665595]]
Updated W1 is: 
 [[-0.2371411   0.5854477 ]
 [-1.65423101 -0.71271114]]
W2 was :
 [[0.1152476 ]
 [1.15832245]]
Updated W2 is: 
 [[0.11329289]
 [1.16121904]]
The mean of the biases b gradient is:
 [4.81358136e-05 3.33732882e-05]
The b biases before the update are:
 [[-2.49062803 -1.56036343]]
The new updated bs are:
 [[-2.49067617 -1.5603968 ]]
The bias c is: 
 [[ 0.12571185]
 [-0.13023775]
 [-0.11680543]
 [ 0.12396011]]
c bias before: [[-0.19844122]]
The mean c bias after update: [[-0.19909842]]
The output is: 
 [[0.50286389]
 [0.47804238]
 [0.53097793]
 [0.49587422]]
Total Loss: 0.49559239588940607
Average Loss: 0.12389809897235152

RUN:
  89
FeedForward


Z1 is:
 [[-2.49067617 -1.5603968 ]
 [-4.14490717 -2.27310794]
 [-2.72781727 -0.9749491 ]
 [-4.38204828 -1.68766024]]
H is:
 [[0.07651441 0.17358972]
 [0.01559776 0.09337477]
 [0.06135174 0.27389514]
 [0.01234542 0.15608379]]
The c is
 [[-0.19909842]]
Z2 is:
 [[ 0.0111458 ]
 [-0.08890274]
 [ 0.12590455]
 [-0.0164523 ]]
Y^ -  the output is:
 [[0.50278642]
 [0.47778894]
 [0.53143462]
 [0.49588702]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50278642]
 [0.47778894]
 [0.53143462]
 [0.49588702]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.1256927 ]
 [-0.13029514]
 [-0.11667834]
 [ 0.12396337]]
W2.T is
 [[0.11329289 1.16121904]]
 D_Error times W2.T
 [[ 0.01424009  0.14595676]
 [-0.01476151 -0.1513012 ]
 [-0.01321883 -0.13548911]
 [ 0.01404417  0.14394862]]
self.H_D_Error_W2 is
 [[ 0.0010062   0.02093842]
 [-0.00022666 -0.01280854]
 [-0.00076124 -0.02694561]
 [ 0.00017124  0.01896115]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.0010062   0.02093842]
 [-0.00022666 -0.01280854]
 [-0.00076124 -0.02694561]
 [ 0.00017124  0.01896115]]
X_H_D_Error_W2 is
 [[-5.90001212e-04 -7.98446383e-03]
 [-5.54146080e-05  6.15260529e-03]]
Using sum gradient........

W1 was :
 [[-0.2371411   0.5854477 ]
 [-1.65423101 -0.71271114]]
Updated W1 is: 
 [[-0.2365511   0.59343216]
 [-1.65417559 -0.71886374]]
W2 was :
 [[0.11329289]
 [1.16121904]]
Updated W2 is: 
 [[0.11133594]
 [1.16417532]]
The mean of the biases b gradient is:
 [4.73868879e-05 3.63533809e-05]
The b biases before the update are:
 [[-2.49067617 -1.5603968 ]]
The new updated bs are:
 [[-2.49072356 -1.56043316]]
The bias c is: 
 [[ 0.1256927 ]
 [-0.13029514]
 [-0.11667834]
 [ 0.12396337]]
c bias before: [[-0.19909842]]
The mean c bias after update: [[-0.19976907]]
The output is: 
 [[0.50278642]
 [0.47778894]
 [0.53143462]
 [0.49588702]]
Total Loss: 0.4954780099692737
Average Loss: 0.12386950249231843

RUN:
  90
FeedForward


Z1 is:
 [[-2.49072356 -1.56043316]
 [-4.14489915 -2.2792969 ]
 [-2.72727466 -0.96700099]
 [-4.38145025 -1.68586474]]
H is:
 [[0.07651106 0.1735845 ]
 [0.01559788 0.09285216]
 [0.061383   0.27547867]
 [0.01235271 0.15632044]]
The c is
 [[-0.19976907]]
Z2 is:
 [[ 0.01083215]
 [-0.08993627]
 [ 0.12777054]
 [-0.01640937]]
Y^ -  the output is:
 [[0.50270801]
 [0.47753108]
 [0.53189925]
 [0.49589775]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50270801]
 [0.47753108]
 [0.53189925]
 [0.49589775]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12567332]
 [-0.13035346]
 [-0.11654887]
 [ 0.12396609]]
W2.T is
 [[0.11133594 1.16417532]]
 D_Error times W2.T
 [[ 0.01399196  0.14630577]
 [-0.01451302 -0.15175428]
 [-0.01297608 -0.13568331]
 [ 0.01380188  0.14431827]]
self.H_D_Error_W2 is
 [[ 0.00098863  0.02098799]
 [-0.00022284 -0.01278236]
 [-0.00074762 -0.02708106]
 [ 0.00016838  0.01903332]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00098863  0.02098799]
 [-0.00022284 -0.01278236]
 [-0.00074762 -0.02708106]
 [ 0.00016838  0.01903332]]
X_H_D_Error_W2 is
 [[-5.79233692e-04 -8.04773380e-03]
 [-5.44569571e-05  6.25096263e-03]]
Using sum gradient........

W1 was :
 [[-0.2365511   0.59343216]
 [-1.65417559 -0.71886374]]
Updated W1 is: 
 [[-0.23597187  0.6014799 ]
 [-1.65412113 -0.72511471]]
W2 was :
 [[0.11133594]
 [1.16417532]]
Updated W2 is: 
 [[0.10937658]
 [1.16719227]]
The mean of the biases b gradient is:
 [4.66390059e-05 3.94743153e-05]
The b biases before the update are:
 [[-2.49072356 -1.56043316]]
The new updated bs are:
 [[-2.49077019 -1.56047263]]
The bias c is: 
 [[ 0.12567332]
 [-0.13035346]
 [-0.11654887]
 [ 0.12396609]]
c bias before: [[-0.19976907]]
The mean c bias after update: [[-0.20045334]]
The output is: 
 [[0.50270801]
 [0.47753108]
 [0.53189925]
 [0.49589775]]
Total Loss: 0.4953610074177706
Average Loss: 0.12384025185444265

RUN:
  91
FeedForward


Z1 is:
 [[-2.49077019 -1.56047263]
 [-4.14489133 -2.28558734]
 [-2.72674206 -0.95899273]
 [-4.3808632  -1.68410744]]
H is:
 [[0.07650776 0.17357884]
 [0.015598   0.09232367]
 [0.06141369 0.27707991]
 [0.01235987 0.15655234]]
The c is
 [[-0.20045334]]
Z2 is:
 [[ 0.0105147 ]
 [-0.09098781]
 [ 0.12966941]
 [-0.01637477]]
Y^ -  the output is:
 [[0.50262865]
 [0.47726873]
 [0.53237201]
 [0.4959064 ]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50262865]
 [0.47726873]
 [0.53237201]
 [0.4959064 ]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12565369]
 [-0.13041272]
 [-0.11641695]
 [ 0.12396829]]
W2.T is
 [[0.10937658 1.16719227]]
 D_Error times W2.T
 [[ 0.01374357  0.14666202]
 [-0.0142641  -0.15221672]
 [-0.01273329 -0.13588096]
 [ 0.01355923  0.14469483]]
self.H_D_Error_W2 is
 [[ 0.00097104  0.02103855]
 [-0.00021902 -0.01275576]
 [-0.00073397 -0.02721786]
 [ 0.00016552  0.01910604]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00097104  0.02103855]
 [-0.00021902 -0.01275576]
 [-0.00073397 -0.02721786]
 [ 0.00016552  0.01910604]]
X_H_D_Error_W2 is
 [[-5.68453827e-04 -8.11181689e-03]
 [-5.35020938e-05  6.35027967e-03]]
Using sum gradient........

W1 was :
 [[-0.23597187  0.6014799 ]
 [-1.65412113 -0.72511471]]
Updated W1 is: 
 [[-0.23540341  0.60959171]
 [-1.65406763 -0.73146499]]
W2 was :
 [[0.10937658]
 [1.16719227]]
Updated W2 is: 
 [[0.10741463]
 [1.17027091]]
The mean of the biases b gradient is:
 [4.58919580e-05 4.27434284e-05]
The b biases before the update are:
 [[-2.49077019 -1.56047263]]
The new updated bs are:
 [[-2.49081609 -1.56051538]]
The bias c is: 
 [[ 0.12565369]
 [-0.13041272]
 [-0.11641695]
 [ 0.12396829]]
c bias before: [[-0.20045334]]
The mean c bias after update: [[-0.20115142]]
The output is: 
 [[0.50262865]
 [0.47726873]
 [0.53237201]
 [0.4959064 ]]
Total Loss: 0.49524131949928185
Average Loss: 0.12381032987482046

RUN:
  92
FeedForward


Z1 is:
 [[-2.49081609 -1.56051538]
 [-4.14488372 -2.29198036]
 [-2.7262195  -0.95092366]
 [-4.38028713 -1.68238865]]
H is:
 [[0.07650452 0.17357271]
 [0.01559812 0.09178933]
 [0.06144382 0.2786991 ]
 [0.01236691 0.15677943]]
The c is
 [[-0.20115142]]
Z2 is:
 [[ 0.01019338]
 [-0.09205757]
 [ 0.131602  ]
 [-0.01634862]]
Y^ -  the output is:
 [[0.50254832]
 [0.47700185]
 [0.5328531 ]
 [0.49591294]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50254832]
 [0.47700185]
 [0.5328531 ]
 [0.49591294]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12563382]
 [-0.13047292]
 [-0.11628252]
 [ 0.12396995]]
W2.T is
 [[0.10741463 1.17027091]]
 D_Error times W2.T
 [[ 0.01349491  0.1470256 ]
 [-0.0140147  -0.15268866]
 [-0.01249044 -0.13608205]
 [ 0.01331619  0.14507843]]
self.H_D_Error_W2 is
 [[ 0.00095344  0.02109012]
 [-0.00021519 -0.01272874]
 [-0.0007203  -0.02735602]
 [ 0.00016264  0.01917932]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00095344  0.02109012]
 [-0.00021519 -0.01272874]
 [-0.0007203  -0.02735602]
 [ 0.00016264  0.01917932]]
X_H_D_Error_W2 is
 [[-5.57661421e-04 -8.17670288e-03]
 [-5.25497465e-05  6.45057165e-03]]
Using sum gradient........

W1 was :
 [[-0.23540341  0.60959171]
 [-1.65406763 -0.73146499]]
Updated W1 is: 
 [[-0.23484575  0.61776842]
 [-1.65401508 -0.73791556]]
W2 was :
 [[0.10741463]
 [1.17027091]]
Updated W2 is: 
 [[0.10544993]
 [1.17341222]]
The mean of the biases b gradient is:
 [4.51455239e-05 4.61682129e-05]
The b biases before the update are:
 [[-2.49081609 -1.56051538]]
The new updated bs are:
 [[-2.49086123 -1.56056154]]
The bias c is: 
 [[ 0.12563382]
 [-0.13047292]
 [-0.11628252]
 [ 0.12396995]]
c bias before: [[-0.20115142]]
The mean c bias after update: [[-0.2018635]]
The output is: 
 [[0.50254832]
 [0.47700185]
 [0.5328531 ]
 [0.49591294]]
Total Loss: 0.4951188760133873
Average Loss: 0.12377971900334682

RUN:
  93
FeedForward


Z1 is:
 [[-2.49086123 -1.56056154]
 [-4.14487631 -2.2984771 ]
 [-2.72570698 -0.94279313]
 [-4.37972207 -1.68070868]]
H is:
 [[0.07650133 0.17356608]
 [0.01559824 0.09124917]
 [0.06147338 0.28033649]
 [0.01237381 0.15700165]]
The c is
 [[-0.2018635]]
Z2 is:
 [[ 0.00986813]
 [-0.09314578]
 [ 0.13356913]
 [-0.01633103]]
Y^ -  the output is:
 [[0.50246701]
 [0.47673038]
 [0.53334273]
 [0.49591733]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50246701]
 [0.47673038]
 [0.53334273]
 [0.49591733]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12561369]
 [-0.13053407]
 [-0.11614552]
 [ 0.12397107]]
W2.T is
 [[0.10544993 1.17341222]]
 D_Error times W2.T
 [[ 0.01324596  0.14739664]
 [-0.01376481 -0.15317027]
 [-0.01224754 -0.13628657]
 [ 0.01307274  0.14546917]]
self.H_D_Error_W2 is
 [[ 0.00093581  0.02114271]
 [-0.00021136 -0.0127013 ]
 [-0.00070661 -0.02749554]
 [ 0.00015976  0.01925315]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00093581  0.02114271]
 [-0.00021136 -0.0127013 ]
 [-0.00070661 -0.02749554]
 [ 0.00015976  0.01925315]]
X_H_D_Error_W2 is
 [[-5.46856294e-04 -8.24238094e-03]
 [-5.15996418e-05  6.55185336e-03]]
Using sum gradient........

W1 was :
 [[-0.23484575  0.61776842]
 [-1.65401508 -0.73791556]]
Updated W1 is: 
 [[-0.2342989   0.6260108 ]
 [-1.65396348 -0.74446741]]
W2 was :
 [[0.10544993]
 [1.17341222]]
Updated W2 is: 
 [[0.10348228]
 [1.17661723]]
The mean of the biases b gradient is:
 [4.43994725e-05 4.97563288e-05]
The b biases before the update are:
 [[-2.49086123 -1.56056154]]
The new updated bs are:
 [[-2.49090563 -1.5606113 ]]
The bias c is: 
 [[ 0.12561369]
 [-0.13053407]
 [-0.11614552]
 [ 0.12397107]]
c bias before: [[-0.2018635]]
The mean c bias after update: [[-0.20258979]]
The output is: 
 [[0.50246701]
 [0.47673038]
 [0.53334273]
 [0.49591733]]
Total Loss: 0.4949936052799454
Average Loss: 0.12374840131998635

RUN:
  94
FeedForward


Z1 is:
 [[-2.49090563 -1.5606113 ]
 [-4.14486911 -2.30507871]
 [-2.72520453 -0.9346005 ]
 [-4.37916801 -1.67906791]]
H is:
 [[0.07649819 0.17355895]
 [0.01559835 0.09070322]
 [0.06150238 0.28199231]
 [0.01238058 0.15721893]]
The c is
 [[-0.20258979]]
Z2 is:
 [[ 0.00953886]
 [-0.09425267]
 [ 0.13557162]
 [-0.01632212]]
Y^ -  the output is:
 [[0.5023847 ]
 [0.47645426]
 [0.53384109]
 [0.49591956]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.5023847 ]
 [0.47645426]
 [0.53384109]
 [0.49591956]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12559332]
 [-0.13059618]
 [-0.11600587]
 [ 0.12397163]]
W2.T is
 [[0.10348228 1.17661723]]
 D_Error times W2.T
 [[ 0.01299668  0.14777526]
 [-0.01351439 -0.15366172]
 [-0.01200455 -0.13649451]
 [ 0.01282887  0.14586716]]
self.H_D_Error_W2 is
 [[ 0.00091817  0.02119633]
 [-0.00020751 -0.01267343]
 [-0.0006929  -0.0276364 ]
 [ 0.00015686  0.01932756]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00091817  0.02119633]
 [-0.00020751 -0.01267343]
 [-0.0006929  -0.0276364 ]
 [ 0.00015686  0.01932756]]
X_H_D_Error_W2 is
 [[-5.36038281e-04 -8.30883957e-03]
 [-5.06515039e-05  6.65413921e-03]]
Using sum gradient........

W1 was :
 [[-0.2342989   0.6260108 ]
 [-1.65396348 -0.74446741]]
Updated W1 is: 
 [[-0.23376286  0.63431964]
 [-1.65391283 -0.75112155]]
W2 was :
 [[0.10348228]
 [1.17661723]]
Updated W2 is: 
 [[0.1015115 ]
 [1.17988696]]
The mean of the biases b gradient is:
 [4.36535626e-05 5.35156134e-05]
The b biases before the update are:
 [[-2.49090563 -1.5606113 ]]
The new updated bs are:
 [[-2.49094928 -1.56066482]]
The bias c is: 
 [[ 0.12559332]
 [-0.13059618]
 [-0.11600587]
 [ 0.12397163]]
c bias before: [[-0.20258979]]
The mean c bias after update: [[-0.20333052]]
The output is: 
 [[0.5023847 ]
 [0.47645426]
 [0.53384109]
 [0.49591956]]
Total Loss: 0.4948654341258729
Average Loss: 0.12371635853146823

RUN:
  95
FeedForward


Z1 is:
 [[-2.49094928 -1.56066482]
 [-4.14486212 -2.31178637]
 [-2.72471214 -0.92634518]
 [-4.37862497 -1.67746673]]
H is:
 [[0.07649511 0.17355127]
 [0.01559845 0.09015151]
 [0.0615308  0.28366679]
 [0.01238723 0.15743121]]
The c is
 [[-0.20333052]]
Z2 is:
 [[ 0.0092055 ]
 [-0.0953785 ]
 [ 0.13761031]
 [-0.01632204]]
Y^ -  the output is:
 [[0.50230136]
 [0.47617343]
 [0.53434839]
 [0.49591958]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50230136]
 [0.47617343]
 [0.53434839]
 [0.49591958]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12557268]
 [-0.13065926]
 [-0.11586352]
 [ 0.12397164]]
W2.T is
 [[0.1015115  1.17988696]]
 D_Error times W2.T
 [[ 0.01274707  0.14816157]
 [-0.01326342 -0.15416316]
 [-0.01176148 -0.13670586]
 [ 0.01258455  0.14627252]]
self.H_D_Error_W2 is
 [[ 0.0009005   0.021251  ]
 [-0.00020366 -0.01264511]
 [-0.00067916 -0.02777862]
 [ 0.00015396  0.01940256]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.0009005   0.021251  ]
 [-0.00020366 -0.01264511]
 [-0.00067916 -0.02777862]
 [ 0.00015396  0.01940256]]
X_H_D_Error_W2 is
 [[-5.25207238e-04 -8.37606654e-03]
 [-4.97050547e-05  6.75744313e-03]]
Using sum gradient........

W1 was :
 [[-0.23376286  0.63431964]
 [-1.65391283 -0.75112155]]
Updated W1 is: 
 [[-0.23323765  0.6426957 ]
 [-1.65386313 -0.75787899]]
W2 was :
 [[0.1015115 ]
 [1.17988696]]
Updated W2 is: 
 [[0.09953739]
 [1.18322242]]
The mean of the biases b gradient is:
 [4.29075423e-05 5.74540923e-05]
The b biases before the update are:
 [[-2.49094928 -1.56066482]]
The new updated bs are:
 [[-2.49099219 -1.56072227]]
The bias c is: 
 [[ 0.12557268]
 [-0.13065926]
 [-0.11586352]
 [ 0.12397164]]
c bias before: [[-0.20333052]]
The mean c bias after update: [[-0.2040859]]
The output is: 
 [[0.50230136]
 [0.47617343]
 [0.53434839]
 [0.49591958]]
Total Loss: 0.4947342878737586
Average Loss: 0.12368357196843965

RUN:
  96
FeedForward


Z1 is:
 [[-2.49099219 -1.56072227]
 [-4.14485532 -2.31860126]
 [-2.72422984 -0.91802657]
 [-4.37809297 -1.67590556]]
H is:
 [[0.07649208 0.17354303]
 [0.01559856 0.08959408]
 [0.06155866 0.28536017]
 [0.01239374 0.1576384 ]]
The c is
 [[-0.2040859]]
Z2 is:
 [[ 0.00886793]
 [-0.09652353]
 [ 0.13968603]
 [-0.01633097]]
Y^ -  the output is:
 [[0.50221697]
 [0.47588784]
 [0.53486484]
 [0.49591735]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50221697]
 [0.47588784]
 [0.53486484]
 [0.49591735]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12555177]
 [-0.13072332]
 [-0.11571839]
 [ 0.12397107]]
W2.T is
 [[0.09953739 1.18322242]]
 D_Error times W2.T
 [[ 0.0124971   0.14855567]
 [-0.01301186 -0.15467477]
 [-0.01151831 -0.1369206 ]
 [ 0.01233976  0.14668535]]
self.H_D_Error_W2 is
 [[ 0.00088281  0.02130672]
 [-0.0001998  -0.01261635]
 [-0.0006654  -0.02792218]
 [ 0.00015104  0.01947813]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00088281  0.02130672]
 [-0.0001998  -0.01261635]
 [-0.0006654  -0.02792218]
 [ 0.00015104  0.01947813]]
X_H_D_Error_W2 is
 [[-5.14363036e-04 -8.44404888e-03]
 [-4.87600129e-05  6.86177859e-03]]
Using sum gradient........

W1 was :
 [[-0.23323765  0.6426957 ]
 [-1.65386313 -0.75787899]]
Updated W1 is: 
 [[-0.23272329  0.65113975]
 [-1.65381437 -0.76474077]]
W2 was :
 [[0.09953739]
 [1.18322242]]
Updated W2 is: 
 [[0.09755978]
 [1.18662464]]
The mean of the biases b gradient is:
 [4.21611496e-05 6.15799881e-05]
The b biases before the update are:
 [[-2.49099219 -1.56072227]]
The new updated bs are:
 [[-2.49103435 -1.56078385]]
The bias c is: 
 [[ 0.12555177]
 [-0.13072332]
 [-0.11571839]
 [ 0.12397107]]
c bias before: [[-0.2040859]]
The mean c bias after update: [[-0.20485618]]
The output is: 
 [[0.50221697]
 [0.47588784]
 [0.53486484]
 [0.49591735]]
Total Loss: 0.4946000903324589
Average Loss: 0.12365002258311472

RUN:
  97
FeedForward


Z1 is:
 [[-2.49103435 -1.56078385]
 [-4.14484872 -2.32552462]
 [-2.72375764 -0.9096441 ]
 [-4.37757201 -1.67438487]]
H is:
 [[0.0764891  0.1735342 ]
 [0.01559866 0.08903097]
 [0.06158594 0.28707267]
 [0.01240011 0.15784044]]
The c is
 [[-0.20485618]]
Z2 is:
 [[ 0.00852603]
 [-0.09768804]
 [ 0.14179963]
 [-0.01634908]]
Y^ -  the output is:
 [[0.5021315 ]
 [0.47559739]
 [0.53539063]
 [0.49591282]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.5021315 ]
 [0.47559739]
 [0.53539063]
 [0.49591282]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12553059]
 [-0.13078838]
 [-0.11557042]
 [ 0.12396992]]
W2.T is
 [[0.09755978 1.18662464]]
 D_Error times W2.T
 [[ 0.01224674  0.14895769]
 [-0.01275968 -0.15519671]
 [-0.01127502 -0.13713871]
 [ 0.01209448  0.14710576]]
self.H_D_Error_W2 is
 [[ 0.00086509  0.02136352]
 [-0.00019593 -0.01258714]
 [-0.00065162 -0.02806708]
 [ 0.00014811  0.0195543 ]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00086509  0.02136352]
 [-0.00019593 -0.01258714]
 [-0.00065162 -0.02806708]
 [ 0.00014811  0.0195543 ]]
X_H_D_Error_W2 is
 [[-5.03505566e-04 -8.51277281e-03]
 [-4.78160939e-05  6.96715855e-03]]
Using sum gradient........

W1 was :
 [[-0.23272329  0.65113975]
 [-1.65381437 -0.76474077]]
Updated W1 is: 
 [[-0.23221978  0.65965253]
 [-1.65376655 -0.77170793]]
W2 was :
 [[0.09755978]
 [1.18662464]]
Updated W2 is: 
 [[0.09557845]
 [1.19009465]]
The mean of the biases b gradient is:
 [4.14141125e-05 6.59017299e-05]
The b biases before the update are:
 [[-2.49103435 -1.56078385]]
The new updated bs are:
 [[-2.49107577 -1.56084975]]
The bias c is: 
 [[ 0.12553059]
 [-0.13078838]
 [-0.11557042]
 [ 0.12396992]]
c bias before: [[-0.20485618]]
The mean c bias after update: [[-0.20564161]]
The output is: 
 [[0.5021315 ]
 [0.47559739]
 [0.53539063]
 [0.49591282]]
Total Loss: 0.49446276378983517
Average Loss: 0.12361569094745879

RUN:
  98
FeedForward


Z1 is:
 [[-2.49107577 -1.56084975]
 [-4.14484232 -2.33255768]
 [-2.72329555 -0.90119723]
 [-4.3770621  -1.67290516]]
H is:
 [[0.07648617 0.17352475]
 [0.01559876 0.0884622 ]
 [0.06161265 0.28880453]
 [0.01240636 0.15803723]]
The c is
 [[-0.20564161]]
Z2 is:
 [[ 0.00817969]
 [-0.09887231]
 [ 0.14395196]
 [-0.01637657]]
Y^ -  the output is:
 [[0.50204491]
 [0.47530204]
 [0.53592597]
 [0.49590595]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50204491]
 [0.47530204]
 [0.53592597]
 [0.49590595]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12550913]
 [-0.13085443]
 [-0.11541954]
 [ 0.12396818]]
W2.T is
 [[0.09557845 1.19009465]]
 D_Error times W2.T
 [[ 0.01199597  0.14936774]
 [-0.01250686 -0.15572916]
 [-0.01103162 -0.13736017]
 [ 0.01184869  0.14753386]]
self.H_D_Error_W2 is
 [[ 0.00084735  0.02142141]
 [-0.00019205 -0.01255748]
 [-0.00063781 -0.0282133 ]
 [ 0.00014518  0.01963107]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00084735  0.02142141]
 [-0.00019205 -0.01255748]
 [-0.00063781 -0.0282133 ]
 [ 0.00014518  0.01963107]]
X_H_D_Error_W2 is
 [[-4.92634740e-04 -8.58222371e-03]
 [-4.68730099e-05  7.07359545e-03]]
Using sum gradient........

W1 was :
 [[-0.23221978  0.65965253]
 [-1.65376655 -0.77170793]]
Updated W1 is: 
 [[-0.23172715  0.66823475]
 [-1.65371968 -0.77878153]]
W2 was :
 [[0.09557845]
 [1.19009465]]
Updated W2 is: 
 [[0.09359322]
 [1.19363348]]
The mean of the biases b gradient is:
 [4.06661486e-05 7.04279621e-05]
The b biases before the update are:
 [[-2.49107577 -1.56084975]]
The new updated bs are:
 [[-2.49111643 -1.56092018]]
The bias c is: 
 [[ 0.12550913]
 [-0.13085443]
 [-0.11541954]
 [ 0.12396818]]
c bias before: [[-0.20564161]]
The mean c bias after update: [[-0.20644244]]
The output is: 
 [[0.50204491]
 [0.47530204]
 [0.53592597]
 [0.49590595]]
Total Loss: 0.4943222290077972
Average Loss: 0.1235805572519493

RUN:
  99
FeedForward


Z1 is:
 [[-2.49111643 -1.56092018]
 [-4.14483611 -2.33970171]
 [-2.72284358 -0.89268543]
 [-4.37656326 -1.67146696]]
H is:
 [[0.0764833  0.17351465]
 [0.01559885 0.08788782]
 [0.06163879 0.29055596]
 [0.01241247 0.15822869]]
The c is
 [[-0.20644244]]
Z2 is:
 [[ 0.00782876]
 [-0.10007665]
 [ 0.14614385]
 [-0.01641366]]
Y^ -  the output is:
 [[0.50195718]
 [0.4750017 ]
 [0.53647107]
 [0.49589668]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50195718]
 [0.4750017 ]
 [0.53647107]
 [0.49589668]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12548737]
 [-0.1309215 ]
 [-0.11526567]
 [ 0.12396582]]
W2.T is
 [[0.09359322 1.19363348]]
 D_Error times W2.T
 [[ 0.01174477  0.14978593]
 [-0.01225336 -0.15627228]
 [-0.01078808 -0.13758497]
 [ 0.01160236  0.14796975]]
self.H_D_Error_W2 is
 [[ 0.00082958  0.0214804 ]
 [-0.00018816 -0.01252734]
 [-0.00062398 -0.02836083]
 [ 0.00014223  0.01970844]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00082958  0.0214804 ]
 [-0.00018816 -0.01252734]
 [-0.00062398 -0.02836083]
 [ 0.00014223  0.01970844]]
X_H_D_Error_W2 is
 [[-4.81750489e-04 -8.65238606e-03]
 [-4.59304686e-05  7.18110117e-03]]
Using sum gradient........

W1 was :
 [[-0.23172715  0.66823475]
 [-1.65371968 -0.77878153]]
Updated W1 is: 
 [[-0.2312454   0.67688714]
 [-1.65367375 -0.78596263]]
W2 was :
 [[0.09359322]
 [1.19363348]]
Updated W2 is: 
 [[0.09160387]
 [1.19724216]]
The mean of the biases b gradient is:
 [3.99169656e-05 7.51675519e-05]
The b biases before the update are:
 [[-2.49111643 -1.56092018]]
The new updated bs are:
 [[-2.49115635 -1.56099535]]
The bias c is: 
 [[ 0.12548737]
 [-0.1309215 ]
 [-0.11526567]
 [ 0.12396582]]
c bias before: [[-0.20644244]]
The mean c bias after update: [[-0.20725895]]
The output is: 
 [[0.50195718]
 [0.4750017 ]
 [0.53647107]
 [0.49589668]]
Total Loss: 0.4941784052198275
Average Loss: 0.12354460130495687

RUN:
  100
FeedForward


Z1 is:
 [[-2.49115635 -1.56099535]
 [-4.1448301  -2.34695797]
 [-2.72240175 -0.88410821]
 [-4.37607549 -1.67007084]]
H is:
 [[0.07648048 0.17350387]
 [0.01559895 0.08730787]
 [0.06166435 0.29232718]
 [0.01241845 0.15841473]]
The c is
 [[-0.20725895]]
Z2 is:
 [[ 0.0074731 ]
 [-0.10130136]
 [ 0.14837617]
 [-0.01646057]]
Y^ -  the output is:
 [[0.50186827]
 [0.4746963 ]
 [0.53702614]
 [0.49588495]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50186827]
 [0.4746963 ]
 [0.53702614]
 [0.49588495]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12546532]
 [-0.13098959]
 [-0.11510876]
 [ 0.12396284]]
W2.T is
 [[0.09160387 1.19724216]]
 D_Error times W2.T
 [[ 0.01149311  0.15021237]
 [-0.01199915 -0.15682626]
 [-0.01054441 -0.13781306]
 [ 0.01135548  0.14841354]]
self.H_D_Error_W2 is
 [[ 0.00081177  0.02154049]
 [-0.00018425 -0.01249673]
 [-0.00061012 -0.02850966]
 [ 0.00013927  0.01978642]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00081177  0.02154049]
 [-0.00018425 -0.01249673]
 [-0.00061012 -0.02850966]
 [ 0.00013927  0.01978642]]
X_H_D_Error_W2 is
 [[-4.70852770e-04 -8.72324338e-03]
 [-4.49881740e-05  7.28968698e-03]]
Using sum gradient........

W1 was :
 [[-0.2312454   0.67688714]
 [-1.65367375 -0.78596263]]
Updated W1 is: 
 [[-0.23077454  0.68561038]
 [-1.65362876 -0.79325231]]
W2 was :
 [[0.09160387]
 [1.19724216]]
Updated W2 is: 
 [[0.0896102 ]
 [1.20092175]]
The mean of the biases b gradient is:
 [3.91662609e-05 8.01295967e-05]
The b biases before the update are:
 [[-2.49115635 -1.56099535]]
The new updated bs are:
 [[-2.49119552 -1.56107548]]
The bias c is: 
 [[ 0.12546532]
 [-0.13098959]
 [-0.11510876]
 [ 0.12396284]]
c bias before: [[-0.20725895]]
The mean c bias after update: [[-0.2080914]]
The output is: 
 [[0.50186827]
 [0.4746963 ]
 [0.53702614]
 [0.49588495]]
Total Loss: 0.4940312101311709
Average Loss: 0.12350780253279273

RUN:
  101
FeedForward


Z1 is:
 [[-2.49119552 -1.56107548]
 [-4.14482428 -2.35432779]
 [-2.72197006 -0.8754651 ]
 [-4.37559882 -1.66871741]]
H is:
 [[0.07647772 0.17349238]
 [0.01559903 0.08672239]
 [0.06168933 0.2941184 ]
 [0.0124243  0.15859526]]
The c is
 [[-0.2080914]]
Z2 is:
 [[ 0.00711255]
 [-0.10254677]
 [ 0.15064978]
 [-0.01651757]]
Y^ -  the output is:
 [[0.50177813]
 [0.47438575]
 [0.53759138]
 [0.4958707 ]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50177813]
 [0.47438575]
 [0.53759138]
 [0.4958707 ]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12544295]
 [-0.13105871]
 [-0.11494872]
 [ 0.12395922]]
W2.T is
 [[0.0896102  1.20092175]]
 D_Error times W2.T
 [[ 0.01124097  0.15064716]
 [-0.0117442  -0.15739126]
 [-0.01030058 -0.13804442]
 [ 0.01110801  0.14886532]]
self.H_D_Error_W2 is
 [[ 0.00079394  0.02160171]
 [-0.00018034 -0.01246564]
 [-0.00059624 -0.02865978]
 [ 0.00013629  0.01986501]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00079394  0.02160171]
 [-0.00018034 -0.01246564]
 [-0.00059624 -0.02865978]
 [ 0.00013629  0.01986501]]
X_H_D_Error_W2 is
 [[-4.59941559e-04 -8.79477824e-03]
 [-4.40458252e-05  7.39936351e-03]]
Using sum gradient........

W1 was :
 [[-0.23077454  0.68561038]
 [-1.65362876 -0.79325231]]
Updated W1 is: 
 [[-0.2303146   0.69440516]
 [-1.65358471 -0.80065168]]
W2 was :
 [[0.0896102 ]
 [1.20092175]]
Updated W2 is: 
 [[0.087612  ]
 [1.20467327]]
The mean of the biases b gradient is:
 [3.84137218e-05 8.53234312e-05]
The b biases before the update are:
 [[-2.49119552 -1.56107548]]
The new updated bs are:
 [[-2.49123393 -1.5611608 ]]
The bias c is: 
 [[ 0.12544295]
 [-0.13105871]
 [-0.11494872]
 [ 0.12395922]]
c bias before: [[-0.2080914]]
The mean c bias after update: [[-0.20894009]]
The output is: 
 [[0.50177813]
 [0.47438575]
 [0.53759138]
 [0.4958707 ]]
Total Loss: 0.49388055992188085
Average Loss: 0.12347013998047021

RUN:
  102
FeedForward


Z1 is:
 [[-2.49123393 -1.5611608 ]
 [-4.14481864 -2.36181248]
 [-2.72154853 -0.86675564]
 [-4.37513325 -1.66740732]]
H is:
 [[0.076475   0.17348014]
 [0.01559912 0.08613142]
 [0.06171374 0.29592983]
 [0.01243002 0.15877016]]
The c is
 [[-0.20894009]]
Z2 is:
 [[ 0.00674693]
 [-0.10381319]
 [ 0.15296554]
 [-0.0165849 ]]
Y^ -  the output is:
 [[0.50168673]
 [0.47406998]
 [0.53816699]
 [0.49585387]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50168673]
 [0.47406998]
 [0.53816699]
 [0.49585387]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12542025]
 [-0.13112889]
 [-0.11478549]
 [ 0.12395494]]
W2.T is
 [[0.087612   1.20467327]]
 D_Error times W2.T
 [[ 0.01098832  0.15109043]
 [-0.01148846 -0.15796746]
 [-0.01005659 -0.13827901]
 [ 0.01085994  0.14932521]]
self.H_D_Error_W2 is
 [[ 0.00077607  0.02166407]
 [-0.00017641 -0.01243406]
 [-0.00058233 -0.02881117]
 [ 0.00013331  0.0199442 ]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00077607  0.02166407]
 [-0.00017641 -0.01243406]
 [-0.00058233 -0.02881117]
 [ 0.00013331  0.0199442 ]]
X_H_D_Error_W2 is
 [[-4.49016858e-04 -8.86697217e-03]
 [-4.31031166e-05  7.51014073e-03]]
Using sum gradient........

W1 was :
 [[-0.2303146   0.69440516]
 [-1.65358471 -0.80065168]]
Updated W1 is: 
 [[-0.22986559  0.70327213]
 [-1.65354161 -0.80816182]]
W2 was :
 [[0.087612  ]
 [1.20467327]]
Updated W2 is: 
 [[0.08560906]
 [1.20849777]]
The mean of the biases b gradient is:
 [3.76590256e-05 9.07586329e-05]
The b biases before the update are:
 [[-2.49123393 -1.5611608 ]]
The new updated bs are:
 [[-2.49127159 -1.56125156]]
The bias c is: 
 [[ 0.12542025]
 [-0.13112889]
 [-0.11478549]
 [ 0.12395494]]
c bias before: [[-0.20894009]]
The mean c bias after update: [[-0.20980529]]
The output is: 
 [[0.50168673]
 [0.47406998]
 [0.53816699]
 [0.49585387]]
Total Loss: 0.49372636925291935
Average Loss: 0.12343159231322984

RUN:
  103
FeedForward


Z1 is:
 [[-2.49127159 -1.56125156]
 [-4.1448132  -2.36941338]
 [-2.72113717 -0.85797943]
 [-4.37467878 -1.66614125]]
H is:
 [[0.07647234 0.17346713]
 [0.0155992  0.08553501]
 [0.06173756 0.29776167]
 [0.0124356  0.15893933]]
The c is
 [[-0.20980529]]
Z2 is:
 [[ 0.00637607]
 [-0.10510099]
 [ 0.15532432]
 [-0.01666287]]
Y^ -  the output is:
 [[0.50159401]
 [0.47374891]
 [0.5387532 ]
 [0.49583438]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50159401]
 [0.47374891]
 [0.5387532 ]
 [0.49583438]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12539723]
 [-0.13120012]
 [-0.11461899]
 [ 0.12394999]]
W2.T is
 [[0.08560906 1.20849777]]
 D_Error times W2.T
 [[ 0.01073514  0.15154227]
 [-0.01123192 -0.15855505]
 [-0.00981242 -0.1385168 ]
 [ 0.01061124  0.14979329]]
self.H_D_Error_W2 is
 [[ 0.00075816  0.02172757]
 [-0.00017248 -0.01240198]
 [-0.00056839 -0.02896382]
 [ 0.00013032  0.02002401]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00075816  0.02172757]
 [-0.00017248 -0.01240198]
 [-0.00056839 -0.02896382]
 [ 0.00013032  0.02002401]]
X_H_D_Error_W2 is
 [[-4.38078696e-04 -8.93980561e-03]
 [-4.21597372e-05  7.62202789e-03]]
Using sum gradient........

W1 was :
 [[-0.22986559  0.70327213]
 [-1.65354161 -0.80816182]]
Updated W1 is: 
 [[-0.22942751  0.71221194]
 [-1.65349945 -0.81578385]]
W2 was :
 [[0.08560906]
 [1.20849777]]
Updated W2 is: 
 [[0.08360116]
 [1.21239629]]
The mean of the biases b gradient is:
 [3.69018391e-05 9.64450279e-05]
The b biases before the update are:
 [[-2.49127159 -1.56125156]]
The new updated bs are:
 [[-2.49130849 -1.561348  ]]
The bias c is: 
 [[ 0.12539723]
 [-0.13120012]
 [-0.11461899]
 [ 0.12394999]]
c bias before: [[-0.20980529]]
The mean c bias after update: [[-0.21068732]]
The output is: 
 [[0.50159401]
 [0.47374891]
 [0.5387532 ]
 [0.49583438]]
Total Loss: 0.49356855127551824
Average Loss: 0.12339213781887956

RUN:
  104
FeedForward


Z1 is:
 [[-2.49130849 -1.561348  ]
 [-4.14480794 -2.37713185]
 [-2.720736   -0.84913607]
 [-4.37423545 -1.66491992]]
H is:
 [[0.07646974 0.1734533 ]
 [0.01559929 0.08493321]
 [0.0617608  0.29961412]
 [0.01244104 0.15910266]]
The c is
 [[-0.21068732]]
Z2 is:
 [[ 0.00599978]
 [-0.10641049]
 [ 0.157727  ]
 [-0.01675175]]
Y^ -  the output is:
 [[0.50149994]
 [0.47342245]
 [0.53935021]
 [0.49581216]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50149994]
 [0.47342245]
 [0.53935021]
 [0.49581216]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12537386]
 [-0.13127243]
 [-0.11444916]
 [ 0.12394434]]
W2.T is
 [[0.08360116 1.21239629]]
 D_Error times W2.T
 [[ 0.0104814   0.1520028 ]
 [-0.01097453 -0.15915421]
 [-0.00956808 -0.13875774]
 [ 0.01036189  0.15026966]]
self.H_D_Error_W2 is
 [[ 0.00074022  0.02179222]
 [-0.00016852 -0.0123694 ]
 [-0.00055444 -0.02911769]
 [ 0.00012731  0.02010443]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00074022  0.02179222]
 [-0.00016852 -0.0123694 ]
 [-0.00055444 -0.02911769]
 [ 0.00012731  0.02010443]]
X_H_D_Error_W2 is
 [[-4.27127127e-04 -9.01325793e-03]
 [-4.12153708e-05  7.73503347e-03]]
Using sum gradient........

W1 was :
 [[-0.22942751  0.71221194]
 [-1.65349945 -0.81578385]]
Updated W1 is: 
 [[-0.22900038  0.72122519]
 [-1.65345823 -0.82351888]]
W2 was :
 [[0.08360116]
 [1.21239629]]
Updated W2 is: 
 [[0.08158809]
 [1.21636988]]
The mean of the biases b gradient is:
 [3.61418188e-05 1.02392695e-04]
The b biases before the update are:
 [[-2.49130849 -1.561348  ]]
The new updated bs are:
 [[-2.49134463 -1.5614504 ]]
The bias c is: 
 [[ 0.12537386]
 [-0.13127243]
 [-0.11444916]
 [ 0.12394434]]
c bias before: [[-0.21068732]]
The mean c bias after update: [[-0.21158647]]
The output is: 
 [[0.50149994]
 [0.47342245]
 [0.53935021]
 [0.49581216]]
Total Loss: 0.4934070176440128
Average Loss: 0.1233517544110032

RUN:
  105
FeedForward


Z1 is:
 [[-2.49134463 -1.5614504 ]
 [-4.14480287 -2.38496928]
 [-2.72034501 -0.8402252 ]
 [-4.37380325 -1.66374408]]
H is:
 [[0.07646719 0.17343862]
 [0.01559936 0.08432607]
 [0.06178346 0.30148736]
 [0.01244635 0.15926004]]
The c is
 [[-0.21158647]]
Z2 is:
 [[ 0.00561786]
 [-0.10774206]
 [ 0.16017446]
 [-0.01685188]]
Y^ -  the output is:
 [[0.50140446]
 [0.47309051]
 [0.53995822]
 [0.49578713]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50140446]
 [0.47309051]
 [0.53995822]
 [0.49578713]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12535013]
 [-0.13134583]
 [-0.11427591]
 [ 0.12393798]]
W2.T is
 [[0.08158809 1.21636988]]
 D_Error times W2.T
 [[ 0.01022708  0.15247212]
 [-0.01071625 -0.15976511]
 [-0.00932355 -0.13900178]
 [ 0.01011186  0.15075443]]
self.H_D_Error_W2 is
 [[ 0.00072224  0.02185805]
 [-0.00016456 -0.01233629]
 [-0.00054045 -0.02927276]
 [ 0.00012429  0.02018546]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00072224  0.02185805]
 [-0.00016456 -0.01233629]
 [-0.00054045 -0.02927276]
 [ 0.00012429  0.02018546]]
X_H_D_Error_W2 is
 [[-4.16162234e-04 -9.08730731e-03]
 [-4.02696953e-05  7.84916513e-03]]
Using sum gradient........

W1 was :
 [[-0.22900038  0.72122519]
 [-1.65345823 -0.82351888]]
Updated W1 is: 
 [[-0.22858422  0.7303125 ]
 [-1.65341796 -0.83136805]]
W2 was :
 [[0.08158809]
 [1.21636988]]
Updated W2 is: 
 [[0.07956962]
 [1.22041958]]
The mean of the biases b gradient is:
 [3.53786107e-05 1.08611971e-04]
The b biases before the update are:
 [[-2.49134463 -1.5614504 ]]
The new updated bs are:
 [[-2.49138001 -1.56155901]]
The bias c is: 
 [[ 0.12535013]
 [-0.13134583]
 [-0.11427591]
 [ 0.12393798]]
c bias before: [[-0.21158647]]
The mean c bias after update: [[-0.21250306]]
The output is: 
 [[0.50140446]
 [0.47309051]
 [0.53995822]
 [0.49578713]]
Total Loss: 0.49324167853236783
Average Loss: 0.12331041963309196

RUN:
  106
FeedForward


Z1 is:
 [[-2.49138001 -1.56155901]
 [-4.14479798 -2.39292705]
 [-2.71996423 -0.83124651]
 [-4.37338219 -1.66261455]]
H is:
 [[0.07646469 0.17342305]
 [0.01559944 0.08371364]
 [0.06180554 0.30338157]
 [0.01245153 0.15941134]]
The c is
 [[-0.21250306]]
Z2 is:
 [[ 0.00523009]
 [-0.10909606]
 [ 0.16266759]
 [-0.01696358]]
Y^ -  the output is:
 [[0.50130752]
 [0.472753  ]
 [0.54057746]
 [0.49575921]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50130752]
 [0.472753  ]
 [0.54057746]
 [0.49575921]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12532602]
 [-0.13142032]
 [-0.11409918]
 [ 0.12393089]]
W2.T is
 [[0.07956962 1.22041958]]
 D_Error times W2.T
 [[ 0.00997214  0.15295033]
 [-0.01045706 -0.16038793]
 [-0.00907883 -0.13924888]
 [ 0.00986113  0.15124768]]
self.H_D_Error_W2 is
 [[ 0.00070421  0.02192505]
 [-0.00016058 -0.01230266]
 [-0.00052644 -0.02942902]
 [ 0.00012126  0.02026709]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00070421  0.02192505]
 [-0.00016058 -0.01230266]
 [-0.00052644 -0.02942902]
 [ 0.00012126  0.02026709]]
X_H_D_Error_W2 is
 [[-4.05184129e-04 -9.16193075e-03]
 [-3.93223823e-05  7.96442968e-03]]
Using sum gradient........

W1 was :
 [[-0.22858422  0.7303125 ]
 [-1.65341796 -0.83136805]]
Updated W1 is: 
 [[-0.22817903  0.73947443]
 [-1.65337864 -0.83933248]]
W2 was :
 [[0.07956962]
 [1.22041958]]
Updated W2 is: 
 [[0.07754552]
 [1.22454643]]
The mean of the biases b gradient is:
 [3.46118505e-05 1.15113449e-04]
The b biases before the update are:
 [[-2.49138001 -1.56155901]]
The new updated bs are:
 [[-2.49141462 -1.56167412]]
The bias c is: 
 [[ 0.12532602]
 [-0.13142032]
 [-0.11409918]
 [ 0.12393089]]
c bias before: [[-0.21250306]]
The mean c bias after update: [[-0.21343741]]
The output is: 
 [[0.50130752]
 [0.472753  ]
 [0.54057746]
 [0.49575921]]
Total Loss: 0.49307244265461847
Average Loss: 0.12326811066365462

RUN:
  107
FeedForward


Z1 is:
 [[-2.49141462 -1.56167412]
 [-4.14479327 -2.4010066 ]
 [-2.71959366 -0.82219969]
 [-4.3729723  -1.66153217]]
H is:
 [[0.07646224 0.17340655]
 [0.01559951 0.08309597]
 [0.06182703 0.30529693]
 [0.01245657 0.15955643]]
The c is
 [[-0.21343741]]
Z2 is:
 [[ 0.00483627]
 [-0.11047287]
 [ 0.16520726]
 [-0.01708721]]
Y^ -  the output is:
 [[0.50120906]
 [0.47240984]
 [0.54120813]
 [0.4957283 ]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50120906]
 [0.47240984]
 [0.54120813]
 [0.4957283 ]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12530153]
 [-0.13149593]
 [-0.11391889]
 [ 0.12392303]]
W2.T is
 [[0.07754552 1.22454643]]
 D_Error times W2.T
 [[ 0.00971657  0.15343755]
 [-0.01019692 -0.16102287]
 [-0.0088339  -0.13949897]
 [ 0.00960968  0.1517495 ]]
self.H_D_Error_W2 is
 [[ 0.00068614  0.02199323]
 [-0.00015659 -0.0122685 ]
 [-0.00051241 -0.02958644]
 [ 0.00011821  0.02034933]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00068614  0.02199323]
 [-0.00015659 -0.0122685 ]
 [-0.00051241 -0.02958644]
 [ 0.00011821  0.02034933]]
X_H_D_Error_W2 is
 [[-3.94192956e-04 -9.23710403e-03]
 [-3.83730974e-05  8.08083302e-03]]
Using sum gradient........

W1 was :
 [[-0.22817903  0.73947443]
 [-1.65337864 -0.83933248]]
Updated W1 is: 
 [[-0.22778484  0.74871154]
 [-1.65334027 -0.84741331]]
W2 was :
 [[0.07754552]
 [1.22454643]]
Updated W2 is: 
 [[0.07551556]
 [1.22875147]]
The mean of the biases b gradient is:
 [3.38411630e-05 1.21907988e-04]
The b biases before the update are:
 [[-2.49141462 -1.56167412]]
The new updated bs are:
 [[-2.49144847 -1.56179603]]
The bias c is: 
 [[ 0.12530153]
 [-0.13149593]
 [-0.11391889]
 [ 0.12392303]]
c bias before: [[-0.21343741]]
The mean c bias after update: [[-0.21438985]]
The output is: 
 [[0.50120906]
 [0.47240984]
 [0.54120813]
 [0.4957283 ]]
Total Loss: 0.49289921728945635
Average Loss: 0.12322480432236409

RUN:
  108
FeedForward


Z1 is:
 [[-2.49144847 -1.56179603]
 [-4.14478873 -2.40920934]
 [-2.7192333  -0.81308449]
 [-4.37257357 -1.6604978 ]]
H is:
 [[0.07645985 0.17338908]
 [0.01559958 0.08247313]
 [0.06184794 0.3072336 ]
 [0.01246148 0.15969518]]
The c is
 [[-0.21438985]]
Z2 is:
 [[ 0.00443615]
 [-0.11187286]
 [ 0.16779437]
 [-0.01722312]]
Y^ -  the output is:
 [[0.50110903]
 [0.47206092]
 [0.54185045]
 [0.49569433]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50110903]
 [0.47206092]
 [0.54185045]
 [0.49569433]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12527664]
 [-0.13157267]
 [-0.11373496]
 [ 0.12391439]]
W2.T is
 [[0.07551556 1.22875147]]
 D_Error times W2.T
 [[ 0.00946034  0.15393386]
 [-0.00993578 -0.16167011]
 [-0.00858876 -0.139752  ]
 [ 0.00935747  0.15225999]]
self.H_D_Error_W2 is
 [[ 0.00066803  0.02206262]
 [-0.00015258 -0.01223379]
 [-0.00049834 -0.02974497]
 [ 0.00011515  0.02043217]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00066803  0.02206262]
 [-0.00015258 -0.01223379]
 [-0.00049834 -0.02974497]
 [ 0.00011515  0.02043217]]
X_H_D_Error_W2 is
 [[-3.83188889e-04 -9.31280167e-03]
 [-3.74214991e-05  8.19838006e-03]]
Using sum gradient........

W1 was :
 [[-0.22778484  0.74871154]
 [-1.65334027 -0.84741331]]
Updated W1 is: 
 [[-0.22740165  0.75802434]
 [-1.65330285 -0.85561169]]
W2 was :
 [[0.07551556]
 [1.22875147]]
Updated W2 is: 
 [[0.07347952]
 [1.23303575]]
The mean of the biases b gradient is:
 [3.30661624e-05 1.29006707e-04]
The b biases before the update are:
 [[-2.49144847 -1.56179603]]
The new updated bs are:
 [[-2.49148153 -1.56192504]]
The bias c is: 
 [[ 0.12527664]
 [-0.13157267]
 [-0.11373496]
 [ 0.12391439]]
c bias before: [[-0.21438985]]
The mean c bias after update: [[-0.2153607]]
The output is: 
 [[0.50110903]
 [0.47206092]
 [0.54185045]
 [0.49569433]]
Total Loss: 0.4927219083091911
Average Loss: 0.12318047707729778

RUN:
  109
FeedForward


Z1 is:
 [[-2.49148153 -1.56192504]
 [-4.14478438 -2.41753672]
 [-2.71888318 -0.8039007 ]
 [-4.37218603 -1.65951239]]
H is:
 [[0.07645752 0.17337059]
 [0.01559965 0.08184517]
 [0.06186826 0.30919174]
 [0.01246625 0.15982746]]
The c is
 [[-0.2153607]]
Z2 is:
 [[ 0.0040295 ]
 [-0.11329643]
 [ 0.17042982]
 [-0.01737171]]
Y^ -  the output is:
 [[0.50100737]
 [0.47170615]
 [0.54250462]
 [0.49565718]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50100737]
 [0.47170615]
 [0.54250462]
 [0.49565718]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12525133]
 [-0.13165054]
 [-0.11354731]
 [ 0.12390495]]
W2.T is
 [[0.07347952 1.23303575]]
 D_Error times W2.T
 [[ 0.00920341  0.15443937]
 [-0.00967362 -0.16232982]
 [-0.0083434  -0.1400079 ]
 [ 0.00910448  0.15277923]]
self.H_D_Error_W2 is
 [[ 0.00064987  0.02213321]
 [-0.00014855 -0.01219852]
 [-0.00048426 -0.0299046 ]
 [ 0.00011208  0.0205156 ]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00064987  0.02213321]
 [-0.00014855 -0.01219852]
 [-0.00048426 -0.0299046 ]
 [ 0.00011208  0.0205156 ]]
X_H_D_Error_W2 is
 [[-3.72172137e-04 -9.38899686e-03]
 [-3.64672392e-05  8.31707471e-03]]
Using sum gradient........

W1 was :
 [[-0.22740165  0.75802434]
 [-1.65330285 -0.85561169]]
Updated W1 is: 
 [[-0.22702948  0.76741333]
 [-1.65326638 -0.86392876]]
W2 was :
 [[0.07347952]
 [1.23303575]]
Updated W2 is: 
 [[0.07143716]
 [1.23740029]]
The mean of the biases b gradient is:
 [3.22864518e-05 1.36420988e-04]
The b biases before the update are:
 [[-2.49148153 -1.56192504]]
The new updated bs are:
 [[-2.49151382 -1.56206146]]
The bias c is: 
 [[ 0.12525133]
 [-0.13165054]
 [-0.11354731]
 [ 0.12390495]]
c bias before: [[-0.2153607]]
The mean c bias after update: [[-0.21635031]]
The output is: 
 [[0.50100737]
 [0.47170615]
 [0.54250462]
 [0.49565718]]
Total Loss: 0.4925404202133244
Average Loss: 0.1231351050533311

RUN:
  110
FeedForward


Z1 is:
 [[-2.49151382 -1.56206146]
 [-4.1447802  -2.42599022]
 [-2.7185433  -0.79464812]
 [-4.37180968 -1.65857689]]
H is:
 [[0.07645524 0.17335104]
 [0.01559971 0.08121216]
 [0.06188799 0.3111715 ]
 [0.01247088 0.15995313]]
The c is
 [[-0.21635031]]
Z2 is:
 [[ 0.00361606]
 [-0.11474396]
 [ 0.1731145 ]
 [-0.01753338]]
Y^ -  the output is:
 [[0.50090402]
 [0.47134544]
 [0.54317086]
 [0.49561677]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50090402]
 [0.47134544]
 [0.54317086]
 [0.49561677]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12522559]
 [-0.13172957]
 [-0.11335588]
 [ 0.12389467]]
W2.T is
 [[0.07143716 1.23740029]]
 D_Error times W2.T
 [[ 0.00894576  0.15495419]
 [-0.00941039 -0.16300221]
 [-0.00809782 -0.1402666 ]
 [ 0.00885068  0.1533073 ]]
self.H_D_Error_W2 is
 [[ 0.00063166  0.02220501]
 [-0.00014451 -0.01216269]
 [-0.00047014 -0.03006528]
 [ 0.000109    0.02059961]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00063166  0.02220501]
 [-0.00014451 -0.01216269]
 [-0.00047014 -0.03006528]
 [ 0.000109    0.02059961]]
X_H_D_Error_W2 is
 [[-3.61142946e-04 -9.46566148e-03]
 [-3.55099619e-05  8.43691976e-03]]
Using sum gradient........

W1 was :
 [[-0.22702948  0.76741333]
 [-1.65326638 -0.86392876]]
Updated W1 is: 
 [[-0.22666834  0.776879  ]
 [-1.65323087 -0.87236568]]
W2 was :
 [[0.07143716]
 [1.23740029]]
Updated W2 is: 
 [[0.06938825]
 [1.24184613]]
The mean of the biases b gradient is:
 [3.15016234e-05 1.44162474e-04]
The b biases before the update are:
 [[-2.49151382 -1.56206146]]
The new updated bs are:
 [[-2.49154532 -1.56220562]]
The bias c is: 
 [[ 0.12522559]
 [-0.13172957]
 [-0.11335588]
 [ 0.12389467]]
c bias before: [[-0.21635031]]
The mean c bias after update: [[-0.21735901]]
The output is: 
 [[0.50090402]
 [0.47134544]
 [0.54317086]
 [0.49561677]]
Total Loss: 0.4923546561669745
Average Loss: 0.12308866404174362

RUN:
  111
FeedForward


Z1 is:
 [[-2.49154532 -1.56220562]
 [-4.14477619 -2.4345713 ]
 [-2.71821366 -0.78532662]
 [-4.37144453 -1.65769231]]
H is:
 [[0.07645301 0.17333038]
 [0.01559977 0.08057417]
 [0.06190713 0.31317301]
 [0.01247538 0.16007202]]
The c is
 [[-0.21735901]]
Z2 is:
 [[ 0.00319559]
 [-0.11621586]
 [ 0.17584931]
 [-0.01770855]]
Y^ -  the output is:
 [[0.5007989 ]
 [0.47097869]
 [0.54384939]
 [0.49557298]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.5007989 ]
 [0.47097869]
 [0.54384939]
 [0.49557298]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.1251994 ]
 [-0.13180977]
 [-0.11316058]
 [ 0.12388353]]
W2.T is
 [[0.06938825 1.24184613]]
 D_Error times W2.T
 [[ 0.00868737  0.1554784 ]
 [-0.00914605 -0.16368745]
 [-0.00785201 -0.14052803]
 [ 0.00859606  0.15384428]]
self.H_D_Error_W2 is
 [[ 0.0006134   0.02227803]
 [-0.00014045 -0.01212629]
 [-0.000456   -0.03022697]
 [ 0.0001059   0.02068421]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.0006134   0.02227803]
 [-0.00014045 -0.01212629]
 [-0.000456   -0.03022697]
 [ 0.0001059   0.02068421]]
X_H_D_Error_W2 is
 [[-3.50101596e-04 -9.54276606e-03]
 [-3.45493040e-05  8.55791687e-03]]
Using sum gradient........

W1 was :
 [[-0.22666834  0.776879  ]
 [-1.65323087 -0.87236568]]
Updated W1 is: 
 [[-0.22631823  0.78642176]
 [-1.65319632 -0.8809236 ]]
W2 was :
 [[0.06938825]
 [1.24184613]]
Updated W2 is: 
 [[0.06733253]
 [1.24637428]]
The mean of the biases b gradient is:
 [3.07112583e-05 1.52243067e-04]
The b biases before the update are:
 [[-2.49154532 -1.56220562]]
The new updated bs are:
 [[-2.49157603 -1.56235786]]
The bias c is: 
 [[ 0.1251994 ]
 [-0.13180977]
 [-0.11316058]
 [ 0.12388353]]
c bias before: [[-0.21735901]]
The mean c bias after update: [[-0.21838716]]
The output is: 
 [[0.5007989 ]
 [0.47097869]
 [0.54384939]
 [0.49557298]]
Total Loss: 0.4921645180443871
Average Loss: 0.12304112951109678

RUN:
  112
FeedForward


Z1 is:
 [[-2.49157603 -1.56235786]
 [-4.14477235 -2.44328146]
 [-2.71789426 -0.7759361 ]
 [-4.37109059 -1.6568597 ]]
H is:
 [[0.07645085 0.17330857]
 [0.01559983 0.07993125]
 [0.06192568 0.31519641]
 [0.01247974 0.160184  ]]
The c is
 [[-0.21838716]]
Z2 is:
 [[ 0.00276781]
 [-0.11771253]
 [ 0.17863515]
 [-0.01789766]]
Y^ -  the output is:
 [[0.50069195]
 [0.4706058 ]
 [0.54454041]
 [0.49552571]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50069195]
 [0.4706058 ]
 [0.54454041]
 [0.49552571]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12517275]
 [-0.13189114]
 [-0.11296134]
 [ 0.12387151]]
W2.T is
 [[0.06733253 1.24637428]]
 D_Error times W2.T
 [[ 0.0084282   0.15601209]
 [-0.00888056 -0.16438573]
 [-0.00760597 -0.1407921 ]
 [ 0.00834058  0.15439026]]
self.H_D_Error_W2 is
 [[ 0.00059508  0.02235228]
 [-0.00013637 -0.0120893 ]
 [-0.00044184 -0.03038964]
 [ 0.00010279  0.02076936]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 0.00059508  0.02235228]
 [-0.00013637 -0.0120893 ]
 [-0.00044184 -0.03038964]
 [ 0.00010279  0.02076936]]
X_H_D_Error_W2 is
 [[-3.39048405e-04 -9.62027971e-03]
 [-3.35848943e-05  8.68006648e-03]]
Using sum gradient........

W1 was :
 [[-0.22631823  0.78642176]
 [-1.65319632 -0.8809236 ]]
Updated W1 is: 
 [[-0.22597919  0.79604204]
 [-1.65316274 -0.88960367]]
W2 was :
 [[0.06733253]
 [1.24637428]]
Updated W2 is: 
 [[0.06526977]
 [1.25098577]]
The mean of the biases b gradient is:
 [2.99149261e-05 1.60674926e-04]
The b biases before the update are:
 [[-2.49157603 -1.56235786]]
The new updated bs are:
 [[-2.49160595 -1.56251854]]
The bias c is: 
 [[ 0.12517275]
 [-0.13189114]
 [-0.11296134]
 [ 0.12387151]]
c bias before: [[-0.21838716]]
The mean c bias after update: [[-0.2194351]]
The output is: 
 [[0.50069195]
 [0.4706058 ]
 [0.54454041]
 [0.49552571]]
Total Loss: 0.49196990647777206
Average Loss: 0.12299247661944301

RUN:
  113
FeedForward


Z1 is:
 [[-2.49160595 -1.56251854]
 [-4.14476868 -2.4521222 ]
 [-2.71758513 -0.7664765 ]
 [-4.37074787 -1.65608016]]
H is:
 [[0.07644873 0.17328555]
 [0.01559989 0.07928349]
 [0.06194364 0.3172418 ]
 [0.01248396 0.16028889]]
The c is
 [[-0.2194351]]
Z2 is:
 [[ 0.00233244]
 [-0.11923438]
 [ 0.18147292]
 [-0.01810116]]
Y^ -  the output is:
 [[0.50058311]
 [0.47022667]
 [0.54524413]
 [0.49547483]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50058311]
 [0.47022667]
 [0.54524413]
 [0.49547483]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12514561]
 [-0.13197371]
 [-0.11275807]
 [ 0.12385856]]
W2.T is
 [[0.06526977 1.25098577]]
 D_Error times W2.T
 [[ 0.00816823  0.15655537]
 [-0.00861389 -0.16509724]
 [-0.00735969 -0.14105874]
 [ 0.00808422  0.1549453 ]]
self.H_D_Error_W2 is
 [[ 5.76712010e-04  2.24277579e-02]
 [-1.32279536e-04 -1.20517059e-02]
 [-4.27646916e-04 -3.05532438e-02]
 [ 9.96631824e-05  2.08550736e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 5.76712010e-04  2.24277579e-02]
 [-1.32279536e-04 -1.20517059e-02]
 [-4.27646916e-04 -3.05532438e-02]
 [ 9.96631824e-05  2.08550736e-02]]
X_H_D_Error_W2 is
 [[-3.27983734e-04 -9.69817016e-03]
 [-3.26163534e-05  8.80336774e-03]]
Using sum gradient........

W1 was :
 [[-0.22597919  0.79604204]
 [-1.65316274 -0.88960367]]
Updated W1 is: 
 [[-0.2256512   0.80574021]
 [-1.65313012 -0.89840703]]
W2 was :
 [[0.06526977]
 [1.25098577]]
Updated W2 is: 
 [[0.06319972]
 [1.2556816 ]]
The mean of the biases b gradient is:
 [2.91121851e-05 1.69470458e-04]
The b biases before the update are:
 [[-2.49160595 -1.56251854]]
The new updated bs are:
 [[-2.49163506 -1.56268801]]
The bias c is: 
 [[ 0.12514561]
 [-0.13197371]
 [-0.11275807]
 [ 0.12385856]]
c bias before: [[-0.2194351]]
The mean c bias after update: [[-0.2205032]]
The output is: 
 [[0.50058311]
 [0.47022667]
 [0.54524413]
 [0.49547483]]
Total Loss: 0.49177072091170293
Average Loss: 0.12294268022792573

RUN:
  114
FeedForward


Z1 is:
 [[-2.49163506 -1.56268801]
 [-4.14476518 -2.46109504]
 [-2.71728626 -0.7569478 ]
 [-4.37041638 -1.65535483]]
H is:
 [[0.07644668 0.17326127]
 [0.01559994 0.07863097]
 [0.06196101 0.3193093 ]
 [0.01248805 0.16038654]]
The c is
 [[-0.2205032]]
Z2 is:
 [[ 0.0018892 ]
 [-0.12078183]
 [ 0.18436353]
 [-0.01831953]]
Y^ -  the output is:
 [[0.5004723 ]
 [0.4698412 ]
 [0.54596077]
 [0.49542025]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.5004723 ]
 [0.4698412 ]
 [0.54596077]
 [0.49542025]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12511796]
 [-0.13205749]
 [-0.1125507 ]
 [ 0.12384467]]
W2.T is
 [[0.06319972 1.2556816 ]]
 D_Error times W2.T
 [[ 0.00790742  0.15710832]
 [-0.008346   -0.16582216]
 [-0.00711317 -0.14132784]
 [ 0.00782695  0.15550947]]
self.H_D_Error_W2 is
 [[ 5.58284308e-04  2.25044800e-02]
 [-1.28165999e-04 -1.20135070e-02]
 [-4.13430687e-04 -3.07177292e-02]
 [ 9.65227051e-05  2.09413255e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 5.58284308e-04  2.25044800e-02]
 [-1.28165999e-04 -1.20135070e-02]
 [-4.13430687e-04 -3.07177292e-02]
 [ 9.65227051e-05  2.09413255e-02]]
X_H_D_Error_W2 is
 [[-3.16907982e-04 -9.77640369e-03]
 [-3.16432934e-05  8.92781846e-03]]
Using sum gradient........

W1 was :
 [[-0.2256512   0.80574021]
 [-1.65313012 -0.89840703]]
Updated W1 is: 
 [[-0.22533429  0.81551662]
 [-1.65309848 -0.90733485]]
W2 was :
 [[0.06319972]
 [1.2556816 ]]
Updated W2 is: 
 [[0.06112213]
 [1.26046278]]
The mean of the biases b gradient is:
 [2.83025820e-05 1.78642314e-04]
The b biases before the update are:
 [[-2.49163506 -1.56268801]]
The new updated bs are:
 [[-2.49166336 -1.56286665]]
The bias c is: 
 [[ 0.12511796]
 [-0.13205749]
 [-0.1125507 ]
 [ 0.12384467]]
c bias before: [[-0.2205032]]
The mean c bias after update: [[-0.22159181]]
The output is: 
 [[0.5004723 ]
 [0.4698412 ]
 [0.54596077]
 [0.49542025]]
Total Loss: 0.4915668596633087
Average Loss: 0.12289171491582718

RUN:
  115
FeedForward


Z1 is:
 [[-2.49166336 -1.56286665]
 [-4.14476184 -2.4702015 ]
 [-2.71699765 -0.74735004]
 [-4.37009613 -1.65468489]]
H is:
 [[0.07644468 0.17323569]
 [0.01559999 0.07797375]
 [0.06197778 0.32139899]
 [0.012492   0.16047678]]
The c is
 [[-0.22159181]]
Z2 is:
 [[ 0.00143778]
 [-0.1223553 ]
 [ 0.18730787]
 [-0.01855327]]
Y^ -  the output is:
 [[0.50035945]
 [0.46944928]
 [0.54669054]
 [0.49536182]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50035945]
 [0.46944928]
 [0.54669054]
 [0.49536182]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.1250898 ]
 [-0.13214249]
 [-0.11233915]
 [ 0.1238298 ]]
W2.T is
 [[0.06112213 1.26046278]]
 D_Error times W2.T
 [[ 0.00764576  0.15767103]
 [-0.00807683 -0.16656069]
 [-0.00686641 -0.14159931]
 [ 0.00756874  0.15608285]]
self.H_D_Error_W2 is
 [[ 5.39797134e-04  2.25824468e-02]
 [-1.24032935e-04 -1.19746881e-02]
 [-3.99189210e-04 -3.08830483e-02]
 [ 9.33676178e-05  2.10281032e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 5.39797134e-04  2.25824468e-02]
 [-1.24032935e-04 -1.19746881e-02]
 [-3.99189210e-04 -3.08830483e-02]
 [ 9.33676178e-05  2.10281032e-02]]
X_H_D_Error_W2 is
 [[-3.05821592e-04 -9.85494513e-03]
 [-3.06653175e-05  9.05341502e-03]]
Using sum gradient........

W1 was :
 [[-0.22533429  0.81551662]
 [-1.65309848 -0.90733485]]
Updated W1 is: 
 [[-0.22502847  0.82537156]
 [-1.65306781 -0.91638827]]
W2 was :
 [[0.06112213]
 [1.26046278]]
Updated W2 is: 
 [[0.05903676]
 [1.26533029]]
The mean of the biases b gradient is:
 [2.74856516e-05 1.88203384e-04]
The b biases before the update are:
 [[-2.49166336 -1.56286665]]
The new updated bs are:
 [[-2.49169085 -1.56305485]]
The bias c is: 
 [[ 0.1250898 ]
 [-0.13214249]
 [-0.11233915]
 [ 0.1238298 ]]
c bias before: [[-0.22159181]]
The mean c bias after update: [[-0.2227013]]
The output is: 
 [[0.50035945]
 [0.46944928]
 [0.54669054]
 [0.49536182]]
Total Loss: 0.49135821998849005
Average Loss: 0.12283955499712251

RUN:
  116
FeedForward


Z1 is:
 [[-2.49169085 -1.56305485]
 [-4.14475866 -2.47944312]
 [-2.71671932 -0.73768329]
 [-4.36978713 -1.65407156]]
H is:
 [[0.07644274 0.17320873]
 [0.01560004 0.07731192]
 [0.06199397 0.32351095]
 [0.01249581 0.16055943]]
The c is
 [[-0.2227013]]
Z2 is:
 [[ 0.00097789]
 [-0.12395521]
 [ 0.19030683]
 [-0.01880288]]
Y^ -  the output is:
 [[0.50024447]
 [0.46905081]
 [0.54743364]
 [0.49529942]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50024447]
 [0.46905081]
 [0.54743364]
 [0.49529942]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12506109]
 [-0.13222873]
 [-0.11212334]
 [ 0.12381391]]
W2.T is
 [[0.05903676 1.26533029]]
 D_Error times W2.T
 [[ 0.0073832   0.15824358]
 [-0.00780635 -0.16731301]
 [-0.0066194  -0.14187306]
 [ 0.00730957  0.15666549]]
self.H_D_Error_W2 is
 [[ 5.21248423e-04  2.26616627e-02]
 [-1.19879702e-04 -1.19352377e-02]
 [-3.84922734e-04 -3.10491479e-02]
 [ 9.01976821e-05  2.11153900e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 5.21248423e-04  2.26616627e-02]
 [-1.19879702e-04 -1.19352377e-02]
 [-3.84922734e-04 -3.10491479e-02]
 [ 9.01976821e-05  2.11153900e-02]]
X_H_D_Error_W2 is
 [[-2.94725052e-04 -9.93375786e-03]
 [-2.96820198e-05  9.18015228e-03]]
Using sum gradient........

W1 was :
 [[-0.22502847  0.82537156]
 [-1.65306781 -0.91638827]]
Updated W1 is: 
 [[-0.22473375  0.83530532]
 [-1.65303813 -0.92556842]]
W2 was :
 [[0.05903676]
 [1.26533029]]
Updated W2 is: 
 [[0.05694333]
 [1.27028511]]
The mean of the biases b gradient is:
 [2.66609173e-05 1.98166780e-04]
The b biases before the update are:
 [[-2.49169085 -1.56305485]]
The new updated bs are:
 [[-2.49171751 -1.56325302]]
The bias c is: 
 [[ 0.12506109]
 [-0.13222873]
 [-0.11212334]
 [ 0.12381391]]
c bias before: [[-0.2227013]]
The mean c bias after update: [[-0.22383203]]
The output is: 
 [[0.50024447]
 [0.46905081]
 [0.54743364]
 [0.49529942]]
Total Loss: 0.49114469815437833
Average Loss: 0.12278617453859458

RUN:
  117
FeedForward


Z1 is:
 [[-2.49171751 -1.56325302]
 [-4.14475564 -2.48882144]
 [-2.71645125 -0.7279477 ]
 [-4.36948938 -1.65351612]]
H is:
 [[0.07644086 0.17318036]
 [0.01560009 0.07664556]
 [0.06200956 0.32564525]
 [0.01249949 0.1606343 ]]
The c is
 [[-0.22383203]]
Z2 is:
 [[ 0.00050919]
 [-0.12558199]
 [ 0.19336131]
 [-0.01906891]]
Y^ -  the output is:
 [[0.5001273 ]
 [0.4686457 ]
 [0.54819027]
 [0.49523292]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.5001273 ]
 [0.4686457 ]
 [0.54819027]
 [0.49523292]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12503182]
 [-0.13231621]
 [-0.11190319]
 [ 0.12379698]]
W2.T is
 [[0.05694333 1.27028511]]
 D_Error times W2.T
 [[ 0.00711973  0.15882605]
 [-0.00753453 -0.16807931]
 [-0.00637214 -0.14214896]
 [ 0.00704941  0.15725745]]
self.H_D_Error_W2 is
 [[ 5.02636102e-04  2.27421312e-02]
 [-1.15705644e-04 -1.18951441e-02]
 [-3.70631556e-04 -3.12159714e-02]
 [ 8.70126594e-05  2.12031677e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 5.02636102e-04  2.27421312e-02]
 [-1.15705644e-04 -1.18951441e-02]
 [-3.70631556e-04 -3.12159714e-02]
 [ 8.70126594e-05  2.12031677e-02]]
X_H_D_Error_W2 is
 [[-2.83618896e-04 -1.00128038e-02]
 [-2.86929850e-05  9.30802358e-03]]
Using sum gradient........

W1 was :
 [[-0.22473375  0.83530532]
 [-1.65303813 -0.92556842]]
Updated W1 is: 
 [[-0.22445013  0.84531812]
 [-1.65300944 -0.93487644]]
W2 was :
 [[0.05694333]
 [1.27028511]]
Updated W2 is: 
 [[0.05484161]
 [1.27532821]]
The mean of the biases b gradient is:
 [2.58278902e-05 2.08545833e-04]
The b biases before the update are:
 [[-2.49171751 -1.56325302]]
The new updated bs are:
 [[-2.49174333 -1.56346157]]
The bias c is: 
 [[ 0.12503182]
 [-0.13231621]
 [-0.11190319]
 [ 0.12379698]]
c bias before: [[-0.22383203]]
The mean c bias after update: [[-0.22498438]]
The output is: 
 [[0.5001273 ]
 [0.4686457 ]
 [0.54819027]
 [0.49523292]]
Total Loss: 0.4909261895182531
Average Loss: 0.12273154737956328

RUN:
  118
FeedForward


Z1 is:
 [[-2.49174333 -1.56346157]
 [-4.14475277 -2.49833801]
 [-2.71619346 -0.71814344]
 [-4.3692029  -1.65301989]]
H is:
 [[0.07643903 0.1731505 ]
 [0.01560013 0.07597477]
 [0.06202455 0.32780194]
 [0.01250302 0.16070122]]
The c is
 [[-0.22498438]]
Z2 is:
 [[ 3.13690495e-05]
 [-1.27236073e-01]
 [ 1.96472205e-01]
 [-1.93518961e-02]]
Y^ -  the output is:
 [[0.50000784]
 [0.46823383]
 [0.54896066]
 [0.49516218]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.50000784]
 [0.46823383]
 [0.54896066]
 [0.49516218]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12500196]
 [-0.13240494]
 [-0.11167863]
 [ 0.12377896]]
W2.T is
 [[0.05484161 1.27532821]]
 D_Error times W2.T
 [[ 0.00685531  0.15941853]
 [-0.0072613  -0.16885976]
 [-0.00612464 -0.14242691]
 [ 0.00678824  0.15785879]]
self.H_D_Error_W2 is
 [[ 4.83958084e-04  2.28238550e-02]
 [-1.11510099e-04 -1.18543955e-02]
 [-3.56316018e-04 -3.13834593e-02]
 [ 8.38123113e-05  2.12914160e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 4.83958084e-04  2.28238550e-02]
 [-1.11510099e-04 -1.18543955e-02]
 [-3.56316018e-04 -3.13834593e-02]
 [ 8.38123113e-05  2.12914160e-02]]
X_H_D_Error_W2 is
 [[-2.72503707e-04 -1.00920433e-02]
 [-2.76977881e-05  9.43702055e-03]]
Using sum gradient........

W1 was :
 [[-0.22445013  0.84531812]
 [-1.65300944 -0.93487644]]
Updated W1 is: 
 [[-0.22417763  0.85541017]
 [-1.65298174 -0.94431346]]
W2 was :
 [[0.05484161]
 [1.27532821]]
Updated W2 is: 
 [[0.05273132]
 [1.28046054]]
The mean of the biases b gradient is:
 [2.49860695e-05 2.19354076e-04]
The b biases before the update are:
 [[-2.49174333 -1.56346157]]
The new updated bs are:
 [[-2.49176832 -1.56368092]]
The bias c is: 
 [[ 0.12500196]
 [-0.13240494]
 [-0.11167863]
 [ 0.12377896]]
c bias before: [[-0.22498438]]
The mean c bias after update: [[-0.22615872]]
The output is: 
 [[0.50000784]
 [0.46823383]
 [0.54896066]
 [0.49516218]]
Total Loss: 0.49070258861311966
Average Loss: 0.12267564715327992

RUN:
  119
FeedForward


Z1 is:
 [[-2.49176832 -1.56368092]
 [-4.14475006 -2.50799438]
 [-2.71594595 -0.70827076]
 [-4.36892768 -1.65258422]]
H is:
 [[0.07643727 0.17311909]
 [0.01560017 0.07529964]
 [0.06203895 0.32998105]
 [0.01250642 0.16075999]]
The c is
 [[-0.22615872]]
Z2 is:
 [[-0.00045591]
 [-0.12891788]
 [ 0.19964039]
 [-0.01965241]]
Y^ -  the output is:
 [[0.49988602]
 [0.46781509]
 [0.54974499]
 [0.49508705]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49988602]
 [0.46781509]
 [0.54974499]
 [0.49508705]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.1249715 ]
 [-0.13249495]
 [-0.11144957]
 [ 0.12375981]]
W2.T is
 [[0.05273132 1.28046054]]
 D_Error times W2.T
 [[ 0.00658991  0.16002107]
 [-0.00698663 -0.16965456]
 [-0.00587688 -0.14270677]
 [ 0.00652602  0.15846956]]
self.H_D_Error_W2 is
 [[ 4.65212276e-04  2.29068361e-02]
 [-1.07292394e-04 -1.18129799e-02]
 [-3.41976514e-04 -3.15515484e-02]
 [ 8.05963999e-05  2.13801131e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 4.65212276e-04  2.29068361e-02]
 [-1.07292394e-04 -1.18129799e-02]
 [-3.41976514e-04 -3.15515484e-02]
 [ 8.05963999e-05  2.13801131e-02]]
X_H_D_Error_W2 is
 [[-2.61380114e-04 -1.01714353e-02]
 [-2.66959939e-05  9.56713315e-03]]
Using sum gradient........

W1 was :
 [[-0.22417763  0.85541017]
 [-1.65298174 -0.94431346]]
Updated W1 is: 
 [[-0.22391624  0.8655816 ]
 [-1.65295504 -0.9538806 ]]
W2 was :
 [[0.05273132]
 [1.28046054]]
Updated W2 is: 
 [[0.0506122 ]
 [1.28568303]]
The mean of the biases b gradient is:
 [2.41349422e-05 2.30605226e-04]
The b biases before the update are:
 [[-2.49176832 -1.56368092]]
The new updated bs are:
 [[-2.49179246 -1.56391153]]
The bias c is: 
 [[ 0.1249715 ]
 [-0.13249495]
 [-0.11144957]
 [ 0.12375981]]
c bias before: [[-0.22615872]]
The mean c bias after update: [[-0.22735542]]
The output is: 
 [[0.49988602]
 [0.46781509]
 [0.54974499]
 [0.49508705]]
Total Loss: 0.4904737892401387
Average Loss: 0.12261844731003467

RUN:
  120
FeedForward


Z1 is:
 [[-2.49179246 -1.56391153]
 [-4.1447475  -2.51779212]
 [-2.7157087  -0.69832993]
 [-4.36866374 -1.65221052]]
H is:
 [[0.07643557 0.17308608]
 [0.01560021 0.07462026]
 [0.06205276 0.33218261]
 [0.01250968 0.16081041]]
The c is
 [[-0.22735542]]
Z2 is:
 [[-0.000953  ]
 [-0.13062785]
 [ 0.20286675]
 [-0.01997105]]
Y^ -  the output is:
 [[0.49976175]
 [0.4673894 ]
 [0.55054346]
 [0.4950074 ]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49976175]
 [0.4673894 ]
 [0.55054346]
 [0.4950074 ]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12494041]
 [-0.13258625]
 [-0.11121593]
 [ 0.12373951]]
W2.T is
 [[0.0506122  1.28568303]]
 D_Error times W2.T
 [[ 0.00632351  0.16063376]
 [-0.00671048 -0.17046389]
 [-0.00562888 -0.14298844]
 [ 0.00626273  0.15908979]]
self.H_D_Error_W2 is
 [[ 4.46396579e-04  2.29910755e-02]
 [-1.03051846e-04 -1.17708854e-02]
 [-3.27613489e-04 -3.17201723e-02]
 [ 7.73646886e-05  2.14692349e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 4.46396579e-04  2.29910755e-02]
 [-1.03051846e-04 -1.17708854e-02]
 [-3.27613489e-04 -3.17201723e-02]
 [ 7.73646886e-05  2.14692349e-02]]
X_H_D_Error_W2 is
 [[-2.50248800e-04 -1.02509374e-02]
 [-2.56871572e-05  9.69834949e-03]]
Using sum gradient........

W1 was :
 [[-0.22391624  0.8655816 ]
 [-1.65295504 -0.9538806 ]]
Updated W1 is: 
 [[-0.223666    0.87583254]
 [-1.65292936 -0.96357895]]
W2 was :
 [[0.0506122 ]
 [1.28568303]]
Updated W2 is: 
 [[0.048484 ]
 [1.2909966]]
The mean of the biases b gradient is:
 [2.32739831e-05 2.42313177e-04]
The b biases before the update are:
 [[-2.49179246 -1.56391153]]
The new updated bs are:
 [[-2.49181573 -1.56415384]]
The bias c is: 
 [[ 0.12494041]
 [-0.13258625]
 [-0.11121593]
 [ 0.12373951]]
c bias before: [[-0.22735542]]
The mean c bias after update: [[-0.22857485]]
The output is: 
 [[0.49976175]
 [0.4673894 ]
 [0.55054346]
 [0.4950074 ]]
Total Loss: 0.4902396845680848
Average Loss: 0.1225599211420212

RUN:
  121
FeedForward


Z1 is:
 [[-2.49181573 -1.56415384]
 [-4.14474509 -2.52773279]
 [-2.71548173 -0.6883213 ]
 [-4.36841108 -1.65190025]]
H is:
 [[0.07643392 0.17305141]
 [0.01560025 0.07393673]
 [0.06206597 0.33440661]
 [0.0125128  0.16085229]]
The c is
 [[-0.22857485]]
Z2 is:
 [[-0.00146025]
 [-0.13236642]
 [ 0.20615215]
 [-0.02030842]]
Y^ -  the output is:
 [[0.49963494]
 [0.46695663]
 [0.55135629]
 [0.49492307]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49963494]
 [0.46695663]
 [0.55135629]
 [0.49492307]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12490867]
 [-0.13267883]
 [-0.11097765]
 [ 0.12371801]]
W2.T is
 [[0.048484  1.2909966]]
 D_Error times W2.T
 [[ 0.00605607  0.16125666]
 [-0.0064328  -0.17128792]
 [-0.00538064 -0.14327176]
 [ 0.00599834  0.15971953]]
self.H_D_Error_W2 is
 [[ 4.27508884e-04  2.30765732e-02]
 [-9.87877637e-05 -1.17280998e-02]
 [-3.13227443e-04 -3.18892611e-02]
 [ 7.41169419e-05  2.15587556e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 4.27508884e-04  2.30765732e-02]
 [-9.87877637e-05 -1.17280998e-02]
 [-3.13227443e-04 -3.18892611e-02]
 [ 7.41169419e-05  2.15587556e-02]]
X_H_D_Error_W2 is
 [[-2.39110501e-04 -1.03305055e-02]
 [-2.46708218e-05  9.83065582e-03]]
Using sum gradient........

W1 was :
 [[-0.223666    0.87583254]
 [-1.65292936 -0.96357895]]
Updated W1 is: 
 [[-0.22342689  0.88616304]
 [-1.65290468 -0.9734096 ]]
W2 was :
 [[0.048484 ]
 [1.2909966]]
Updated W2 is: 
 [[0.04634644]
 [1.29640215]]
The mean of the biases b gradient is:
 [2.2402655e-05 2.5449197e-04]
The b biases before the update are:
 [[-2.49181573 -1.56415384]]
The new updated bs are:
 [[-2.49183813 -1.56440833]]
The bias c is: 
 [[ 0.12490867]
 [-0.13267883]
 [-0.11097765]
 [ 0.12371801]]
c bias before: [[-0.22857485]]
The mean c bias after update: [[-0.2298174]]
The output is: 
 [[0.49963494]
 [0.46695663]
 [0.55135629]
 [0.49492307]]
Total Loss: 0.4900001672399961
Average Loss: 0.12250004180999903

RUN:
  122
FeedForward


Z1 is:
 [[-2.49183813 -1.56440833]
 [-4.14474282 -2.53781793]
 [-2.71526502 -0.67824529]
 [-4.3681697  -1.65165489]]
H is:
 [[0.07643234 0.17301499]
 [0.01560029 0.07324916]
 [0.06207859 0.33665305]
 [0.01251579 0.16088541]]
The c is
 [[-0.2298174]]
Z2 is:
 [[-0.00197803]
 [-0.13413401]
 [ 0.20949746]
 [-0.02066515]]
Y^ -  the output is:
 [[0.49950549]
 [0.46651668]
 [0.55218365]
 [0.4948339 ]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49950549]
 [0.46651668]
 [0.55218365]
 [0.4948339 ]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12487625]
 [-0.13277272]
 [-0.11073463]
 [ 0.12369527]]
W2.T is
 [[0.04634644 1.29640215]]
 D_Error times W2.T
 [[ 0.00578757  0.16188984]
 [-0.00615354 -0.17212684]
 [-0.00513216 -0.14355661]
 [ 0.00573284  0.16035881]]
self.H_D_Error_W2 is
 [[ 4.08547083e-04  2.31633283e-02]
 [-9.44994472e-05 -1.16846109e-02]
 [-2.98818930e-04 -3.20587416e-02]
 [ 7.08529261e-05  2.16486473e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 4.08547083e-04  2.31633283e-02]
 [-9.44994472e-05 -1.16846109e-02]
 [-2.98818930e-04 -3.20587416e-02]
 [ 7.08529261e-05  2.16486473e-02]]
X_H_D_Error_W2 is
 [[-2.27966004e-04 -1.04100943e-02]
 [-2.36465211e-05  9.96403641e-03]]
Using sum gradient........

W1 was :
 [[-0.22342689  0.88616304]
 [-1.65290468 -0.9734096 ]]
Updated W1 is: 
 [[-0.22319892  0.89657314]
 [-1.65288104 -0.98337364]]
W2 was :
 [[0.04634644]
 [1.29640215]]
Updated W2 is: 
 [[0.04419925]
 [1.30190056]]
The mean of the biases b gradient is:
 [2.15204079e-05 2.67155781e-04]
The b biases before the update are:
 [[-2.49183813 -1.56440833]]
The new updated bs are:
 [[-2.49185965 -1.56467549]]
The bias c is: 
 [[ 0.12487625]
 [-0.13277272]
 [-0.11073463]
 [ 0.12369527]]
c bias before: [[-0.2298174]]
The mean c bias after update: [[-0.23108344]]
The output is: 
 [[0.49950549]
 [0.46651668]
 [0.55218365]
 [0.4948339 ]]
Total Loss: 0.48975512948715183
Average Loss: 0.12243878237178796

RUN:
  123
FeedForward


Z1 is:
 [[-2.49185965 -1.56467549]
 [-4.14474069 -2.54804913]
 [-2.71505857 -0.66810235]
 [-4.36793961 -1.65147599]]
H is:
 [[0.07643082 0.17297677]
 [0.01560032 0.07255766]
 [0.06209061 0.33892189]
 [0.01251863 0.16090957]]
The c is
 [[-0.23108344]]
Z2 is:
 [[-0.00250671]
 [-0.13593107]
 [ 0.21290351]
 [-0.02104188]]
Y^ -  the output is:
 [[0.49937332]
 [0.46606946]
 [0.55302573]
 [0.49473972]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49937332]
 [0.46606946]
 [0.55302573]
 [0.49473972]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12484313]
 [-0.13286793]
 [-0.1104868 ]
 [ 0.12367124]]
W2.T is
 [[0.04419925 1.30190056]]
 D_Error times W2.T
 [[ 0.00551797  0.16253335]
 [-0.00587266 -0.17298083]
 [-0.00488343 -0.14384282]
 [ 0.00546618  0.16100766]]
self.H_D_Error_W2 is
 [[ 3.89509062e-04  2.32513390e-02]
 [-9.01861868e-05 -1.16404066e-02]
 [-2.84388565e-04 -3.22285369e-02]
 [ 6.75724099e-05  2.17388801e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 3.89509062e-04  2.32513390e-02]
 [-9.01861868e-05 -1.16404066e-02]
 [-2.84388565e-04 -3.22285369e-02]
 [ 6.75724099e-05  2.17388801e-02]]
X_H_D_Error_W2 is
 [[-2.16816155e-04 -1.04896568e-02]
 [-2.26137768e-05  1.00984735e-02]]
Using sum gradient........

W1 was :
 [[-0.22319892  0.89657314]
 [-1.65288104 -0.98337364]]
Updated W1 is: 
 [[-0.2229821   0.90706279]
 [-1.65285842 -0.99347211]]
W2 was :
 [[0.04419925]
 [1.30190056]]
Updated W2 is: 
 [[0.04204217]
 [1.30749269]]
The mean of the biases b gradient is:
 [2.06266800e-05 2.80318895e-04]
The b biases before the update are:
 [[-2.49185965 -1.56467549]]
The new updated bs are:
 [[-2.49188028 -1.56495581]]
The bias c is: 
 [[ 0.12484313]
 [-0.13286793]
 [-0.1104868 ]
 [ 0.12367124]]
c bias before: [[-0.23108344]]
The mean c bias after update: [[-0.23237336]]
The output is: 
 [[0.49937332]
 [0.46606946]
 [0.55302573]
 [0.49473972]]
Total Loss: 0.4895044632505061
Average Loss: 0.12237611581262653

RUN:
  124
FeedForward


Z1 is:
 [[-2.49188028 -1.56495581]
 [-4.1447387  -2.55842792]
 [-2.71486238 -0.65789301]
 [-4.36772081 -1.65136512]]
H is:
 [[0.07642937 0.17293667]
 [0.01560035 0.07186233]
 [0.06210204 0.34121308]
 [0.01252134 0.16092453]]
The c is
 [[-0.23237336]]
Z2 is:
 [[-0.00304667]
 [-0.13775802]
 [ 0.21637115]
 [-0.02143928]]
Y^ -  the output is:
 [[0.49923833]
 [0.46561486]
 [0.55388273]
 [0.49464039]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49923833]
 [0.46561486]
 [0.55388273]
 [0.49464039]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12480929]
 [-0.13296446]
 [-0.11023408]
 [ 0.12364589]]
W2.T is
 [[0.04204217 1.30749269]]
 D_Error times W2.T
 [[ 0.00524725  0.16318724]
 [-0.00559011 -0.17385006]
 [-0.00463448 -0.14413026]
 [ 0.00519834  0.16166609]]
self.H_D_Error_W2 is
 [[ 3.70392708e-04  2.33406021e-02]
 [-8.58472644e-05 -1.15954745e-02]
 [-2.69937020e-04 -3.23985666e-02]
 [ 6.42751648e-05  2.18294217e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 3.70392708e-04  2.33406021e-02]
 [-8.58472644e-05 -1.15954745e-02]
 [-2.69937020e-04 -3.23985666e-02]
 [ 6.42751648e-05  2.18294217e-02]]
X_H_D_Error_W2 is
 [[-2.05661856e-04 -1.05691449e-02]
 [-2.15720997e-05  1.02339472e-02]]
Using sum gradient........

W1 was :
 [[-0.2229821   0.90706279]
 [-1.65285842 -0.99347211]]
Updated W1 is: 
 [[-0.22277644  0.91763194]
 [-1.65283685 -1.00370606]]
W2 was :
 [[0.04204217]
 [1.30749269]]
Updated W2 is: 
 [[0.03987492]
 [1.31317938]]
The mean of the biases b gradient is:
 [1.97208969e-05 2.93995678e-04]
The b biases before the update are:
 [[-2.49188028 -1.56495581]]
The new updated bs are:
 [[-2.4919    -1.5652498]]
The bias c is: 
 [[ 0.12480929]
 [-0.13296446]
 [-0.11023408]
 [ 0.12364589]]
c bias before: [[-0.23237336]]
The mean c bias after update: [[-0.23368752]]
The output is: 
 [[0.49923833]
 [0.46561486]
 [0.55388273]
 [0.49464039]]
Total Loss: 0.4892480603096701
Average Loss: 0.12231201507741753

RUN:
  125
FeedForward


Z1 is:
 [[-2.4919     -1.5652498 ]
 [-4.14473685 -2.56895586]
 [-2.71467644 -0.64761786]
 [-4.36751329 -1.65132392]]
H is:
 [[0.07642797 0.17289462]
 [0.01560038 0.07116329]
 [0.06211287 0.34352655]
 [0.0125239  0.1609301 ]]
The c is
 [[-0.23368752]]
Z2 is:
 [[-0.0035983 ]
 [-0.13961529]
 [ 0.21990121]
 [-0.02185804]]
Y^ -  the output is:
 [[0.49910043]
 [0.46515276]
 [0.55475483]
 [0.49453571]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49910043]
 [0.46515276]
 [0.55475483]
 [0.49453571]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.1247747 ]
 [-0.13306233]
 [-0.10997641]
 [ 0.12361916]]
W2.T is
 [[0.03987492 1.31317938]]
 D_Error times W2.T
 [[ 0.00497538  0.16385157]
 [-0.00530585 -0.1747347 ]
 [-0.0043853  -0.14441875]
 [ 0.0049293   0.16233413]]
self.H_D_Error_W2 is
 [[ 3.51195907e-04  2.34311137e-02]
 [-8.14819536e-05 -1.15498025e-02]
 [-2.55465031e-04 -3.25687469e-02]
 [ 6.09609652e-05  2.19202379e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 3.51195907e-04  2.34311137e-02]
 [-8.14819536e-05 -1.15498025e-02]
 [-2.55465031e-04 -3.25687469e-02]
 [ 6.09609652e-05  2.19202379e-02]]
X_H_D_Error_W2 is
 [[-1.94504066e-04 -1.06485089e-02]
 [-2.05209884e-05  1.03704354e-02]]
Using sum gradient........

W1 was :
 [[-0.22277644  0.91763194]
 [-1.65283685 -1.00370606]]
Updated W1 is: 
 [[-0.22258194  0.92828045]
 [-1.65281633 -1.01407649]]
W2 was :
 [[0.03987492]
 [1.31317938]]
Updated W2 is: 
 [[0.03769722]
 [1.31896143]]
The mean of the biases b gradient is:
 [1.88024718e-05 3.08200557e-04]
The b biases before the update are:
 [[-2.4919    -1.5652498]]
The new updated bs are:
 [[-2.4919188 -1.565558 ]]
The bias c is: 
 [[ 0.1247747 ]
 [-0.13306233]
 [-0.10997641]
 [ 0.12361916]]
c bias before: [[-0.23368752]]
The mean c bias after update: [[-0.2350263]]
The output is: 
 [[0.49910043]
 [0.46515276]
 [0.55475483]
 [0.49453571]]
Total Loss: 0.4889858124195171
Average Loss: 0.12224645310487928

RUN:
  126
FeedForward


Z1 is:
 [[-2.4919188  -1.565558  ]
 [-4.14473513 -2.5796345 ]
 [-2.71450074 -0.63727755]
 [-4.36731707 -1.65135405]]
H is:
 [[0.07642665 0.17285056]
 [0.0156004  0.07046067]
 [0.0621231  0.34586221]
 [0.01252633 0.16092603]]
The c is
 [[-0.2350263]]
Z2 is:
 [[-0.00416201]
 [-0.14150331]
 [ 0.22349448]
 [-0.02229886]]
Y^ -  the output is:
 [[0.4989595 ]
 [0.46468308]
 [0.5556422 ]
 [0.49442551]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.4989595 ]
 [0.46468308]
 [0.5556422 ]
 [0.49442551]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12473933]
 [-0.13316154]
 [-0.10971369]
 [ 0.12359101]]
W2.T is
 [[0.03769722 1.31896143]]
 D_Error times W2.T
 [[ 0.00470233  0.16452637]
 [-0.00501982 -0.17563493]
 [-0.0041359  -0.14470813]
 [ 0.00465904  0.16301178]]
self.H_D_Error_W2 is
 [[ 3.31916551e-04  2.35228685e-02]
 [-7.70895192e-05 -1.15033785e-02]
 [-2.40973396e-04 -3.27389902e-02]
 [ 5.76295893e-05  2.20112921e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 3.31916551e-04  2.35228685e-02]
 [-7.70895192e-05 -1.15033785e-02]
 [-2.40973396e-04 -3.27389902e-02]
 [ 5.76295893e-05  2.20112921e-02]]
X_H_D_Error_W2 is
 [[-1.83343807e-04 -1.07276981e-02]
 [-1.94599299e-05  1.05079137e-02]]
Using sum gradient........

W1 was :
 [[-0.22258194  0.92828045]
 [-1.65281633 -1.01407649]]
Updated W1 is: 
 [[-0.22239859  0.93900815]
 [-1.65279687 -1.02458441]]
W2 was :
 [[0.03769722]
 [1.31896143]]
Updated W2 is: 
 [[0.03550879]
 [1.32483962]]
The mean of the biases b gradient is:
 [1.78708061e-05 3.22947983e-04]
The b biases before the update are:
 [[-2.4919188 -1.565558 ]]
The new updated bs are:
 [[-2.49193667 -1.56588095]]
The bias c is: 
 [[ 0.12473933]
 [-0.13316154]
 [-0.10971369]
 [ 0.12359101]]
c bias before: [[-0.2350263]]
The mean c bias after update: [[-0.23639008]]
The output is: 
 [[0.4989595 ]
 [0.46468308]
 [0.5556422 ]
 [0.49442551]]
Total Loss: 0.48871761145445536
Average Loss: 0.12217940286361384

RUN:
  127
FeedForward


Z1 is:
 [[-2.49193667 -1.56588095]
 [-4.14473355 -2.59046536]
 [-2.71433527 -0.6268728 ]
 [-4.36713214 -1.65145721]]
H is:
 [[0.07642539 0.17280439]
 [0.01560043 0.06975458]
 [0.06213275 0.34821996]
 [0.01252862 0.1609121 ]]
The c is
 [[-0.23639008]]
Z2 is:
 [[-0.0047382 ]
 [-0.14342249]
 [ 0.22715178]
 [-0.02276248]]
Y^ -  the output is:
 [[0.49881545]
 [0.46420571]
 [0.55654502]
 [0.49430963]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49881545]
 [0.46420571]
 [0.55654502]
 [0.49430963]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12470316]
 [-0.1332621 ]
 [-0.10944587]
 [ 0.1235614 ]]
W2.T is
 [[0.03550879 1.32483962]]
 D_Error times W2.T
 [[ 0.00442806  0.16521169]
 [-0.00473198 -0.1765509 ]
 [-0.00388629 -0.14499822]
 [ 0.00438752  0.16369904]]
self.H_D_Error_W2 is
 [[ 3.12552533e-04  2.36158600e-02]
 [-7.26692186e-05 -1.14561902e-02]
 [-2.26462980e-04 -3.29092056e-02]
 [ 5.42808198e-05  2.21025455e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 3.12552533e-04  2.36158600e-02]
 [-7.26692186e-05 -1.14561902e-02]
 [-2.26462980e-04 -3.29092056e-02]
 [ 5.42808198e-05  2.21025455e-02]]
X_H_D_Error_W2 is
 [[-1.72182161e-04 -1.08066601e-02]
 [-1.83883988e-05  1.06463553e-02]]
Using sum gradient........

W1 was :
 [[-0.22239859  0.93900815]
 [-1.65279687 -1.02458441]]
Updated W1 is: 
 [[-0.22222641  0.94981481]
 [-1.65277848 -1.03523076]]
W2 was :
 [[0.03550879]
 [1.32483962]]
Updated W2 is: 
 [[0.03330937]
 [1.33081472]]
The mean of the biases b gradient is:
 [1.69252886e-05 3.38252407e-04]
The b biases before the update are:
 [[-2.49193667 -1.56588095]]
The new updated bs are:
 [[-2.4919536 -1.5662192]]
The bias c is: 
 [[ 0.12470316]
 [-0.1332621 ]
 [-0.10944587]
 [ 0.1235614 ]]
c bias before: [[-0.23639008]]
The mean c bias after update: [[-0.23777923]]
The output is: 
 [[0.49881545]
 [0.46420571]
 [0.55654502]
 [0.49430963]]
Total Loss: 0.48844334956038116
Average Loss: 0.12211083739009529

RUN:
  128
FeedForward


Z1 is:
 [[-2.4919536  -1.5662192 ]
 [-4.14473208 -2.60144997]
 [-2.71418001 -0.6164044 ]
 [-4.36695849 -1.65163516]]
H is:
 [[0.07642419 0.17275604]
 [0.01560045 0.06904516]
 [0.06214179 0.35059966]
 [0.01253077 0.16088808]]
The c is
 [[-0.23777923]]
Z2 is:
 [[-0.0053273 ]
 [-0.14537327]
 [ 0.23087386]
 [-0.02324962]]
Y^ -  the output is:
 [[0.49866818]
 [0.46372055]
 [0.55746344]
 [0.49418786]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49866818]
 [0.46372055]
 [0.55746344]
 [0.49418786]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12466616]
 [-0.13336401]
 [-0.10917286]
 [ 0.12353027]]
W2.T is
 [[0.03330937 1.33081472]]
 D_Error times W2.T
 [[ 0.00415255  0.16590756]
 [-0.00444227 -0.17748279]
 [-0.00363648 -0.14528885]
 [ 0.00411472  0.1643959 ]]
self.H_D_Error_W2 is
 [[ 2.93101757e-04  2.37100805e-02]
 [-6.82203010e-05 -1.14082258e-02]
 [-2.11934715e-04 -3.30792986e-02]
 [ 5.09144436e-05  2.21939569e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 2.93101757e-04  2.37100805e-02]
 [-6.82203010e-05 -1.14082258e-02]
 [-2.11934715e-04 -3.30792986e-02]
 [ 5.09144436e-05  2.21939569e-02]]
X_H_D_Error_W2 is
 [[-1.61020271e-04 -1.08853417e-02]
 [-1.73058574e-05  1.07857310e-02]]
Using sum gradient........

W1 was :
 [[-0.22222641  0.94981481]
 [-1.65277848 -1.03523076]]
Updated W1 is: 
 [[-0.22206539  0.96070015]
 [-1.65276118 -1.04601649]]
W2 was :
 [[0.03330937]
 [1.33081472]]
Updated W2 is: 
 [[0.03109867]
 [1.33688745]]
The mean of the biases b gradient is:
 [1.59652963e-05 3.54128238e-04]
The b biases before the update are:
 [[-2.4919536 -1.5662192]]
The new updated bs are:
 [[-2.49196956 -1.56657333]]
The bias c is: 
 [[ 0.12466616]
 [-0.13336401]
 [-0.10917286]
 [ 0.12353027]]
c bias before: [[-0.23777923]]
The mean c bias after update: [[-0.23919412]]
The output is: 
 [[0.49866818]
 [0.46372055]
 [0.55746344]
 [0.49418786]]
Total Loss: 0.48816291931429434
Average Loss: 0.12204072982857359

RUN:
  129
FeedForward


Z1 is:
 [[-2.49196956 -1.56657333]
 [-4.14473074 -2.61258983]
 [-2.71403496 -0.60587318]
 [-4.36679613 -1.65188968]]
H is:
 [[0.07642306 0.17270544]
 [0.01560047 0.06833254]
 [0.06215025 0.35300115]
 [0.01253277 0.16085372]]
The c is
 [[-0.23919412]]
Z2 is:
 [[-0.00592973]
 [-0.14735604]
 [ 0.23466149]
 [-0.02376105]]
Y^ -  the output is:
 [[0.49851757]
 [0.4632275 ]
 [0.55839764]
 [0.49406002]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49851757]
 [0.4632275 ]
 [0.55839764]
 [0.49406002]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.1246283 ]
 [-0.13346729]
 [-0.1088946 ]
 [ 0.12349757]]
W2.T is
 [[0.03109867 1.33688745]]
 D_Error times W2.T
 [[ 0.00387577  0.16661401]
 [-0.00415066 -0.17843075]
 [-0.00338648 -0.14557982]
 [ 0.00384061  0.16510235]]
self.H_D_Error_W2 is
 [[ 2.73562134e-04  2.38055211e-02]
 [-6.37420086e-05 -1.13594734e-02]
 [-1.97389601e-04 -3.32491711e-02]
 [ 4.75302534e-05  2.22854827e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 2.73562134e-04  2.38055211e-02]
 [-6.37420086e-05 -1.13594734e-02]
 [-1.97389601e-04 -3.32491711e-02]
 [ 4.75302534e-05  2.22854827e-02]]
X_H_D_Error_W2 is
 [[-1.49859348e-04 -1.09636885e-02]
 [-1.62117552e-05  1.09260092e-02]]
Using sum gradient........

W1 was :
 [[-0.22206539  0.96070015]
 [-1.65276118 -1.04601649]]
Updated W1 is: 
 [[-0.22191553  0.97166384]
 [-1.65274497 -1.0569425 ]]
W2 was :
 [[0.03109867]
 [1.33688745]]
Updated W2 is: 
 [[0.0288764]
 [1.3430585]]
The mean of the biases b gradient is:
 [1.49901943e-05 3.70589814e-04]
The b biases before the update are:
 [[-2.49196956 -1.56657333]]
The new updated bs are:
 [[-2.49198455 -1.56694392]]
The bias c is: 
 [[ 0.1246283 ]
 [-0.13346729]
 [-0.1088946 ]
 [ 0.12349757]]
c bias before: [[-0.23919412]]
The mean c bias after update: [[-0.24063511]]
The output is: 
 [[0.49851757]
 [0.4632275 ]
 [0.55839764]
 [0.49406002]]
Total Loss: 0.48787621389152336
Average Loss: 0.12196905347288084

RUN:
  130
FeedForward


Z1 is:
 [[-2.49198455 -1.56694392]
 [-4.14472952 -2.62388642]
 [-2.71390009 -0.59528008]
 [-4.36664505 -1.65222259]]
H is:
 [[0.07642201 0.1726525 ]
 [0.01560049 0.06761686]
 [0.06215811 0.35542428]
 [0.01253464 0.16080879]]
The c is
 [[-0.24063511]]
Z2 is:
 [[-0.00654592]
 [-0.14937122]
 [ 0.23851539]
 [-0.02429755]]
Y^ -  the output is:
 [[0.49836353]
 [0.46272647]
 [0.55934776]
 [0.49392591]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49836353]
 [0.46272647]
 [0.55934776]
 [0.49392591]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12458955]
 [-0.13357194]
 [-0.10861101]
 [ 0.12346325]]
W2.T is
 [[0.0288764 1.3430585]]
 D_Error times W2.T
 [[ 0.0035977   0.16733105]
 [-0.00385708 -0.17939493]
 [-0.0031363  -0.14587095]
 [ 0.00356517  0.16581837]]
self.H_D_Error_W2 is
 [[ 2.53931585e-04  2.39021716e-02]
 [-5.92335764e-05 -1.13099214e-02]
 [-1.82828712e-04 -3.34187217e-02]
 [ 4.41280475e-05  2.23770769e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 2.53931585e-04  2.39021716e-02]
 [-5.92335764e-05 -1.13099214e-02]
 [-1.82828712e-04 -3.34187217e-02]
 [ 4.41280475e-05  2.23770769e-02]]
X_H_D_Error_W2 is
 [[-1.38700664e-04 -1.10416448e-02]
 [-1.51055290e-05  1.10671555e-02]]
Using sum gradient........

W1 was :
 [[-0.22191553  0.97166384]
 [-1.65274497 -1.0569425 ]]
Updated W1 is: 
 [[-0.22177683  0.98270548]
 [-1.65272986 -1.06800966]]
W2 was :
 [[0.0288764]
 [1.3430585]]
Updated W2 is: 
 [[0.0266423 ]
 [1.34932853]]
The mean of the biases b gradient is:
 [1.39993362e-05 3.87651358e-04]
The b biases before the update are:
 [[-2.49198455 -1.56694392]]
The new updated bs are:
 [[-2.49199855 -1.56733157]]
The bias c is: 
 [[ 0.12458955]
 [-0.13357194]
 [-0.10861101]
 [ 0.12346325]]
c bias before: [[-0.24063511]]
The mean c bias after update: [[-0.24210257]]
The output is: 
 [[0.49836353]
 [0.46272647]
 [0.55934776]
 [0.49392591]]
Total Loss: 0.48758312724046937
Average Loss: 0.12189578181011734

RUN:
  131
FeedForward


Z1 is:
 [[-2.49199855 -1.56733157]
 [-4.14472841 -2.63534123]
 [-2.71377538 -0.58462609]
 [-4.36650524 -1.65263575]]
H is:
 [[0.07642102 0.17259713]
 [0.01560051 0.06689826]
 [0.06216538 0.35786883]
 [0.01253637 0.16075304]]
The c is
 [[-0.24210257]]
Z2 is:
 [[-0.00717631]
 [-0.1514192 ]
 [ 0.24243627]
 [-0.02485991]]
Y^ -  the output is:
 [[0.49820593]
 [0.46221736]
 [0.56031394]
 [0.49378534]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49820593]
 [0.46221736]
 [0.56031394]
 [0.49378534]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12454988]
 [-0.13367796]
 [-0.10832204]
 [ 0.12342726]]
W2.T is
 [[0.0266423  1.34932853]]
 D_Error times W2.T
 [[ 0.00331829  0.16805871]
 [-0.00356149 -0.18037549]
 [-0.00288595 -0.14616201]
 [ 0.00328839  0.16654393]]
self.H_D_Error_W2 is
 [[ 2.34208047e-04  2.40000202e-02]
 [-5.46942329e-05 -1.12595583e-02]
 [-1.68253190e-04 -3.35878453e-02]
 [ 4.07076306e-05  2.24686912e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 2.34208047e-04  2.40000202e-02]
 [-5.46942329e-05 -1.12595583e-02]
 [-1.68253190e-04 -3.35878453e-02]
 [ 4.07076306e-05  2.24686912e-02]]
X_H_D_Error_W2 is
 [[-1.27545559e-04 -1.11191541e-02]
 [-1.39866022e-05  1.12091329e-02]]
Using sum gradient........

W1 was :
 [[-0.22177683  0.98270548]
 [-1.65272986 -1.06800966]]
Updated W1 is: 
 [[-0.22164929  0.99382464]
 [-1.65271587 -1.07921879]]
W2 was :
 [[0.0266423 ]
 [1.34932853]]
Updated W2 is: 
 [[0.02439606]
 [1.35569818]]
The mean of the biases b gradient is:
 [1.29920638e-05 4.05326941e-04]
The b biases before the update are:
 [[-2.49199855 -1.56733157]]
The new updated bs are:
 [[-2.49201155 -1.5677369 ]]
The bias c is: 
 [[ 0.12454988]
 [-0.13367796]
 [-0.10832204]
 [ 0.12342726]]
c bias before: [[-0.24210257]]
The mean c bias after update: [[-0.24359686]]
The output is: 
 [[0.49820593]
 [0.46221736]
 [0.56031394]
 [0.49378534]]
Total Loss: 0.48728355426473846
Average Loss: 0.12182088856618462

RUN:
  132
FeedForward


Z1 is:
 [[-2.49201155 -1.5677369 ]
 [-4.14472742 -2.64695569]
 [-2.71366083 -0.57391226]
 [-4.3663767  -1.65313106]]
H is:
 [[0.0764201  0.17253925]
 [0.01560052 0.06617689]
 [0.06217206 0.36033458]
 [0.01253797 0.16068623]]
The c is
 [[-0.24359686]]
Z2 is:
 [[-0.00782136]
 [-0.15350038]
 [ 0.24642483]
 [-0.02544896]]
Y^ -  the output is:
 [[0.49804467]
 [0.46170008]
 [0.56129633]
 [0.4936381 ]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49804467]
 [0.46170008]
 [0.56129633]
 [0.4936381 ]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12450926]
 [-0.13378536]
 [-0.1080276 ]
 [ 0.12338955]]
W2.T is
 [[0.02439606 1.35569818]]
 D_Error times W2.T
 [[ 0.00303754  0.16879698]
 [-0.00326384 -0.18137256]
 [-0.00263545 -0.14645282]
 [ 0.00301022  0.16727898]]
self.H_D_Error_W2 is
 [[ 2.14389471e-04  2.40990539e-02]
 [-5.01232002e-05 -1.12083730e-02]
 [-1.53664254e-04 -3.37564339e-02]
 [ 3.72688148e-05  2.25602748e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 2.14389471e-04  2.40990539e-02]
 [-5.01232002e-05 -1.12083730e-02]
 [-1.53664254e-04 -3.37564339e-02]
 [ 3.72688148e-05  2.25602748e-02]]
X_H_D_Error_W2 is
 [[-1.16395439e-04 -1.11961591e-02]
 [-1.28543854e-05  1.13519018e-02]]
Using sum gradient........

W1 was :
 [[-0.22164929  0.99382464]
 [-1.65271587 -1.07921879]]
Updated W1 is: 
 [[-0.22153289  1.00502079]
 [-1.65270302 -1.09057069]]
W2 was :
 [[0.02439606]
 [1.35569818]]
Updated W2 is: 
 [[0.02213742]
 [1.36216802]]
The mean of the biases b gradient is:
 [1.19677080e-05 4.23630437e-04]
The b biases before the update are:
 [[-2.49201155 -1.5677369 ]]
The new updated bs are:
 [[-2.49202351 -1.56816053]]
The bias c is: 
 [[ 0.12450926]
 [-0.13378536]
 [-0.1080276 ]
 [ 0.12338955]]
c bias before: [[-0.24359686]]
The mean c bias after update: [[-0.24511832]]
The output is: 
 [[0.49804467]
 [0.46170008]
 [0.56129633]
 [0.4936381 ]]
Total Loss: 0.4869773910124947
Average Loss: 0.12174434775312368

RUN:
  133
FeedForward


Z1 is:
 [[-2.49202351 -1.56816053]
 [-4.14472653 -2.65873122]
 [-2.7135564  -0.56313973]
 [-4.36625942 -1.65371043]]
H is:
 [[0.07641926 0.17247878]
 [0.01560054 0.0654529 ]
 [0.06217815 0.3628213 ]
 [0.01253942 0.1606081 ]]
The c is
 [[-0.24511832]]
Z2 is:
 [[-0.00848152]
 [-0.15561512]
 [ 0.25048171]
 [-0.02606551]]
Y^ -  the output is:
 [[0.49787963]
 [0.46117454]
 [0.56229506]
 [0.49348399]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49787963]
 [0.46117454]
 [0.56229506]
 [0.49348399]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12446767]
 [-0.13389413]
 [-0.10772764]
 [ 0.12335005]]
W2.T is
 [[0.02213742 1.36216802]]
 D_Error times W2.T
 [[ 0.00275539  0.16954588]
 [-0.00296407 -0.1823863 ]
 [-0.00238481 -0.14674315]
 [ 0.00273065  0.16802349]]
self.H_D_Error_W2 is
 [[ 1.94473828e-04  2.41992581e-02]
 [-4.55196950e-05 -1.11563546e-02]
 [-1.39063198e-04 -3.39243758e-02]
 [ 3.38114194e-05  2.26517742e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 1.94473828e-04  2.41992581e-02]
 [-4.55196950e-05 -1.11563546e-02]
 [-1.39063198e-04 -3.39243758e-02]
 [ 3.38114194e-05  2.26517742e-02]]
X_H_D_Error_W2 is
 [[-1.05251779e-04 -1.12726016e-02]
 [-1.17082756e-05  1.14954196e-02]]
Using sum gradient........

W1 was :
 [[-0.22153289  1.00502079]
 [-1.65270302 -1.09057069]]
Updated W1 is: 
 [[-0.22142764  1.0162934 ]
 [-1.65269131 -1.10206611]]
W2 was :
 [[0.02213742]
 [1.36216802]]
Updated W2 is: 
 [[0.01986608]
 [1.36873861]]
The mean of the biases b gradient is:
 [1.09255887e-05 4.42575480e-04]
The b biases before the update are:
 [[-2.49202351 -1.56816053]]
The new updated bs are:
 [[-2.49203444 -1.5686031 ]]
The bias c is: 
 [[ 0.12446767]
 [-0.13389413]
 [-0.10772764]
 [ 0.12335005]]
c bias before: [[-0.24511832]]
The mean c bias after update: [[-0.24666731]]
The output is: 
 [[0.49787963]
 [0.46117454]
 [0.56229506]
 [0.49348399]]
Total Loss: 0.4866645348728194
Average Loss: 0.12166613371820485

RUN:
  134
FeedForward


Z1 is:
 [[-2.49203444 -1.5686031 ]
 [-4.14472575 -2.67066922]
 [-2.71346208 -0.55230971]
 [-4.36615339 -1.65437582]]
H is:
 [[0.07641849 0.17241562]
 [0.01560055 0.06472644]
 [0.06218365 0.36532871]
 [0.01254073 0.16051842]]
The c is
 [[-0.24666731]]
Z2 is:
 [[-0.00915725]
 [-0.1577638 ]
 [ 0.25460754]
 [-0.02671041]]
Y^ -  the output is:
 [[0.4977107 ]
 [0.46064065]
 [0.56331025]
 [0.49332279]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.4977107 ]
 [0.46064065]
 [0.56331025]
 [0.49332279]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12442507]
 [-0.13400428]
 [-0.1074221 ]
 [ 0.1233087 ]]
W2.T is
 [[0.01986608 1.36873861]]
 D_Error times W2.T
 [[ 0.00247184  0.17030539]
 [-0.00266214 -0.18341684]
 [-0.00213406 -0.14703278]
 [ 0.00244966  0.16877738]]
self.H_D_Error_W2 is
 [[ 1.74459110e-04  2.43006169e-02]
 [-4.08829286e-05 -1.11034926e-02]
 [-1.24451392e-04 -3.40915563e-02]
 [ 3.03352725e-05  2.27431336e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 1.74459110e-04  2.43006169e-02]
 [-4.08829286e-05 -1.11034926e-02]
 [-1.24451392e-04 -3.40915563e-02]
 [ 3.03352725e-05  2.27431336e-02]]
X_H_D_Error_W2 is
 [[-9.41161197e-05 -1.13484227e-02]
 [-1.05476561e-05  1.16396410e-02]]
Using sum gradient........

W1 was :
 [[-0.22142764  1.0162934 ]
 [-1.65269131 -1.10206611]]
Updated W1 is: 
 [[-0.22133352  1.02764182]
 [-1.65268076 -1.11370575]]
W2 was :
 [[0.01986608]
 [1.36873861]]
Updated W2 is: 
 [[0.01758176]
 [1.37541047]]
The mean of the biases b gradient is:
 [9.86501546e-06 4.62175414e-04]
The b biases before the update are:
 [[-2.49203444 -1.5686031 ]]
The new updated bs are:
 [[-2.4920443  -1.56906528]]
The bias c is: 
 [[ 0.12442507]
 [-0.13400428]
 [-0.1074221 ]
 [ 0.1233087 ]]
c bias before: [[-0.24666731]]
The mean c bias after update: [[-0.24824415]]
The output is: 
 [[0.4977107 ]
 [0.46064065]
 [0.56331025]
 [0.49332279]]
Total Loss: 0.4863448847788206
Average Loss: 0.12158622119470515

RUN:
  135
FeedForward


Z1 is:
 [[-2.4920443  -1.56906528]
 [-4.14472507 -2.68277103]
 [-2.71337783 -0.54142346]
 [-4.36605859 -1.65512922]]
H is:
 [[0.07641779 0.17234968]
 [0.01560056 0.06399769]
 [0.06218856 0.36785651]
 [0.01254191 0.16041693]]
The c is
 [[-0.24824415]]
Z2 is:
 [[-0.00984903]
 [-0.15994678]
 [ 0.25880293]
 [-0.02738453]]
Y^ -  the output is:
 [[0.49753776]
 [0.46009834]
 [0.564342  ]
 [0.4931543 ]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49753776]
 [0.46009834]
 [0.564342  ]
 [0.4931543 ]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12438142]
 [-0.13411582]
 [-0.10711092]
 [ 0.12326546]]
W2.T is
 [[0.01758176 1.37541047]]
 D_Error times W2.T
 [[ 0.00218684  0.17107551]
 [-0.00235799 -0.1844643 ]
 [-0.0018832  -0.14732148]
 [ 0.00216722  0.16954061]]
self.H_D_Error_W2 is
 [[ 1.54343331e-04  2.44031128e-02]
 [-3.62121078e-05 -1.10497769e-02]
 [-1.09830284e-04 -3.42578576e-02]
 [ 2.68402109e-05  2.28342947e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 1.54343331e-04  2.44031128e-02]
 [-3.62121078e-05 -1.10497769e-02]
 [-1.09830284e-04 -3.42578576e-02]
 [ 2.68402109e-05  2.28342947e-02]]
X_H_D_Error_W2 is
 [[-8.29900735e-05 -1.14235629e-02]
 [-9.37189687e-06  1.17845178e-02]]
Using sum gradient........

W1 was :
 [[-0.22133352  1.02764182]
 [-1.65268076 -1.11370575]]
Updated W1 is: 
 [[-0.22125053  1.03906538]
 [-1.65267139 -1.12549027]]
W2 was :
 [[0.01758176]
 [1.37541047]]
Updated W2 is: 
 [[0.01528418]
 [1.38218406]]
The mean of the biases b gradient is:
 [8.78528753e-06 4.82443248e-04]
The b biases before the update are:
 [[-2.4920443  -1.56906528]]
The new updated bs are:
 [[-2.49205309 -1.56954772]]
The bias c is: 
 [[ 0.12438142]
 [-0.13411582]
 [-0.10711092]
 [ 0.12326546]]
c bias before: [[-0.24824415]]
The mean c bias after update: [[-0.24984919]]
The output is: 
 [[0.49753776]
 [0.46009834]
 [0.564342  ]
 [0.4931543 ]]
Total Loss: 0.48601834141719147
Average Loss: 0.12150458535429787

RUN:
  136
FeedForward


Z1 is:
 [[-2.49205309 -1.56954772]
 [-4.14472448 -2.695038  ]
 [-2.71330362 -0.53048234]
 [-4.36597501 -1.65597261]]
H is:
 [[0.07641717 0.17228088]
 [0.01560057 0.06326679]
 [0.06219289 0.3704044 ]
 [0.01254294 0.16030337]]
The c is
 [[-0.24984919]]
Z2 is:
 [[-0.01055734]
 [-0.1621644 ]
 [ 0.26306843]
 [-0.02808873]]
Y^ -  the output is:
 [[0.49736069]
 [0.45954751]
 [0.56539043]
 [0.49297828]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49736069]
 [0.45954751]
 [0.56539043]
 [0.49297828]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12433671]
 [-0.13422872]
 [-0.10679404]
 [ 0.12322026]]
W2.T is
 [[0.01528418 1.38218406]]
 D_Error times W2.T
 [[ 0.00190038  0.17185622]
 [-0.00205158 -0.1855288 ]
 [-0.00163226 -0.14760902]
 [ 0.00188332  0.17031308]]
self.H_D_Error_W2 is
 [[ 1.34124534e-04  2.45067266e-02]
 [-3.15064350e-05 -1.09951978e-02]
 [-9.52014015e-05 -3.44231590e-02]
 [ 2.33260811e-05  2.29251966e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 1.34124534e-04  2.45067266e-02]
 [-3.15064350e-05 -1.09951978e-02]
 [-9.52014015e-05 -3.44231590e-02]
 [ 2.33260811e-05  2.29251966e-02]]
X_H_D_Error_W2 is
 [[-7.18753204e-05 -1.14979624e-02]
 [-8.18035391e-06  1.19299988e-02]]
Using sum gradient........

W1 was :
 [[-0.22125053  1.03906538]
 [-1.65267139 -1.12549027]]
Updated W1 is: 
 [[-0.22117866  1.05056334]
 [-1.65266321 -1.13742027]]
W2 was :
 [[0.01528418]
 [1.38218406]]
Updated W2 is: 
 [[0.01297305]
 [1.3890598 ]]
The mean of the biases b gradient is:
 [7.68569462e-06 5.03391601e-04]
The b biases before the update are:
 [[-2.49205309 -1.56954772]]
The new updated bs are:
 [[-2.49206078 -1.57005112]]
The bias c is: 
 [[ 0.12433671]
 [-0.13422872]
 [-0.10679404]
 [ 0.12322026]]
c bias before: [[-0.24984919]]
The mean c bias after update: [[-0.25148274]]
The output is: 
 [[0.49736069]
 [0.45954751]
 [0.56539043]
 [0.49297828]]
Total Loss: 0.4856848074438689
Average Loss: 0.12142120186096722

RUN:
  137
FeedForward


Z1 is:
 [[-2.49206078 -1.57005112]
 [-4.14472399 -2.70747139]
 [-2.71323943 -0.51948777]
 [-4.36590264 -1.65690804]]
H is:
 [[0.07641663 0.1722091 ]
 [0.01560057 0.06253392]
 [0.06219663 0.37297202]
 [0.01254384 0.16017749]]
The c is
 [[-0.25148274]]
Z2 is:
 [[-0.01128264]
 [-0.164417  ]
 [ 0.26740457]
 [-0.0288239 ]]
Y^ -  the output is:
 [[0.49717937]
 [0.4589881 ]
 [0.56645562]
 [0.49279452]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49717937]
 [0.4589881 ]
 [0.56645562]
 [0.49279452]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12429089]
 [-0.13434301]
 [-0.10647141]
 [ 0.12317305]]
W2.T is
 [[0.01297305 1.3890598 ]]
 D_Error times W2.T
 [[ 0.00161243  0.17264747]
 [-0.00174284 -0.18661047]
 [-0.00138126 -0.14789516]
 [ 0.00159793  0.17109473]]
self.H_D_Error_W2 is
 [[ 1.13800789e-04  2.46114377e-02]
 [-2.67651096e-05 -1.09397461e-02]
 [-8.05663500e-05 -3.45873370e-02]
 [ 1.97927400e-05  2.30157761e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 1.13800789e-04  2.46114377e-02]
 [-2.67651096e-05 -1.09397461e-02]
 [-8.05663500e-05 -3.45873370e-02]
 [ 1.97927400e-05  2.30157761e-02]]
X_H_D_Error_W2 is
 [[-6.07736100e-05 -1.15715609e-02]
 [-6.97236954e-06  1.20760299e-02]]
Using sum gradient........

W1 was :
 [[-0.22117866  1.05056334]
 [-1.65266321 -1.13742027]]
Updated W1 is: 
 [[-0.22111788  1.06213491]
 [-1.65265624 -1.1494963 ]]
W2 was :
 [[0.01297305]
 [1.3890598 ]]
Updated W2 is: 
 [[0.01064809]
 [1.39603808]]
The mean of the biases b gradient is:
 [6.56551727e-06 5.25032653e-04]
The b biases before the update are:
 [[-2.49206078 -1.57005112]]
The new updated bs are:
 [[-2.49206734 -1.57057615]]
The bias c is: 
 [[ 0.12429089]
 [-0.13434301]
 [-0.10647141]
 [ 0.12317305]]
c bias before: [[-0.25148274]]
The mean c bias after update: [[-0.25314512]]
The output is: 
 [[0.49717937]
 [0.4589881 ]
 [0.56645562]
 [0.49279452]]
Total Loss: 0.485344187705398
Average Loss: 0.1213360469263495

RUN:
  138
FeedForward


Z1 is:
 [[-2.49206734 -1.57057615]
 [-4.14472358 -2.72007245]
 [-2.71318522 -0.50844124]
 [-4.36584146 -1.65793754]]
H is:
 [[0.07641616 0.17213427]
 [0.01560058 0.06179927]
 [0.06219979 0.37555901]
 [0.01254459 0.16003905]]
The c is
 [[-0.25314512]]
Z2 is:
 [[-0.01202544]
 [-0.16670488]
 [ 0.27181186]
 [-0.02959094]]
Y^ -  the output is:
 [[0.49699368]
 [0.45842003]
 [0.56753766]
 [0.49260281]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49699368]
 [0.45842003]
 [0.56753766]
 [0.49260281]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12424393]
 [-0.13445866]
 [-0.10614298]
 [ 0.12312375]]
W2.T is
 [[0.01064809 1.39603808]]
 D_Error times W2.T
 [[ 0.00132296  0.17344925]
 [-0.00143173 -0.18770941]
 [-0.00113022 -0.14817964]
 [ 0.00131103  0.17188544]]
self.H_D_Error_W2 is
 [[ 9.33701987e-05  2.47172237e-02]
 [-2.19873277e-05 -1.08834132e-02]
 [-6.59268169e-05 -3.47502653e-02]
 [ 1.62400555e-05  2.31059672e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 9.33701987e-05  2.47172237e-02]
 [-2.19873277e-05 -1.08834132e-02]
 [-6.59268169e-05 -3.47502653e-02]
 [ 1.62400555e-05  2.31059672e-02]]
X_H_D_Error_W2 is
 [[-4.96867614e-05 -1.16442981e-02]
 [-5.74727221e-06  1.22225539e-02]]
Using sum gradient........

W1 was :
 [[-0.22111788  1.06213491]
 [-1.65265624 -1.1494963 ]]
Updated W1 is: 
 [[-0.2210682   1.0737792 ]
 [-1.65265049 -1.16171885]]
W2 was :
 [[0.01064809]
 [1.39603808]]
Updated W2 is: 
 [[0.00830901]
 [1.40311923]]
The mean of the biases b gradient is:
 [5.42402740e-06 5.47378089e-04]
The b biases before the update are:
 [[-2.49206734 -1.57057615]]
The new updated bs are:
 [[-2.49207276 -1.57112353]]
The bias c is: 
 [[ 0.12424393]
 [-0.13445866]
 [-0.10614298]
 [ 0.12312375]]
c bias before: [[-0.25314512]]
The mean c bias after update: [[-0.25483663]]
The output is: 
 [[0.49699368]
 [0.45842003]
 [0.56753766]
 [0.49260281]]
Total Loss: 0.4849963894655596
Average Loss: 0.1212490973663899

RUN:
  139
FeedForward


Z1 is:
 [[-2.49207276 -1.57112353]
 [-4.14472326 -2.73284238]
 [-2.71314096 -0.49734432]
 [-4.36579145 -1.65906318]]
H is:
 [[0.07641578 0.17205628]
 [0.01560059 0.06106299]
 [0.06220238 0.37816497]
 [0.01254521 0.15988779]]
The c is
 [[-0.25483663]]
Z2 is:
 [[-0.01278621]
 [-0.16902834]
 [ 0.27629075]
 [-0.03039075]]
Y^ -  the output is:
 [[0.49680349]
 [0.45784324]
 [0.56863662]
 [0.4924029 ]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49680349]
 [0.45784324]
 [0.56863662]
 [0.4924029 ]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.1241958 ]
 [-0.13457567]
 [-0.1058087 ]
 [ 0.1230723 ]]
W2.T is
 [[0.00830901 1.40311923]]
 D_Error times W2.T
 [[ 0.00103194  0.17426151]
 [-0.00111819 -0.18882572]
 [-0.00087917 -0.14846222]
 [ 0.00102261  0.17268512]]
self.H_D_Error_W2 is
 [[ 7.28309024e-05  2.48240608e-02]
 [-1.71722837e-05 -1.08261911e-02]
 [-5.12845701e-05 -3.49118153e-02]
 [ 1.26679071e-05  2.31957017e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 7.28309024e-05  2.48240608e-02]
 [-1.71722837e-05 -1.08261911e-02]
 [-5.12845701e-05 -3.49118153e-02]
 [ 1.26679071e-05  2.31957017e-02]]
X_H_D_Error_W2 is
 [[-3.86166630e-05 -1.17161135e-02]
 [-4.50437654e-06  1.23695107e-02]]
Using sum gradient........

W1 was :
 [[-0.2210682   1.0737792 ]
 [-1.65265049 -1.16171885]]
Updated W1 is: 
 [[-0.22102958  1.08549532]
 [-1.65264599 -1.17408837]]
W2 was :
 [[0.00830901]
 [1.40311923]]
Updated W2 is: 
 [[0.00595553]
 [1.41030354]]
The mean of the biases b gradient is:
 [4.26048895e-06 5.70439046e-04]
The b biases before the update are:
 [[-2.49207276 -1.57112353]]
The new updated bs are:
 [[-2.49207703 -1.57169396]]
The bias c is: 
 [[ 0.1241958 ]
 [-0.13457567]
 [-0.1058087 ]
 [ 0.1230723 ]]
c bias before: [[-0.25483663]]
The mean c bias after update: [[-0.25655756]]
The output is: 
 [[0.49680349]
 [0.45784324]
 [0.56863662]
 [0.4924029 ]]
Total Loss: 0.4846413226367732
Average Loss: 0.1211603306591933

RUN:
  140
FeedForward


Z1 is:
 [[-2.49207703 -1.57169396]
 [-4.14472301 -2.74578233]
 [-2.71310661 -0.48619865]
 [-4.36575259 -1.66028701]]
H is:
 [[0.07641548 0.17197504]
 [0.01560059 0.06032529]
 [0.06220438 0.38078948]
 [0.0125457  0.15972347]]
The c is
 [[-0.25655756]]
Z2 is:
 [[-0.01356546]
 [-0.17138768]
 [ 0.28084165]
 [-0.03122427]]
Y^ -  the output is:
 [[0.49660869]
 [0.45725765]
 [0.56975255]
 [0.49219457]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49660869]
 [0.45725765]
 [0.56975255]
 [0.49219457]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12414646]
 [-0.13469405]
 [-0.10546853]
 [ 0.12301865]]
W2.T is
 [[0.00595553 1.41030354]]
 D_Error times W2.T
 [[ 0.00073936  0.17508419]
 [-0.00080217 -0.18995949]
 [-0.00062812 -0.14874264]
 [ 0.00073264  0.17349364]]
self.H_D_Error_W2 is
 [[ 5.21810762e-05  2.49319232e-02]
 [-1.23191703e-05 -1.07680722e-02]
 [-3.66414590e-05 -3.50718559e-02]
 [ 9.07618690e-06  2.32849091e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 5.21810762e-05  2.49319232e-02]
 [-1.23191703e-05 -1.07680722e-02]
 [-3.66414590e-05 -3.50718559e-02]
 [ 9.07618690e-06  2.32849091e-02]]
X_H_D_Error_W2 is
 [[-2.75652721e-05 -1.17869468e-02]
 [-3.24298336e-06  1.25168369e-02]]
Using sum gradient........

W1 was :
 [[-0.22102958  1.08549532]
 [-1.65264599 -1.17408837]]
Updated W1 is: 
 [[-0.22100201  1.09728226]
 [-1.65264274 -1.1866052 ]]
W2 was :
 [[0.00595553]
 [1.41030354]]
Updated W2 is: 
 [[0.00358738]
 [1.41759125]]
The mean of the biases b gradient is:
 [3.07415847e-06 5.94226055e-04]
The b biases before the update are:
 [[-2.49207703 -1.57169396]]
The new updated bs are:
 [[-2.4920801  -1.57228819]]
The bias c is: 
 [[ 0.12414646]
 [-0.13469405]
 [-0.10546853]
 [ 0.12301865]]
c bias before: [[-0.25655756]]
The mean c bias after update: [[-0.2583082]]
The output is: 
 [[0.49660869]
 [0.45725765]
 [0.56975255]
 [0.49219457]]
Total Loss: 0.48427890001573626
Average Loss: 0.12106972500393406

RUN:
  141
FeedForward


Z1 is:
 [[-2.4920801  -1.57228819]
 [-4.14472284 -2.75889339]
 [-2.71308211 -0.47500593]
 [-4.36572486 -1.66161113]]
H is:
 [[0.07641526 0.17189044]
 [0.01560059 0.05958635]
 [0.06220581 0.38343209]
 [0.01254604 0.15954584]]
The c is
 [[-0.2583082]]
Z2 is:
 [[-0.01436369]
 [-0.17378315]
 [ 0.28546494]
 [-0.0320924 ]]
Y^ -  the output is:
 [[0.49640914]
 [0.45666322]
 [0.57088551]
 [0.49197759]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49640914]
 [0.45666322]
 [0.57088551]
 [0.49197759]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12409588]
 [-0.13481377]
 [-0.10512243]
 [ 0.12296273]]
W2.T is
 [[0.00358738 1.41759125]]
 D_Error times W2.T
 [[ 0.00044518  0.17591724]
 [-0.00048363 -0.19111081]
 [-0.00037711 -0.14902063]
 [ 0.00044111  0.17431089]]
self.H_D_Error_W2 is
 [[ 3.14189378e-05  2.50407836e-02]
 [-7.42717970e-06 -1.07090498e-02]
 [-2.19994145e-05 -3.52302539e-02]
 [ 5.46479996e-06  2.33735161e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 3.14189378e-05  2.50407836e-02]
 [-7.42717970e-06 -1.07090498e-02]
 [-2.19994145e-05 -3.52302539e-02]
 [ 5.46479996e-06  2.33735161e-02]]
X_H_D_Error_W2 is
 [[-1.65346145e-05 -1.18567378e-02]
 [-1.96237974e-06  1.26644663e-02]]
Using sum gradient........

W1 was :
 [[-0.22100201  1.09728226]
 [-1.65264274 -1.1866052 ]]
Updated W1 is: 
 [[-0.22098548  1.109139  ]
 [-1.65264078 -1.19926967]]
W2 was :
 [[0.00358738]
 [1.41759125]]
Updated W2 is: 
 [[1.20426433e-03]
 [1.42498253e+00]]
The mean of the biases b gradient is:
 [1.86428590e-06 6.18748987e-04]
The b biases before the update are:
 [[-2.4920801  -1.57228819]]
The new updated bs are:
 [[-2.49208196 -1.57290694]]
The bias c is: 
 [[ 0.12409588]
 [-0.13481377]
 [-0.10512243]
 [ 0.12296273]]
c bias before: [[-0.2583082]]
The mean c bias after update: [[-0.2600888]]
The output is: 
 [[0.49640914]
 [0.45666322]
 [0.57088551]
 [0.49197759]]
Total Loss: 0.48390903752271774
Average Loss: 0.12097725938067944

RUN:
  142
FeedForward


Z1 is:
 [[-2.49208196 -1.57290694]
 [-4.14472274 -2.77217661]
 [-2.71306744 -0.46376794]
 [-4.36570822 -1.66303761]]
H is:
 [[0.07641513 0.17180238]
 [0.01560059 0.05884635]
 [0.06220667 0.38609235]
 [0.01254625 0.15935466]]
The c is
 [[-0.2600888]]
Z2 is:
 [[-0.01518139]
 [-0.176215  ]
 [ 0.29016096]
 [-0.0329961 ]]
Y^ -  the output is:
 [[0.49620473]
 [0.45605989]
 [0.57203554]
 [0.49175172]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49620473]
 [0.45605989]
 [0.57203554]
 [0.49175172]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12404403]
 [-0.13493482]
 [-0.10477036]
 [ 0.12290448]]
W2.T is
 [[1.20426433e-03 1.42498253e+00]]
 D_Error times W2.T
 [[ 1.49381805e-04  1.76760581e-01]
 [-1.62497194e-04 -1.92279766e-01]
 [-1.26171204e-04 -1.49295929e-01]
 [ 1.48009475e-04  1.75136730e-01]]
self.H_D_Error_W2 is
 [[ 1.05427492e-05  2.51506129e-02]
 [-2.49550438e-06 -1.06491181e-02]
 [-7.36044904e-06 -3.53868744e-02]
 [ 1.83366526e-06  2.34614476e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[ 1.05427492e-05  2.51506129e-02]
 [-2.49550438e-06 -1.06491181e-02]
 [-7.36044904e-06 -3.53868744e-02]
 [ 1.83366526e-06  2.34614476e-02]]
X_H_D_Error_W2 is
 [[-5.52678378e-06 -1.19254268e-02]
 [-6.61839115e-07  1.28123295e-02]]
Using sum gradient........

W1 was :
 [[-0.22098548  1.109139  ]
 [-1.65264078 -1.19926967]]
Updated W1 is: 
 [[-0.22097995  1.12106443]
 [-1.65264012 -1.212082  ]]
W2 was :
 [[1.20426433e-03]
 [1.42498253e+00]]
Updated W2 is: 
 [[-1.19408858e-03]
 [ 1.43247752e+00]]
The mean of the biases b gradient is:
 [6.30115262e-07 6.44016992e-04]
The b biases before the update are:
 [[-2.49208196 -1.57290694]]
The new updated bs are:
 [[-2.49208259 -1.57355096]]
The bias c is: 
 [[ 0.12404403]
 [-0.13493482]
 [-0.10477036]
 [ 0.12290448]]
c bias before: [[-0.2600888]]
The mean c bias after update: [[-0.26189964]]
The output is: 
 [[0.49620473]
 [0.45605989]
 [0.57203554]
 [0.49175172]]
Total Loss: 0.4835316544438805
Average Loss: 0.12088291361097013

RUN:
  143
FeedForward


Z1 is:
 [[-2.49208259 -1.57355096]
 [-4.14472271 -2.78563296]
 [-2.71306255 -0.45248653]
 [-4.36570267 -1.66456853]]
H is:
 [[0.07641509 0.17171076]
 [0.01560059 0.0581055 ]
 [0.06220695 0.38876973]
 [0.01254631 0.15914968]]
The c is
 [[-0.26189964]]
Z2 is:
 [[-0.01601907]
 [-0.17868344]
 [ 0.29492999]
 [-0.03393628]]
Y^ -  the output is:
 [[0.49599532]
 [0.45544761]
 [0.57320265]
 [0.49151674]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49599532]
 [0.45544761]
 [0.57320265]
 [0.49151674]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12399087]
 [-0.13505721]
 [-0.10441229]
 [ 0.12284381]]
W2.T is
 [[-1.19408858e-03  1.43247752e+00]]
 D_Error times W2.T
 [[-1.48056088e-04  1.77614141e-01]
 [ 1.61270268e-04 -1.93466412e-01]
 [ 1.24677524e-04 -1.49568260e-01]
 [-1.46686395e-04  1.75971002e-01]]
self.H_D_Error_W2 is
 [[-1.04491800e-05  2.52613804e-02]
 [ 2.47666228e-06 -1.05882717e-02]
 [ 7.27334343e-06 -3.55415807e-02]
 [-1.81728369e-06  2.35486258e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[-1.04491800e-05  2.52613804e-02]
 [ 2.47666228e-06 -1.05882717e-02]
 [ 7.27334343e-06 -3.55415807e-02]
 [-1.81728369e-06  2.35486258e-02]]
X_H_D_Error_W2 is
 [[ 5.45605974e-06 -1.19929548e-02]
 [ 6.59378585e-07  1.29603541e-02]]
Using sum gradient........

W1 was :
 [[-0.22097995  1.12106443]
 [-1.65264012 -1.212082  ]]
Updated W1 is: 
 [[-0.22098541  1.13305738]
 [-1.65264078 -1.22504235]]
W2 was :
 [[-1.19408858e-03]
 [ 1.43247752e+00]]
Updated W2 is: 
 [[-0.00360796]
 [ 1.44007631]]
The mean of the biases b gradient is:
 [-6.29114495e-07  6.70038445e-04]
The b biases before the update are:
 [[-2.49208259 -1.57355096]]
The new updated bs are:
 [[-2.49208196 -1.574221  ]]
The bias c is: 
 [[ 0.12399087]
 [-0.13505721]
 [-0.10441229]
 [ 0.12284381]]
c bias before: [[-0.26189964]]
The mean c bias after update: [[-0.26374093]]
The output is: 
 [[0.49599532]
 [0.45544761]
 [0.57320265]
 [0.49151674]]
Total Loss: 0.48314667367595343
Average Loss: 0.12078666841898836

RUN:
  144
FeedForward


Z1 is:
 [[-2.49208196 -1.574221  ]
 [-4.14472274 -2.79926335]
 [-2.71306737 -0.44116361]
 [-4.36570815 -1.66620596]]
H is:
 [[0.07641513 0.17161549]
 [0.01560059 0.057364  ]
 [0.06220667 0.39146374]
 [0.01254625 0.15893068]]
The c is
 [[-0.26374093]]
Z2 is:
 [[-0.01687724]
 [-0.18118869]
 [ 0.29977228]
 [-0.0349139 ]]
Y^ -  the output is:
 [[0.49578079]
 [0.45482635]
 [0.57438685]
 [0.49127241]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49578079]
 [0.45482635]
 [0.57438685]
 [0.49127241]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12393637]
 [-0.1351809 ]
 [-0.1040482 ]
 [ 0.12278068]]
W2.T is
 [[-0.00360796  1.44007631]]
 D_Error times W2.T
 [[-0.00044716  0.17847783]
 [ 0.00048773 -0.19467081]
 [ 0.0003754  -0.14983735]
 [-0.00044299  0.17681355]]
self.H_D_Error_W2 is
 [[-3.15584895e-05  2.53730534e-02]
 [ 7.49012491e-06 -1.05265064e-02]
 [ 2.18997881e-05 -3.56942345e-02]
 [-5.48809815e-06  2.36349712e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[-3.15584895e-05  2.53730534e-02]
 [ 7.49012491e-06 -1.05265064e-02]
 [ 2.18997881e-05 -3.56942345e-02]
 [-5.48809815e-06  2.36349712e-02]]
X_H_D_Error_W2 is
 [[ 1.64116900e-05 -1.20592634e-02]
 [ 2.00202676e-06  1.31084647e-02]]
Using sum gradient........

W1 was :
 [[-0.22098541  1.13305738]
 [-1.65264078 -1.22504235]]
Updated W1 is: 
 [[-0.22100182  1.14511665]
 [-1.65264278 -1.23815082]]
W2 was :
 [[-0.00360796]
 [ 1.44007631]]
Updated W2 is: 
 [[-0.00603761]
 [ 1.4477789 ]]
The mean of the biases b gradient is:
 [-1.91416865e-06  6.96820888e-04]
The b biases before the update are:
 [[-2.49208196 -1.574221  ]]
The new updated bs are:
 [[-2.49208005 -1.57491782]]
The bias c is: 
 [[ 0.12393637]
 [-0.1351809 ]
 [-0.1040482 ]
 [ 0.12278068]]
c bias before: [[-0.26374093]]
The mean c bias after update: [[-0.26561292]]
The output is: 
 [[0.49578079]
 [0.45482635]
 [0.57438685]
 [0.49127241]]
Total Loss: 0.4827540219725463
Average Loss: 0.12068850549313657

RUN:
  145
FeedForward


Z1 is:
 [[-2.49208005 -1.57491782]
 [-4.14472283 -2.81306863]
 [-2.71308187 -0.42980117]
 [-4.36572465 -1.66795199]]
H is:
 [[0.07641527 0.17151645]
 [0.01560059 0.05662204]
 [0.06220582 0.39417381]
 [0.01254604 0.15869742]]
The c is
 [[-0.26561292]]
Z2 is:
 [[-0.01775639]
 [-0.18373091]
 [ 0.30468803]
 [-0.03592989]]
Y^ -  the output is:
 [[0.49556102]
 [0.45419605]
 [0.57558814]
 [0.49101849]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49556102]
 [0.45419605]
 [0.57558814]
 [0.49101849]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12388049]
 [-0.13530589]
 [-0.10367806]
 [ 0.12271501]]
W2.T is
 [[-0.00603761  1.4477789 ]]
 D_Error times W2.T
 [[-0.00074794  0.17935156]
 [ 0.00081692 -0.19589301]
 [ 0.00062597 -0.15010291]
 [-0.00074091  0.17766421]]
self.H_D_Error_W2 is
 [[-5.27867649e-05  2.54855976e-02]
 [ 1.25456853e-05 -1.04638188e-02]
 [ 3.65166299e-05 -3.58446967e-02]
 [-9.17881343e-06  2.37204017e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[-5.27867649e-05  2.54855976e-02]
 [ 1.25456853e-05 -1.04638188e-02]
 [ 3.65166299e-05 -3.58446967e-02]
 [-9.17881343e-06  2.37204017e-02]]
X_H_D_Error_W2 is
 [[ 2.73378164e-05 -1.21242949e-02]
 [ 3.36687192e-06  1.32565830e-02]]
Using sum gradient........

W1 was :
 [[-0.22100182  1.14511665]
 [-1.65264278 -1.23815082]]
Updated W1 is: 
 [[-0.22102916  1.15724094]
 [-1.65264615 -1.2514074 ]]
W2 was :
 [[-0.00603761]
 [ 1.4477789 ]]
Updated W2 is: 
 [[-0.00848333]
 [ 1.45558528]]
The mean of the biases b gradient is:
 [-3.22581578e-06  7.24370972e-04]
The b biases before the update are:
 [[-2.49208005 -1.57491782]]
The new updated bs are:
 [[-2.49207682 -1.57564219]]
The bias c is: 
 [[ 0.12388049]
 [-0.13530589]
 [-0.10367806]
 [ 0.12271501]]
c bias before: [[-0.26561292]]
The mean c bias after update: [[-0.26751581]]
The output is: 
 [[0.49556102]
 [0.45419605]
 [0.57558814]
 [0.49101849]]
Total Loss: 0.48235363019134664
Average Loss: 0.12058840754783666

RUN:
  146
FeedForward


Z1 is:
 [[-2.49207682 -1.57564219]
 [-4.14472297 -2.82704959]
 [-2.71310598 -0.41840125]
 [-4.36575213 -1.66980865]]
H is:
 [[0.07641549 0.17141354]
 [0.01560059 0.05587985]
 [0.06220442 0.39689938]
 [0.0125457  0.15844969]]
The c is
 [[-0.26751581]]
Z2 is:
 [[-0.01865704]
 [-0.18631027]
 [ 0.30967738]
 [-0.0369852 ]]
Y^ -  the output is:
 [[0.49533587]
 [0.4535567 ]
 [0.57680651]
 [0.49075475]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49533587]
 [0.4535567 ]
 [0.57680651]
 [0.49075475]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12382319]
 [-0.13543216]
 [-0.10330185]
 [ 0.12264674]]
W2.T is
 [[-0.00848333  1.45558528]]
 D_Error times W2.T
 [[-0.00105043  0.18023522]
 [ 0.00114892 -0.19713306]
 [ 0.00087634 -0.15036465]
 [-0.00104045  0.17852279]]
self.H_D_Error_W2 is
 [[-7.41355348e-05  2.55989769e-02]
 [ 1.76441416e-05 -1.04002062e-02]
 [ 5.11215341e-05 -3.59928268e-02]
 [-1.28894482e-05  2.38048337e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[-7.41355348e-05  2.55989769e-02]
 [ 1.76441416e-05 -1.04002062e-02]
 [ 5.11215341e-05 -3.59928268e-02]
 [-1.28894482e-05  2.38048337e-02]]
X_H_D_Error_W2 is
 [[ 3.82320859e-05 -1.21879931e-02]
 [ 4.75469340e-06  1.34046275e-02]]
Using sum gradient........

W1 was :
 [[-0.22102916  1.15724094]
 [-1.65264615 -1.2514074 ]]
Updated W1 is: 
 [[-0.22106739  1.16942893]
 [-1.6526509  -1.26481203]]
W2 was :
 [[-0.00848333]
 [ 1.45558528]]
Updated W2 is: 
 [[-0.01094538]
 [ 1.46349533]]
The mean of the biases b gradient is:
 [-4.56482682e-06  7.52694402e-04]
The b biases before the update are:
 [[-2.49207682 -1.57564219]]
The new updated bs are:
 [[-2.49207226 -1.57639488]]
The bias c is: 
 [[ 0.12382319]
 [-0.13543216]
 [-0.10330185]
 [ 0.12264674]]
c bias before: [[-0.26751581]]
The mean c bias after update: [[-0.26944979]]
The output is: 
 [[0.49533587]
 [0.4535567 ]
 [0.57680651]
 [0.49075475]]
Total Loss: 0.4819454335414086
Average Loss: 0.12048635838535215

RUN:
  147
FeedForward


Z1 is:
 [[-2.49207226 -1.57639488]
 [-4.14472316 -2.84120691]
 [-2.71313965 -0.40696595]
 [-4.36579055 -1.67177797]]
H is:
 [[0.07641582 0.17130666]
 [0.01560059 0.05513763]
 [0.06220245 0.39963985]
 [0.01254523 0.15818727]]
The c is
 [[-0.26944979]]
Z2 is:
 [[-0.01957969]
 [-0.18892689]
 [ 0.31474044]
 [-0.03808077]]
Y^ -  the output is:
 [[0.49510523]
 [0.45290827]
 [0.57804192]
 [0.49048096]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49510523]
 [0.45290827]
 [0.57804192]
 [0.49048096]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12376445]
 [-0.13555969]
 [-0.10291957]
 [ 0.1225758 ]]
W2.T is
 [[-0.01094538  1.46349533]]
 D_Error times W2.T
 [[-0.00135465  0.18112869]
 [ 0.00148375 -0.19839097]
 [ 0.00112649 -0.1506223 ]
 [-0.00134164  0.17938911]]
self.H_D_Error_W2 is
 [[-9.56062671e-05  2.57131535e-02]
 [ 2.27862869e-05 -1.03356673e-02]
 [ 6.57120878e-05 -3.61384842e-02]
 [-1.66200038e-05  2.38881815e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[-9.56062671e-05  2.57131535e-02]
 [ 2.27862869e-05 -1.03356673e-02]
 [ 6.57120878e-05 -3.61384842e-02]
 [-1.66200038e-05  2.38881815e-02]]
X_H_D_Error_W2 is
 [[ 4.90920840e-05 -1.22503027e-02]
 [ 6.16628311e-06  1.35525142e-02]]
Using sum gradient........

W1 was :
 [[-0.22106739  1.16942893]
 [-1.6526509  -1.26481203]]
Updated W1 is: 
 [[-0.22111648  1.18167924]
 [-1.65265707 -1.27836454]]
W2 was :
 [[-0.01094538]
 [ 1.46349533]]
Updated W2 is: 
 [[-0.01342402]
 [ 1.47150893]]
The mean of the biases b gradient is:
 [-5.93197405e-06  7.81795883e-04]
The b biases before the update are:
 [[-2.49207226 -1.57639488]]
The new updated bs are:
 [[-2.49206633 -1.57717668]]
The bias c is: 
 [[ 0.12376445]
 [-0.13555969]
 [-0.10291957]
 [ 0.1225758 ]]
c bias before: [[-0.26944979]]
The mean c bias after update: [[-0.27141504]]
The output is: 
 [[0.49510523]
 [0.45290827]
 [0.57804192]
 [0.49048096]]
Total Loss: 0.4815293718297055
Average Loss: 0.12038234295742638

RUN:
  148
FeedForward


Z1 is:
 [[-2.49206633 -1.57717668]
 [-4.1447234  -2.85554122]
 [-2.71318281 -0.39549744]
 [-4.36583988 -1.67386198]]
H is:
 [[0.07641623 0.17119571]
 [0.01560058 0.05439559]
 [0.06219994 0.40239461]
 [0.01254461 0.15790996]]
The c is
 [[-0.27141504]]
Z2 is:
 [[-0.02052485]
 [-0.19158087]
 [ 0.31987724]
 [-0.03921753]]
Y^ -  the output is:
 [[0.49486897]
 [0.45225074]
 [0.57929434]
 [0.49019687]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49486897]
 [0.45225074]
 [0.57929434]
 [0.49019687]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12370421]
 [-0.13568845]
 [-0.10253119]
 [ 0.12250211]]
W2.T is
 [[-0.01342402  1.47150893]]
 D_Error times W2.T
 [[-0.00166061  0.18203185]
 [ 0.00182148 -0.19966677]
 [ 0.00137638 -0.15087556]
 [-0.00164447  0.18026295]]
self.H_D_Error_W2 is
 [[-1.17200366e-04  2.58280877e-02]
 [ 2.79729089e-05 -1.02702016e-02]
 [ 8.02858011e-05 -3.62815275e-02]
 [-2.03704638e-05  2.39703577e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[-1.17200366e-04  2.58280877e-02]
 [ 2.79729089e-05 -1.02702016e-02]
 [ 8.02858011e-05 -3.62815275e-02]
 [-2.03704638e-05  2.39703577e-02]]
X_H_D_Error_W2 is
 [[ 5.99153373e-05 -1.23111698e-02]
 [ 7.60244511e-06  1.37001560e-02]]
Using sum gradient........

W1 was :
 [[-0.22111648  1.18167924]
 [-1.65265707 -1.27836454]]
Updated W1 is: 
 [[-0.2211764   1.19399041]
 [-1.65266467 -1.2920647 ]]
W2 was :
 [[-0.01342402]
 [ 1.47150893]]
Updated W2 is: 
 [[-0.01591952]
 [ 1.47962585]]
The mean of the biases b gradient is:
 [-7.32803002e-06  8.11679067e-04]
The b biases before the update are:
 [[-2.49206633 -1.57717668]]
The new updated bs are:
 [[-2.492059   -1.57798836]]
The bias c is: 
 [[ 0.12370421]
 [-0.13568845]
 [-0.10253119]
 [ 0.12250211]]
c bias before: [[-0.27141504]]
The mean c bias after update: [[-0.27341171]]
The output is: 
 [[0.49486897]
 [0.45225074]
 [0.57929434]
 [0.49019687]]
Total Loss: 0.48110538970608674
Average Loss: 0.12027634742652168

RUN:
  149
FeedForward


Z1 is:
 [[-2.492059   -1.57798836]
 [-4.14472367 -2.87005305]
 [-2.7132354  -0.38399795]
 [-4.36590007 -1.67606265]]
H is:
 [[0.07641675 0.17108057]
 [0.01560058 0.05365396]
 [0.06219687 0.405163  ]
 [0.01254387 0.15761754]]
The c is
 [[-0.27341171]]
Z2 is:
 [[-0.021493  ]
 [-0.19427228]
 [ 0.3250878 ]
 [-0.04039641]]
Y^ -  the output is:
 [[0.49462696]
 [0.45158411]
 [0.58056368]
 [0.48990227]]


BackProp

input X is
 [[0 0]
 [0 1]
 [1 0]
 [1 1]]
input y is 
 [[0]
 [1]
 [1]
 [0]]
Y is
 [[0]
 [1]
 [1]
 [0]]
Y^ is
 [[0.49462696]
 [0.45158411]
 [0.58056368]
 [0.48990227]]
D_Error (Y^)(1-Y^)(Y^-Y) is:
 [[ 0.12364246]
 [-0.13581843]
 [-0.10213672]
 [ 0.12242562]]
W2.T is
 [[-0.01591952  1.47962585]]
 D_Error times W2.T
 [[-0.00196833  0.18294458]
 [ 0.00216216 -0.20096046]
 [ 0.00162597 -0.15112414]
 [-0.00194896  0.1811441 ]]
self.H_D_Error_W2 is
 [[-1.38919169e-04  2.59437381e-02]
 [ 3.32047880e-05 -1.02038099e-02]
 [ 9.48401085e-05 -3.64218154e-02]
 [-2.41407928e-05  2.40512733e-02]]
X transpose is
 [[0 0 1 1]
 [0 1 0 1]]
self.H_D_Error_W2 is
 [[-1.38919169e-04  2.59437381e-02]
 [ 3.32047880e-05 -1.02038099e-02]
 [ 9.48401085e-05 -3.64218154e-02]
 [-2.41407928e-05  2.40512733e-02]]
X_H_D_Error_W2 is
 [[ 7.06993157e-05 -1.23705422e-02]
 [ 9.06399525e-06  1.38474634e-02]]
Using sum gradient........

W1 was :
 [[-0.2211764   1.19399041]
 [-1.65266467 -1.2920647 ]]
Updated W1 is: 
 [[-0.2212471   1.20636095]
 [-1.65267374 -1.30591216]]
W2 was :
 [[-0.01591952]
 [ 1.47962585]]
Updated W2 is: 
 [[-0.01843213]
 [ 1.48784582]]
The mean of the biases b gradient is:
 [-8.75376644e-06  8.42346500e-04]
The b biases before the update are:
 [[-2.492059   -1.57798836]]
The new updated bs are:
 [[-2.49205025 -1.5788307 ]]
The bias c is: 
 [[ 0.12364246]
 [-0.13581843]
 [-0.10213672]
 [ 0.12242562]]
c bias before: [[-0.27341171]]
The mean c bias after update: [[-0.27543994]]
The output is: 
 [[0.49462696]
 [0.45158411]
 [0.58056368]
 [0.48990227]]
Total Loss: 0.4806734369057517
Average Loss: 0.12016835922643793</code></pre>
</div>
</div>
<section id="apply-filter-to-output" class="level2">
<h2 class="anchored" data-anchor-id="apply-filter-to-output">Apply filter to output</h2>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> FormatOutput(V):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    V[V <span class="op">&gt;=</span> <span class="fl">.5</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    V[V <span class="op">&lt;</span> <span class="fl">.5</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> V</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The final prediction is</span><span class="ch">\n</span><span class="st">"</span>, FormatOutput(output))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The final prediction is
 [[0.]
 [0.]
 [1.]
 [0.]]</code></pre>
</div>
</div>
</section>
</section>
<section id="output-and-vis" class="level1">
<h1>Output and Vis</h1>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Total Loss List:"</span>, TotalLoss) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Total Loss List: [0.506703560618369, 0.5058706911542734, 0.5051385196246267, 0.5044949977898465, 0.5039293668651307, 0.503432053591885, 0.5029945661635381, 0.5026093927316053, 0.502269904386843, 0.5019702638516874, 0.5017053406100518, 0.5014706328148621, 0.5012621960297272, 0.5010765786586594, 0.5009107637792787, 0.5007621170058001, 0.5006283399563055, 0.5005074288746452, 0.5003976379531488, 0.5002974469121719, 0.5002055324117833, 0.5001207428961465, 0.500042076499783, 0.49996866167506504, 0.499899740230613, 0.4998346524998631, 0.499772824387298, 0.49971375606632906, 0.49965701212736113, 0.4996022129970854, 0.4995490274705271, 0.4994971662158775, 0.4994463761287683, 0.49939643542752143, 0.49934714939414604, 0.49929834667762374, 0.4992498760864252, 0.499201603806395, 0.499153410988237, 0.49910519165595113, 0.49905685089382074, 0.4990083032750232, 0.4989594714997327, 0.498910285214771, 0.49886067999051975, 0.49881059643400155, 0.49875997941981604, 0.49870877742304287, 0.4986569419403276, 0.4986044269872053, 0.4985511886613039, 0.49849718476246274, 0.49844237446199446, 0.4983867180143682, 0.49833017650549527, 0.49827271163258374, 0.49821428551120756, 0.4981548605058267, 0.49809439908050823, 0.4980328636670349, 0.49797021654797674, 0.4979064197526319, 0.4978414349640269, 0.4977752234354218, 0.4977077459149743, 0.49763896257740864, 0.49756883296169513, 0.49749731591388596, 0.4974243695343738, 0.4973499511289465, 0.4972740171631032, 0.49719652321917285, 0.4971174239558502, 0.4970366730698219, 0.4969542232592066, 0.496870026188583, 0.4967840324554173, 0.49669619155773614, 0.4966064518629268, 0.49651476057756805, 0.4964210637182274, 0.49632530608317627, 0.49622743122499635, 0.4961273814240729, 0.4960250976629814, 0.495920519601792, 0.4958135855543325, 0.49570423246545997, 0.49559239588940607, 0.4954780099692737, 0.4953610074177706, 0.49524131949928185, 0.4951188760133873, 0.4949936052799454, 0.4948654341258729, 0.4947342878737586, 0.4946000903324589, 0.49446276378983517, 0.4943222290077972, 0.4941784052198275, 0.4940312101311709, 0.49388055992188085, 0.49372636925291935, 0.49356855127551824, 0.4934070176440128, 0.49324167853236783, 0.49307244265461847, 0.49289921728945635, 0.4927219083091911, 0.4925404202133244, 0.4923546561669745, 0.4921645180443871, 0.49196990647777206, 0.49177072091170293, 0.4915668596633087, 0.49135821998849005, 0.49114469815437833, 0.4909261895182531, 0.49070258861311966, 0.4904737892401387, 0.4902396845680848, 0.4900001672399961, 0.48975512948715183, 0.4895044632505061, 0.4892480603096701, 0.4889858124195171, 0.48871761145445536, 0.48844334956038116, 0.48816291931429434, 0.48787621389152336, 0.48758312724046937, 0.48728355426473846, 0.4869773910124947, 0.4866645348728194, 0.4863448847788206, 0.48601834141719147, 0.4856848074438689, 0.485344187705398, 0.4849963894655596, 0.4846413226367732, 0.48427890001573626, 0.48390903752271774, 0.4835316544438805, 0.48314667367595343, 0.4827540219725463, 0.48235363019134664, 0.4819454335414086, 0.4815293718297055, 0.48110538970608674, 0.4806734369057517]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>fig1 <span class="op">=</span> plt.figure()</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plt.axes()</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Total Loss"</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, Epochs, Epochs)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>ax.plot(x, TotalLoss)    </span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>fig2 <span class="op">=</span> plt.figure()</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plt.axes()</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Average Loss"</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, Epochs, Epochs)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>ax.plot(x, AvgLoss)  </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="XOR_NN_files/figure-html/cell-9-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="XOR_NN_files/figure-html/cell-9-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="extra-matrix-multiplication" class="level1">
<h1>Extra (matrix multiplication)</h1>
<p>This is just to show examples for how matrix mult works</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>M1<span class="op">=</span>np.array([[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">0</span>, <span class="dv">1</span>]])</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.mean(M1, axis<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.mean(M1, axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>M2 <span class="op">=</span> np.array( [[<span class="dv">3</span>, <span class="dv">4</span>], [<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>]])</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>V1 <span class="op">=</span> np.array([[<span class="dv">3</span>, <span class="dv">4</span>]])</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>N1 <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"M1 * M2 = </span><span class="ch">\n</span><span class="st">"</span>,M1<span class="op">*</span>M2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0.5 1.5]
[1.5 0.5]
M1 * M2 = 
 [[ 3  8]
 [ 0 -1]]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"M1 @ M2 = </span><span class="ch">\n</span><span class="st">"</span>,M1<span class="op">@</span>M2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>M1 @ M2 = 
 [[ 1  2]
 [-1 -1]]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"M1 * V1 = </span><span class="ch">\n</span><span class="st">"</span>,M1<span class="op">*</span>V1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>M1 * V1 = 
 [[3 8]
 [0 4]]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"M1 * N1 = </span><span class="ch">\n</span><span class="st">"</span>,M1<span class="op">*</span>N1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>M1 * N1 = 
 [[ 5 10]
 [ 0  5]]</code></pre>
</div>
</div>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>