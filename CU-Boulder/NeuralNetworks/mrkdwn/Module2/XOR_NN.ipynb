{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: XOR Problem using NN\n",
        "author: Professor Ami Gates\n",
        "format:\n",
        "  html:\n",
        "    code-fold: false\n",
        "execute:\n",
        "  output: true\n",
        "toc: true\n",
        "---"
      ],
      "id": "7d61eaea"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "XOR - BP\n"
      ],
      "id": "c8f36dde"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "id": "f1dca828",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setting up $X$ and $y$\n",
        "\n",
        "$X$ can be whatever YOU want it to be.....\n",
        "\n",
        "For XOR"
      ],
      "id": "8f6cab5c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"STARTING\\n.......\")\n",
        "X = np.array( [[0, 0], [0, 1], [1, 0] , [1, 1] ])\n",
        "print(\"X is:\\n\", X)   \n",
        "print(\"The shape of X is\\n\", X.shape)"
      ],
      "id": "24678183",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set $y$ to match with your choice of $X$\n",
        "\n",
        "These are just examples I was playing with...."
      ],
      "id": "0c5a52c5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y = np.array(( [[0], [1], [1], [0]] ))\n",
        "print(\"y is:\\n\", y) \n",
        "print(\"The shape of Y is\\n\", y.shape)"
      ],
      "id": "36544812",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Network Class object code\n",
        "\n",
        "**NOTE** \n",
        "\n",
        "You can set the parameters randomly or by hand\n",
        "Both options are below.\n"
      ],
      "id": "2491f0d3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class NeuralNetwork(object):\n",
        "    def __init__(self):\n",
        "        \n",
        "        self.InputNumColumns = 2  ## columns\n",
        "        self.OutputSize = 1\n",
        "        self.HiddenUnits = 2  ## one layer with h units\n",
        "        self.n = 4  ## number of training examples, n\n",
        "        \n",
        "        #Random W1\n",
        "        self.W1 = np.random.randn(self.InputNumColumns, self.HiddenUnits) # c by h  \n",
        "        ##self.W1=[[1, 1], [1, 1]] # If YOU want to control these values\n",
        "        print(\"W1 is\\n\", self.W1)\n",
        "        \n",
        "        self.W2 = np.random.randn(self.HiddenUnits, self.OutputSize) # h by o \n",
        "        #self.W2=np.array(( [[1], [1]] )) # If YOU want to control these values\n",
        "        print(\"W2 is:\\n\", self.W2)\n",
        "        \n",
        "        self.b = np.random.randn(self.OutputSize, self.HiddenUnits)\n",
        "        #self.b = [[1,1]] # If YOU want to control these values\n",
        "        print(\"The b's are:\\n\", self.b)\n",
        "        ## biases for layer 1\n",
        "        \n",
        "        self.c = np.random.randn(1, self.OutputSize)\n",
        "        #self.c =1 # If YOU want to control this value\n",
        "        print(\"The c is\\n\", self.c)\n",
        "        ## bias for last layer\n",
        "        \n",
        "        self.GA=False ## set this to True if you want the \n",
        "        ## average gradient over all examples rather than\n",
        "        ## the sum\n",
        "        \n",
        "    def FeedForward(self, X):\n",
        "        print(\"FeedForward\\n\\n\")\n",
        "        self.z = (np.dot(X, self.W1)) + self.b \n",
        "        #X is n by c   W1  is c by h -->  n by h\n",
        "        print(\"Z1 is:\\n\", self.z)        \n",
        "        self.h = self.Sigmoid(self.z) #activation function, shape: n by h\n",
        "        print(\"H is:\\n\", self.h)\n",
        "        print(\"The c is\\n\", self.c)\n",
        "        self.z2 = (np.dot(self.h, self.W2)) + self.c# n by h  @  h by o  -->  n by o  \n",
        "        print(\"Z2 is:\\n\", self.z2)\n",
        "        output = self.Sigmoid(self.z2)  \n",
        "        print(\"Y^ -  the output is:\\n\", output)\n",
        "        return output\n",
        "        \n",
        "    def Sigmoid(self, s, deriv=False):\n",
        "        if (deriv == True):\n",
        "            return s * (1 - s)\n",
        "        return 1/(1 + np.exp(-s))\n",
        "    \n",
        "    def BackProp(self, X, y, output):\n",
        "        print(\"\\n\\nBackProp\\n\")\n",
        "        print(\"input X is\\n\", X)\n",
        "        print(\"input y is \\n\", y)\n",
        "        \n",
        "        self.LR = 1\n",
        "        # Y^ - Y\n",
        "        self.output_error = output - y    \n",
        "        #print(\"Y^ - Y\\n\", self.output_error)\n",
        "        #print(\"SIG Y^\\n\", self.Sigmoid(output, deriv=True))\n",
        "        \n",
        "        ##(Y^ - Y)(Y^)(1-Y^)\n",
        "        print(\"Y is\\n\", y)\n",
        "        print(\"Y^ is\\n\", output)\n",
        "        self.output_delta = self.output_error * self.Sigmoid(output, deriv=True) \n",
        "        print(\"D_Error (Y^)(1-Y^)(Y^-Y) is:\\n\", self.output_delta)\n",
        "        \n",
        "        ##(Y^ - Y)(Y^)(1-Y^)(W2)\n",
        "        self.D_Error_W2 = self.output_delta.dot(self.W2.T) #  D_Error times W2\n",
        "        \n",
        "        print(\"W2.T is\\n\", self.W2.T)\n",
        "        print(\" D_Error times W2.T\\n\", self.D_Error_W2)\n",
        "        \n",
        "        ## (H)(1 - H) (Y^ - Y)(Y^)(1-Y^)(W2)\n",
        "        self.H_D_Error_W2 = self.D_Error_W2 * self.Sigmoid(self.h, deriv=True) \n",
        "        ## Note that * will multiply respective values together in each matrix\n",
        "        #print(\"Derivative sig H is:\\n\", self.Sigmoid(self.h, deriv=True))\n",
        "        print(\"self.H_D_Error_W2 is\\n\", self.H_D_Error_W2)\n",
        "        self.H_D_Error_W2_mean=np.mean(self.H_D_Error_W2, axis=0)\n",
        "        #print(\"self.H_D_Error_W2 mean is\\n\", self.H_D_Error_W2_mean)\n",
        "        #print(\"shape\",self.H_D_Error_W2_mean.shape )\n",
        "        \n",
        "        \n",
        "    \n",
        "        \n",
        "        ##  XT  (H)(1 - H) (Y^ - Y)(Y^)(1-Y^)(W2)\n",
        "        print(\"X transpose is\\n\", X.T)\n",
        "        print(\"self.H_D_Error_W2 is\\n\", self.H_D_Error_W2)\n",
        "        self.X_H_D_Error_W2 = X.T @ (self.H_D_Error_W2) ## this is dW1\n",
        "        print(\"X_H_D_Error_W2 is\\n\", self.X_H_D_Error_W2)\n",
        "        \n",
        "        ## (H)T (Y^ - Y)(Y^)(1-Y^)\n",
        "        self.h_output_delta = self.h.T.dot(self.output_delta) ## this is dW2\n",
        "        \n",
        "        #print(\"the gradient :\\n\", self.X_H_D_Error_W2)\n",
        "        #print(\"the gradient average:\\n\", self.X_H_D_Error_W2/self.n)\n",
        "        \n",
        "        if(self.GA==True):\n",
        "            print(\"Using average gradient........\\n\")\n",
        "            self.W1 = self.W1 - self.LR*(self.X_H_D_Error_W2/self.n)\n",
        "            self.W2 = self.W2 - self.LR*(self.h_output_delta/self.n) ## average the gradients\n",
        "        # #print(\"New W1: \\n\", self.W1)\n",
        "        else: \n",
        "            print(\"Using sum gradient........\\n\")\n",
        "            print(\"W1 was :\\n\", self.W1)\n",
        "            self.W1 = self.W1 - self.LR*(self.X_H_D_Error_W2) # c by h  adjusting first set (input -> hidden) weights\n",
        "            print(\"Updated W1 is: \\n\", self.W1)\n",
        "            print(\"W2 was :\\n\", self.W2)\n",
        "            self.W2 = self.W2 - self.LR*(self.h_output_delta) # adjusting second set (hidden -> output) weights\n",
        "            print(\"Updated W2 is: \\n\", self.W2)\n",
        "        \n",
        "        \n",
        "        \n",
        "        #print(\"The W1 gradient is: \\n\", self.X_H_D_Error_W2)\n",
        "        #print(\"The W1 gradient average is: \\n\", self.X_H_D_Error_W2/self.n)\n",
        "        #print(\"The W2 gradient  is: \\n\", self.h_output_delta)\n",
        "        #print(\"The W2 gradient average is: \\n\", self.h_output_delta/self.n)\n",
        "        print(\"The mean of the biases b gradient is:\\n\",self.H_D_Error_W2_mean )\n",
        "        print(\"The b biases before the update are:\\n\", self.b)\n",
        "        self.b = self.b  - self.LR*self.H_D_Error_W2_mean\n",
        "        print(\"The new updated bs are:\\n\", self.b)\n",
        "        \n",
        "        print(\"The bias c is: \\n\", self.output_delta)\n",
        "        print(\"c bias before:\", self.c)\n",
        "        self.c = self.c - np.mean(self.output_delta)\n",
        "        print(\"The mean c bias after update:\", self.c)\n",
        "       \n",
        "        \n",
        "        ################################################################\n",
        "        \n",
        "    def TrainNetwork(self, X, y):\n",
        "        output = self.FeedForward(X)\n",
        "        self.BackProp(X, y, output)\n",
        "        return output"
      ],
      "id": "7c2b131c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Use run for ____ epochs\n"
      ],
      "id": "2006b24c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "MyNN = NeuralNetwork()\n",
        "\n",
        "TotalLoss=[]\n",
        "AvgLoss=[]\n",
        "Epochs=800\n",
        "for i in range(Epochs): \n",
        "    print(\"\\nRUN:\\n \", i)\n",
        "    output=MyNN.TrainNetwork(X, y)\n",
        "   \n",
        "    #print(\"The y is ...\\n\", y)\n",
        "    print(\"The output is: \\n\", output)\n",
        "    print(\"Total Loss:\", .5*(np.sum(np.square(output-y))))\n",
        "    TotalLoss.append( .5*(np.sum(np.square(output-y))))\n",
        "    \n",
        "    print(\"Average Loss:\", .5*(np.mean(np.square((output-y)))))\n",
        "    AvgLoss.append(.5*(np.mean(np.square((output-y)))))"
      ],
      "id": "5f78e96a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Apply filter to output"
      ],
      "id": "a10070fe"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def FormatOutput(V):\n",
        "    V[V >= .5] = 1\n",
        "    V[V < .5] = 0\n",
        "    return V\n",
        "\n",
        "print(\"The final prediction is\\n\", FormatOutput(output))"
      ],
      "id": "6b6f6e6b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Output and Vis   "
      ],
      "id": "50d13c74"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Total Loss List:\", TotalLoss) "
      ],
      "id": "779d360d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig1 = plt.figure()\n",
        "ax = plt.axes()\n",
        "plt.title(\"Total Loss\")\n",
        "x = np.linspace(0, Epochs, Epochs)\n",
        "ax.plot(x, TotalLoss)    \n",
        "\n",
        "fig2 = plt.figure()\n",
        "ax = plt.axes()\n",
        "plt.title(\"Average Loss\")\n",
        "x = np.linspace(0, Epochs, Epochs)\n",
        "ax.plot(x, AvgLoss)  "
      ],
      "id": "4d24ce9a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Extra (matrix multiplication)\n",
        "\n",
        "This is just to show examples for how matrix mult works\n"
      ],
      "id": "99d3789f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "M1=np.array([[1, 2], [0, 1]])\n",
        "print(np.mean(M1, axis=0))\n",
        "print(np.mean(M1, axis=1))\n",
        "M2 = np.array( [[3, 4], [-1, -1]])\n",
        "V1 = np.array([[3, 4]])\n",
        "N1 = 5\n",
        "\n",
        "print(\"M1 * M2 = \\n\",M1*M2)"
      ],
      "id": "4f1b39a9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"M1 @ M2 = \\n\",M1@M2)"
      ],
      "id": "816dacf0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"M1 * V1 = \\n\",M1*V1)"
      ],
      "id": "7d1a5ed1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\n",
        "print(\"M1 * N1 = \\n\",M1*N1)\n",
        "```"
      ],
      "id": "f1e7cc14"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}