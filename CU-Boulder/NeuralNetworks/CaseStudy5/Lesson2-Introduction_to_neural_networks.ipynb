{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to neural networks\n",
    "\n",
    "## Basic building block of deep learning : - \n",
    "### Perceptron\n",
    "Perceptron is the building block of neural networks and deep learning is lot of neural networks stacked together. So before diving into neural networks lets take some time to study perceptrons. \n",
    "<img src=\"./images/perceptron.png\" style=\"width:600px;height:400px;\">\n",
    "\n",
    "Perceptron takes an input, aggregates it (weighted sum) and returns 1 only if the aggregated sum is more than some threshold else returns 0. A way you can think about the perceptron is that it's a device that makes decisions by weighing up evidence. By varying the weights and the threshold, we can get different models of decision-making.\n",
    "<br>\n",
    "Lets implement one function to give you more insight to perceptron. \n",
    "### AND Function Using A Perceptron : - \n",
    "We know the truth table for AND gate. It takes two binary inputs and gives their product as output. Let us think of weights w1 and w2 and threshold(bias) which will give the correct output.\n",
    "<img src=\"./images/andpreceptron.png\" style=\"width:600px;height:300px;\">\n",
    "\n",
    "W1 = 1.0<br>\n",
    "W2 = 1.0<br>\n",
    "threshold = 2.0<br>\n",
    "Using these values for each value\n",
    "<img src=\"./images/andsolution.png\" style=\"width:800px;height:300px;\">\n",
    "\n",
    "### Sigmoid neuron\n",
    "\n",
    "The issue with perceptron is they are very harsh step functions giving output only as 0 or 1 because of this small changes in weights can make a large difference in output, on the other hand in sigmoid neuron small change in weights will make only small change in output. There is a slight difference between the sigmoid neuron and perceptron, instead of using thresholds we use sigmoid function to get output from the weighted sum. Sigmoid function and its output is given by,\n",
    "<img src=\"./images/sigmoid.png\" style=\"width:800px;height:300px;\">\n",
    "Sigmoid function and its graph\n",
    "So we get the output as,\n",
    "\n",
    "<img src=\"./images/sigmoid_output.png\" style=\"width:800px;height:300px;\">\n",
    "\n",
    "## What is neural network?\n",
    "A neural network also known as artificial neural network(ANN) is the basic building block of deep learning. It consists of layers of sigmoid neuron stacked together to form a bigger architecture. \n",
    "<img src=\"./images/neuralnetwork.png\" style=\"width:800px;height:300px;\">\n",
    "Each circle in the above image is a sigmoid neuron. It consists of 3 types of layers, an input layer, an output layer, and hidden layers. All the previous layers are fully connected with the next layer as can be seen in the image so it is sometimes also referred to as the fully connected neural network. Each neuron has its own weight values. The first layer(input) takes the independent variable of data as input. Output layer predicts the class. The number of hidden layers and number of neurons in hidden layers is not fixed and you can choose any number and tinker to get the best results.\n",
    "\n",
    "## How neural networks learn?\n",
    "Neural networks are just the weighted sum of the inputs. So the learning of neural networks is based on updating these weights. We need a method to update the weights. It is based on how good the neural network is performing. Performance of the neural network means how good are the predictions based on the actual labels that are to be predicted. The value at the output layer is calculated by crossing through the neural network and finding the value of each neuron. This process of crossing through the neural network is called forward propagation. \n",
    "<img src=\"./images/forwardprop.png\" style=\"width:800px;height:300px;\">\n",
    "For measuring the performance of a neural network we introduce loss function which calculates how bad the neural network is. Loss function depends on the task we are trying to solve, there are plenty of loss functions but we will discuss two most used ones. \n",
    "\n",
    "1. Cross-entropy loss:- It is used for the classification problem and calculates some value using a function based on the true input label and the predicted output label. The function is given by, \n",
    "\n",
    "    cross entropy loss = −(y*log(p)+(1−y)*log(1−p))\n",
    "    Here y is true label and p is predicted label.\n",
    "2. Mean Squared Error(MSE):- It is used in the regression problem and calculates the distance between the true value and predicted value. It is given as,\n",
    "<img src=\"./images/mse.png\" style=\"width:800px;height:300px;\">\n",
    "Once the loss is calculated we need a method to change the weights of the neural network with respect to the calculated loss. We backpropagate through the neural network and update the weights. based on the relative contribution that each neuron has contributed to the original output. This process is repeated, layer by layer, until all the neurons in the network have received a loss signal that describes their relative contribution to the total loss. The change to be made in weights is calculated using gradient descent. Gradient descent is an important concept to understand the neural network so before going forward please watch this <a href=\"https://www.youtube.com/watch?v=IHZwWFHWa-w&t=682s\">video</a>. The complete learning process of neural network is given as,\n",
    "<img src=\"./images/nngradient_descent.png\" style=\"width:800px;height:300px;\">\n",
    "<br>\n",
    "In the later chapters, we will discuss how to improve the neural networks using various techniques and also discuss some better optimization algorithms such as adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn(2, 3) # a.shape = (2, 3)\n",
    "b = np.random.randn(2, 1) # b.shape = (2, 1)\n",
    "c = a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn(3, 3)\n",
    "b = np.random.randn(3, 1)\n",
    "c = a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.04032934, -0.10979807,  0.12888539],\n",
       "       [ 0.00671015, -0.07563936, -0.06385819],\n",
       "       [-0.28658713, -0.3994585 , -1.39003048]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
