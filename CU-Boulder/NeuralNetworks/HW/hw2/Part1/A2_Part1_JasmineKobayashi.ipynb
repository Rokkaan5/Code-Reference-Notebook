{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Module 2 Assignment - Part 1: One Layer NN with One Output\"\n",
    "author: \"Jasmine Kobayashi\"\n",
    "format:\n",
    "    html:\n",
    "        code-fold: false\n",
    "execute:\n",
    "    output: true\n",
    "    warning: false\n",
    "toc: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module 2 Assignment - Part 1: One Layer NN with One Output\n",
    "---\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "This assignment has a two large parts with many sub-parts. Each part will enable you to practice with a neural networks and to practice for Exam 2.\n",
    "\n",
    "This Assignment will TAKE TIME so use all of the time you have to work on it. Please avoid waiting until the last few days as this will not be enough time. You need weeks - not days :)\n",
    "\n",
    "Be sure to show all work and (when requested) to code using Python (and no NN/TF/Keras packages). Do not worry, we will use packages in coming modules but first it is best to learn about what is going on inside the model. \n",
    "\n",
    "You may use examples and code that I have shared as a reference. Using my code will not be considered cheating although I strongly recommend that you write as much (if not all) of your own code so that you learn the concepts more robustly. \n",
    "\n",
    "This is not a team assignment. Please work and code alone. You may discuss concepts, but you cannot share code or work with others. Papers that look too similar with split the grade (first offense) and can suffer less pleasant outcomes for further offenses. Keep it simple and smart - do your own work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NN_architecture](network.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Fill out this network by hand and create all the needed matrices, etc. Show all multiplications and shapes.\n",
    "\n",
    "In other words, illustrate the $X$, $y$, $W1$, $B$, $Z_1$, $H_1$, $W2$, $Z_2$, $C$, $\\hat{y}$, $\\hat{y}-y$, and $L$. Assume that $H_1$ uses the Sigmoid Function. \n",
    "\n",
    "As you do this - ask yourself:\n",
    "\n",
    "- How many columns does $X$ have?\n",
    "- How many hidden layers are there in this architecture?\n",
    "- How many hidden units are there in the hidden layer?\n",
    "- How many outputs are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JK's answer:\n",
    "\n",
    "### Input Matrix $X$\n",
    "\n",
    "\\begin{equation} \n",
    "X = \n",
    "\n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{12} & x_{13} \\\\\n",
    "\\vdots  & \\vdots  & \\vdots  \\\\\n",
    "x_{n1} & x_{n2} & x_{n3} \\\\\n",
    "\\end{bmatrix}\n",
    "\\nonumber\n",
    "\\end{equation}\n",
    "\n",
    "Where $n$ is any integer.\n",
    "\n",
    "In any case, the point being: this architecture implies $X$ has 3 columns (but gives no restriction/information about the number of rows of $X$.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Targets $y$ \n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{y} = \n",
    "\\begin{bmatrix} \n",
    "y_1 \\\\\n",
    "\\vdots \\\\\n",
    "y_n\n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight matrix $W1$\n",
    "\n",
    "The weight matrix connecting inputs $X$ with the first hidden layer (or in this case, the only hidden layer).\n",
    "\n",
    "I will use both notations $W1 = W^{(1)}$, to be clear and be able to distinct weights $w$ in $W1$ and $W2$\n",
    "\n",
    "\\begin{align*}\n",
    "W1 = W^{(1)} = \n",
    "\\begin{bmatrix}\n",
    "w_{11} & w_{12} & w_{13} & w_{14} \\\\\n",
    "w_{21} & w_{22} & w_{23} & w_{24} \\\\\n",
    "w_{31} & w_{32} & w_{33} & w_{34}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "When creating the matrix to match the subscript notation so that the first number in the subscript of $w$ is associated with the connected $x$ value and the second number is associated with the hidden unit $h$ (i.e. $w_{12}$ is the weight connecting $x_1$ to $h_2$), then the $W^{(1)}$ matrix is an $m \\times p$ matrix, where $m$ is the number of columns of $X$ and $p$ is the number of units in the first hidden layer (which is also the number of rows of $H_1$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias $B$\n",
    "\n",
    "The \"bias\". Column vector with length that matches the number of rows in $X$.\n",
    "\n",
    "$$\\mathbf{B} = [b_1,\\cdots, b_n]^\\top$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $Z_1$\n",
    "\n",
    "\\begin{align*} \n",
    "Z_1 & =  X\\cdot W_1 + B\n",
    "\n",
    "\\\\\n",
    "& = \n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{12} & x_{13} \\\\\n",
    "\\vdots  & \\vdots  & \\vdots  \\\\\n",
    "x_{n1} & x_{n2} & x_{n3} \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix}\n",
    "w_{11} & w_{12} & w_{13} & w_{14} \\\\\n",
    "w_{21} & w_{22} & w_{23} & w_{24} \\\\\n",
    "w_{31} & w_{32} & w_{33} & w_{34}\n",
    "\\end{bmatrix}\n",
    "\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\\n",
    "\\vdots \\\\\n",
    "b_n\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\\\\n",
    "\n",
    "& = \n",
    "\\begin{bmatrix}\n",
    "z_{11} & z_{12} & z_{13} & z_{14} \\\\\n",
    "z_{21} & z_{22} & z_{23} & z_{24} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "z_{n1} & z_{n2} & z_{n3} & z_{n4}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Where we have:\n",
    "\n",
    "\\begin{align*}\n",
    "& z_{11} = x_{11}w_{11} + x_{12}w_{21} + x_{13}w_{31} + b_1 & & \\\\\n",
    "& z_{12} = x_{11}w_{12} + x_{12}w_{22} + x_{13}w_{32} + b_1 & &\\\\\n",
    "\\end{align*}\n",
    "\n",
    "$$\\vdots$$\n",
    "\n",
    "\\begin{align*}\n",
    "z_{n4} = x_{n1}w_{14} + x_{n2}w_{24} + x_{n3}w_{34} + b_n \n",
    "\\end{align*}\n",
    "\n",
    "In other words, I guess any $Z$ element can be summarized as,\n",
    "\n",
    "$$z_{j,k} = \\sum_i^3 x_{j,i}w_{i,k} + b_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $H_1$\n",
    "\n",
    "We're assuming the first hidden layer uses the Simoid activation function. Thus $S(z)$ is the Sigmoid of $z$. \n",
    "\n",
    "(Very) Generically we have,\n",
    "$$h = S(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "Thus the matrix is as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "H_1 & = \n",
    "\\begin{bmatrix}\n",
    "S(z_{11}) & S(z_{12}) & S(z_{13}) & S(z_{14}) \\\\\n",
    "\\vdots    &   \\vdots  &   \\vdots  &   \\vdots  \\\\\n",
    "S(z_{n1}) & S(z_{n2}) & S(z_{n3}) & S(z_{n4}) \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\\\\n",
    "\n",
    "& = \n",
    "\\begin{bmatrix}\n",
    "h_{11} & h_{12} & h_{13} & h_{14} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "h_{n1} & h_{n2} & h_{n3} & h_{n4} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "The number of columns in $H_1$ is the number of hidden units in the hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $W2$\n",
    "\n",
    "The weight matrix connecting the hidden layer and the last node.\n",
    "\n",
    "Again, I will use both notations $W1 = W^{(1)}$ and $W2 = W^{(2)}$, to be able to distinguish weights from each weight matrix.\n",
    "\n",
    "\\begin{align*}\n",
    "W2 = W^{(2)} =\n",
    "\\begin{bmatrix}\n",
    "w_1^{(2)} \\\\[6pt]\n",
    "w_2^{(2)} \\\\[6pt]\n",
    "w_3^{(2)} \\\\[6pt]\n",
    "w_4^{(2)} \\\\[6pt]\n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $C$ \n",
    "\n",
    "I believe this is the \"bias\" associated to calculate $Z_2$. So much like $B$ it's going to be a column vector with length that matches the number of rows in $X$.\n",
    "\n",
    "\\begin{equation*}\n",
    "C = \n",
    "\\begin{bmatrix}\n",
    "c_1 \\\\\n",
    "\\vdots \\\\\n",
    "c_n\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $Z_2$\n",
    "\n",
    "\\begin{align*}\n",
    "Z_2 &= H_1 W^{(2)} + C \\\\\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "h_{11} & h_{12} & h_{13} & h_{14} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "h_{n1} & h_{n2} & h_{n3} & h_{n4} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "w_1^{(2)} \\\\[6pt]\n",
    "w_2^{(2)} \\\\[6pt]\n",
    "w_3^{(2)} \\\\[6pt]\n",
    "w_4^{(2)} \\\\[6pt]\n",
    "\\end{bmatrix}\n",
    "\n",
    "+\n",
    "\n",
    "\\begin{bmatrix}\n",
    "c_1 \\\\\n",
    "\\vdots \\\\\n",
    "c_n\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "h_{11}w_1^{(2)} + h_{12}w_2^{(2)} + h_{13}w_3^{(2)} + h_{14}w_4^{(2)} + c_1\\\\\n",
    "\\vdots \\\\\n",
    "h_{n1}w_1^{(2)} + h_{n2}w_2^{(2)} + h_{n3}w_3^{(2)} + h_{n4}w_4^{(2)} + c_n\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "z_1^{(2)} \\\\\n",
    "\\vdots \\\\\n",
    "z_n^{(2)}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\mathbf{\\hat{y}}$\n",
    "\n",
    "In terms of notation,\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{\\hat{y}} = \n",
    "\\begin{bmatrix}\n",
    "\\hat{y}_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\hat{y}_n \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "(Should have the same shape as $\\mathbf{y}$)\n",
    "\n",
    "I'm guessing I have to do sigmoid again to get probability for binary labels?\n",
    "\n",
    "\\begin{align*}\n",
    "\\Rightarrow\n",
    "\\mathbf{\\hat{y}} = \n",
    "\\begin{bmatrix}\n",
    "\\hat{y}_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\hat{y}_n \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "S(z_1^{(2)}) \\\\\n",
    "\\vdots \\\\\n",
    "S(z_n^{(2)})\n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\mathbf{\\hat{y}} - \\mathbf{y}$\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{\\hat{y}} - \\mathbf{y} = \n",
    "\\begin{bmatrix}\n",
    "\\hat{y}_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\hat{y}_n \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "-\n",
    "\n",
    "\\begin{bmatrix} \n",
    "y_1 \\\\\n",
    "\\vdots \\\\\n",
    "y_n\n",
    "\\end{bmatrix}\n",
    "\n",
    "=\n",
    "\n",
    "\\begin{bmatrix}\n",
    "\\hat{y}_1 - y_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\hat{y}_n - y_n \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss $L$\n",
    "\n",
    "There's really no limit of what Loss function $L$ to use. (A NN architecture doesn't define this, the NN model builder does :)) It could be LCE or a simple one.\n",
    "\n",
    "For the purpose of this assignment I'll use a simple MSE (that's supposedly pretty commonly used as well), since it uses $\\mathbf{\\hat{y}} - \\mathbf{y}$.\n",
    "\n",
    "\\begin{align*}\n",
    "Loss = L &= \\frac{1}{2} (\\mathbf{y} - \\mathbf{\\hat{y}})^2 \\\\\n",
    "&= \\frac{1}{2}\n",
    "\\begin{bmatrix}\n",
    "\\hat{y}_1 - y_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\hat{y}_n - y_n\n",
    "\\end{bmatrix}^2\n",
    "\\\\\n",
    "&=\\frac{1}{2}\n",
    "\\begin{bmatrix}\n",
    "\\hat{y}_1 - y_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\hat{y}_n - y_n\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\hat{y}_1 - y_1, & \\cdots, & \\hat{y}_1 - y_1\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "&= \\frac{1}{2} \\sum_i^n (\\hat{y}_i - y_i)^2\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Use Python to code the **Feed-Forward** portion of this NN. \n",
    "\n",
    "Print out $X$, $y$, $W1$, $B$, $Z_1$, $H_1$, $W_2$, $Z_2$, $C$, $\\hat{y}$, $\\hat{y}-y$, and $L$.\n",
    "\n",
    "**Assumptions and Notes**:\n",
    "\n",
    "(a) Create your own small labeled dataset. Have the right number of columns and have a column called \"LABEL\" which is 0 or 1. \n",
    "\n",
    "(b) Read in the data and create $X$ and $y$\n",
    "\n",
    "(c) YOU create the dataset so that everyone will have a unique dataset. Have between 5 and 7 rows (no more), assure that your labels \"make sense\" for the data, and have a balance of labels (so nearish 50% $0$ and 50% $1$). You want your NN to \"work\" so again make sure your labeled data has a predictable pattern. *INVENT* the data yourself - type it into a `.csv` file - save the file as **A2_Data_YourName.csv**. You will submit this dataset with your Assignment submission.\n",
    "\n",
    "(d) Create $H_1$ using Sigmoid. You will need to **code** a function for the Sigmoid (and its derivative) into your code. Do this using `def`.\n",
    "\n",
    "(e) Use $L = \\frac{1}{n} \\displaystyle \\sum^n (\\hat{y} - y)^2$ as the Loss Function when $n$ is the number of rows in your dataset. Do not \"hard code\". The TAs or I should be able to use your code to read ANY dataset that has 3 columns of data, one label column called \"LABEL\" (of 0 or 1), and any number of rows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***JK answer:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.preprocessing import LabelEncoder,MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix  \n",
    "import seaborn as sns\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class OneLayer_OneOutput_FF():\n",
    "    def __init__(self,\n",
    "                 data_filename = \"A2_Data_JasmineKobayashi.csv\", \n",
    "                 scale_data = True,\n",
    "                 verbose = True):\n",
    "        # Read Data\n",
    "        self.df = pd.read_csv(data_filename)\n",
    "        \n",
    "        # create X and y, assuming data has column \"LABEL\" as column with labels\n",
    "        ## X matrix\n",
    "        if scale_data:\n",
    "            mm_scaler = MinMaxScaler()\n",
    "            X = mm_scaler.fit_transform(self.df.drop(columns=\"LABEL\",axis=1))\n",
    "        else:\n",
    "            X = np.array(self.df.drop(columns=['LABEL'],axis=1))\n",
    "        self.X = X\n",
    "\n",
    "        ## y\n",
    "        self.y = np.array(self.df[['LABEL']])\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Shape of X:\", self.X.shape)\n",
    "            print(\"X matrix: \\n\", self.X)\n",
    "            print(\"Shape of y:\", self.y.shape)\n",
    "            print(\"y vector: \\n\", self.y)\n",
    "\n",
    "    def MSE_loss(self,y_hat,y):\n",
    "        # L = mean((y_hat - y)^2)\n",
    "        return np.mean((y_hat - y)**2)\n",
    "        \n",
    "    def lin_eq(self,X,W,B):\n",
    "        assert X.shape[1] == W.shape[0], \"Linear eq: z = X @ W + B. Shape of X: {}; Shape of W: {}\".format(X.shape,W.shape)\n",
    "        assert B.shape[0] == X.shape[0], \"Linear eq: z = X @ W + B. Length of B should match X rows; Length B = {}; Shape of X = {}\".format(B.shape[0],X.shape)\n",
    "        return X @ W + B\n",
    "    \n",
    "    def Sigmoid(self,z,\n",
    "                derivative = False):\n",
    "        if derivative:\n",
    "            return self.Sigmoid(z) * (1 - self.Sigmoid(z)) # dS/dz = S(z)(1-S(z))\n",
    "        # Sigmoid\n",
    "        return 1/(1 + math.e**(-z)) # S(z) = 1/(1 + e^(-z))\n",
    "    \n",
    "    def FeedForward(self,\n",
    "                    hidden_units= 4,\n",
    "                    randomize_parameters = False,\n",
    "                    activation = \"sigmoid\",\n",
    "                    verbose = True):\n",
    "        # Shape of W1\n",
    "        w1_ncol = hidden_units        # Number of W1 cols should be number of hidden units in H1\n",
    "        w1_nrow = self.X.shape[1]     # Number of W1 rows should be number of X columns (dim of X)\n",
    "        # Shape of B (column vector)\n",
    "        b_length = self.X.shape[0]    # length of X rows\n",
    "        # Shape of W2:\n",
    "        w2_ncol = 1                   # Number of W2 cols should be number of outputs (in this case, only one output)\n",
    "        w2_nrow = hidden_units        # Number of W2 rows should be number of hidden units in H1\n",
    "        # Shape of C (column vector)\n",
    "        c_length = self.X.shape[0]    # length of X rows\n",
    "        \n",
    "        if randomize_parameters: \n",
    "            # TODO: Finish potential code of randomized parameters\n",
    "            # # bounds of randomized weights\n",
    "            # bound = 1/np.sqrt(w1_nrow)   # I found online that supposedly this is a standard range for randomized weights\n",
    "            # rand_w = [np.random.uniform(-bound,bound) for i in range(w1_ncol)]\n",
    "            # self.W1 = np.array([rand_w]*w1_nrow)\n",
    "\n",
    "            # self.W2 = np.array([]*w2_nrow)\n",
    "            # print('All parameters were randomized')\n",
    "            pass\n",
    "        else:   # These are the parameters defined by the question 2 in Assignment2\n",
    "            self.W1 = np.array([[1]*w1_ncol]*w1_nrow) # otherwise, all weights of W1 = 1\n",
    "            self.B = np.array([[0]]*b_length)         # otherwise, all bias (B) = 0\n",
    "            self.W2 = np.array([[2]*w2_ncol]*w2_nrow) # otherwise, all weights of W2 = 2\n",
    "            self.C = np.array([[0]]*c_length)\n",
    "            print(\"Parameters built for simple calculation (not randomized)\")\n",
    "        \n",
    "        if verbose:\n",
    "            # print(\"# of X cols (should be # of W1 rows):\", self.X.shape[1])\n",
    "            # print(\"# of hidden units (should be # of W1 cols):\", hidden_units)\n",
    "            print(\"W1 has shape:\", self.W1.shape)\n",
    "            print(\"W1 matrix: \\n\", self.W1)\n",
    "            print(\"Shape of B:\", self.B.shape)\n",
    "            print(\"B (bias vector for Z1): \\n\", self.B)\n",
    "            print(\"Shape of W2:\",self.W2.shape)\n",
    "            print(\"W2 matrix: \\n\", self.W2)\n",
    "            print(\"Shape of C :\", self.C.shape)\n",
    "            print(\"C (bias vector for Z2): \\n\",self.C)\n",
    "\n",
    "        # Create H1\n",
    "        # Z1 = X @ W1 + B\n",
    "        self.Z1 = self.lin_eq(self.X,self.W1,self.B)\n",
    "        if verbose:\n",
    "            print(\"Shape of Z1:\",self.Z1.shape)\n",
    "            print(\"Z1 matrix: \\n\",self.Z1)\n",
    "        self.H1 = self.Sigmoid(self.Z1)\n",
    "        # TODO: Add flexibility with other activation functions\n",
    "        if verbose: \n",
    "            print(\"Activation function:\",activation)\n",
    "            print(\"H1 matrix: \\n\", self.H1)\n",
    "\n",
    "        # Z2 and y-hat\n",
    "        # Z2 = H1 @ W2 + C\n",
    "        self.Z2 = self.lin_eq(self.H1,self.W2,self.C)\n",
    "        self.y_hat = self.Sigmoid(self.Z2)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Shape of Z2:\", self.Z2.shape)\n",
    "            print(\"Z2 matrix: \\n\", self.Z2)\n",
    "            print(\"y_hat:\\n\",self.y_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (6, 3)\n",
      "X matrix: \n",
      " [[0.86363636 0.         0.07894737]\n",
      " [0.09090909 0.4        0.81578947]\n",
      " [0.         0.8        1.        ]\n",
      " [1.         0.2        0.05263158]\n",
      " [0.90909091 1.         0.        ]\n",
      " [0.04545455 0.6        0.94736842]]\n",
      "Shape of y: (6, 1)\n",
      "y vector: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "Parameters built for simple calculation (not randomized)\n",
      "W1 has shape: (3, 4)\n",
      "W1 matrix: \n",
      " [[1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]]\n",
      "Shape of B: (6, 1)\n",
      "B (bias vector for Z1): \n",
      " [[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "Shape of W2: (4, 1)\n",
      "W2 matrix: \n",
      " [[2]\n",
      " [2]\n",
      " [2]\n",
      " [2]]\n",
      "Shape of C : (6, 1)\n",
      "C (bias vector for Z2): \n",
      " [[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "Shape of Z1: (6, 4)\n",
      "Z1 matrix: \n",
      " [[0.94258373 0.94258373 0.94258373 0.94258373]\n",
      " [1.30669856 1.30669856 1.30669856 1.30669856]\n",
      " [1.8        1.8        1.8        1.8       ]\n",
      " [1.25263158 1.25263158 1.25263158 1.25263158]\n",
      " [1.90909091 1.90909091 1.90909091 1.90909091]\n",
      " [1.59282297 1.59282297 1.59282297 1.59282297]]\n",
      "Activation function: sigmoid\n",
      "H1 matrix: \n",
      " [[0.71962126 0.71962126 0.71962126 0.71962126]\n",
      " [0.78696018 0.78696018 0.78696018 0.78696018]\n",
      " [0.85814894 0.85814894 0.85814894 0.85814894]\n",
      " [0.77775507 0.77775507 0.77775507 0.77775507]\n",
      " [0.87091698 0.87091698 0.87091698 0.87091698]\n",
      " [0.8310129  0.8310129  0.8310129  0.8310129 ]]\n",
      "Shape of Z2: (6, 1)\n",
      "Z2 matrix: \n",
      " [[5.75697011]\n",
      " [6.29568146]\n",
      " [6.86519148]\n",
      " [6.22204054]\n",
      " [6.96733585]\n",
      " [6.64810323]]\n",
      "y_hat:\n",
      " [[0.99684928]\n",
      " [0.99815914]\n",
      " [0.99895761]\n",
      " [0.99801874]\n",
      " [0.99905873]\n",
      " [0.9987052 ]]\n",
      "MSE Loss: 0.4979790666878168\n"
     ]
    }
   ],
   "source": [
    "NN = OneLayer_OneOutput_FF()\n",
    "NN.FeedForward()\n",
    "print(\"MSE Loss:\",NN.MSE_loss(y_hat=NN.y_hat,y=NN.y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Compare *by-hand* and code\n",
    "\n",
    "Here, will compare your by-hand work in (1) with your code in (2). \n",
    "\n",
    "To do this, use your dataset values (just the first row) and your label (for the first row) to FF *by hand on paper* through the network. Show numeric and clear results for $X$, $y$, $W_1$, $B$, $Z_1$, $H_1$, $W_2$, $Z_2$, $C$, $\\hat{y}$, $\\hat{y}-y$, and $L$.\n",
    "\n",
    "Then, using your code and the same $X$ (first row) and $y$ (first label), print out $X$, $y$, $W_1$, $B$, $Z_1$, $H_1$, $W_2$, $Z_2$, $C$, $\\hat{y}$, $\\hat{y}-y$, and $L$.\n",
    "\n",
    "Both the \"by hand on paper\" and the \"by code\" results should be the same. \n",
    "\n",
    "If they are not, find and fix your bugs until they are the same.\n",
    "\n",
    "**Notes**: $\\mathbf{W_1}$, $\\mathbf{W_2}$, $\\mathbf{B}$ (which is $b_1$, $b_2$, $b_3$, $b_4$), and $C$ (which is $c_1$) would normally be randomly initialized. However, to make your code and \"by hand\" comparison easier, let all the values in $W_1$ be $1$, let all the values in $W_2$ be 2, let all the $\\mathbf{B}$ values be $0$, and let $c$ be $-1$. You will code this in and will use this by hand so that your code and by-hand results will be the same. \n",
    "\n",
    "Print out all key matrices and results. \n",
    "\n",
    "Illustrate and compare your \"by-hand\" results with your code print outs to show that they are the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JK's answer\n",
    "\n",
    "### X Matrix\n",
    "\n",
    "\\begin{align*} \n",
    "X &= \n",
    "\n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{12} & x_{13} \\\\\n",
    "x_{21} & x_{22} & x_{23} \\\\\n",
    "\\vdots  & \\vdots  & \\vdots  \\\\\n",
    "x_{n1} & x_{n2} & x_{n3} \\\\\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "22 & 10 & 24 \\\\\n",
    "5  & 12 & 52 \\\\\n",
    "3  & 15 & 59 \\\\\n",
    "25 & 11 & 23 \\\\\n",
    "23 & 14 & 21 \\\\\n",
    "4  & 13 & 57 \n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-scaled X:\n",
      " [[22 10 24]\n",
      " [ 5 12 52]\n",
      " [ 3 14 59]\n",
      " [25 11 23]\n",
      " [23 15 21]\n",
      " [ 4 13 57]]\n"
     ]
    }
   ],
   "source": [
    "NN = OneLayer_OneOutput_FF(scale_data=False,verbose=False)\n",
    "print(\"non-scaled X:\\n\",NN.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid errors with Sigmoid, so going to Min-Max scale the data. I'm not going to calculate individually I\"m going to leave it to the Sklearn package min-max normalizer and trust it works.\n",
    "\n",
    "But this is the formula I would use if I did calculate by hand:\n",
    "\n",
    "$$x' = \\frac{x - min}{max - min}$$\n",
    "\n",
    "Where $x'$ is the scaled value, $x$ is the value to scale. ($min$ & $max$ are the minimum value and maximum value of the list of values to scale to.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max Scaled X:\n",
      " [[0.86363636 0.         0.07894737]\n",
      " [0.09090909 0.4        0.81578947]\n",
      " [0.         0.8        1.        ]\n",
      " [1.         0.2        0.05263158]\n",
      " [0.90909091 1.         0.        ]\n",
      " [0.04545455 0.6        0.94736842]]\n"
     ]
    }
   ],
   "source": [
    "NN = OneLayer_OneOutput_FF(scale_data=True,verbose=False)\n",
    "print(\"Min-Max Scaled X:\\n\",NN.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Targets $\\mathbf{y}$\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{y} &= \n",
    "\\begin{bmatrix} \n",
    "y_1 \\\\\n",
    "\\vdots \\\\\n",
    "y_n\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "&=\n",
    "\\begin{bmatrix} \n",
    "0 \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y target vector:\n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"y target vector:\\n\",NN.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $W^{(1)}$ Matrix\n",
    "For this assignment we're using all weights in $W^{(1)}$ are equal to $1$\n",
    "\n",
    "\\begin{align*}\n",
    "W^{(1)} &= \n",
    "\\begin{bmatrix}\n",
    "w_{11} & w_{12} & w_{13} & w_{14} \\\\\n",
    "w_{21} & w_{22} & w_{23} & w_{24} \\\\\n",
    "w_{31} & w_{32} & w_{33} & w_{34}\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 & 1 \\\\\n",
    "1 & 1 & 1 & 1 \\\\\n",
    "1 & 1 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters built for simple calculation (not randomized)\n",
      "W1 matrix: \n",
      " [[1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "NN.FeedForward(verbose=False)\n",
    "print(\"W1 matrix: \\n\",NN.W1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias vector $\\mathbf{B}$\n",
    "\n",
    "For this assignment we using all bias = 0\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{B} =\n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\\n",
    "\\vdots \\\\\n",
    "b_n\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias vector B: \n",
      " [[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Bias vector B: \\n\",NN.B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $Z_1$ Matrix\n",
    "\n",
    "\\begin{align*}\n",
    "Z_1 &= X \\cdot W^{(1)} + B \\\\\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{12} & x_{13} \\\\\n",
    "x_{21} & x_{22} & x_{23} \\\\\n",
    "\\vdots  & \\vdots  & \\vdots  \\\\\n",
    "x_{n1} & x_{n2} & x_{n3} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "w_{11} & w_{12} & w_{13} & w_{14} \\\\\n",
    "w_{21} & w_{22} & w_{23} & w_{24} \\\\\n",
    "w_{31} & w_{32} & w_{33} & w_{34}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\\n",
    "\\vdots \\\\\n",
    "b_n\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "0.863 & 0.0 & 0.0789 \\\\\n",
    "0.091 & 0.4 & 0.816  \\\\\n",
    "0.0   & 0.8 & 1.0    \\\\\n",
    "1.0   & 0.2 & 0.0526 \\\\\n",
    "0.909 & 1.0 & 0.0    \\\\\n",
    "0.045 & 0.6 & 0.947 \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 & 1 \\\\\n",
    "1 & 1 & 1 & 1 \\\\\n",
    "1 & 1 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "0.942 & 0.942 & 0.942 & 0.942 \\\\\n",
    "1.306 & 1.306 & 1.306 & 1.306 \\\\\n",
    "1.8   & 1.8   & 1.8   & 1.8   \\\\\n",
    "1.253 & 1.253 & 1.253 & 1.253 \\\\\n",
    "1.91  & 1.91  & 1.91  & 1.91  \\\\\n",
    "1.597 & 1.597 & 1.597 & 1.597 \n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94258373, 0.94258373, 0.94258373, 0.94258373],\n",
       "       [1.30669856, 1.30669856, 1.30669856, 1.30669856],\n",
       "       [1.8       , 1.8       , 1.8       , 1.8       ],\n",
       "       [1.25263158, 1.25263158, 1.25263158, 1.25263158],\n",
       "       [1.90909091, 1.90909091, 1.90909091, 1.90909091],\n",
       "       [1.59282297, 1.59282297, 1.59282297, 1.59282297]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN.Z1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $H_1$ Matrix\n",
    "\n",
    "Using the Sigmoid as the activation function which is defined as follows,\n",
    "\n",
    "\\begin{align*}\n",
    "S(z) &= \\frac{e^z}{1+e^z} \\\\[6pt] \n",
    "&= \\frac{1}{1+e^{-z}}\n",
    "\\end{align*}\n",
    "\n",
    "And $h=S(z)$\n",
    "\n",
    "\\begin{align*}\n",
    "H_1 & = \n",
    "\\begin{bmatrix}\n",
    "S(z_{11}) & S(z_{12}) & S(z_{13}) & S(z_{14}) \\\\\n",
    "\\vdots    &   \\vdots  &   \\vdots  &   \\vdots  \\\\\n",
    "S(z_{n1}) & S(z_{n2}) & S(z_{n3}) & S(z_{n4}) \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\\\\n",
    "\n",
    "& = \n",
    "\\begin{bmatrix}\n",
    "0.7195 & 0.7195 & 0.7195 & 0.7195 \\\\\n",
    "0.787  & 0.787  & 0.787  & 0.787  \\\\\n",
    "0.858  & 0.858  & 0.858  & 0.858  \\\\\n",
    "0.778  & 0.778  & 0.778  & 0.778  \\\\\n",
    "0.871  & 0.871  & 0.871  & 0.871  \\\\\n",
    "0.832  & 0.832  & 0.832  & 0.832  \n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H1 matrix: \n",
      " [[0.71962126 0.71962126 0.71962126 0.71962126]\n",
      " [0.78696018 0.78696018 0.78696018 0.78696018]\n",
      " [0.85814894 0.85814894 0.85814894 0.85814894]\n",
      " [0.77775507 0.77775507 0.77775507 0.77775507]\n",
      " [0.87091698 0.87091698 0.87091698 0.87091698]\n",
      " [0.8310129  0.8310129  0.8310129  0.8310129 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"H1 matrix: \\n\",NN.H1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $W^{(2)}$ Matrix\n",
    "\n",
    "For this assignment we're using all weights in $W^{(2)}$ are equal to $2$\n",
    "\n",
    "\\begin{align*}\n",
    "W^{(2)} =\n",
    "\\begin{bmatrix}\n",
    "w_1^{(2)} \\\\[6pt]\n",
    "w_2^{(2)} \\\\[6pt]\n",
    "w_3^{(2)} \\\\[6pt]\n",
    "w_4^{(2)} \\\\[6pt]\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "2 \\\\\n",
    "2 \\\\\n",
    "2 \\\\\n",
    "2 \n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2 matrix: \n",
      " [[2]\n",
      " [2]\n",
      " [2]\n",
      " [2]]\n"
     ]
    }
   ],
   "source": [
    "print(\"W2 matrix: \\n\",NN.W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\mathbf{C}$ bias vector\n",
    "\n",
    "For this assignment we using all bias = 0\n",
    "\n",
    "\\begin{equation*}\n",
    "C = \n",
    "\\begin{bmatrix}\n",
    "c_1 \\\\\n",
    "\\vdots \\\\\n",
    "c_n\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \n",
    "\\end{bmatrix}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C bias vector: \n",
      " [[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"C bias vector: \\n\",NN.C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $Z_2$ Matrix\n",
    "\n",
    "\\begin{align*}\n",
    "Z_2 &= H_1 W^{(2)} + C \\\\\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "h_{11} & h_{12} & h_{13} & h_{14} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "h_{n1} & h_{n2} & h_{n3} & h_{n4} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "w_1^{(2)} \\\\[6pt]\n",
    "w_2^{(2)} \\\\[6pt]\n",
    "w_3^{(2)} \\\\[6pt]\n",
    "w_4^{(2)} \\\\[6pt]\n",
    "\\end{bmatrix}\n",
    "\n",
    "+\n",
    "\n",
    "\\begin{bmatrix}\n",
    "c_1 \\\\\n",
    "\\vdots \\\\\n",
    "c_n\n",
    "\\end{bmatrix}\n",
    "\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "z_1^{(2)} \\\\\n",
    "\\vdots \\\\\n",
    "z_n^{(2)}\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "0.7195 & 0.7195 & 0.7195 & 0.7195 \\\\\n",
    "0.787  & 0.787  & 0.787  & 0.787  \\\\\n",
    "0.858  & 0.858  & 0.858  & 0.858  \\\\\n",
    "0.778  & 0.778  & 0.778  & 0.778  \\\\\n",
    "0.871  & 0.871  & 0.871  & 0.871  \\\\\n",
    "0.832  & 0.832  & 0.832  & 0.832  \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "2 \\\\\n",
    "2 \\\\\n",
    "2 \\\\\n",
    "2 \n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "5.756 \\\\\n",
    "6.296 \\\\\n",
    "6.864 \\\\\n",
    "6.224 \\\\\n",
    "6.968 \\\\\n",
    "6.656\n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z2 matrix: \n",
      " [[5.75697011]\n",
      " [6.29568146]\n",
      " [6.86519148]\n",
      " [6.22204054]\n",
      " [6.96733585]\n",
      " [6.64810323]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Z2 matrix: \\n\",NN.Z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\mathbf{\\hat{y}}$ prediction vector\n",
    "\n",
    "I'm guessing I have to do sigmoid again to get probability for binary labels.\n",
    "\n",
    "So something like,\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{\\hat{y}} &= \n",
    "\\begin{bmatrix}\n",
    "\\hat{y}_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\hat{y}_n \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "S(z_1^{(2)}) \\\\\n",
    "\\vdots \\\\\n",
    "S(z_n^{(2)})\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "& = \n",
    "\\begin{bmatrix}\n",
    "0.997 \\\\\n",
    "0.998 \\\\\n",
    "0.999 \\\\\n",
    "0.998 \\\\\n",
    "0.999 \\\\\n",
    "0.999 \n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y-hat prediction vector:\n",
      " [[0.99684928]\n",
      " [0.99815914]\n",
      " [0.99895761]\n",
      " [0.99801874]\n",
      " [0.99905873]\n",
      " [0.9987052 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"y-hat prediction vector:\\n\",NN.y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE Loss\n",
    "\n",
    "We're using MSE\n",
    "\n",
    "\\begin{align*}\n",
    "L = \\frac{1}{n} \\displaystyle \\sum_i^n (\\hat{y}_i - y_i)^2\n",
    "\\end{align*}\n",
    "\n",
    "Gonna start with $\\hat{y} - y$\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{\\hat{y}} - \\mathbf{y} &= \n",
    "\\begin{bmatrix}\n",
    "\\hat{y}_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\hat{y}_n \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "-\n",
    "\n",
    "\\begin{bmatrix} \n",
    "y_1 \\\\\n",
    "\\vdots \\\\\n",
    "y_n\n",
    "\\end{bmatrix}\n",
    "\n",
    "=\n",
    "\n",
    "\\begin{bmatrix}\n",
    "\\hat{y}_1 - y_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\hat{y}_n - y_n \\\\\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "0.997 \\\\\n",
    "0.998 \\\\\n",
    "0.999 \\\\\n",
    "0.998 \\\\\n",
    "0.999 \\\\\n",
    "0.999 \n",
    "\\end{bmatrix}\n",
    "-\n",
    "\\begin{bmatrix} \n",
    "0 \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.997  \\\\\n",
    "-0.002 \\\\\n",
    "-0.001 \\\\\n",
    "0.998  \\\\\n",
    "0.999  \\\\\n",
    "-0.001 \n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(y_hat) - y:\n",
      " [[ 0.99684928]\n",
      " [-0.00184086]\n",
      " [-0.00104239]\n",
      " [ 0.99801874]\n",
      " [ 0.99905873]\n",
      " [-0.0012948 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"(y_hat) - y:\\n\", NN.y_hat-NN.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\Rightarrow\n",
    "L  \\frac{1}{n} \\displaystyle \\sum_i^n (\\hat{y}_i - y_i)^2\n",
    "\\approx 0.498\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss: 0.4979790666878168\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE Loss:\",NN.MSE_loss(y_hat=NN.y_hat,y=NN.y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Writing out the derivatives by hand.\n",
    "\n",
    "For this part and in order to help you to code the back propagation, write out the derivatives for updating $\\mathbf{W_1}$, $\\mathbf{W_2}$, $\\mathbf{B}$, and $C$.\n",
    "\n",
    "Please write them clearly :)\n",
    "\n",
    "You can do this \"matrix style\" - so for example, $\\frac{\\partial L}{\\partial \\mathbf{W_1}}$, or you can write them out individually, such as $\\frac{\\partial L}{dw_{11}}$, $\\frac{\\partial L}{\\partial w_{12}}$, etc...\n",
    "\n",
    "Either way, show all your work and illustrate how the matrices (such as $\\mathbf{X}$, $\\mathbf{W_1}$, $\\mathbf{W_2}$, etc. etc.) will all fit together to create the final derivative.\n",
    "\n",
    "**Hints:** There are many examples on the PowerPoints. We reviewed this in class. You can also see examples of this in my code examples which are all here: [Gates Bolton Analytics - Neural Networks Page](https://gatesboltonanalytics.com/?page_id=680)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JK answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Adding Back Propagation to your code so that $\\mathbf{W_1}$, $\\mathbf{B}$, and $C$ are updated.\n",
    "\n",
    "For this step, you will add back propagation to your code. Specifically, you already have code **(2)** above. In your code (so far) you have read in your dataset, have created $X$ and $y$, and have function (or class if you OO programming) for Feed Forward.\n",
    "\n",
    "In this next step, you will add a function for Back Propagation that will update your parameters. Recall that the parameters for this NN architecture are $\\mathbf{W_1}$, $\\mathbf{W_2}$, $\\mathbf{B}$, and $C$. Each must be updated (using a learning rate) and the derivatives you calculated in **(4)**.\n",
    "\n",
    "Once you create code for the Back Propagation, add further code for epochs so that your code will \"update\" (FF and BP and calculate the Loss each time) for as many iterations (epochs) as you wish.\n",
    "\n",
    "Be sure to have a visualization of the change in the average Loss over epochs. \n",
    "\n",
    "Be sure to have a final visualization that is a confusion matrix that shows the label predictions.\n",
    "\n",
    "Be sure to **print out** some of your matrices so that a viewer can see what you are doing. While there are many ways to do this, you can consider printing $\\mathbf{W_1}$, the updates for $\\mathbf{W_1}$, the new $\\mathbf{W_1}$ (which will be the derivative updates times the learning rate added to $\\mathbf{W_1}$). In other words, **make your code verbose (shows/tells what its doing)** so you can print out the last epoch steps. (An example on Canvas) \n",
    "\n",
    "\n",
    "**NOTE:** At this point, you should have code that will read in any dataset with 3 columns of numeric data and one column called \"LABEL\" (of $0$ or $1$) and will perform NN modeling and a prediction result on the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***JK answer:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Test model\n",
    "\n",
    "Finally, create a test dataset and use your model to predict the labels of that test dataset. Then create a confusion matrix to show the prediction accuracy. Remember, to do this you will need labeled data, BUT you will remove the labels from the data before trying to predict it. Once the model makes predictions, you can then compare the predictions to the actual labels. \n",
    "\n",
    "(1) EXPLAIN in words what you plan to do\n",
    "\n",
    "(2) Code it\n",
    "\n",
    "(3) Give the results as a confusion matrix\n",
    "\n",
    "(4) Include your test dataset with your submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***JK answer:***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
