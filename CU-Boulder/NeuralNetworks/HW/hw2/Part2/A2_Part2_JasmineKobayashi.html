<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jasmine Kobayashi">

<title>Module 2 Assignment - Part 2: Multinomial NN with Softmax, Categorical Cross Entropy, and One-Hot Encoding</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="A2_Part2_JasmineKobayashi_files/libs/clipboard/clipboard.min.js"></script>
<script src="A2_Part2_JasmineKobayashi_files/libs/quarto-html/quarto.js"></script>
<script src="A2_Part2_JasmineKobayashi_files/libs/quarto-html/popper.min.js"></script>
<script src="A2_Part2_JasmineKobayashi_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="A2_Part2_JasmineKobayashi_files/libs/quarto-html/anchor.min.js"></script>
<link href="A2_Part2_JasmineKobayashi_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="A2_Part2_JasmineKobayashi_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="A2_Part2_JasmineKobayashi_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="A2_Part2_JasmineKobayashi_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="A2_Part2_JasmineKobayashi_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#draw-nn-architecture" id="toc-draw-nn-architecture" class="nav-link active" data-scroll-target="#draw-nn-architecture">1) Draw NN architecture</a></li>
  <li><a href="#write-out-all-the-feed-forward-equations" id="toc-write-out-all-the-feed-forward-equations" class="nav-link" data-scroll-target="#write-out-all-the-feed-forward-equations">2) Write out all the Feed Forward equations</a>
  <ul class="collapse">
  <li><a href="#jks-answer" id="toc-jks-answer" class="nav-link" data-scroll-target="#jks-answer">JK’s answer</a>
  <ul class="collapse">
  <li><a href="#z_1" id="toc-z_1" class="nav-link" data-scroll-target="#z_1"><span class="math inline">\(Z_1\)</span></a></li>
  <li><a href="#h_1" id="toc-h_1" class="nav-link" data-scroll-target="#h_1"><span class="math inline">\(H_1\)</span></a></li>
  <li><a href="#z_2" id="toc-z_2" class="nav-link" data-scroll-target="#z_2"><span class="math inline">\(Z_2\)</span></a></li>
  <li><a href="#mathbfhaty" id="toc-mathbfhaty" class="nav-link" data-scroll-target="#mathbfhaty"><span class="math inline">\(\mathbf{\hat{y}}\)</span></a></li>
  </ul></li>
  </ul></li>
  <li><a href="#what-are-the-following-derivatives" id="toc-what-are-the-following-derivatives" class="nav-link" data-scroll-target="#what-are-the-following-derivatives">3) What are the following derivatives</a>
  <ul class="collapse">
  <li><a href="#jks-answer-1" id="toc-jks-answer-1" class="nav-link" data-scroll-target="#jks-answer-1">JK’s answer:</a>
  <ul class="collapse">
  <li><a href="#fracpartial-lpartial-c" id="toc-fracpartial-lpartial-c" class="nav-link" data-scroll-target="#fracpartial-lpartial-c"><span class="math inline">\(\frac{\partial L}{\partial C}\)</span></a></li>
  <li><a href="#fracpartial-lpartial-w2" id="toc-fracpartial-lpartial-w2" class="nav-link" data-scroll-target="#fracpartial-lpartial-w2"><span class="math inline">\(\frac{\partial L}{\partial W^{(2)}}\)</span></a></li>
  <li><a href="#fracpartial-lpartial-b" id="toc-fracpartial-lpartial-b" class="nav-link" data-scroll-target="#fracpartial-lpartial-b"><span class="math inline">\(\frac{\partial L}{\partial B}\)</span></a></li>
  <li><a href="#fracpartial-lpartial-w1" id="toc-fracpartial-lpartial-w1" class="nav-link" data-scroll-target="#fracpartial-lpartial-w1"><span class="math inline">\(\frac{\partial L}{\partial W^{(1)}}\)</span></a></li>
  </ul></li>
  </ul></li>
  <li><a href="#calculate-softmax-results" id="toc-calculate-softmax-results" class="nav-link" data-scroll-target="#calculate-softmax-results">4) Calculate Softmax results</a>
  <ul class="collapse">
  <li><a href="#jks-answer-2" id="toc-jks-answer-2" class="nav-link" data-scroll-target="#jks-answer-2">JK’s answer</a>
  <ul class="collapse">
  <li><a href="#textsoftmax1.1" id="toc-textsoftmax1.1" class="nav-link" data-scroll-target="#textsoftmax1.1"><span class="math inline">\(\text{softmax}(1.1)\)</span></a></li>
  <li><a href="#textsoftmax2.2" id="toc-textsoftmax2.2" class="nav-link" data-scroll-target="#textsoftmax2.2"><span class="math inline">\(\text{softmax}(2.2)\)</span></a></li>
  <li><a href="#textsoftmax0.2" id="toc-textsoftmax0.2" class="nav-link" data-scroll-target="#textsoftmax0.2"><span class="math inline">\(\text{softmax}(0.2)\)</span></a></li>
  <li><a href="#textsoftmax-1.7" id="toc-textsoftmax-1.7" class="nav-link" data-scroll-target="#textsoftmax-1.7"><span class="math inline">\(\text{softmax}(-1.7)\)</span></a></li>
  </ul></li>
  </ul></li>
  <li><a href="#derivative-of-softmax" id="toc-derivative-of-softmax" class="nav-link" data-scroll-target="#derivative-of-softmax">5) Derivative of Softmax</a>
  <ul class="collapse">
  <li><a href="#jks-answer-3" id="toc-jks-answer-3" class="nav-link" data-scroll-target="#jks-answer-3">JK’s answer:</a>
  <ul class="collapse">
  <li><a href="#case-1-ij" id="toc-case-1-ij" class="nav-link" data-scroll-target="#case-1-ij"><em>Case 1:</em> <span class="math inline">\(i=j\)</span></a></li>
  <li><a href="#case-2-i-neq-j" id="toc-case-2-i-neq-j" class="nav-link" data-scroll-target="#case-2-i-neq-j"><em>Case 2:</em> <span class="math inline">\(i \neq j\)</span></a></li>
  <li><a href="#jacobian" id="toc-jacobian" class="nav-link" data-scroll-target="#jacobian">Jacobian</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#combination" id="toc-combination" class="nav-link" data-scroll-target="#combination">6) Combination</a></li>
  <li><a href="#code" id="toc-code" class="nav-link" data-scroll-target="#code">7) Code</a></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Module 2 Assignment - Part 2: Multinomial NN with Softmax, Categorical Cross Entropy, and One-Hot Encoding</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Jasmine Kobayashi </p>
          </div>
  </div>
    
  
    
  </div>
  

</header>

<p>Module 2 Assignment - Part 2: Multinomial NN with Softmax, Categorical Cross Entropy, and One-Hot Encoding</p>
<hr>
<p><strong>Instructions:</strong></p>
<p>This assignment has a two large parts with many sub-parts. Each part will enable you to practice with a neural networks and to practice for Exam 2.</p>
<p>This Assignment will TAKE TIME so use all of the time you have to work on it. Please avoid waiting until the last few days as this will not be enough time. You need weeks - not days :)</p>
<p>Be sure to show all work and (when requested) to code using Python (and no NN/TF/Keras packages). Do not worry, we will use packages in coming modules but first it is best to learn about what is going on inside the model.</p>
<p>You may use examples and code that I have shared as a reference. Using my code will not be considered cheating although I strongly recommend that you write as much (if not all) of your own code so that you learn the concepts more robustly.</p>
<p>This is not a team assignment. Please work and code alone. You may discuss concepts, but you cannot share code or work with others. Papers that look too similar with split the grade (first offense) and can suffer less pleasant outcomes for further offenses. Keep it simple and smart - do your own work.</p>
<section id="draw-nn-architecture" class="level1">
<h1>1) Draw NN architecture</h1>
<p>In Part 1 above, I created (with an illustration) the NN architecture for you. In this part, you will illustrate (draw) the architecture.</p>
<ol type="a">
<li><p>Your NN architecture (draw this) will expect input vectors of three values (3D data). It will have one hidden layer with two units. It will create four outputs (for four categories), it will use softmax to convert those 4 outputs into a probability distribution, and it will choose the max as the prediction.</p></li>
<li><p>The hidden layer (H1) will be activated with reLU.</p></li>
</ol>
<p>Draw the NN.</p>
<p>Use my slides and what we did during class lecture to help you. Label everything - including the x’s, the W1’s, the B, the C, the H1 values, the Z2 values, the y^ values (after softmax), and the final output. Again, the class slides have an exact example of this.</p>
<p><strong><em>JK:</em></strong> <em>Check Word Document</em></p>
</section>
<section id="write-out-all-the-feed-forward-equations" class="level1">
<h1>2) Write out all the Feed Forward equations</h1>
<p>You will have equations for <span class="math inline">\(Z_1\)</span>, <span class="math inline">\(H_1\)</span>, <span class="math inline">\(Z_2\)</span>, <span class="math inline">\(\hat{Y}\)</span>.</p>
<section id="jks-answer" class="level2">
<h2 class="anchored" data-anchor-id="jks-answer">JK’s answer</h2>
<section id="z_1" class="level3">
<h3 class="anchored" data-anchor-id="z_1"><span class="math inline">\(Z_1\)</span></h3>
<p><span class="math display">\[\begin{align*}
Z_1 &amp; =  X\cdot W_1 + B

\\
&amp; =
\begin{bmatrix}
x_{11} &amp; x_{12} &amp; x_{13} \\
\vdots  &amp; \vdots  &amp; \vdots  \\
x_{n1} &amp; x_{n2} &amp; x_{n3} \\
\end{bmatrix}

\begin{bmatrix}
w_{11} &amp; w_{12}  \\
w_{21} &amp; w_{22}  \\
w_{31} &amp; w_{32}
\end{bmatrix}

+
\begin{bmatrix}
b_1 \\
\vdots \\
b_n
\end{bmatrix}

\\

&amp; =
\begin{bmatrix}
z_{11} &amp; z_{12}  \\
z_{21} &amp; z_{22}  \\
\vdots &amp; \vdots  \\
z_{n1} &amp; z_{n2}
\end{bmatrix}
\end{align*}\]</span></p>
<p>Where we have:</p>
<p><span class="math display">\[\begin{align*}
&amp; z_{11} = x_{11}w_{11} + x_{12}w_{21} + x_{13}w_{31} + b_1 &amp; &amp; \\
&amp; z_{12} = x_{11}w_{12} + x_{12}w_{22} + x_{13}w_{32} + b_1 &amp; &amp;\\
\end{align*}\]</span></p>
<p><span class="math display">\[\vdots\]</span></p>
<p><span class="math display">\[\begin{align*}
z_{n2} = x_{n1}w_{12} + x_{n2}w_{22} + x_{n3}w_{32} + b_n
\end{align*}\]</span></p>
<p>In other words, any <span class="math inline">\(Z\)</span> element can be summarized as,</p>
<p><span class="math display">\[z_{i,k} = \sum_j^3 x_{i,j}w_{j,k} + b_i\]</span></p>
</section>
<section id="h_1" class="level3">
<h3 class="anchored" data-anchor-id="h_1"><span class="math inline">\(H_1\)</span></h3>
<p>We’re assuming the first hidden layer uses the reLU activation function.</p>
<p>reLU function:</p>
<p><span class="math display">\[\begin{equation*}
f =
\begin{cases}
x   &amp;\text{if } x &gt; 0 \\
0   &amp;\text{else}
\end{cases}
\end{equation*}\]</span></p>
<p><span class="math inline">\(h\)</span> is the same value as <span class="math inline">\(x\)</span> if it’s positive and 0 otherwise.</p>
<p>The <span class="math inline">\(H1\)</span> matrix is as follows:</p>
<p><span class="math display">\[\begin{align*}
H_1 &amp; =
\begin{bmatrix}
f(z_{11}) &amp; f(z_{12}) \\
\vdots    &amp;   \vdots  \\
f(z_{n1}) &amp; f(z_{n2}) \\
\end{bmatrix}

\\

&amp; =
\begin{bmatrix}
h_{11} &amp; h_{12}  \\
\vdots &amp; \vdots  \\
h_{n1} &amp; h_{n2}  \\
\end{bmatrix}
\end{align*}\]</span></p>
<p>The number of columns in <span class="math inline">\(H_1\)</span> is the number of hidden units in the hidden layer.</p>
</section>
<section id="z_2" class="level3">
<h3 class="anchored" data-anchor-id="z_2"><span class="math inline">\(Z_2\)</span></h3>
<p><span class="math display">\[\begin{align*}
Z^{(2)} &amp;= H_1 W^{(2)} + C \\
&amp;=
\begin{bmatrix}
h_{11} &amp; h_{12} \\
\vdots &amp; \vdots \\
h_{n1} &amp; h_{n2} \\
\end{bmatrix}

\begin{bmatrix}
w_{11}^{(2)} &amp; w_{12}^{(2)} &amp; w_{13}^{(2)} &amp; w_{14}^{(2)}  \\[6pt]
w_{21}^{(2)} &amp; w_{22}^{(2)} &amp; w_{23}^{(2)} &amp; w_{24}^{(2)}  \\[6pt]
\end{bmatrix}

+

\begin{bmatrix}
c_1 \\
\vdots \\
c_n
\end{bmatrix}\\
&amp;=
\begin{bmatrix}
z_{11}^{(2)} &amp; z_{12}^{(2)} &amp; z_{13}^{(2)} &amp; z_{14}^{(2)} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
z_{n1}^{(2)} &amp; z_{n2}^{(2)} &amp; z_{n3}^{(2)} &amp; z_{n4}^{(2)}  
\end{bmatrix}
\end{align*}\]</span></p>
<p>Where,</p>
<p><span class="math display">\[\begin{align*}
&amp; z_{11}^{(2)} = h_{11}w_{11}^{(2)} + h_{12}w_{21}^{(2)} + c_1 &amp; &amp; \\
&amp; z_{12}^{(2)} = h_{11}w_{12}^{(2)} + h_{12}w_{22}^{(2)} + c_1 &amp; &amp;\\
\end{align*}\]</span></p>
<p><span class="math display">\[\vdots\]</span></p>
<p><span class="math display">\[\begin{align*}
z_{n4}^{(2)} = h_{n1}w_{14}^{(2)} + x_{n2}w_{24}^{(2)} + c_n
\end{align*}\]</span></p>
<p>Or in other words, each <span class="math inline">\(Z^{(2)}\)</span> element can be summarized as,</p>
<p><span class="math display">\[\begin{align*}
z_{i,l}^{(2)} = \sum_k^2 h_{i,k}w_{k,l}^{(2)} + c_i
\end{align*}\]</span></p>
</section>
<section id="mathbfhaty" class="level3">
<h3 class="anchored" data-anchor-id="mathbfhaty"><span class="math inline">\(\mathbf{\hat{y}}\)</span></h3>
<p>And lastly we have the predicted output <span class="math inline">\(\mathbf{\hat{y}}\)</span>.</p>
<p>In the case of this architecture, we’re expecting 4 outputs.</p>
<p>And for this assignment, we said we would use the Softmax function, which is defined as follows.</p>
<p><span class="math display">\[\begin{align*}
\hat{y}_{i,l} = \text{softmax}(z_{i,l}^{(2)}) = \frac{\text{exp}(z_{i,l}^{(2)})}{\displaystyle \sum_l^4 \text{exp}(z_{i,l}^{(2)})}
\end{align*}\]</span></p>
<p>Where <span class="math inline">\(\text{exp}(x) = e^x\)</span> (I just used that notation to make the variables easier to see in the fraction.)</p>
<p>The summation from <span class="math inline">\(l=1\)</span> to <span class="math inline">\(l=4\)</span> is due to the fact that we have four possible classes/outcomes for this architecture. (So the formula above is specific to this assignment.)</p>
<p>The softmax function gives the probability distributions of the possible outcomes. So in this case we would have 4 probabilities of the 4 labels from the softmax. (The probabilities are pdfs too, so the probabilities for each instance are non-negative and all sum up to one.)</p>
<p><span class="math display">\[\begin{align*}
\Rightarrow
\mathbf{\hat{y}} &amp;=
\begin{bmatrix}
[\hat{y}_{11} &amp; \hat{y}_{12} &amp; \hat{y}_{13} &amp; \hat{y}_{14}] \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
[\hat{y}_{n1} &amp; \hat{y}_{n2} &amp; \hat{y}_{n3} &amp; \hat{y}_{n4}] \\
\end{bmatrix}\\
&amp;=
\begin{bmatrix}
[\text{softmax}(z_{11}^{(2)}) &amp; \text{softmax}(z_{12}^{(2)}) &amp; \text{softmax}(z_{13}^{(2)}) &amp; \text{softmax}(z_{14}^{(2)})] \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
[\text{softmax}(z_{n1}^{(2)}) &amp; \text{softmax}(z_{n2}^{(2)}) &amp; \text{softmax}(z_{n3}^{(2)}) &amp; \text{softmax}(z_{n4}^{(2)})]
\end{bmatrix}
\end{align*}\]</span></p>
</section>
</section>
</section>
<section id="what-are-the-following-derivatives" class="level1">
<h1>3) What are the following derivatives</h1>
<p><span class="math inline">\(\frac{\partial L}{\partial W1} = ?\)</span></p>
<p><span class="math inline">\(\frac{\partial L}{\partial W2} = ?\)</span></p>
<p><span class="math inline">\(\frac{\partial L}{\partial B} = ?\)</span></p>
<p><span class="math inline">\(\frac{\partial L}{\partial C} = ?\)</span></p>
<p>(using reLU to activate <span class="math inline">\(Z_1\)</span> to <span class="math inline">\(H_1\)</span>, and using Softmax to activate <span class="math inline">\(Z_2\)</span> to <span class="math inline">\(\hat{Y}\)</span>)</p>
<p><strong>Hint:</strong> The slides show a very similar example. The only difference is that the slides use Sigmoid and not reLU. So you will need to update this part. Start by determining the derivative of the reLU.</p>
<p>Use the Categorical Cross Entropy Loss function.</p>
<p>Use One-Hot Encoding for <span class="math inline">\(\mathbf{y}\)</span> (your label)</p>
<p>Show all your work</p>
<section id="jks-answer-1" class="level2">
<h2 class="anchored" data-anchor-id="jks-answer-1">JK’s answer:</h2>
<p>Parameters we want to update via back propagation (and gradient descent):</p>
<span class="math display">\[\begin{aligned}
\begin{array}{ccc}
\hline
\text{Parameter} &amp; \text{Matrix/Vector form} &amp; \text{Shape} \\
\hline
\\

W^{(1)} &amp;
\begin{bmatrix}
w_{11} &amp; w_{12}  \\
w_{21} &amp; w_{22}  \\
w_{31} &amp; w_{32}
\end{bmatrix}
&amp;
[3 \times 2]\\

\\
\hline
\\

\mathbf{B} &amp;
\begin{bmatrix}
b_1 \\
\vdots \\
b_n
\end{bmatrix}
&amp;
[n \times 1]\\

\\
\hline
\\

W^{(2)} &amp;
\begin{bmatrix}
w_{11}^{(2)} &amp; w_{12}^{(2)} &amp; w_{13}^{(2)} &amp; w_{14}^{(2)}  \\[6pt]
w_{21}^{(2)} &amp; w_{22}^{(2)} &amp; w_{23}^{(2)} &amp; w_{24}^{(2)}  \\[6pt]
\end{bmatrix}
&amp;
[2 \times 4]\\

\\
\hline
\\

\mathbf{C} &amp;
\begin{bmatrix}
c_1 \\
\vdots \\
c_n
\end{bmatrix}
&amp;
[n \times 1]

\end{array}
\end{aligned}\]</span>
<p>And I believe for gradient descent to work properly, I think the (final matrix/vector form of the) partial derivatives of each parameter needs to match the shape of the original parameter.</p>
<p>Other relevant formulas:</p>
<span class="math display">\[\begin{aligned}
\begin{array}{c|c|c|c}
\hline
\text{Variable (matrix/vector form)}&amp; \text{Shape} &amp; \text{Vector/Matrix Formula} &amp; \text{Element-wise formula} \\
\hline \\

L_{CCE} &amp; \text{(scalar)} &amp; -log(\mathbf{\hat{y}}) &amp; L = -log(\hat{y_i}) \\
\\
\hline \\

\mathbf{\hat{y}} =
\begin{bmatrix}
[\hat{y}_{11} &amp; \hat{y}_{12} &amp; \hat{y}_{13} &amp; \hat{y}_{14}] \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
[\hat{y}_{n1} &amp; \hat{y}_{n2} &amp; \hat{y}_{n3} &amp; \hat{y}_{n4}] \\
\end{bmatrix}
&amp;
[n \times 4]
&amp;
\mathbf{\hat{y}} = \text{softmax}(Z^{(2)})
&amp; \hat{y}_{i,l} =  \frac{\textstyle{\text{exp}}(z_{i,l}^{(2)})}{\displaystyle \sum_l^4 \text{exp}(z_{i,l}^{(2)})}\\
\\
\hline
\\

Z^{(2)} =
\begin{bmatrix}
z_{11}^{(2)} &amp; z_{12}^{(2)} &amp; z_{13}^{(2)} &amp; z_{14}^{(2)} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
z_{n1}^{(2)} &amp; z_{n2}^{(2)} &amp; z_{n3}^{(2)} &amp; z_{n4}^{(2)}  
\end{bmatrix}
&amp;
[n \times 4]
&amp;
Z^{(2)} = H_1 W^{(2)} + C
&amp; z_{i,l}^{(2)} = \displaystyle \sum_k^2 h_{i,k}w_{k,l}^{(2)} + c_i \\

\\
\hline
\\

H_1 =
\begin{bmatrix}
h_{11} &amp; h_{12}  \\
\vdots &amp; \vdots  \\
h_{n1} &amp; h_{n2}  \\
\end{bmatrix}
&amp;
[n \times 2]
&amp;
H_1 = \text{reLU}(Z^{(1)})
&amp;
h_{i,k} =
\begin{cases}
z_{i,k}^{(1)}   &amp;\text{if } z &gt; 0 \\
0   &amp;\text{else}
\end{cases} \\

\\
\hline
\\

Z^{(1)} =
\begin{bmatrix}
z_{11} &amp; z_{12} &amp; z_{13} &amp; z_{14} \\
z_{21} &amp; z_{22} &amp; z_{23} &amp; z_{24} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
z_{n1} &amp; z_{n2} &amp; z_{n3} &amp; z_{n4}
\end{bmatrix}
&amp;
[n \times 4]
&amp;
Z^{(1)} =  X\cdot W_1 + B
&amp;
z_{i,k}^{(1)} = \displaystyle \sum_j^3 x_{i,j}w_{j,k}^{(1)} + b_i \\

\end{array}
\end{aligned}\]</span>
<p>Reminder (to self) the following are specific to the architecture of this assignment:</p>
<ul>
<li><span class="math inline">\(2\)</span> comes from the number of units in the hidden layer</li>
<li><span class="math inline">\(3\)</span> comes from the number of inputs/columns in <span class="math inline">\(X\)</span></li>
<li><span class="math inline">\(4\)</span> comes from the number of possible labels/outputs in <span class="math inline">\(y\)</span> and <span class="math inline">\(\hat{y}\)</span></li>
<li>(<span class="math inline">\(n\)</span> in this is supposed to be 6 for the dataset I created)</li>
</ul>
<p><strong>Other potentially useful multiplication notation to match with python.</strong></p>
<p>I will try my best to match the multiplication notation as done in Professor Gates’ lecture slides.</p>
<p>Thus, I will use an “o-dot” (<span class="math inline">\(\odot\)</span>) to represent matrix multiplcation, (equivalent to <code>@</code> in Python) i.e.:</p>
<p><span class="math display">\[\begin{align*}
A \odot B =
\begin{bmatrix}
a_{11} &amp; a_{12} \\
a_{21} &amp; a_{22}\\
\end{bmatrix}
\odot
\begin{bmatrix}
b_{11} &amp; b_{12} \\
b_{21} &amp; b_{22} \\
\end{bmatrix}
=
\begin{bmatrix}
a_{11}b_{11} + a_{12}b_{21} &amp; a_{11}b_{12} + a_{12}b_{22} \\
a_{21}b_{11} + a_{22}b_{21} &amp; a_{21}b_{12} + a_{22}b_{22}
\end{bmatrix}
\end{align*}\]</span></p>
<p>And I will use an asterisk (<span class="math inline">\(\ast\)</span>) for element-wise (“regular”) multiplication, (equivalent to <code>*</code> in Python) i.e.:</p>
<p><span class="math display">\[\begin{align*}
A \ast B =
\begin{bmatrix}
a_{11} &amp; a_{12} \\
a_{21} &amp; a_{22}\\
\end{bmatrix}
\ast
\begin{bmatrix}
b_{11} &amp; b_{12} \\
b_{21} &amp; b_{22} \\
\end{bmatrix}
=
\begin{bmatrix}
a_{11}b_{11} &amp; a_{12}b_{12} \\
a_{21}b_{21} &amp; a_{22}b_{22}
\end{bmatrix}
\end{align*}\]</span></p>
<section id="fracpartial-lpartial-c" class="level3">
<h3 class="anchored" data-anchor-id="fracpartial-lpartial-c"><span class="math inline">\(\frac{\partial L}{\partial C}\)</span></h3>
<p><span class="math display">\[\begin{align*}
\frac{\partial L}{\partial C} = \frac{\partial L}{\partial \mathbf{\hat{y}_i}} \cdot \frac{\partial \mathbf{\hat{y}_i}}{\partial Z^{(2)}} \cdot \frac{\partial Z^{(2)}}{\partial C}
\end{align*}\]</span></p>
<p>We find the derivative of the softmax in question 5, and in question 6 we remind ourselves from the lectures that when we use a combination of Softmax, Categorical Cross entropy, and One-Hot Encoding, then <span class="math inline">\(\frac{\partial L}{\partial Z^{(2)}} = \hat{y} - y\)</span> (aka “error” in the lecture slides). So I’m going to skip those parts and just find the rest of the relevant partial derivatives.</p>
<p><span class="math display">\[\begin{align*}
z_{i,l}^{(2)} &amp;= \sum_k^2 h_{i,k}w_{k,l}^{(2)} + c_i \\
\Rightarrow
\frac{\partial z_{i,l}^{(2)}}{\partial c_i} &amp;= 0 + (1)\\
\Rightarrow
\frac{\partial Z^{(2)}}{\partial C} &amp;=
\begin{pmatrix}
1 \\
\vdots \\
1
\end{pmatrix}
=
\vec{1}_n
\end{align*}\]</span></p>
<p>So in matrix form</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial L}{\partial C} &amp;= \frac{\partial L}{\partial \mathbf{\hat{y}_i}} \cdot \frac{\partial \mathbf{\hat{y}_i}}{\partial Z^{(2)}} \cdot \frac{\partial Z^{(2)}}{\partial C}\\
&amp;= [\mathbf{\hat{y}} - \mathbf{y}] \ast \vec{1}_n
\end{align*}\]</span></p>
</section>
<section id="fracpartial-lpartial-w2" class="level3">
<h3 class="anchored" data-anchor-id="fracpartial-lpartial-w2"><span class="math inline">\(\frac{\partial L}{\partial W^{(2)}}\)</span></h3>
<p><span class="math display">\[\begin{align*}
\frac{\partial L}{\partial W^{(2)}} &amp;= \underbrace{\frac{\partial L}{\partial \mathbf{\hat{y}_i}} \cdot \frac{\partial \mathbf{\hat{y}_i}}{\partial Z^{(2)}}}_{\frac{\partial L}{\partial Z^{(2)}}} \cdot \frac{\partial Z^{(2)}}{\partial W^{(2)}}\\
&amp;= \frac{\partial L}{\partial Z^{(2)}} \cdot \frac{\partial Z^{(2)}}{\partial W^{(2)}}
\end{align*}\]</span></p>
<p>Again, focusing on the other derivatives:</p>
<p><span class="math display">\[\begin{align*}
z_{i,l}^{(2)} &amp;= \sum_k^2 h_{i,k}w_{k,l}^{(2)} + c_i \\
\Rightarrow
\frac{\partial z_{i,l}^{(2)}}{\partial w_{kl}} &amp;= h_{i,k}(1) + 0\\
\Rightarrow
\frac{\partial Z^{(2)}}{\partial W^{(2)}} &amp;= H_1
\end{align*}\]</span></p>
<p>Matrix form:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial L}{\partial W^{(2)}} &amp;= \frac{\partial L}{\partial \mathbf{\hat{y}_i}} \cdot \frac{\partial \mathbf{\hat{y}_i}}{\partial Z^{(2)}} \cdot \frac{\partial Z^{(2)}}{\partial W^{(2)}}\\
&amp;= [\mathbf{\hat{y}} - \mathbf{y}] \odot H_1
\end{align*}\]</span></p>
</section>
<section id="fracpartial-lpartial-b" class="level3">
<h3 class="anchored" data-anchor-id="fracpartial-lpartial-b"><span class="math inline">\(\frac{\partial L}{\partial B}\)</span></h3>
<p><span class="math display">\[\begin{align*}
\frac{\partial L}{\partial B} &amp;= \underbrace{\frac{\partial L}{\partial \mathbf{\hat{y}}} \cdot \frac{\partial \mathbf{\hat{y}}}{\partial Z^{(2)}}}_{\frac{\partial L}{\partial Z^{(2)}}} \cdot \frac{\partial Z^{(2)}}{\partial H_1} \cdot \frac{\partial H_1}{\partial Z^{(1)}} \cdot \frac{\partial Z^{(1)}}{\partial B} \\
\end{align*}\]</span></p>
<p>Starting with <span class="math inline">\(\frac{\partial Z^{(2)}}{\partial H_1}\)</span></p>
<p><span class="math display">\[\begin{align*}
z_{i,l}^{(2)} &amp;= \displaystyle \sum_k^2 h_{i,k}w_{k,l}^{(2)} + c_i \\
\Rightarrow
\frac{\partial z_{i,l}^{(2)}}{\partial h_{i,k}} &amp;= (1)w_{k,l}^{(2)} + 0 \\
\Rightarrow
\frac{\partial Z^{(2)}}{\partial H_1} &amp;= W^{(2)}
\end{align*}\]</span></p>
<p>Then for <span class="math inline">\(\frac{\partial H_1}{\partial Z^{(1)}}\)</span></p>
<p><span class="math display">\[\begin{align*}
h_{i,k} = \text{reLU}(z_{i,k}^{(1)}) =
\begin{cases}
z_{i,k}^{(1)}   &amp;\text{if } z &gt; 0 \\
0   &amp;\text{else}
\end{cases}
\end{align*}\]</span></p>
<p>We need to start with finding the derivative of the reLU function.</p>
<p>Generically,</p>
<p><span class="math display">\[\begin{align*}
f(z) =
\begin{cases}
z   &amp;\text{if } z &gt; 0 \\
0   &amp;\text{else}
\end{cases} \\
\Rightarrow
f'(z) =
\begin{cases}
1   &amp;\text{if } z &gt; 0 \\
0   &amp;\text{else}
\end{cases} \\
\end{align*}\]</span></p>
<p>So I can summarize this with a matrix, and I can call this matrix, <span class="math inline">\(\Gamma\)</span>.</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial H_1}{\partial Z^{(1)}} =
\begin{bmatrix}
f'(z_{11}^{(1)}) &amp; f'(z_{12}^{(1)}) &amp; f'(z_{13}^{(1)}) &amp; f'(z_{14}^{(1)}) \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
f'(z_{n1}^{(1)}) &amp; f'(z_{n2}^{(1)}) &amp; f'(z_{n3}^{(1)}) &amp; f'(z_{n4}^{(1)})
\end{bmatrix}
= \Gamma
\end{align*}\]</span></p>
<p>Then lastly for <span class="math inline">\(\frac{\partial Z^{(1)}}{\partial B}\)</span></p>
<p><span class="math display">\[\begin{align*}
z_{i,k}^{(1)} &amp;= \displaystyle \sum_j^3 x_{i,j}w_{j,k}^{(1)} + b_i \\
\Rightarrow
\frac{\partial z_{i,k}^{(1)}}{\partial b_i} &amp;= 0 + (1) \\
\Rightarrow
\frac{\partial Z^{(1)}}{\partial B} &amp;=
\begin{bmatrix}
1 \\
\vdots \\
1
\end{bmatrix}
= \vec{1}_n

\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
\frac{\partial L}{\partial B} &amp;=
\underbrace{\frac{\partial L}{\partial \mathbf{\hat{y}}}
\cdot
\frac{\partial \mathbf{\hat{y}}}{\partial Z^{(2)}}}_{\frac{\partial L}{\partial Z^{(2)}}}

\cdot

\underbrace{\frac{\partial Z^{(2)}}{\partial H_1}}_{W^{(2)}}

\cdot

\underbrace{\frac{\partial H_1}{\partial Z^{(1)}}}_{\Gamma}

\cdot

\underbrace{\frac{\partial Z^{(1)}}{\partial B}}_{\vec{1}_n} \\
\end{align*}\]</span></p>
</section>
<section id="fracpartial-lpartial-w1" class="level3">
<h3 class="anchored" data-anchor-id="fracpartial-lpartial-w1"><span class="math inline">\(\frac{\partial L}{\partial W^{(1)}}\)</span></h3>
<p><span class="math display">\[\begin{align*}
\frac{\partial L}{\partial W^{(1)}} &amp;=
\underbrace{\frac{\partial L}{\partial \mathbf{\hat{y}}}
\cdot
\frac{\partial \mathbf{\hat{y}}}{\partial Z^{(2)}}}_{\frac{\partial L}{\partial Z^{(2)}}}

\cdot

\underbrace{\frac{\partial Z^{(2)}}{\partial H_1}}_{W^{(2)}}

\cdot

\underbrace{\frac{\partial H_1}{\partial Z^{(1)}}}_{\Gamma}

\cdot

\frac{\partial Z^{(1)}}{\partial W^{(1)}} \\
\end{align*}\]</span></p>
<p>The only derivative I need to find for <span class="math inline">\(\frac{\partial Z^{(1)}}{\partial W^{(1)}}\)</span></p>
<p><span class="math display">\[\begin{align*}
z_{i,k}^{(1)} &amp;= \displaystyle \sum_j^3 x_{i,j}w_{j,k}^{(1)} + b_i \\
\Rightarrow
\frac{\partial z_{i,k}^{(1)}}{\partial w_{j,k}^{(1)}} &amp;= x_{i,j}(1) + 0 = x_{i,j}\\
\Rightarrow
\frac{\partial Z^{(1)}}{\partial W^{(1)}} &amp;=
X

\end{align*}\]</span></p>
<p>Therefore,</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial L}{\partial W^{(1)}} &amp;=
\underbrace{\frac{\partial L}{\partial \mathbf{\hat{y}}}
\cdot
\frac{\partial \mathbf{\hat{y}}}{\partial Z^{(2)}}}_{\frac{\partial L}{\partial Z^{(2)}}}

\cdot

\underbrace{\frac{\partial Z^{(2)}}{\partial H_1}}_{W^{(2)}}

\cdot

\underbrace{\frac{\partial H_1}{\partial Z^{(1)}}}_{\Gamma}

\cdot

\underbrace{\frac{\partial Z^{(1)}}{\partial W^{(1)}}}_{X} \\
\end{align*}\]</span></p>
</section>
</section>
</section>
<section id="calculate-softmax-results" class="level1">
<h1>4) Calculate Softmax results</h1>
<p>If your four <span class="math inline">\(Z_2\)</span> values are <span class="math inline">\(1.1, 2.2, 0.2\)</span> and <span class="math inline">\(-1.7\)</span>, calculate (by hand and show your work) the four softmax results.</p>
<p>(Yes you can use a calculator or Google for the math - but SHOW THE WORK and steps) If you show just the answer, you will get -50%.</p>
<section id="jks-answer-2" class="level2">
<h2 class="anchored" data-anchor-id="jks-answer-2">JK’s answer</h2>
<p>Softmax formula:</p>
<p><span class="math display">\[\begin{align*}
\text{softmax}(z) = \frac{\text{exp}(z)}{\displaystyle \sum_i \text{exp}(z_i)}
\end{align*}\]</span></p>
<p><span class="math inline">\(Z2 = [1.1,2.2,0.2,-1.7]\)</span></p>
<section id="textsoftmax1.1" class="level3">
<h3 class="anchored" data-anchor-id="textsoftmax1.1"><span class="math inline">\(\text{softmax}(1.1)\)</span></h3>
<p><span class="math display">\[\begin{align*}
\text{softmax}(1.1) &amp;= \frac{e^{1.1}}{e^{1.1} + e^{2.2} + e^{0.2} + e^{-1.7}} \\
&amp; \approx \frac{3.0042}{13.43327} \\
&amp; \approx 0.2236
\end{align*}\]</span></p>
</section>
<section id="textsoftmax2.2" class="level3">
<h3 class="anchored" data-anchor-id="textsoftmax2.2"><span class="math inline">\(\text{softmax}(2.2)\)</span></h3>
<p><span class="math display">\[\begin{align*}
\text{softmax}(2.2) &amp;= \frac{e^{2.2}}{e^{1.1} + e^{2.2} + e^{0.2} + e^{-1.7}} \\
&amp; \approx \frac{9.025}{13.43327} \\
&amp; \approx 0.6718
\end{align*}\]</span></p>
</section>
<section id="textsoftmax0.2" class="level3">
<h3 class="anchored" data-anchor-id="textsoftmax0.2"><span class="math inline">\(\text{softmax}(0.2)\)</span></h3>
<p><span class="math display">\[\begin{align*}
\text{softmax}(0.2) &amp;= \frac{e^{0.2}}{e^{1.1} + e^{2.2} + e^{0.2} + e^{-1.7}} \\
&amp; \approx \frac{1.2214}{13.43327} \\
&amp; \approx 0.0909
\end{align*}\]</span></p>
</section>
<section id="textsoftmax-1.7" class="level3">
<h3 class="anchored" data-anchor-id="textsoftmax-1.7"><span class="math inline">\(\text{softmax}(-1.7)\)</span></h3>
<p><span class="math display">\[\begin{align*}
\text{softmax}(-1.7) &amp;= \frac{e^{-1.7}}{e^{1.1} + e^{2.2} + e^{0.2} + e^{-1.7}} \\
&amp; \approx \frac{0.18268}{13.43327} \\
&amp; \approx 0.0136
\end{align*}\]</span></p>
</section>
</section>
</section>
<section id="derivative-of-softmax" class="level1">
<h1>5) Derivative of Softmax</h1>
<p>Write out the derivative of the Softmax for the case where <span class="math inline">\(i=j\)</span> and for the case <span class="math inline">\(i\)</span> does not equal <span class="math inline">\(j\)</span>. Show all work and steps.</p>
<p>Find and paste in the Jacobian for the derivative of the Softmax and assure that your results (for <span class="math inline">\(i=j\)</span> and <span class="math inline">\(i\neq j\)</span>) match the Jacobian values.</p>
<section id="jks-answer-3" class="level2">
<h2 class="anchored" data-anchor-id="jks-answer-3">JK’s answer:</h2>
<p>Reminder:</p>
<p><span class="math display">\[\begin{align*}
\hat{y}_i = \text{softmax}(z_i) = \frac{\text{exp}(z_i)}{\displaystyle \sum_{j=1}^k \text{exp}(z_j)}
\end{align*}\]</span></p>
<p>where <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> go from 1 to <span class="math inline">\(k\)</span>, and <span class="math inline">\(k\)</span> is the number of categories.</p>
<p>Thus, we need <em>two derivatives</em>, for <span class="math inline">\(\frac{\partial \hat{y}_i}{\partial z_j}\)</span>. Cases for when <span class="math inline">\(i=j\)</span> <strong>AND</strong> when <span class="math inline">\(i \neq j\)</span>.</p>
<section id="case-1-ij" class="level3">
<h3 class="anchored" data-anchor-id="case-1-ij"><em>Case 1:</em> <span class="math inline">\(i=j\)</span></h3>
<p>For simpler notation I’m going to state that all summations are <span class="math inline">\(\displaystyle \sum_{j=1}^k\)</span></p>
<p><span class="math display">\[\begin{align*}
\frac{\partial \hat{y}_i}{\partial z_j} &amp;= \frac{(e^{z_i})(\sum e^{z_j}) - (e^{z_i})(e^{z_i})}{(\sum e^{z_j})^2}\\[6pt]
&amp;=
\frac{e^{z_i}}{\sum e^{z_j}} \cdot \left(1 - \frac{e^{z_i}}{\sum e^{z_j}}\right) \\[8pt]

\Rightarrow
\frac{\partial \hat{y}_i}{\partial z_j} &amp;= \hat{y}_i(1 - \hat{y}_i)
\end{align*}\]</span></p>
</section>
<section id="case-2-i-neq-j" class="level3">
<h3 class="anchored" data-anchor-id="case-2-i-neq-j"><em>Case 2:</em> <span class="math inline">\(i \neq j\)</span></h3>
<p><span class="math display">\[\begin{align*}
\frac{\partial \hat{y}_i}{\partial z_j} &amp;= \frac{(0)(\sum e^{z_j}) - (e^{z_j})(e^{z_i})}{(\sum e^{z_j})^2}\\[6pt]
&amp;=
\frac{-e^{z_j}e^{z_i}}{(\sum e^{z_j})^2}\\[6pt]

&amp;=
\frac{e^{z_i}}{\sum e^{z_j}} \cdot \left(- \frac{e^{z_j}}{\sum e^{z_j}}\right) \\[8pt]

\Rightarrow
\frac{\partial \hat{y}_i}{\partial z_j} &amp;= \hat{y}_i(- \hat{y}_j) = -\hat{y}_i\hat{y}_j
\end{align*}\]</span></p>
</section>
<section id="jacobian" class="level3">
<h3 class="anchored" data-anchor-id="jacobian">Jacobian</h3>
<p>(For the case in this assignment)</p>
<p><span class="math display">\[\begin{align*}
J &amp;=
\begin{bmatrix}
\frac{\partial \hat{y}_1}{\partial z_1} &amp; \frac{\partial \hat{y}_1}{\partial z_2} &amp; \frac{\partial \hat{y}_1}{\partial z_3} &amp; \frac{\partial \hat{y}_1}{\partial z_4} \\[6pt]

\frac{\partial \hat{y}_2}{\partial z_1} &amp; \frac{\partial \hat{y}_2}{\partial z_2} &amp; \frac{\partial \hat{y}_2}{\partial z_3} &amp; \frac{\partial \hat{y}_2}{\partial z_4} \\[6pt]

\frac{\partial \hat{y}_3}{\partial z_1} &amp; \frac{\partial \hat{y}_3}{\partial z_2} &amp; \frac{\partial \hat{y}_3}{\partial z_3} &amp; \frac{\partial \hat{y}_3}{\partial z_4} \\[6pt]

\frac{\partial \hat{y}_4}{\partial z_1} &amp; \frac{\partial \hat{y}_4}{\partial z_2} &amp; \frac{\partial \hat{y}_4}{\partial z_3} &amp; \frac{\partial \hat{y}_4}{\partial z_4} \\[6pt]
\end{bmatrix}\\

&amp;=
\begin{bmatrix}
\hat{y}_1(1-\hat{y}_1) &amp; -\hat{y}_1\hat{y}_2 &amp; -\hat{y}_1\hat{y}_3 &amp; -\hat{y}_1\hat{y}_4 \\[4pt]

-\hat{y}_2\hat{y}_1 &amp; \hat{y}_2(1-\hat{y}_2) &amp; -\hat{y}_2\hat{y}_3 &amp; -\hat{y}_2\hat{y}_4 \\[4pt]

-\hat{y}_3\hat{y}_1 &amp; -\hat{y}_3\hat{y}_2 &amp; \hat{y}_3(1-\hat{y}_3) &amp; -\hat{y}_3\hat{y}_4 \\[4pt]

-\hat{y}_4\hat{y}_1 &amp; -\hat{y}_4\hat{y}_2 &amp; -\hat{y}_4\hat{y}_3 &amp; \hat{y}_4(1-\hat{y}_4) \\[4pt]
\end{bmatrix}\\

\end{align*}\]</span></p>
Lecture slide on Jacobians is as follows:
<div>
<p><img src="j_lecture.PNG" width="800"></p>
</div>
<p>The jacobian matrix matches the above calculations.</p>
</section>
</section>
</section>
<section id="combination" class="level1">
<h1>6) Combination</h1>
<p>When using the combination of Softmax, Categorical Cross Entropy, and One-Hot Encoding, what is <span class="math inline">\(\frac{\partial L}{\partial Z_2}\)</span> (sometimes written as <span class="math inline">\(\frac{dL}{dz}\)</span> or as “Error” on the slides)?</p>
<p>(Just give the final answer here. You are not required to show work for this part.)</p>
<p><strong><em>JK:</em></strong></p>
<p><span class="math inline">\(\frac{\partial L}{\partial Z_2} = \hat{y} - y\)</span></p>
</section>
<section id="code" class="level1">
<h1>7) Code</h1>
<p>In <strong>(1)</strong> above, you created a NN architecture for 3D data, a 4-category label (0,1,2,3), one hidden layer (with two units). Write code for this NN without using any NN packages (so by hand)</p>
<ul>
<li>Use One-hot Encoding for the label.</li>
<li>For the Loss Fucntion, use Categorical Cross Entropy.</li>
<li>For the activation from <span class="math inline">\(Z_1\)</span> to <span class="math inline">\(H_1\)</span>, use reLU.</li>
<li>Run it for 1000 epochs and include the same visualizations as were required in <strong>Part 1 <em>(5)</em></strong> above.</li>
<li><strong>YOU</strong> create the dataset and also submit the dataset with your submission.</li>
<li>Be sure to illustrate some of your results so that the viewer can see what you did.</li>
</ul>
<p>This code will help (a lot). Here again, I suggest that you try not to use my code :) Try to write this yourself and only use my code as a reference or if you must. (<a href="https://gatesboltonanalytics.com/?page_id=707">Prof Gates’ Code Example</a>)</p>
<p>(Note - my code uses Sigmoid - not reLU - so you will need to make some updates anyway).</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#%% libraries</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> LabelEncoder,MinMaxScaler,StandardScaler,OneHotEncoder</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix  </span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># %%</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> OneLayer_MultiOutput():</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>                 data_filename <span class="op">=</span> <span class="st">"A2_Part2_Data_JasmineKobayashi.csv"</span>, </span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>                 data_normalizer <span class="op">=</span> StandardScaler(),</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>                 hidden_units<span class="op">=</span> <span class="dv">2</span>,</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>                 randomize_initial_parameters <span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>                 verbose <span class="op">=</span> <span class="va">True</span>):</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Read Data</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.df <span class="op">=</span> pd.read_csv(data_filename)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># create X and y, assuming data has column "LABEL" as column with labels</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        <span class="co">## X matrix</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> data_normalizer <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.data_scaler <span class="op">=</span> data_normalizer</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>            X <span class="op">=</span> <span class="va">self</span>.data_scaler.fit_transform(<span class="va">self</span>.df.drop(columns<span class="op">=</span><span class="st">"LABEL"</span>,axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"X was normalized using </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(data_normalizer))</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.data_scaler <span class="op">=</span> <span class="va">None</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>            X <span class="op">=</span> np.array(<span class="va">self</span>.df.drop(columns<span class="op">=</span>[<span class="st">'LABEL'</span>],axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"X was not normalized"</span>)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.X <span class="op">=</span> X</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>        <span class="co">## y</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.OHE <span class="op">=</span> OneHotEncoder()</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.y <span class="op">=</span> <span class="va">self</span>.OHE.fit_transform(<span class="va">self</span>.df[[<span class="st">'LABEL'</span>]]).toarray()</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> verbose:</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Shape of X:"</span>, <span class="va">self</span>.X.shape)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"X matrix: </span><span class="ch">\n</span><span class="st">"</span>, <span class="va">self</span>.X)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Shape of y:"</span>, <span class="va">self</span>.y.shape)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"y vector: </span><span class="ch">\n</span><span class="st">"</span>, <span class="va">self</span>.y)</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Shape of W1</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>        w1_ncol <span class="op">=</span> hidden_units        <span class="co"># Number of W1 cols should be number of hidden units in H1</span></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>        w1_nrow <span class="op">=</span> <span class="va">self</span>.X.shape[<span class="dv">1</span>]     <span class="co"># Number of W1 rows should be number of X columns (dim of X)</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Shape of B (column vector)</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>        b_length <span class="op">=</span> <span class="va">self</span>.X.shape[<span class="dv">0</span>]    <span class="co"># length of X rows</span></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Shape of W2:</span></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>        w2_ncol <span class="op">=</span> <span class="va">self</span>.y.shape[<span class="dv">1</span>]                 <span class="co"># Number of W2 cols should be number of outputs (in this case, only one output)</span></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>        w2_nrow <span class="op">=</span> hidden_units        <span class="co"># Number of W2 rows should be number of hidden units in H1</span></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Shape of C (column vector)</span></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>        c_length <span class="op">=</span> <span class="va">self</span>.X.shape[<span class="dv">0</span>]    <span class="co"># length of X rows</span></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> randomize_initial_parameters: </span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>            <span class="co"># </span><span class="al">TODO</span><span class="co">: Finish potential code of randomized parameters</span></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>            <span class="co"># # bounds of randomized weights</span></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>            <span class="co"># bound = 1/np.sqrt(w1_nrow)   # I found online that supposedly this is a standard range for randomized weights</span></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>            <span class="co"># rand_w = [np.random.uniform(-bound,bound) for i in range(w1_ncol)]</span></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>            <span class="co"># self.W1 = np.array([rand_w]*w1_nrow)</span></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>            <span class="co"># self.W2 = np.array([]*w2_nrow)</span></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>            <span class="co"># print('All parameters were randomized')</span></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>            <span class="cf">pass</span></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:   <span class="co"># These are the parameters defined by the question 2 in Assignment2</span></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.W1 <span class="op">=</span> np.array([[<span class="dv">1</span>]<span class="op">*</span>w1_ncol]<span class="op">*</span>w1_nrow) <span class="co"># otherwise, all weights of W1 = 1</span></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.B <span class="op">=</span> np.array([[<span class="dv">0</span>]]<span class="op">*</span>b_length)         <span class="co"># otherwise, all bias (B) = 0</span></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.W2 <span class="op">=</span> np.array([[<span class="dv">2</span>]<span class="op">*</span>w2_ncol]<span class="op">*</span>w2_nrow) <span class="co"># otherwise, all weights of W2 = 2</span></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.C <span class="op">=</span> np.array([[<span class="dv">0</span>]]<span class="op">*</span>c_length)</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Parameters built for simple calculation (not randomized)"</span>)</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> verbose:</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>            <span class="co"># print("# of X cols (should be # of W1 rows):", self.X.shape[1])</span></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>            <span class="co"># print("# of hidden units (should be # of W1 cols):", hidden_units)</span></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"W1 has shape:"</span>, <span class="va">self</span>.W1.shape)</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"W1 matrix: </span><span class="ch">\n</span><span class="st">"</span>, <span class="va">self</span>.W1)</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Shape of B:"</span>, <span class="va">self</span>.B.shape)</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"B (bias vector for Z1): </span><span class="ch">\n</span><span class="st">"</span>, <span class="va">self</span>.B)</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Shape of W2:"</span>,<span class="va">self</span>.W2.shape)</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"W2 matrix: </span><span class="ch">\n</span><span class="st">"</span>, <span class="va">self</span>.W2)</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Shape of C :"</span>, <span class="va">self</span>.C.shape)</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"C (bias vector for Z2): </span><span class="ch">\n</span><span class="st">"</span>,<span class="va">self</span>.C)</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> MSE_loss(<span class="va">self</span>,y_hat,y):</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>        <span class="co"># L = mean((y_hat - y)^2)</span></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.mean((y_hat <span class="op">-</span> y)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> CCE_loss(<span class="va">self</span>,y_hat,y):</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>        <span class="co"># each row of y and y^ have to be passed individually</span></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>        <span class="co"># loss=[]</span></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>        <span class="co"># for j in range(y.shape[0]):</span></span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     loss.append(-y[j]*np.log(y_hat[j]))</span></span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.mean(<span class="op">-</span>y <span class="op">*</span> np.log(y_hat))</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> lin_eq(<span class="va">self</span>,X,W,B):</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> X.shape[<span class="dv">1</span>] <span class="op">==</span> W.shape[<span class="dv">0</span>], <span class="st">"Linear eq: z = X @ W + B. Shape of X: </span><span class="sc">{}</span><span class="st">; Shape of W: </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(X.shape,W.shape)</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> B.shape[<span class="dv">0</span>] <span class="op">==</span> X.shape[<span class="dv">0</span>], <span class="st">"Linear eq: z = X @ W + B. Length of B should match X rows; Length B = </span><span class="sc">{}</span><span class="st">; Shape of X = </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(B.shape[<span class="dv">0</span>],X.shape)</span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> X <span class="op">@</span> W <span class="op">+</span> B</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> Sigmoid(<span class="va">self</span>,z,</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>                derivative <span class="op">=</span> <span class="va">False</span>):</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> derivative:</span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>.Sigmoid(z) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.Sigmoid(z)) <span class="co"># dS/dz = S(z)(1-S(z))</span></span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Sigmoid</span></span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span> <span class="op">+</span> math.e<span class="op">**</span>(<span class="op">-</span>z)) <span class="co"># S(z) = 1/(1 + e^(-z))</span></span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reLU(<span class="va">self</span>,z,</span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>             derivative <span class="op">=</span> <span class="va">False</span>):</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>        relulist <span class="op">=</span> []</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> row <span class="kw">in</span> z:</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> derivative:</span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a>                relulist.append([<span class="dv">1</span> <span class="cf">if</span> value <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">0</span> <span class="cf">for</span> value <span class="kw">in</span> row])</span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a>                relulist.append([<span class="bu">max</span>(<span class="fl">0.0</span>,value) <span class="cf">for</span> value <span class="kw">in</span> row])</span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.array(relulist)</span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> softmax(<span class="va">self</span>,Z2):</span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a>        <span class="co"># SMlist = []</span></span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a>        <span class="co"># for k in range(Z2.shape[0]):</span></span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     z2_row = Z2[k]</span></span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     smrow = []</span></span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     for l in range(Z2.shape[1]):</span></span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a>        <span class="co">#         denominator = np.sum(math.e**(z2_row))</span></span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a>        <span class="co">#         SMz2 = (math.e**z2_row[l])/denominator</span></span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a>        <span class="co">#         smrow.append(SMz2)</span></span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     SMlist.append(smrow)</span></span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return np.array(SMlist)</span></span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a>        expZ <span class="op">=</span> np.exp(Z2)</span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>        SM<span class="op">=</span>expZ<span class="op">/</span>np.<span class="bu">sum</span>(expZ, axis<span class="op">=</span><span class="dv">1</span>)[:,<span class="va">None</span>]</span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> SM </span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> FeedForward(<span class="va">self</span>,</span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a>                    activation <span class="op">=</span> <span class="st">"reLU"</span>,</span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>                    verbose <span class="op">=</span> <span class="va">True</span>):</span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create H1</span></span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Z1 = X @ W1 + B</span></span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Z1 <span class="op">=</span> <span class="va">self</span>.lin_eq(<span class="va">self</span>.X,<span class="va">self</span>.W1,<span class="va">self</span>.B)</span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> verbose:</span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Shape of Z1:"</span>,<span class="va">self</span>.Z1.shape)</span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Z1 matrix: </span><span class="ch">\n</span><span class="st">"</span>,<span class="va">self</span>.Z1)</span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.H1 <span class="op">=</span> <span class="va">self</span>.reLU(<span class="va">self</span>.Z1)</span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">TODO</span><span class="co">: Add flexibility with other activation functions</span></span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> verbose: </span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Activation function:"</span>,activation)</span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"H1 matrix: </span><span class="ch">\n</span><span class="st">"</span>, <span class="va">self</span>.H1)</span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Z2 and y-hat</span></span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Z2 = H1 @ W2 + C</span></span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Z2 <span class="op">=</span> <span class="va">self</span>.lin_eq(<span class="va">self</span>.H1,<span class="va">self</span>.W2,<span class="va">self</span>.C)</span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.y_hat <span class="op">=</span> <span class="va">self</span>.softmax(<span class="va">self</span>.Z2)</span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> verbose:</span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Shape of Z2:"</span>, <span class="va">self</span>.Z2.shape)</span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Z2 matrix: </span><span class="ch">\n</span><span class="st">"</span>, <span class="va">self</span>.Z2)</span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"y_hat:</span><span class="ch">\n</span><span class="st">"</span>,<span class="va">self</span>.y_hat)</span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Gradient descent----------------------------------------------</span></span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> grad_desc(<span class="va">self</span>,y_hat,y,LR,</span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a>                  verbose <span class="op">=</span> <span class="va">True</span>):</span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a>        <span class="co"># dL/dZ2  = [y^-y]</span></span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dL_dZ2 <span class="op">=</span> y_hat <span class="op">-</span> y                                   <span class="co"># shape =  [n x 1]</span></span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dH_dZ1 <span class="op">=</span> <span class="va">self</span>.reLU(<span class="va">self</span>.Z1,derivative<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a>        <span class="co"># dL/dC = [dL/dy^][dy^/dZ2][dZ2/C] </span></span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a>        <span class="co">#       = [y^-y][S(Z2)(1 - S(Z2))][1]</span></span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dZ2_dC <span class="op">=</span> np.array([[<span class="dv">1</span>]]<span class="op">*</span><span class="va">self</span>.C.shape[<span class="dv">0</span>])           <span class="co"># vector of 1s with same shape of C</span></span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dL_dC <span class="op">=</span> <span class="va">self</span>.dL_dZ2 <span class="op">*</span> <span class="va">self</span>.dZ2_dC                            <span class="co"># should match shape of C (n x 1)</span></span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a>        <span class="co"># I don't see a good way to make these matrices match the shape of C, </span></span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a>        <span class="co">#  so instead I'm going to redefine dL/dC to be a single column vector with the averages </span></span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a>        <span class="co">#  of each row from the above calculation</span></span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dL_dC <span class="op">=</span> np.array([[np.mean(<span class="va">self</span>.dL_dC)]]<span class="op">*</span><span class="va">self</span>.C.shape[<span class="dv">0</span>])</span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a>        <span class="co"># dL/dW2 = [dL/dy^][dy^/dZ2][dZ2/W2] </span></span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a>        <span class="co">#        = [y^-y][S(Z2)(1 - S(Z2))][H1]</span></span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dL_dW2 <span class="op">=</span>  <span class="va">self</span>.H1.T <span class="op">@</span> <span class="va">self</span>.dL_dZ2                         <span class="co"># should match shape of W2 </span></span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a>        <span class="co"># dL/dB = [dL/dy^][dy^/dZ2][dZ2/dH1][dH1/dZ1][dZ1/dB] </span></span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a>        <span class="co">#       = [dL/dZ2][gamma][W2][1]</span></span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a>        <span class="co">#       = [y^-y] * ([gamma] @ [W2]) * [1] </span></span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Gamma <span class="op">=</span> <span class="va">self</span>.reLU(<span class="va">self</span>.Z1,derivative <span class="op">=</span> <span class="va">True</span>)      <span class="co"># should match shape of Z1 (n x 2)</span></span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dZ1_dB <span class="op">=</span> np.array([[<span class="dv">1</span>]]<span class="op">*</span><span class="va">self</span>.B.shape[<span class="dv">0</span>])           <span class="co"># vector of 1s with same shape of B</span></span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dL_dB <span class="op">=</span> <span class="va">self</span>.dL_dZ2 <span class="op">*</span> (<span class="va">self</span>.Gamma <span class="op">@</span> <span class="va">self</span>.W2)  <span class="op">*</span> <span class="va">self</span>.dZ1_dB        <span class="co"># should match shape of B (n x 1)</span></span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a>        <span class="co"># I have the same problem with dL/dB as with dL/dC, so I will do the same thing as did there</span></span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a>        <span class="co">#self.dL_dB = np.array([[np.mean(row)] for row in self.dL_dB])</span></span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dL_dB <span class="op">=</span> np.array([[np.mean(<span class="va">self</span>.dL_dB)]]<span class="op">*</span><span class="va">self</span>.B.shape[<span class="dv">0</span>])</span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a>        <span class="co"># dL/dW1 = [dL/dy^][dy^/dZ2][dZ2/dH1][dH1/dZ1][dZ1/dW1] </span></span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a>        <span class="co">#        = [y^-y][S(Z2)(1 - S(Z2))][W2][S(Z1)(1-S(Z1))][X] </span></span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a>        <span class="co">#        = [X]^T @ [([[y^-y] * [S(Z2)(1 - S(Z2))]] @ [W2]) * [S(Z1)(1-S(Z1))]] </span></span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a>        <span class="co">#        = [X]^T @ [([E * Phi] * [W2]^T) * Omega]</span></span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dL_W1 <span class="op">=</span> <span class="va">self</span>.X.T <span class="op">@</span> (( (<span class="va">self</span>.dL_dZ2) <span class="op">@</span> <span class="va">self</span>.W2.T) <span class="op">*</span> <span class="va">self</span>.Gamma)    <span class="co"># should match shape of W1 (3 x 2)</span></span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Gradient descent</span></span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.C <span class="op">=</span> <span class="va">self</span>.C <span class="op">-</span> (LR<span class="op">*</span><span class="va">self</span>.dL_dC)</span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W2 <span class="op">=</span> <span class="va">self</span>.W2 <span class="op">-</span> (LR<span class="op">*</span><span class="va">self</span>.dL_dW2)</span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.B <span class="op">=</span> <span class="va">self</span>.B <span class="op">-</span> (LR<span class="op">*</span><span class="va">self</span>.dL_dB)</span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W1 <span class="op">=</span> <span class="va">self</span>.W1 <span class="op">-</span> (LR<span class="op">*</span><span class="va">self</span>.dL_W1)</span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> verbose:</span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a>            <span class="co"># print("# of X cols (should be # of W1 rows):", self.X.shape[1])</span></span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a>            <span class="co"># print("# of hidden units (should be # of W1 cols):", hidden_units)</span></span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Updated W1 has shape:"</span>, <span class="va">self</span>.W1.shape)</span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Updated W1 matrix: </span><span class="ch">\n</span><span class="st">"</span>, <span class="va">self</span>.W1)</span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Shape of updated B:"</span>, <span class="va">self</span>.B.shape)</span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Updated B (bias vector for Z1): </span><span class="ch">\n</span><span class="st">"</span>, <span class="va">self</span>.B)</span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Shape of updated W2:"</span>,<span class="va">self</span>.W2.shape)</span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Updated W2 matrix: </span><span class="ch">\n</span><span class="st">"</span>, <span class="va">self</span>.W2)</span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Shape of updated  C :"</span>, <span class="va">self</span>.C.shape)</span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Updated C (bias vector for Z2): </span><span class="ch">\n</span><span class="st">"</span>,<span class="va">self</span>.C)</span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Visualizations (confusion matrix &amp; Lce plot)==================</span></span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Confusion matrix-------------------------------------------------------------</span></span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> ConfusionMatrix(<span class="va">self</span>, y_hat,y):</span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.prediction <span class="op">=</span> y_hat</span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.prediction[y_hat &gt;= .5] = 1</span></span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.prediction[y_hat &lt; .5] = 0</span></span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.labels <span class="op">=</span> y</span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cm <span class="op">=</span> confusion_matrix(<span class="va">self</span>.labels, <span class="va">self</span>.prediction)</span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="va">self</span>.cm)</span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a>        plt.figure()</span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a>        ax<span class="op">=</span> plt.subplot()</span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a>        sns.heatmap(<span class="va">self</span>.cm, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">'g'</span>, ax<span class="op">=</span>ax, cmap<span class="op">=</span><span class="st">'Blues'</span>)  </span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a>        <span class="co">#annot=True to annotate cells, ftm='g' to disable scientific notation</span></span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a>        <span class="co"># labels, title and ticks</span></span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a>        ax.set_xlabel(<span class="st">"Predicted labels"</span>)</span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a>        ax.set_ylabel(<span class="st">"True labels"</span>)</span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a>        ax.set_title(<span class="st">"Confusion Matrix"</span>)</span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a>        ax.xaxis.set_ticklabels([<span class="st">"0"</span>, <span class="st">"1"</span>])</span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a>        ax.yaxis.set_ticklabels([<span class="st">"0"</span>, <span class="st">"1"</span>])</span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot of Lce vs. Epochs--------------------------------------------------------</span></span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> plot_LCE(<span class="va">self</span>):</span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a>        plt.figure()</span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a>        plt.plot(np.arange(<span class="dv">0</span>,<span class="bu">len</span>(<span class="va">self</span>.loss_record)),<span class="va">self</span>.loss_record,<span class="st">'-'</span>,label<span class="op">=</span><span class="st">"LR = </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(<span class="va">self</span>.LR))</span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a>        plt.xlabel(<span class="st">"epochs"</span>)</span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a>        plt.ylabel(<span class="st">'Loss'</span>)</span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a>        plt.title(<span class="st">"Loss over epochs"</span>)</span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a>        plt.legend()</span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Run multiple iterations (with specified epochs, etc.)========================================================</span></span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> run_model(<span class="va">self</span>, </span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a>                  epochs,</span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a>                  LR<span class="op">=</span> <span class="dv">1</span>,</span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a>                  hidden_units<span class="op">=</span> <span class="dv">4</span>,</span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a>                  randomize_parameters <span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a>                  activation <span class="op">=</span> <span class="st">"sigmoid"</span>, </span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a>                  verbose <span class="op">=</span> <span class="va">True</span>):</span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Save LCE values (for plotting)------------------------------------</span></span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.loss_record <span class="op">=</span> []</span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a>        <span class="co">#-------------------------------------------------------------------</span></span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.LR <span class="op">=</span> LR</span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Running model for </span><span class="sc">{}</span><span class="st"> epochs..."</span>.<span class="bu">format</span>(epochs))</span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Epoch"</span>,i)</span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.FeedForward(activation<span class="op">=</span> activation,</span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a>                             verbose<span class="op">=</span>verbose)</span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb1-275"><a href="#cb1-275" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.loss_record.append(<span class="va">self</span>.CCE_loss(y_hat<span class="op">=</span><span class="va">self</span>.y_hat,</span>
<span id="cb1-276"><a href="#cb1-276" aria-hidden="true" tabindex="-1"></a>                                                  y<span class="op">=</span><span class="va">self</span>.y))</span>
<span id="cb1-277"><a href="#cb1-277" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> verbose:</span>
<span id="cb1-278"><a href="#cb1-278" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="st">"Loss:"</span>,<span class="va">self</span>.loss_record[i])</span>
<span id="cb1-279"><a href="#cb1-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-280"><a href="#cb1-280" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Update params with gradient descent---------------------------</span></span>
<span id="cb1-281"><a href="#cb1-281" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.grad_desc(y_hat<span class="op">=</span><span class="va">self</span>.y_hat,</span>
<span id="cb1-282"><a href="#cb1-282" aria-hidden="true" tabindex="-1"></a>                           y<span class="op">=</span><span class="va">self</span>.y,</span>
<span id="cb1-283"><a href="#cb1-283" aria-hidden="true" tabindex="-1"></a>                           LR<span class="op">=</span><span class="va">self</span>.LR,</span>
<span id="cb1-284"><a href="#cb1-284" aria-hidden="true" tabindex="-1"></a>                           verbose<span class="op">=</span>verbose)</span>
<span id="cb1-285"><a href="#cb1-285" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Finished running all epochs"</span>)</span>
<span id="cb1-286"><a href="#cb1-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-287"><a href="#cb1-287" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Visualizations</span></span>
<span id="cb1-288"><a href="#cb1-288" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ConfusionMatrix(y_hat<span class="op">=</span><span class="va">self</span>.y_hat,y<span class="op">=</span><span class="va">self</span>.y)</span>
<span id="cb1-289"><a href="#cb1-289" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.plot_LCE()</span>
<span id="cb1-290"><a href="#cb1-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-291"><a href="#cb1-291" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test_model(<span class="va">self</span>,</span>
<span id="cb1-292"><a href="#cb1-292" aria-hidden="true" tabindex="-1"></a>                   test_data,</span>
<span id="cb1-293"><a href="#cb1-293" aria-hidden="true" tabindex="-1"></a>                   verbose <span class="op">=</span> <span class="va">True</span>):</span>
<span id="cb1-294"><a href="#cb1-294" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.test_df <span class="op">=</span> pd.read_csv(test_data)</span>
<span id="cb1-295"><a href="#cb1-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-296"><a href="#cb1-296" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"X is now input from test data"</span>)</span>
<span id="cb1-297"><a href="#cb1-297" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.data_scaler <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb1-298"><a href="#cb1-298" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.X <span class="op">=</span> <span class="va">self</span>.data_scaler.transform(<span class="va">self</span>.test_df.drop(columns<span class="op">=</span><span class="st">"LABEL"</span>,axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb1-299"><a href="#cb1-299" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Test input X was normalized using same normalizer as training (</span><span class="sc">{}</span><span class="st">)"</span>.<span class="bu">format</span>(<span class="va">self</span>.data_scaler))</span>
<span id="cb1-300"><a href="#cb1-300" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb1-301"><a href="#cb1-301" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.X <span class="op">=</span> <span class="va">self</span>.test_df.drop(columns<span class="op">=</span><span class="st">"LABEL"</span>,axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-302"><a href="#cb1-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-303"><a href="#cb1-303" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.y <span class="op">=</span> np.array(<span class="va">self</span>.test_df[[<span class="st">'LABEL'</span>]])</span>
<span id="cb1-304"><a href="#cb1-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-305"><a href="#cb1-305" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> verbose:</span>
<span id="cb1-306"><a href="#cb1-306" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Shape of X:"</span>, <span class="va">self</span>.X.shape)</span>
<span id="cb1-307"><a href="#cb1-307" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"X matrix: </span><span class="ch">\n</span><span class="st">"</span>, <span class="va">self</span>.X)</span>
<span id="cb1-308"><a href="#cb1-308" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Shape of y:"</span>, <span class="va">self</span>.y.shape)</span>
<span id="cb1-309"><a href="#cb1-309" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"y vector: </span><span class="ch">\n</span><span class="st">"</span>, <span class="va">self</span>.y)</span>
<span id="cb1-310"><a href="#cb1-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-311"><a href="#cb1-311" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Testing model with test data"</span>)</span>
<span id="cb1-312"><a href="#cb1-312" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.FeedForward()</span>
<span id="cb1-313"><a href="#cb1-313" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Finished testing model"</span>)</span>
<span id="cb1-314"><a href="#cb1-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-315"><a href="#cb1-315" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ConfusionMatrix(y_hat<span class="op">=</span><span class="va">self</span>.y_hat,</span>
<span id="cb1-316"><a href="#cb1-316" aria-hidden="true" tabindex="-1"></a>                             y<span class="op">=</span><span class="va">self</span>.y)</span>
<span id="cb1-317"><a href="#cb1-317" aria-hidden="true" tabindex="-1"></a>        </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>NN <span class="op">=</span> OneLayer_MultiOutput()</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>NN.FeedForward()</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>NN.grad_desc(y_hat<span class="op">=</span>NN.y_hat,y<span class="op">=</span>NN.y,LR<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>X was normalized using StandardScaler()
Shape of X: (16, 3)
X matrix: 
 [[ 0.92819243 -1.27475541  1.61048338]
 [ 0.30643674 -0.63663915 -0.75580778]
 [ 0.46631677  0.56869158 -0.2328705 ]
 [-1.87859043  1.25407572 -0.88218428]
 [ 0.87489909 -1.20385361  1.48410687]
 [ 0.27090784 -0.61300521 -0.55099068]
 [ 0.50184567  0.68686126  0.06781843]
 [ 0.96372133 -1.22748754  1.66713492]
 [ 0.43078788  0.49778977 -0.17186116]
 [-1.6121237   1.39587933 -0.98677174]
 [-1.48777256  1.23044178 -0.97369831]
 [ 0.94595688 -1.34565722  1.78479581]
 [-1.73647484  1.20680785 -1.03470766]
 [ 0.32420118 -0.68390702 -0.46383447]
 [ 0.44855232  0.7104952  -0.0236956 ]
 [ 0.25314339 -0.56573734 -0.53791725]]
Shape of y: (16, 4)
y vector: 
 [[0. 1. 0. 0.]
 [0. 0. 0. 1.]
 [1. 0. 0. 0.]
 [0. 0. 1. 0.]
 [0. 1. 0. 0.]
 [0. 0. 0. 1.]
 [1. 0. 0. 0.]
 [0. 1. 0. 0.]
 [1. 0. 0. 0.]
 [0. 0. 1. 0.]
 [0. 0. 1. 0.]
 [0. 1. 0. 0.]
 [0. 0. 1. 0.]
 [0. 0. 0. 1.]
 [1. 0. 0. 0.]
 [0. 0. 0. 1.]]
Parameters built for simple calculation (not randomized)
W1 has shape: (3, 2)
W1 matrix: 
 [[1 1]
 [1 1]
 [1 1]]
Shape of B: (16, 1)
B (bias vector for Z1): 
 [[0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]]
Shape of W2: (2, 4)
W2 matrix: 
 [[2 2 2 2]
 [2 2 2 2]]
Shape of C : (16, 1)
C (bias vector for Z2): 
 [[0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]]
Shape of Z1: (16, 2)
Z1 matrix: 
 [[ 1.2639204   1.2639204 ]
 [-1.08601019 -1.08601019]
 [ 0.80213785  0.80213785]
 [-1.50669899 -1.50669899]
 [ 1.15515236  1.15515236]
 [-0.89308805 -0.89308805]
 [ 1.25652536  1.25652536]
 [ 1.40336871  1.40336871]
 [ 0.75671649  0.75671649]
 [-1.2030161  -1.2030161 ]
 [-1.23102908 -1.23102908]
 [ 1.38509547  1.38509547]
 [-1.56437465 -1.56437465]
 [-0.8235403  -0.8235403 ]
 [ 1.13535192  1.13535192]
 [-0.85051119 -0.85051119]]
Activation function: reLU
H1 matrix: 
 [[1.2639204  1.2639204 ]
 [0.         0.        ]
 [0.80213785 0.80213785]
 [0.         0.        ]
 [1.15515236 1.15515236]
 [0.         0.        ]
 [1.25652536 1.25652536]
 [1.40336871 1.40336871]
 [0.75671649 0.75671649]
 [0.         0.        ]
 [0.         0.        ]
 [1.38509547 1.38509547]
 [0.         0.        ]
 [0.         0.        ]
 [1.13535192 1.13535192]
 [0.         0.        ]]
Shape of Z2: (16, 4)
Z2 matrix: 
 [[5.05568161 5.05568161 5.05568161 5.05568161]
 [0.         0.         0.         0.        ]
 [3.2085514  3.2085514  3.2085514  3.2085514 ]
 [0.         0.         0.         0.        ]
 [4.62060942 4.62060942 4.62060942 4.62060942]
 [0.         0.         0.         0.        ]
 [5.02610143 5.02610143 5.02610143 5.02610143]
 [5.61347483 5.61347483 5.61347483 5.61347483]
 [3.02686597 3.02686597 3.02686597 3.02686597]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [5.54038187 5.54038187 5.54038187 5.54038187]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [4.5414077  4.5414077  4.5414077  4.5414077 ]
 [0.         0.         0.         0.        ]]
y_hat:
 [[0.25 0.25 0.25 0.25]
 [0.25 0.25 0.25 0.25]
 [0.25 0.25 0.25 0.25]
 [0.25 0.25 0.25 0.25]
 [0.25 0.25 0.25 0.25]
 [0.25 0.25 0.25 0.25]
 [0.25 0.25 0.25 0.25]
 [0.25 0.25 0.25 0.25]
 [0.25 0.25 0.25 0.25]
 [0.25 0.25 0.25 0.25]
 [0.25 0.25 0.25 0.25]
 [0.25 0.25 0.25 0.25]
 [0.25 0.25 0.25 0.25]
 [0.25 0.25 0.25 0.25]
 [0.25 0.25 0.25 0.25]
 [0.25 0.25 0.25 0.25]]
Updated W1 has shape: (3, 2)
Updated W1 matrix: 
 [[1. 1.]
 [1. 1.]
 [1. 1.]]
Shape of updated B: (16, 1)
Updated B (bias vector for Z1): 
 [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Shape of updated W2: (2, 4)
Updated W2 matrix: 
 [[ 3.66116448  4.91796979 -0.28956714 -0.28956714]
 [ 3.66116448  4.91796979 -0.28956714 -0.28956714]]
Shape of updated  C : (16, 1)
Updated C (bias vector for Z2): 
 [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span><span class="dv">10</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    NN.FeedForward()</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    NN.grad_desc(y_hat<span class="op">=</span>NN.y_hat,y<span class="op">=</span>NN.y,LR<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Shape of Z1: (16, 2)
Z1 matrix: 
 [[ 1.2639204   1.2639204 ]
 [-1.08601019 -1.08601019]
 [ 0.80213785  0.80213785]
 [-1.50669899 -1.50669899]
 [ 1.15515236  1.15515236]
 [-0.89308805 -0.89308805]
 [ 1.25652536  1.25652536]
 [ 1.40336871  1.40336871]
 [ 0.75671649  0.75671649]
 [-1.2030161  -1.2030161 ]
 [-1.23102908 -1.23102908]
 [ 1.38509547  1.38509547]
 [-1.56437465 -1.56437465]
 [-0.8235403  -0.8235403 ]
 [ 1.13535192  1.13535192]
 [-0.85051119 -0.85051119]]
Activation function: reLU
H1 matrix: 
 [[1.2639204  1.2639204 ]
 [0.         0.        ]
 [0.80213785 0.80213785]
 [0.         0.        ]
 [1.15515236 1.15515236]
 [0.         0.        ]
 [1.25652536 1.25652536]
 [1.40336871 1.40336871]
 [0.75671649 0.75671649]
 [0.         0.        ]
 [0.         0.        ]
 [1.38509547 1.38509547]
 [0.         0.        ]
 [0.         0.        ]
 [1.13535192 1.13535192]
 [0.         0.        ]]
Shape of Z2: (16, 4)
Z2 matrix: 
 [[ 9.25484097 12.43184471 -0.73197963 -0.73197963]
 [ 0.          0.          0.          0.        ]
 [ 5.87351721  7.88977942 -0.46454552 -0.46454552]
 [ 0.          0.          0.          0.        ]
 [ 8.45840556 11.36200879 -0.66898833 -0.66898833]
 [ 0.          0.          0.          0.        ]
 [ 9.20069202 12.3591075  -0.7276969  -0.7276969 ]
 [10.27592734 13.80344983 -0.81273892 -0.81273892]
 [ 5.5409271   7.44301771 -0.43824046 -0.43824046]
 [ 0.          0.          0.          0.        ]
 [ 0.          0.          0.          0.        ]
 [10.14212466 13.62371533 -0.80215626 -0.80215626]
 [ 0.          0.          0.          0.        ]
 [ 0.          0.          0.          0.        ]
 [ 8.31342029 11.16725294 -0.65752122 -0.65752122]
 [ 0.          0.          0.          0.        ]]
y_hat:
 [[4.00401954e-02 9.59956121e-01 1.84193843e-06 1.84193843e-06]
 [2.50000000e-01 2.50000000e-01 2.50000000e-01 2.50000000e-01]
 [1.17457243e-01 8.82127493e-01 2.07631926e-04 2.07631926e-04]
 [2.50000000e-01 2.50000000e-01 2.50000000e-01 2.50000000e-01]
 [5.19751428e-02 9.48013563e-01 5.64701437e-06 5.64701437e-06]
 [2.50000000e-01 2.50000000e-01 2.50000000e-01 2.50000000e-01]
 [4.07608004e-02 9.59235224e-01 1.98791669e-06 1.98791669e-06]
 [2.85391710e-02 9.71459957e-01 4.36209161e-07 4.36209161e-07]
 [1.29786720e-01 8.69556317e-01 3.28481399e-04 3.28481399e-04]
 [2.50000000e-01 2.50000000e-01 2.50000000e-01 2.50000000e-01]
 [2.50000000e-01 2.50000000e-01 2.50000000e-01 2.50000000e-01]
 [2.98405637e-02 9.70158382e-01 5.26946118e-07 5.26946118e-07]
 [2.50000000e-01 2.50000000e-01 2.50000000e-01 2.50000000e-01]
 [2.50000000e-01 2.50000000e-01 2.50000000e-01 2.50000000e-01]
 [5.44827860e-02 9.45503370e-01 6.92195730e-06 6.92195730e-06]
 [2.50000000e-01 2.50000000e-01 2.50000000e-01 2.50000000e-01]]
Updated W1 has shape: (3, 2)
Updated W1 matrix: 
 [[-0.94991379 -0.94991379]
 [-2.08197147 -2.08197147]
 [ 1.69642389  1.69642389]]
Shape of updated B: (16, 1)
Updated B (bias vector for Z1): 
 [[-0.13756248]
 [-0.13756248]
 [-0.13756248]
 [-0.13756248]
 [-0.13756248]
 [-0.13756248]
 [-0.13756248]
 [-0.13756248]
 [-0.13756248]
 [-0.13756248]
 [-0.13756248]
 [-0.13756248]
 [-0.13756248]
 [-0.13756248]
 [-0.13756248]
 [-0.13756248]]
Shape of updated W2: (2, 4)
Updated W2 matrix: 
 [[ 7.1143635   1.46564211 -0.29000281 -0.29000281]
 [ 7.1143635   1.46564211 -0.29000281 -0.29000281]]
Shape of updated  C : (16, 1)
Updated C (bias vector for Z2): 
 [[-3.46944695e-18]
 [-3.46944695e-18]
 [-3.46944695e-18]
 [-3.46944695e-18]
 [-3.46944695e-18]
 [-3.46944695e-18]
 [-3.46944695e-18]
 [-3.46944695e-18]
 [-3.46944695e-18]
 [-3.46944695e-18]
 [-3.46944695e-18]
 [-3.46944695e-18]
 [-3.46944695e-18]
 [-3.46944695e-18]
 [-3.46944695e-18]
 [-3.46944695e-18]]
Shape of Z1: (16, 2)
Z1 matrix: 
 [[ 4.36680162  4.36680162]
 [-0.38535679 -0.38535679]
 [-2.15956994 -2.15956994]
 [-2.4605719  -2.4605719 ]
 [ 4.05542204  4.05542204]
 [-0.05335596 -0.05335596]
 [-1.92924934 -1.92924934]
 [ 4.3307469   4.3307469 ]
 [-1.87470729 -1.87470729]
 [-3.18634805 -3.18634805]
 [-2.93785657 -2.93785657]
 [ 4.79325023  4.79325023]
 [-2.75590338 -2.75590338]
 [ 0.19148937  0.19148937]
 [-2.08307701 -2.08307701]
 [-0.11271355 -0.11271355]]
Activation function: reLU
H1 matrix: 
 [[4.36680162 4.36680162]
 [0.         0.        ]
 [0.         0.        ]
 [0.         0.        ]
 [4.05542204 4.05542204]
 [0.         0.        ]
 [0.         0.        ]
 [4.3307469  4.3307469 ]
 [0.         0.        ]
 [0.         0.        ]
 [0.         0.        ]
 [4.79325023 4.79325023]
 [0.         0.        ]
 [0.19148937 0.19148937]
 [0.         0.        ]
 [0.         0.        ]]
Shape of Z2: (16, 4)
Z2 matrix: 
 [[ 6.21340282e+01  1.28003367e+01 -2.53276944e+00 -2.53276944e+00]
 [-3.46944695e-18 -3.46944695e-18 -3.46944695e-18 -3.46944695e-18]
 [-3.46944695e-18 -3.46944695e-18 -3.46944695e-18 -3.46944695e-18]
 [-3.46944695e-18 -3.46944695e-18 -3.46944695e-18 -3.46944695e-18]
 [ 5.77034931e+01  1.18875946e+01 -2.35216754e+00 -2.35216754e+00]
 [-3.46944695e-18 -3.46944695e-18 -3.46944695e-18 -3.46944695e-18]
 [-3.46944695e-18 -3.46944695e-18 -3.46944695e-18 -3.46944695e-18]
 [ 6.16210153e+01  1.26946500e+01 -2.51185750e+00 -2.51185750e+00]
 [-3.46944695e-18 -3.46944695e-18 -3.46944695e-18 -3.46944695e-18]
 [-3.46944695e-18 -3.46944695e-18 -3.46944695e-18 -3.46944695e-18]
 [-3.46944695e-18 -3.46944695e-18 -3.46944695e-18 -3.46944695e-18]
 [ 6.82018490e+01  1.40503787e+01 -2.78011203e+00 -2.78011203e+00]
 [-3.46944695e-18 -3.46944695e-18 -3.46944695e-18 -3.46944695e-18]
 [ 2.72465004e+00  5.61309782e-01 -1.11064912e-01 -1.11064912e-01]
 [-3.46944695e-18 -3.46944695e-18 -3.46944695e-18 -3.46944695e-18]
 [-3.46944695e-18 -3.46944695e-18 -3.46944695e-18 -3.46944695e-18]]
y_hat:
 [[1.00000000e+00 3.75534647e-22 8.23316146e-29 8.23316146e-29]
 [2.50000000e-01 2.50000000e-01 2.50000000e-01 2.50000000e-01]
 [2.50000000e-01 2.50000000e-01 2.50000000e-01 2.50000000e-01]
 [2.50000000e-01 2.50000000e-01 2.50000000e-01 2.50000000e-01]
 [1.00000000e+00 1.26592540e-20 8.28243402e-27 8.28243402e-27]
 [2.50000000e-01 2.50000000e-01 2.50000000e-01 2.50000000e-01]
 [2.50000000e-01 2.50000000e-01 2.50000000e-01 2.50000000e-01]
 [1.00000000e+00 5.64351310e-22 1.40425904e-28 1.40425904e-28]
 [2.50000000e-01 2.50000000e-01 2.50000000e-01 2.50000000e-01]
 [2.50000000e-01 2.50000000e-01 2.50000000e-01 2.50000000e-01]
 [2.50000000e-01 2.50000000e-01 2.50000000e-01 2.50000000e-01]
 [1.00000000e+00 3.03609453e-24 1.48910739e-31 1.48910739e-31]
 [2.50000000e-01 2.50000000e-01 2.50000000e-01 2.50000000e-01]
 [8.11494878e-01 9.32736663e-02 4.76157280e-02 4.76157280e-02]
 [2.50000000e-01 2.50000000e-01 2.50000000e-01 2.50000000e-01]
 [2.50000000e-01 2.50000000e-01 2.50000000e-01 2.50000000e-01]]
Updated W1 has shape: (3, 2)
Updated W1 matrix: 
 [[-23.92340227 -23.92340227]
 [ 30.67529904  30.67529904]
 [-32.42009557 -32.42009557]]
Shape of updated B: (16, 1)
Updated B (bias vector for Z1): 
 [[-1.03653892]
 [-1.03653892]
 [-1.03653892]
 [-1.03653892]
 [-1.03653892]
 [-1.03653892]
 [-1.03653892]
 [-1.03653892]
 [-1.03653892]
 [-1.03653892]
 [-1.03653892]
 [-1.03653892]
 [-1.03653892]
 [-1.03653892]
 [-1.03653892]
 [-1.03653892]]
Shape of updated W2: (2, 4)
Updated W2 matrix: 
 [[-10.58724993  18.99400198  -0.29912071  -0.10763134]
 [-10.58724993  18.99400198  -0.29912071  -0.10763134]]
Shape of updated  C : (16, 1)
Updated C (bias vector for Z2): 
 [[-3.46944695e-18]
 [-3.46944695e-18]
 [-3.46944695e-18]
 [-3.46944695e-18]
 [-3.46944695e-18]
 [-3.46944695e-18]
 [-3.46944695e-18]
 [-3.46944695e-18]
 [-3.46944695e-18]
 [-3.46944695e-18]
 [-3.46944695e-18]
 [-3.46944695e-18]
 [-3.46944695e-18]
 [-3.46944695e-18]
 [-3.46944695e-18]
 [-3.46944695e-18]]
Shape of Z1: (16, 2)
Z1 matrix: 
 [[-114.55758854 -114.55758854]
 [  -3.39328407   -3.39328407]
 [  12.80204564   12.80204564]
 [ 110.97538207  110.97538207]
 [-107.0105578  -107.0105578 ]
 [  -8.45852383   -8.45852383]
 [   5.82859989    5.82859989]
 [-115.79425281 -115.79425281]
 [   9.49915468    9.49915468]
 [ 112.34119493  112.34119493]
 [ 103.86760435  103.86760435]
 [-122.80873419 -122.80873419]
 [ 111.07035987  111.07035987]
 [ -14.73402884  -14.73402884]
 [  10.79542944   10.79542944]
 [  -7.00742359   -7.00742359]]
Activation function: reLU
H1 matrix: 
 [[  0.           0.        ]
 [  0.           0.        ]
 [ 12.80204564  12.80204564]
 [110.97538207 110.97538207]
 [  0.           0.        ]
 [  0.           0.        ]
 [  5.82859989   5.82859989]
 [  0.           0.        ]
 [  9.49915468   9.49915468]
 [112.34119493 112.34119493]
 [103.86760435 103.86760435]
 [  0.           0.        ]
 [111.07035987 111.07035987]
 [  0.           0.        ]
 [ 10.79542944  10.79542944]
 [  0.           0.        ]]
Shape of Z2: (16, 4)
Z2 matrix: 
 [[-3.46944695e-18 -3.46944695e-18 -3.46944695e-18 -3.46944695e-18]
 [-3.46944695e-18 -3.46944695e-18 -3.46944695e-18 -3.46944695e-18]
 [-2.71076914e+02  4.86324161e+02 -7.65871400e+00 -2.75580258e+00]
 [-2.34984821e+03  4.21573325e+03 -6.63900705e+01 -2.38888575e+01]
 [-3.46944695e-18 -3.46944695e-18 -3.46944695e-18 -3.46944695e-18]
 [-3.46944695e-18 -3.46944695e-18 -3.46944695e-18 -3.46944695e-18]
 [-1.23417687e+02  2.21416876e+02 -3.48690989e+00 -1.25468000e+00]
 [-3.46944695e-18 -3.46944695e-18 -3.46944695e-18 -3.46944695e-18]
 [-2.01139850e+02  3.60853926e+02 -5.68278781e+00 -2.04481344e+00]
 [-2.37876862e+03  4.26761776e+03 -6.72071563e+01 -2.41828660e+01]
 [-2.19934457e+03  3.94572297e+03 -6.21379034e+01 -2.23588182e+01]
 [-3.46944695e-18 -3.46944695e-18 -3.46944695e-18 -3.46944695e-18]
 [-2.35185932e+03  4.21934127e+03 -6.64468902e+01 -2.39093027e+01]
 [-3.46944695e-18 -3.46944695e-18 -3.46944695e-18 -3.46944695e-18]
 [-2.28587819e+02  4.10096816e+02 -6.45827307e+00 -2.32385301e+00]
 [-3.46944695e-18 -3.46944695e-18 -3.46944695e-18 -3.46944695e-18]]
y_hat:
 [[2.50000000e-001 2.50000000e-001 2.50000000e-001 2.50000000e-001]
 [2.50000000e-001 2.50000000e-001 2.50000000e-001 2.50000000e-001]
 [0.00000000e+000 1.00000000e+000 2.92390616e-215 3.93795555e-213]
 [0.00000000e+000             nan 0.00000000e+000 0.00000000e+000]
 [2.50000000e-001 2.50000000e-001 2.50000000e-001 2.50000000e-001]
 [2.50000000e-001 2.50000000e-001 2.50000000e-001 2.50000000e-001]
 [1.73880986e-150 1.00000000e+000 2.11605533e-098 1.97229623e-097]
 [2.50000000e-001 2.50000000e-001 2.50000000e-001 2.50000000e-001]
 [8.49580603e-245 1.00000000e+000 6.53322970e-160 2.48359128e-158]
 [0.00000000e+000             nan 0.00000000e+000 0.00000000e+000]
 [0.00000000e+000             nan 0.00000000e+000 0.00000000e+000]
 [2.50000000e-001 2.50000000e-001 2.50000000e-001 2.50000000e-001]
 [0.00000000e+000             nan 0.00000000e+000 0.00000000e+000]
 [2.50000000e-001 2.50000000e-001 2.50000000e-001 2.50000000e-001]
 [4.19553309e-278 1.00000000e+000 1.23715266e-181 7.72643422e-180]
 [2.50000000e-001 2.50000000e-001 2.50000000e-001 2.50000000e-001]]
Updated W1 has shape: (3, 2)
Updated W1 matrix: 
 [[nan nan]
 [nan nan]
 [nan nan]]
Shape of updated B: (16, 1)
Updated B (bias vector for Z1): 
 [[nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]]
Shape of updated W2: (2, 4)
Updated W2 matrix: 
 [[ 2.83379797e+01             nan  4.37955421e+02 -1.07631337e-01]
 [ 2.83379797e+01             nan  4.37955421e+02 -1.07631337e-01]]
Shape of updated  C : (16, 1)
Updated C (bias vector for Z2): 
 [[nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]]
Shape of Z1: (16, 2)
Z1 matrix: 
 [[nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]]
Activation function: reLU
H1 matrix: 
 [[0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]]
Shape of Z2: (16, 4)
Z2 matrix: 
 [[nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]]
y_hat:
 [[nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]]
Updated W1 has shape: (3, 2)
Updated W1 matrix: 
 [[nan nan]
 [nan nan]
 [nan nan]]
Shape of updated B: (16, 1)
Updated B (bias vector for Z1): 
 [[nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]]
Shape of updated W2: (2, 4)
Updated W2 matrix: 
 [[nan nan nan nan]
 [nan nan nan nan]]
Shape of updated  C : (16, 1)
Updated C (bias vector for Z2): 
 [[nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]]
Shape of Z1: (16, 2)
Z1 matrix: 
 [[nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]]
Activation function: reLU
H1 matrix: 
 [[0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]]
Shape of Z2: (16, 4)
Z2 matrix: 
 [[nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]]
y_hat:
 [[nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]]
Updated W1 has shape: (3, 2)
Updated W1 matrix: 
 [[nan nan]
 [nan nan]
 [nan nan]]
Shape of updated B: (16, 1)
Updated B (bias vector for Z1): 
 [[nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]]
Shape of updated W2: (2, 4)
Updated W2 matrix: 
 [[nan nan nan nan]
 [nan nan nan nan]]
Shape of updated  C : (16, 1)
Updated C (bias vector for Z2): 
 [[nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]]
Shape of Z1: (16, 2)
Z1 matrix: 
 [[nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]]
Activation function: reLU
H1 matrix: 
 [[0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]]
Shape of Z2: (16, 4)
Z2 matrix: 
 [[nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]]
y_hat:
 [[nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]]
Updated W1 has shape: (3, 2)
Updated W1 matrix: 
 [[nan nan]
 [nan nan]
 [nan nan]]
Shape of updated B: (16, 1)
Updated B (bias vector for Z1): 
 [[nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]]
Shape of updated W2: (2, 4)
Updated W2 matrix: 
 [[nan nan nan nan]
 [nan nan nan nan]]
Shape of updated  C : (16, 1)
Updated C (bias vector for Z2): 
 [[nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]]
Shape of Z1: (16, 2)
Z1 matrix: 
 [[nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]]
Activation function: reLU
H1 matrix: 
 [[0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]]
Shape of Z2: (16, 4)
Z2 matrix: 
 [[nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]]
y_hat:
 [[nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]]
Updated W1 has shape: (3, 2)
Updated W1 matrix: 
 [[nan nan]
 [nan nan]
 [nan nan]]
Shape of updated B: (16, 1)
Updated B (bias vector for Z1): 
 [[nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]]
Shape of updated W2: (2, 4)
Updated W2 matrix: 
 [[nan nan nan nan]
 [nan nan nan nan]]
Shape of updated  C : (16, 1)
Updated C (bias vector for Z2): 
 [[nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]]
Shape of Z1: (16, 2)
Z1 matrix: 
 [[nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]]
Activation function: reLU
H1 matrix: 
 [[0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]]
Shape of Z2: (16, 4)
Z2 matrix: 
 [[nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]]
y_hat:
 [[nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]]
Updated W1 has shape: (3, 2)
Updated W1 matrix: 
 [[nan nan]
 [nan nan]
 [nan nan]]
Shape of updated B: (16, 1)
Updated B (bias vector for Z1): 
 [[nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]]
Shape of updated W2: (2, 4)
Updated W2 matrix: 
 [[nan nan nan nan]
 [nan nan nan nan]]
Shape of updated  C : (16, 1)
Updated C (bias vector for Z2): 
 [[nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]]
Shape of Z1: (16, 2)
Z1 matrix: 
 [[nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]]
Activation function: reLU
H1 matrix: 
 [[0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]]
Shape of Z2: (16, 4)
Z2 matrix: 
 [[nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]]
y_hat:
 [[nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]]
Updated W1 has shape: (3, 2)
Updated W1 matrix: 
 [[nan nan]
 [nan nan]
 [nan nan]]
Shape of updated B: (16, 1)
Updated B (bias vector for Z1): 
 [[nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]]
Shape of updated W2: (2, 4)
Updated W2 matrix: 
 [[nan nan nan nan]
 [nan nan nan nan]]
Shape of updated  C : (16, 1)
Updated C (bias vector for Z2): 
 [[nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]]
Shape of Z1: (16, 2)
Z1 matrix: 
 [[nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]
 [nan nan]]
Activation function: reLU
H1 matrix: 
 [[0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]]
Shape of Z2: (16, 4)
Z2 matrix: 
 [[nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]]
y_hat:
 [[nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]]
Updated W1 has shape: (3, 2)
Updated W1 matrix: 
 [[nan nan]
 [nan nan]
 [nan nan]]
Shape of updated B: (16, 1)
Updated B (bias vector for Z1): 
 [[nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]]
Shape of updated W2: (2, 4)
Updated W2 matrix: 
 [[nan nan nan nan]
 [nan nan nan nan]]
Shape of updated  C : (16, 1)
Updated C (bias vector for Z2): 
 [[nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]
 [nan]]</code></pre>
</div>
</div>
<p><strong>JK:</strong> For some reason I’m getting <code>NaN</code>s when certain amount of epochs for this code.</p>
<p>But I’m afraid this is far as I’ll have to go. I admittedly did not give myself adequate time to finish this assignment. And I have gotten stuck for several hours on this particular code, and am feeling too sick to try to work through it anymore myself.</p>
<p>(I will try my best to get something working by the exam, but as far as this assignment is concerned this is as much as I’m going to have to submit).</p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>