{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Module 2 Assignment - Part 2: Multinomial NN with Softmax, Categorical Cross Entropy, and One-Hot Encoding\"\n",
    "author: \"Jasmine Kobayashi\"\n",
    "format:\n",
    "    html:\n",
    "        code-fold: false\n",
    "execute:\n",
    "    output: true\n",
    "    warning: false\n",
    "toc: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module 2 Assignment - Part 2: Multinomial NN with Softmax, Categorical Cross Entropy, and One-Hot Encoding\n",
    "---\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "This assignment has a two large parts with many sub-parts. Each part will enable you to practice with a neural networks and to practice for Exam 2.\n",
    "\n",
    "This Assignment will TAKE TIME so use all of the time you have to work on it. Please avoid waiting until the last few days as this will not be enough time. You need weeks - not days :)\n",
    "\n",
    "Be sure to show all work and (when requested) to code using Python (and no NN/TF/Keras packages). Do not worry, we will use packages in coming modules but first it is best to learn about what is going on inside the model. \n",
    "\n",
    "You may use examples and code that I have shared as a reference. Using my code will not be considered cheating although I strongly recommend that you write as much (if not all) of your own code so that you learn the concepts more robustly. \n",
    "\n",
    "This is not a team assignment. Please work and code alone. You may discuss concepts, but you cannot share code or work with others. Papers that look too similar with split the grade (first offense) and can suffer less pleasant outcomes for further offenses. Keep it simple and smart - do your own work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Draw NN architecture\n",
    "In Part 1 above, I created (with an illustration) the NN architecture for you. In this part, you will illustrate (draw) the architecture. \n",
    "\n",
    "(a) Your NN architecture (draw this) will expect input vectors of three values (3D data). It will have one hidden layer with two units. It will create four outputs (for four categories), it will use softmax to convert those 4 outputs into a probability distribution, and it will choose the max as the prediction. \n",
    "\n",
    "(b) The hidden layer (H1) will be activated with reLU. \n",
    "\n",
    "Draw the NN.\n",
    "\n",
    "Use my slides and what we did during class lecture to help you. Label everything - including the x's, the W1's, the B, the C, the H1 values, the Z2 values, the y^ values (after softmax), and the final output. Again, the class slides have an exact example of this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***JK:*** *Check Word Document*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Write out all the Feed Forward equations\n",
    "\n",
    "You will have equations for $Z_1$, $H_1$, $Z_2$, $\\hat{Y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JK's answer\n",
    "\n",
    "### $Z_1$\n",
    "\n",
    "\\begin{align*} \n",
    "Z_1 & =  X\\cdot W_1 + B\n",
    "\n",
    "\\\\\n",
    "& = \n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{12} & x_{13} \\\\\n",
    "\\vdots  & \\vdots  & \\vdots  \\\\\n",
    "x_{n1} & x_{n2} & x_{n3} \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix}\n",
    "w_{11} & w_{12}  \\\\\n",
    "w_{21} & w_{22}  \\\\\n",
    "w_{31} & w_{32} \n",
    "\\end{bmatrix}\n",
    "\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\\n",
    "\\vdots \\\\\n",
    "b_n\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\\\\n",
    "\n",
    "& = \n",
    "\\begin{bmatrix}\n",
    "z_{11} & z_{12}  \\\\\n",
    "z_{21} & z_{22}  \\\\\n",
    "\\vdots & \\vdots  \\\\\n",
    "z_{n1} & z_{n2} \n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Where we have:\n",
    "\n",
    "\\begin{align*}\n",
    "& z_{11} = x_{11}w_{11} + x_{12}w_{21} + x_{13}w_{31} + b_1 & & \\\\\n",
    "& z_{12} = x_{11}w_{12} + x_{12}w_{22} + x_{13}w_{32} + b_1 & &\\\\\n",
    "\\end{align*}\n",
    "\n",
    "$$\\vdots$$\n",
    "\n",
    "\\begin{align*}\n",
    "z_{n2} = x_{n1}w_{12} + x_{n2}w_{22} + x_{n3}w_{32} + b_n \n",
    "\\end{align*}\n",
    "\n",
    "In other words, any $Z$ element can be summarized as,\n",
    "\n",
    "$$z_{i,k} = \\sum_j^3 x_{i,j}w_{j,k} + b_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $H_1$\n",
    "\n",
    "We're assuming the first hidden layer uses the reLU activation function.  \n",
    "\n",
    "reLU function:\n",
    "\n",
    "\\begin{equation*}\n",
    "f = \n",
    "\\begin{cases}\n",
    "x   &\\text{if } x > 0 \\\\\n",
    "0   &\\text{else}\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "$h$ is the same value as $x$ if it's positive and 0 otherwise. \n",
    "\n",
    "The $H1$ matrix is as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "H_1 & = \n",
    "\\begin{bmatrix}\n",
    "f(z_{11}) & f(z_{12}) \\\\\n",
    "\\vdots    &   \\vdots  \\\\\n",
    "f(z_{n1}) & f(z_{n2}) \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\\\\n",
    "\n",
    "& = \n",
    "\\begin{bmatrix}\n",
    "h_{11} & h_{12}  \\\\\n",
    "\\vdots & \\vdots  \\\\\n",
    "h_{n1} & h_{n2}  \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "The number of columns in $H_1$ is the number of hidden units in the hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $Z_2$\n",
    "\n",
    "\\begin{align*}\n",
    "Z^{(2)} &= H_1 W^{(2)} + C \\\\\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "h_{11} & h_{12} \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "h_{n1} & h_{n2} \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix}\n",
    "w_{11}^{(2)} & w_{12}^{(2)} & w_{13}^{(2)} & w_{14}^{(2)}  \\\\[6pt]\n",
    "w_{21}^{(2)} & w_{22}^{(2)} & w_{23}^{(2)} & w_{24}^{(2)}  \\\\[6pt]\n",
    "\\end{bmatrix}\n",
    "\n",
    "+\n",
    "\n",
    "\\begin{bmatrix}\n",
    "c_1 \\\\\n",
    "\\vdots \\\\\n",
    "c_n\n",
    "\\end{bmatrix}\\\\\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "z_{11}^{(2)} & z_{12}^{(2)} & z_{13}^{(2)} & z_{14}^{(2)} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "z_{n1}^{(2)} & z_{n2}^{(2)} & z_{n3}^{(2)} & z_{n4}^{(2)}  \n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Where,\n",
    "\n",
    "\\begin{align*}\n",
    "& z_{11}^{(2)} = h_{11}w_{11}^{(2)} + h_{12}w_{21}^{(2)} + c_1 & & \\\\\n",
    "& z_{12}^{(2)} = h_{11}w_{12}^{(2)} + h_{12}w_{22}^{(2)} + c_1 & &\\\\\n",
    "\\end{align*}\n",
    "\n",
    "$$\\vdots$$\n",
    "\n",
    "\\begin{align*}\n",
    "z_{n4}^{(2)} = h_{n1}w_{14}^{(2)} + x_{n2}w_{24}^{(2)} + c_n \n",
    "\\end{align*}\n",
    "\n",
    "Or in other words, each $Z^{(2)}$ element can be summarized as,\n",
    "\n",
    "\\begin{align*}\n",
    "z_{i,l}^{(2)} = \\sum_k^2 h_{i,k}w_{k,l}^{(2)} + c_i\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\mathbf{\\hat{y}}$\n",
    "\n",
    "And lastly we have the predicted output $\\mathbf{\\hat{y}}$.\n",
    "\n",
    "In the case of this architecture, we're expecting 4 outputs. \n",
    "\n",
    "And for this assignment, we said we would use the Softmax function, which is defined as follows.\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{y}_{i,l} = \\text{softmax}(z_{i,l}^{(2)}) = \\frac{\\text{exp}(z_{i,l}^{(2)})}{\\displaystyle \\sum_l^4 \\text{exp}(z_{i,l}^{(2)})}\n",
    "\\end{align*}\n",
    "\n",
    "Where $\\text{exp}(x) = e^x$ (I just used that notation to make the variables easier to see in the fraction.)\n",
    "\n",
    "The summation from $l=1$ to $l=4$ is due to the fact that we have four possible classes/outcomes for this architecture. (So the formula above is specific to this assignment.)\n",
    "\n",
    "The softmax function gives the probability distributions of the possible outcomes. So in this case we would have 4 probabilities of the 4 labels from the softmax. (The probabilities are pdfs too, so the probabilities for each instance are non-negative and all sum up to one.)\n",
    "\n",
    "\\begin{align*}\n",
    "\\Rightarrow\n",
    "\\mathbf{\\hat{y}} &= \n",
    "\\begin{bmatrix}\n",
    "[\\hat{y}_{11} & \\hat{y}_{12} & \\hat{y}_{13} & \\hat{y}_{14}] \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "[\\hat{y}_{n1} & \\hat{y}_{n2} & \\hat{y}_{n3} & \\hat{y}_{n4}] \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "[\\text{softmax}(z_{11}^{(2)}) & \\text{softmax}(z_{12}^{(2)}) & \\text{softmax}(z_{13}^{(2)}) & \\text{softmax}(z_{14}^{(2)})] \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "[\\text{softmax}(z_{n1}^{(2)}) & \\text{softmax}(z_{n2}^{(2)}) & \\text{softmax}(z_{n3}^{(2)}) & \\text{softmax}(z_{n4}^{(2)})]\n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) What are the following derivatives\n",
    "(using reLU to activate $Z_1$ to $H_1$, and using Softmax to activate $Z_2$ to $\\hat{Y}$)\n",
    "\n",
    "**Hint:** The slides show a very similar example. The only difference is that the slides use Sigmoid and not reLU. So you will need to update this part. Start by determining the derivative of the reLU.\n",
    "\n",
    "Use the Categorical Cross Entropy Loss function.\n",
    "\n",
    "Use One-Hot Encoding for $\\mathbf{y}$ (your label)\n",
    "\n",
    "Show all your work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JK's answer:\n",
    "\n",
    "Parameters we want to update via back propagation (and gradient descent):\n",
    "\n",
    "\\begin{aligned}\n",
    "\\begin{array}{ccc}\n",
    "\\hline\n",
    "\\text{Parameter} & \\text{Matrix/Vector form} & \\text{Shape} \\\\\n",
    "\\hline \n",
    "\\\\\n",
    "\n",
    "W^{(1)} & \n",
    "\\begin{bmatrix}\n",
    "w_{11} & w_{12}  \\\\\n",
    "w_{21} & w_{22}  \\\\\n",
    "w_{31} & w_{32} \n",
    "\\end{bmatrix}\n",
    "&\n",
    "[3 \\times 2]\\\\\n",
    "\n",
    "\\\\\n",
    "\\hline \n",
    "\\\\\n",
    "\n",
    "\\mathbf{B} &\n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\\n",
    "\\vdots \\\\\n",
    "b_n\n",
    "\\end{bmatrix}\n",
    "&\n",
    "[n \\times 1]\\\\\n",
    "\n",
    "\\\\\n",
    "\\hline \n",
    "\\\\\n",
    "\n",
    "W^{(2)} & \n",
    "\\begin{bmatrix}\n",
    "w_{11}^{(2)} & w_{12}^{(2)} & w_{13}^{(2)} & w_{14}^{(2)}  \\\\[6pt]\n",
    "w_{21}^{(2)} & w_{22}^{(2)} & w_{23}^{(2)} & w_{24}^{(2)}  \\\\[6pt]\n",
    "\\end{bmatrix}\n",
    "&\n",
    "[2 \\times 4]\\\\\n",
    "\n",
    "\\\\\n",
    "\\hline \n",
    "\\\\\n",
    "\n",
    "\\mathbf{C} &\n",
    "\\begin{bmatrix}\n",
    "c_1 \\\\\n",
    "\\vdots \\\\\n",
    "c_n\n",
    "\\end{bmatrix}\n",
    "&\n",
    "[n \\times 1]\n",
    "\n",
    "\\end{array}\n",
    "\\end{aligned}\n",
    "\n",
    "And I believe for gradient descent to work properly, I think the (final matrix/vector form of the) partial derivatives of each parameter needs to match the shape of the original parameter. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other relevant formulas:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\begin{array}{c|c|c|c}\n",
    "\\hline\n",
    "\\text{Variable (matrix/vector form)}& \\text{Shape} & \\text{Vector/Matrix Formula} & \\text{Element-wise formula} \\\\\n",
    "\\hline \\\\\n",
    "\n",
    "L_{CCE} & \\text{(scalar)} & -log(\\mathbf{\\hat{y}}) & L = -log(\\hat{y_i}) \\\\\n",
    "\\\\\n",
    "\\hline \\\\\n",
    "\n",
    "\\mathbf{\\hat{y}} = \n",
    "\\begin{bmatrix}\n",
    "[\\hat{y}_{11} & \\hat{y}_{12} & \\hat{y}_{13} & \\hat{y}_{14}] \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "[\\hat{y}_{n1} & \\hat{y}_{n2} & \\hat{y}_{n3} & \\hat{y}_{n4}] \\\\\n",
    "\\end{bmatrix}\n",
    "&\n",
    "[n \\times 4]\n",
    "&\n",
    "\\mathbf{\\hat{y}} = \\text{softmax}(Z^{(2)})\n",
    "& \\hat{y}_{i,l} =  \\frac{\\textstyle{\\text{exp}}(z_{i,l}^{(2)})}{\\displaystyle \\sum_l^4 \\text{exp}(z_{i,l}^{(2)})}\\\\\n",
    "\\\\\n",
    "\\hline \n",
    "\\\\\n",
    "\n",
    "Z^{(2)} = \n",
    "\\begin{bmatrix}\n",
    "z_{11}^{(2)} & z_{12}^{(2)} & z_{13}^{(2)} & z_{14}^{(2)} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "z_{n1}^{(2)} & z_{n2}^{(2)} & z_{n3}^{(2)} & z_{n4}^{(2)}  \n",
    "\\end{bmatrix}\n",
    "&\n",
    "[n \\times 4]\n",
    "&\n",
    "Z^{(2)} = H_1 W^{(2)} + C \n",
    "& z_{i,l}^{(2)} = \\displaystyle \\sum_k^2 h_{i,k}w_{k,l}^{(2)} + c_i \\\\\n",
    "\n",
    "\\\\\n",
    "\\hline\n",
    "\\\\\n",
    "\n",
    "H_1 = \n",
    "\\begin{bmatrix}\n",
    "h_{11} & h_{12}  \\\\\n",
    "\\vdots & \\vdots  \\\\\n",
    "h_{n1} & h_{n2}  \\\\\n",
    "\\end{bmatrix}\n",
    "&\n",
    "[n \\times 2]\n",
    "&\n",
    "H_1 = \\text{reLU}(Z^{(1)})\n",
    "&\n",
    "h_{i,k} = \n",
    "\\begin{cases}\n",
    "z_{i,k}^{(1)}   &\\text{if } z > 0 \\\\\n",
    "0   &\\text{else}\n",
    "\\end{cases} \\\\\n",
    "\n",
    "\\\\\n",
    "\\hline\n",
    "\\\\\n",
    "\n",
    "Z^{(1)} =\n",
    "\\begin{bmatrix}\n",
    "z_{11} & z_{12} & z_{13} & z_{14} \\\\\n",
    "z_{21} & z_{22} & z_{23} & z_{24} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "z_{n1} & z_{n2} & z_{n3} & z_{n4}\n",
    "\\end{bmatrix}\n",
    "&\n",
    "[n \\times 4]\n",
    "& \n",
    "Z^{(1)} =  X\\cdot W_1 + B\n",
    "&\n",
    "z_{i,k}^{(1)} = \\displaystyle \\sum_j^3 x_{i,j}w_{j,k}^{(1)} + b_i \\\\\n",
    "\n",
    "\\end{array}\n",
    "\\end{aligned}\n",
    "\n",
    "Reminder (to self) the following are specific to the architecture of this assignment: \n",
    "\n",
    "- $2$ comes from the number of units in the hidden layer\n",
    "- $3$ comes from the number of inputs/columns in $X$\n",
    "- $4$ comes from the number of possible labels/outputs in $y$ and $\\hat{y}$\n",
    "- ($n$ in this is supposed to be 6 for the dataset I created)\n",
    "\n",
    "**Other potentially useful multiplication notation to match with python.**\n",
    "\n",
    "I will try my best to match the multiplication notation as done in Professor Gates' lecture slides.\n",
    "\n",
    "Thus, I will use an \"o-dot\" ($\\odot$) to represent matrix multiplcation, (equivalent to `@` in Python) i.e.:\n",
    "\n",
    "\\begin{align*}\n",
    "A \\odot B = \n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22}\\\\\n",
    "\\end{bmatrix}\n",
    "\\odot\n",
    "\\begin{bmatrix}\n",
    "b_{11} & b_{12} \\\\\n",
    "b_{21} & b_{22} \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22} \\\\\n",
    "a_{21}b_{11} + a_{22}b_{21} & a_{21}b_{12} + a_{22}b_{22}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "And I will use an asterisk ($\\ast$) for element-wise (\"regular\") multiplication, (equivalent to `*` in Python) i.e.:\n",
    "\n",
    "\\begin{align*}\n",
    "A \\ast B = \n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22}\\\\\n",
    "\\end{bmatrix}\n",
    "\\ast\n",
    "\\begin{bmatrix}\n",
    "b_{11} & b_{12} \\\\\n",
    "b_{21} & b_{22} \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{11}b_{11} & a_{12}b_{12} \\\\\n",
    "a_{21}b_{21} & a_{22}b_{22}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Calculate Softmax results\n",
    "\n",
    "If your four $Z_2$ values are $1.1, 2.2, 0.2$ and $-1.7$, calculate (by hand and show your work) the four softmax results.\n",
    "\n",
    "(Yes you can use a calculator or Google for the math - but SHOW THE WORK and steps) If you show just the answer, you will get -50%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***JK:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Derivative of Softmax\n",
    "\n",
    "Write out the derivative of the Softmax for the case where $i=j$ and for the case $i$ does not equal $j$. Show all work and steps.\n",
    "\n",
    "Find and paste in the Jacobian for the derivative of the Softmax and assure that your results (for $i=j$ and $i\\neq j$) match the Jacobian values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***JK:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Combination\n",
    "\n",
    "When using the combination of Softmax, Categorical Cross Entropy, and One-Hot Encoding, what is $\\frac{\\partial L}{\\partial Z_2}$ (sometimes written as $\\frac{dL}{dz}$ or as \"Error\" on the slides)?\n",
    "\n",
    "(Just give the final answer here. You are not required to show work for this part.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***JK:***\n",
    "\n",
    "$\\frac{\\partial L}{\\partial Z_2} = ?$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Code\n",
    "\n",
    "In **(1)** above, you created a NN architecture for 3D data, a 4-category label (0,1,2,3), one hidden layer (with two units). Write code for this NN without using any NN packages (so by hand)\n",
    "\n",
    "- Use One-hot Encoding for the label.\n",
    "- For the Loss Fucntion, use Categorical Cross Entropy.\n",
    "- For the activation from $Z_1$ to $H_1$, use reLU.\n",
    "- Run it for 1000 epochs and include the same visualizations as were required in **Part 1 *(5)*** above.\n",
    "- **YOU** create the dataset and also submit the dataset with your submission.\n",
    "- Be sure to illustrate some of your results so that the viewer can see what you did.\n",
    "\n",
    "This code will help (a lot). Here again, I suggest that you try not to use my code :) Try to write this yourself and only use my code as a reference or if you must. ([Prof Gates' Code Example](https://gatesboltonanalytics.com/?page_id=707))\n",
    "\n",
    "(Note - my code uses Sigmoid - not reLU - so you will need to make some updates anyway)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
