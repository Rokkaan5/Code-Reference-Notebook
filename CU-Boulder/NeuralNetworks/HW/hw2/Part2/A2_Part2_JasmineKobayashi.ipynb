{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Module 2 Assignment - Part 2: Multinomial NN with Softmax, Categorical Cross Entropy, and One-Hot Encoding\"\n",
    "author: \"Jasmine Kobayashi\"\n",
    "format:\n",
    "    html:\n",
    "        code-fold: false\n",
    "execute:\n",
    "    output: true\n",
    "    warning: false\n",
    "toc: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module 2 Assignment - Part 2: Multinomial NN with Softmax, Categorical Cross Entropy, and One-Hot Encoding\n",
    "---\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "This assignment has a two large parts with many sub-parts. Each part will enable you to practice with a neural networks and to practice for Exam 2.\n",
    "\n",
    "This Assignment will TAKE TIME so use all of the time you have to work on it. Please avoid waiting until the last few days as this will not be enough time. You need weeks - not days :)\n",
    "\n",
    "Be sure to show all work and (when requested) to code using Python (and no NN/TF/Keras packages). Do not worry, we will use packages in coming modules but first it is best to learn about what is going on inside the model. \n",
    "\n",
    "You may use examples and code that I have shared as a reference. Using my code will not be considered cheating although I strongly recommend that you write as much (if not all) of your own code so that you learn the concepts more robustly. \n",
    "\n",
    "This is not a team assignment. Please work and code alone. You may discuss concepts, but you cannot share code or work with others. Papers that look too similar with split the grade (first offense) and can suffer less pleasant outcomes for further offenses. Keep it simple and smart - do your own work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Draw NN architecture\n",
    "In Part 1 above, I created (with an illustration) the NN architecture for you. In this part, you will illustrate (draw) the architecture. \n",
    "\n",
    "(a) Your NN architecture (draw this) will expect input vectors of three values (3D data). It will have one hidden layer with two units. It will create four outputs (for four categories), it will use softmax to convert those 4 outputs into a probability distribution, and it will choose the max as the prediction. \n",
    "\n",
    "(b) The hidden layer (H1) will be activated with reLU. \n",
    "\n",
    "Draw the NN.\n",
    "\n",
    "Use my slides and what we did during class lecture to help you. Label everything - including the x's, the W1's, the B, the C, the H1 values, the Z2 values, the y^ values (after softmax), and the final output. Again, the class slides have an exact example of this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***JK:*** *Check Word Document*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Write out all the Feed Forward equations\n",
    "\n",
    "You will have equations for $Z_1$, $H_1$, $Z_2$, $\\hat{Y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***JK:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) What are the following derivatives\n",
    "(using reLU to activate $Z_1$ to $H_1$, and using Softmax to activate $Z_2$ to $\\hat{Y}$)\n",
    "\n",
    "**Hint:** The slides show a very similar example. The only difference is that the slides use Sigmoid and not reLU. So you will need to update this part. Start by determining the derivative of the reLU.\n",
    "\n",
    "Use the Categorical Cross Entropy Loss function.\n",
    "\n",
    "Use One-Hot Encoding for $\\mathbf{y}$ (your label)\n",
    "\n",
    "Show all your work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***JK:***\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_1} = ?$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_2} = ?$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial B} = ?$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial C} = ?$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Calculate Softmax results\n",
    "\n",
    "If your four $Z_2$ values are $1.1, 2.2, 0.2$ and $-1.7$, calculate (by hand and show your work) the four softmax results.\n",
    "\n",
    "(Yes you can use a calculator or Google for the math - but SHOW THE WORK and steps) If you show just the answer, you will get -50%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***JK:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Derivative of Softmax\n",
    "\n",
    "Write out the derivative of the Softmax for the case where $i=j$ and for the case $i$ does not equal $j$. Show all work and steps.\n",
    "\n",
    "Find and paste in the Jacobian for the derivative of the Softmax and assure that your results (for $i=j$ and $i\\neq j$) match the Jacobian values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***JK:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Combination\n",
    "\n",
    "When using the combination of Softmax, Categorical Cross Entropy, and One-Hot Encoding, what is $\\frac{\\partial L}{\\partial Z_2}$ (sometimes written as $\\frac{dL}{dz}$ or as \"Error\" on the slides)?\n",
    "\n",
    "(Just give the final answer here. You are not required to show work for this part.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***JK:***\n",
    "\n",
    "$\\frac{\\partial L}{\\partial Z_2} = ?$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Code\n",
    "\n",
    "In **(1)** above, you created a NN architecture for 3D data, a 4-category label (0,1,2,3), one hidden layer (with two units). Write code for this NN without using any NN packages (so by hand)\n",
    "\n",
    "- Use One-hot Encoding for the label.\n",
    "- For the Loss Fucntion, use Categorical Cross Entropy.\n",
    "- For the activation from $Z_1$ to $H_1$, use reLU.\n",
    "- Run it for 1000 epochs and include the same visualizations as were required in **Part 1 *(5)*** above.\n",
    "- **YOU** create the dataset and also submit the dataset with your submission.\n",
    "- Be sure to illustrate some of your results so that the viewer can see what you did.\n",
    "\n",
    "This code will help (a lot). Here again, I suggest that you try not to use my code :) Try to write this yourself and only use my code as a reference or if you must. ([Prof Gates' Code Example](https://gatesboltonanalytics.com/?page_id=707))\n",
    "\n",
    "(Note - my code uses Sigmoid - not reLU - so you will need to make some updates anyway)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
