{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Module 3 Assignment - Part 3: Use TF/Keras to Create RNN and LSTM and Compare Results\"\n",
    "author: Jasmine Kobayashi\n",
    "format:\n",
    "  html:\n",
    "    code-fold: false\n",
    "execute:\n",
    "  output: true\n",
    "toc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module 3 Assignment - Part 4: Use TF/Keras to Create both an RNN and an LSTM and Compare Results\n",
    "---\n",
    "\n",
    "For this part, you can choose any dataset you like. I recommend choosing text data (such as movie reviews: <https://keras.io/api/datasets/imdb/>) and predicting sentiment. However, I will allow you to select any sequential dataset you like as long as it is appropriate for RNNs.\n",
    "\n",
    "Use TF/Keras to create two models. The first will be a simple RNN. The second will be a LSTM model.\n",
    "\n",
    "There are great examples online that you can review and learn from (but not copy :)). You can also have a look at one of my examples that compares ANN, CNN, RNN, and LSTM: <https://gatesboltonanalytics.com/?page_id=903>\n",
    "\n",
    "Just as in Parts 1 and 2 (I will not repeat the requirements again here - but please assume them) you will illustrate your process and results. You will also discuss and illustrate a comparison between the RNN and the LSTM models.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "I started messing around with the movie data, but ended struggling for a while since I'm less familar about working with text data, etc. So instead, I'm going to be using data from my undergrad research work, which is a time series data: NASA's [OMNI dataset](https://omniweb.gsfc.nasa.gov/).\n",
    "\n",
    "However, one thing to note. The problem I tackle with this data is a **regression problem not a classification problem**. So I won't be outputting a confusion matrix, but I will be sure to use another form of performance evaluation visualization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "If I submitted as I intended, the csv data should've been included in my submission (titled \"`omni_df.csv`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "omni_df = pd.read_csv('omni_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>BX_GSE</th>\n",
       "      <th>BY_GSE</th>\n",
       "      <th>BZ_GSE</th>\n",
       "      <th>Vx</th>\n",
       "      <th>Vy</th>\n",
       "      <th>Vz</th>\n",
       "      <th>proton_density</th>\n",
       "      <th>T</th>\n",
       "      <th>AE_INDEX</th>\n",
       "      <th>AL_INDEX</th>\n",
       "      <th>AU_INDEX</th>\n",
       "      <th>SYM_H</th>\n",
       "      <th>ASY_H</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-01 00:00:00</td>\n",
       "      <td>-5.94</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>668.0</td>\n",
       "      <td>-486.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>-44.0</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-01-01 00:01:00</td>\n",
       "      <td>-5.88</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.53</td>\n",
       "      <td>-662.6</td>\n",
       "      <td>7.3</td>\n",
       "      <td>-46.5</td>\n",
       "      <td>3.12</td>\n",
       "      <td>343841.0</td>\n",
       "      <td>638.0</td>\n",
       "      <td>-487.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>-45.0</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-01-01 00:02:00</td>\n",
       "      <td>-5.71</td>\n",
       "      <td>3.23</td>\n",
       "      <td>1.44</td>\n",
       "      <td>-661.4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>-46.3</td>\n",
       "      <td>3.24</td>\n",
       "      <td>326583.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>-527.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>-45.0</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-01-01 00:03:00</td>\n",
       "      <td>-5.33</td>\n",
       "      <td>3.80</td>\n",
       "      <td>1.84</td>\n",
       "      <td>-659.8</td>\n",
       "      <td>-8.4</td>\n",
       "      <td>-56.2</td>\n",
       "      <td>3.11</td>\n",
       "      <td>306470.0</td>\n",
       "      <td>615.0</td>\n",
       "      <td>-474.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>-45.0</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-01-01 00:04:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>554.0</td>\n",
       "      <td>-418.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>-45.0</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Epoch  BX_GSE  BY_GSE  BZ_GSE     Vx   Vy    Vz  \\\n",
       "0  2000-01-01 00:00:00   -5.94    0.24   -0.15    NaN  NaN   NaN   \n",
       "1  2000-01-01 00:01:00   -5.88    2.17    0.53 -662.6  7.3 -46.5   \n",
       "2  2000-01-01 00:02:00   -5.71    3.23    1.44 -661.4  2.4 -46.3   \n",
       "3  2000-01-01 00:03:00   -5.33    3.80    1.84 -659.8 -8.4 -56.2   \n",
       "4  2000-01-01 00:04:00     NaN     NaN     NaN    NaN  NaN   NaN   \n",
       "\n",
       "   proton_density         T  AE_INDEX  AL_INDEX  AU_INDEX  SYM_H  ASY_H  \n",
       "0             NaN       NaN     668.0    -486.0     182.0  -44.0   48.0  \n",
       "1            3.12  343841.0     638.0    -487.0     151.0  -45.0   48.0  \n",
       "2            3.24  326583.0     666.0    -527.0     139.0  -45.0   48.0  \n",
       "3            3.11  306470.0     615.0    -474.0     141.0  -45.0   48.0  \n",
       "4             NaN       NaN     554.0    -418.0     136.0  -45.0   47.0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omni_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "### Data columns and their meanings:\n",
    "\n",
    "**JK Note to TAs:** It's not crucial to this work for you to understand these for the purposes of this assignment, but just in case it might be helpful. \n",
    "\n",
    "Can also refer to their [about OMNI data page](https://omniweb.gsfc.nasa.gov/html/ow_data.html)\n",
    "\n",
    "Essentially this is all solar wind data\n",
    "\n",
    "- `Epoch`: Date and Time (datetime index)\n",
    "- Interplanetary Magnetic Field (units = nano-Teslas (nT))\n",
    "    - `BX_GSE`: x-component of Magnetic Field in [GSE coordinate system](https://www.spenvis.oma.be/help/background/coortran/coortran.html#GSE) \n",
    "    - `BY_GSE`: y-component of Magnetic Field in GSE coordinate system\n",
    "    - `BX_GSE`: z-component of Magnetic Field in GSE coordinate system\n",
    "- Solar Wind velocity (km/sec)\n",
    "    - `Vx`: Solar wind velocity in x\n",
    "    - `Vy`: Solar wind velocity in y\n",
    "    - `Vz`: Solar wind velocity in z\n",
    "- `proton_density`: Proton density in the solar wind (n/cc)\n",
    "- `T`: Temperature (K)\n",
    "- Geomagnetic Indices \n",
    "    - all units = nano Teslas (nT)\n",
    "    - [Auroral Electrojet](https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ngdc.stp.indices:G00584#:~:text=The%20AU%20and%20AL%20indices%20are%20intended%20to%20express%20the,Show%20more...)\n",
    "        - `AE_INDEX`: Auroral Electrojet Index (AE = AU-AL)\n",
    "        - `AU_INDEX`: Auroral Electrojet upper\n",
    "        - `AL_INDEX`: Auroral Electrojet lower\n",
    "    - [SYM/ASY H](https://wdc.kugi.kyoto-u.ac.jp/aeasy/asy.pdf)\n",
    "        - `SYM_H`: Longitudinally symmetric disturbance of geomagnetic H-component\n",
    "            - Sym-H is the same is the [Dst Index](https://www.ngdc.noaa.gov/stp/geomag/dst.html), but at higher resolution\n",
    "        - `ASY_H`: Longitudinally asymmetric disturbance of geomagnetic H-component\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my purposes the features (predictor variables, model input, etc. however you want to call it) are:\n",
    "`BX_GSE`, `BY_GSE`, `BZ_GSE`, `AE_INDEX`, `AL_INDEX`, `AU_INDEX`, `SYM_H`, `ASY_H`\n",
    "\n",
    "And the targets (\"label\", the model output, etc.) are: `Vx`, `Vy`, `Vz`, `proton_density`, `T`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BX_GSE</th>\n",
       "      <th>BY_GSE</th>\n",
       "      <th>BZ_GSE</th>\n",
       "      <th>Vx</th>\n",
       "      <th>Vy</th>\n",
       "      <th>Vz</th>\n",
       "      <th>proton_density</th>\n",
       "      <th>T</th>\n",
       "      <th>AE_INDEX</th>\n",
       "      <th>AL_INDEX</th>\n",
       "      <th>AU_INDEX</th>\n",
       "      <th>SYM_H</th>\n",
       "      <th>ASY_H</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8.888632e+06</td>\n",
       "      <td>8.888632e+06</td>\n",
       "      <td>8.888632e+06</td>\n",
       "      <td>7.679877e+06</td>\n",
       "      <td>7.679877e+06</td>\n",
       "      <td>7.679877e+06</td>\n",
       "      <td>7.679877e+06</td>\n",
       "      <td>7.673164e+06</td>\n",
       "      <td>9.552960e+06</td>\n",
       "      <td>9.552960e+06</td>\n",
       "      <td>9.552960e+06</td>\n",
       "      <td>9.552960e+06</td>\n",
       "      <td>9.552960e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.586364e-03</td>\n",
       "      <td>5.997560e-02</td>\n",
       "      <td>2.418269e-03</td>\n",
       "      <td>-4.353644e+02</td>\n",
       "      <td>-1.359368e+00</td>\n",
       "      <td>-2.238432e+00</td>\n",
       "      <td>5.999525e+00</td>\n",
       "      <td>9.880642e+04</td>\n",
       "      <td>1.792767e+02</td>\n",
       "      <td>-1.114364e+02</td>\n",
       "      <td>6.784032e+01</td>\n",
       "      <td>-1.203162e+01</td>\n",
       "      <td>2.011680e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.660013e+00</td>\n",
       "      <td>4.227383e+00</td>\n",
       "      <td>3.395529e+00</td>\n",
       "      <td>1.053745e+02</td>\n",
       "      <td>2.552232e+01</td>\n",
       "      <td>2.266174e+01</td>\n",
       "      <td>4.988173e+00</td>\n",
       "      <td>9.744486e+04</td>\n",
       "      <td>2.102548e+02</td>\n",
       "      <td>1.550540e+02</td>\n",
       "      <td>7.383967e+01</td>\n",
       "      <td>1.971669e+01</td>\n",
       "      <td>1.593993e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-4.951000e+01</td>\n",
       "      <td>-5.109000e+01</td>\n",
       "      <td>-6.064000e+01</td>\n",
       "      <td>-1.136500e+03</td>\n",
       "      <td>-8.649000e+02</td>\n",
       "      <td>-3.104000e+02</td>\n",
       "      <td>3.000000e-02</td>\n",
       "      <td>5.430000e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-4.141000e+03</td>\n",
       "      <td>-9.750000e+02</td>\n",
       "      <td>-4.900000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-2.650000e+00</td>\n",
       "      <td>-2.670000e+00</td>\n",
       "      <td>-1.730000e+00</td>\n",
       "      <td>-4.948000e+02</td>\n",
       "      <td>-1.630000e+01</td>\n",
       "      <td>-1.490000e+01</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.551900e+04</td>\n",
       "      <td>4.200000e+01</td>\n",
       "      <td>-1.470000e+02</td>\n",
       "      <td>1.900000e+01</td>\n",
       "      <td>-1.900000e+01</td>\n",
       "      <td>1.100000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-4.126000e+02</td>\n",
       "      <td>-3.500000e+00</td>\n",
       "      <td>-2.100000e+00</td>\n",
       "      <td>4.600000e+00</td>\n",
       "      <td>7.002000e+04</td>\n",
       "      <td>9.300000e+01</td>\n",
       "      <td>-4.500000e+01</td>\n",
       "      <td>4.100000e+01</td>\n",
       "      <td>-9.000000e+00</td>\n",
       "      <td>1.600000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.630000e+00</td>\n",
       "      <td>2.760000e+00</td>\n",
       "      <td>1.730000e+00</td>\n",
       "      <td>-3.556000e+02</td>\n",
       "      <td>1.140000e+01</td>\n",
       "      <td>1.020000e+01</td>\n",
       "      <td>7.240000e+00</td>\n",
       "      <td>1.308720e+05</td>\n",
       "      <td>2.400000e+02</td>\n",
       "      <td>-1.800000e+01</td>\n",
       "      <td>9.000000e+01</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>2.400000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.995000e+01</td>\n",
       "      <td>5.509000e+01</td>\n",
       "      <td>6.438000e+01</td>\n",
       "      <td>-9.340000e+01</td>\n",
       "      <td>3.291000e+02</td>\n",
       "      <td>4.516000e+02</td>\n",
       "      <td>8.095000e+01</td>\n",
       "      <td>7.617765e+06</td>\n",
       "      <td>4.192000e+03</td>\n",
       "      <td>7.900000e+01</td>\n",
       "      <td>2.063000e+03</td>\n",
       "      <td>1.510000e+02</td>\n",
       "      <td>9.840000e+02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             BX_GSE        BY_GSE        BZ_GSE            Vx            Vy  \\\n",
       "count  8.888632e+06  8.888632e+06  8.888632e+06  7.679877e+06  7.679877e+06   \n",
       "mean   2.586364e-03  5.997560e-02  2.418269e-03 -4.353644e+02 -1.359368e+00   \n",
       "std    3.660013e+00  4.227383e+00  3.395529e+00  1.053745e+02  2.552232e+01   \n",
       "min   -4.951000e+01 -5.109000e+01 -6.064000e+01 -1.136500e+03 -8.649000e+02   \n",
       "25%   -2.650000e+00 -2.670000e+00 -1.730000e+00 -4.948000e+02 -1.630000e+01   \n",
       "50%    0.000000e+00  2.000000e-02  0.000000e+00 -4.126000e+02 -3.500000e+00   \n",
       "75%    2.630000e+00  2.760000e+00  1.730000e+00 -3.556000e+02  1.140000e+01   \n",
       "max    3.995000e+01  5.509000e+01  6.438000e+01 -9.340000e+01  3.291000e+02   \n",
       "\n",
       "                 Vz  proton_density             T      AE_INDEX      AL_INDEX  \\\n",
       "count  7.679877e+06    7.679877e+06  7.673164e+06  9.552960e+06  9.552960e+06   \n",
       "mean  -2.238432e+00    5.999525e+00  9.880642e+04  1.792767e+02 -1.114364e+02   \n",
       "std    2.266174e+01    4.988173e+00  9.744486e+04  2.102548e+02  1.550540e+02   \n",
       "min   -3.104000e+02    3.000000e-02  5.430000e+02  1.000000e+00 -4.141000e+03   \n",
       "25%   -1.490000e+01    3.000000e+00  3.551900e+04  4.200000e+01 -1.470000e+02   \n",
       "50%   -2.100000e+00    4.600000e+00  7.002000e+04  9.300000e+01 -4.500000e+01   \n",
       "75%    1.020000e+01    7.240000e+00  1.308720e+05  2.400000e+02 -1.800000e+01   \n",
       "max    4.516000e+02    8.095000e+01  7.617765e+06  4.192000e+03  7.900000e+01   \n",
       "\n",
       "           AU_INDEX         SYM_H         ASY_H  \n",
       "count  9.552960e+06  9.552960e+06  9.552960e+06  \n",
       "mean   6.784032e+01 -1.203162e+01  2.011680e+01  \n",
       "std    7.383967e+01  1.971669e+01  1.593993e+01  \n",
       "min   -9.750000e+02 -4.900000e+02  0.000000e+00  \n",
       "25%    1.900000e+01 -1.900000e+01  1.100000e+01  \n",
       "50%    4.100000e+01 -9.000000e+00  1.600000e+01  \n",
       "75%    9.000000e+01 -1.000000e+00  2.400000e+01  \n",
       "max    2.063000e+03  1.510000e+02  9.840000e+02  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omni_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch                   0\n",
       "BX_GSE             664328\n",
       "BY_GSE             664328\n",
       "BZ_GSE             664328\n",
       "Vx                1873083\n",
       "Vy                1873083\n",
       "Vz                1873083\n",
       "proton_density    1873083\n",
       "T                 1879796\n",
       "AE_INDEX                0\n",
       "AL_INDEX                0\n",
       "AU_INDEX                0\n",
       "SYM_H                   0\n",
       "ASY_H                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of missing values in each column\n",
    "omni_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data clearly has some missing values, so we'll use the I'll combat by interpolating the input parameters, and dropping the other rows with missing target values. Can do so with the `interpolate_input()` function I defined below.\n",
    "\n",
    "(The following is the same code I used to clean my data for work.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_input(df,features,targets,interpolation_method = 'linear',has_time_col=True,\n",
    "                      drop_target_nan=True,includes_TH=False,th_len=15,include_target_th = False):\n",
    "    \n",
    "    #Creating X and y df from Time History data th_df (if applicable)-----------------------------------------------\n",
    "    if includes_TH:\n",
    "        X_col = []\n",
    "        y_col =[]\n",
    "        \n",
    "        for f in features:\n",
    "            for i in range(0,th_len+1):\n",
    "                X_col.append(f+'_m{}'.format(i))\n",
    "        if include_target_th:\n",
    "            for tt in targets:\n",
    "                for j in range(1,th_len+1):\n",
    "                    X_col.append(tt+'_m{}'.format(j))\n",
    "        for tt in targets:\n",
    "            y_col.append(tt+'_m0')\n",
    "            \n",
    "        features = X_col\n",
    "        targets =y_col\n",
    "    #---------------------------------------------------------------------------------------------------------------    \n",
    "    \n",
    "    \n",
    "    #separate df into features only & targets only dataframes========================================================\n",
    "    \n",
    "    feat_df = df[features].astype('float32')\n",
    "    targ_df = df[targets].astype('float32')\n",
    "    if has_time_col:\n",
    "        if includes_TH:\n",
    "            time = df['time']\n",
    "        else:\n",
    "            time = df['Epoch']      #may want to change this so it doesn't only for dfs with column = 'Epoch'\n",
    "    \n",
    "    #interpolate features-only dataframe==============================================================================\n",
    "    interpolated_feat_df = feat_df.interpolate(interpolation_method)    #if method not specified when function \n",
    "                                                                        # is passed, defaults to linear interpolation\n",
    "    #combine df's together again=====================================================================================\n",
    "    if has_time_col:\n",
    "        concat_df = pd.concat([time,interpolated_feat_df,targ_df],axis=1)\n",
    "    else:\n",
    "        concat_df = pd.concat([interpolated_feat_df,targ_df],axis=1)\n",
    "        \n",
    "    print(\"New df with interpolated input created\")\n",
    "    # TO drop or NOT to drop... the nan of target parameters---------------------------------------------------------\n",
    "    if drop_target_nan:\n",
    "        new_df = concat_df.dropna()\n",
    "        print(\"Target Nan's were dropped (no missing values in df;can be used for model training & testing)\")\n",
    "    else:\n",
    "        new_df = concat_df\n",
    "        print(\"Target NaN's were NOT dropped (dataframe still contains missing values)\")\n",
    "    \n",
    "        \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['BX_GSE','BY_GSE','BZ_GSE','AE_INDEX','AL_INDEX','AU_INDEX','SYM_H','ASY_H']\n",
    "targets = ['Vx','Vy','Vz','proton_density','T']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New df with interpolated input created\n",
      "Target Nan's were dropped (no missing values in df;can be used for model training & testing)\n"
     ]
    }
   ],
   "source": [
    "df = interpolate_input(omni_df,features=features,\n",
    "                  targets=targets, \n",
    "                  has_time_col=True,\n",
    "                  drop_target_nan=True,\n",
    "                  includes_TH=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch             0\n",
       "BX_GSE            0\n",
       "BY_GSE            0\n",
       "BZ_GSE            0\n",
       "AE_INDEX          0\n",
       "AL_INDEX          0\n",
       "AU_INDEX          0\n",
       "SYM_H             0\n",
       "ASY_H             0\n",
       "Vx                0\n",
       "Vy                0\n",
       "Vz                0\n",
       "proton_density    0\n",
       "T                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viola! No more missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing sets\n",
    "\n",
    "The following is also code directly from my work code (so there might be some uneccessary things)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_dataframes(df,features,targets,split_type='random',test_size=0.2,scale=None,\n",
    "                          includes_TH=False,th_len=15,include_target_th = False):\n",
    "    \n",
    "    #Creating X and y df from Time History data th_df (if applicable)-----------------------------------------------\n",
    "    if includes_TH:\n",
    "        X_col = []\n",
    "        y_col =[]\n",
    "        \n",
    "        for f in features:\n",
    "            for i in range(0,th_len+1):\n",
    "                X_col.append(f+'_m{}'.format(i))\n",
    "        if include_target_th:\n",
    "            for tt in targets:\n",
    "                for j in range(1,th_len+1):\n",
    "                    X_col.append(tt+'_m{}'.format(j))\n",
    "        for tt in targets:\n",
    "            y_col.append(tt+'_m0')\n",
    "    else:\n",
    "        X_col = features\n",
    "        y_col = targets\n",
    "    #--------------------------------------------------------------------------------------------------------------- \n",
    "    \n",
    "    X = df[X_col].astype('float32')\n",
    "    y = df[y_col].astype('float32')\n",
    "    \n",
    "    # Training-Test Split=====================================================\n",
    "    #(Random) Train-Test Split-----------------------------------------------\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size= test_size,random_state=123)\n",
    "    split = 'Random split'\n",
    "        \n",
    "    # (Sequential) Train-Test Split------------------------------------------\n",
    "    if split_type == 'sequential':\n",
    "    \n",
    "        train_size = 1-test_size\n",
    "        \n",
    "        X_train = X[:int(train_size*len(X))]   #values up to % indicated by train_size\n",
    "        X_test = X[int(train_size*len(X)):]    #remaining sequential values from X\n",
    "        y_train = y[:int(train_size*len(y))]\n",
    "        y_test = y[int(train_size*len(y)):]\n",
    "        \n",
    "        split = 'Sequential split'\n",
    "    # Print split type--------------------------------------------------------\n",
    "    print('Train-Test split was:', split)\n",
    "    \n",
    "    #Scaling==================================================================\n",
    "    if scale is not None:\n",
    "        scaler = scale\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        print('X_train & X_test have been scaled using {}'.format(str(scale)))\n",
    "    else:\n",
    "        print('X_train & X_test are not scaled')\n",
    "        \n",
    "    print('Dataframes are complete')  \n",
    "    \n",
    "    return X_train,X_test,y_train,y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Test split was: Sequential split\n",
      "X_train & X_test have been scaled using StandardScaler()\n",
      "Dataframes are complete\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_dataframes(df,\n",
    "                      features=features,\n",
    "                      targets=targets,\n",
    "                      split_type='sequential',\n",
    "                      test_size=0.2,\n",
    "                      scale=StandardScaler(),\n",
    "                      includes_TH=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (6734530, 8)\n",
      "y_train shape: (6734530, 5)\n",
      "X_test shape: (1683633, 8)\n",
      "y_test shape: (1683633, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\",X_train.shape)\n",
    "print(\"y_train shape:\",y_train.shape)\n",
    "print(\"X_test shape:\",X_test.shape)\n",
    "print(\"y_test shape:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 19:55:56.289532: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: UNKNOWN ERROR (100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 5)           40        \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 50)                2800      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,891\n",
      "Trainable params: 2,891\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "RNN_Model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Embedding(input_dim=8,output_dim=5),\n",
    "  tf.keras.layers.SimpleRNN(units =50),\n",
    "  ## If not using Embedding, you would use SimpleRNN(units, input_shape=(x,y))\n",
    "])\n",
    "\n",
    "RNN_Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = keras.losses.MeanSquaredError\n",
    "RNN_Model.compile(\n",
    "                 loss=loss_function,\n",
    "                 metrics=[\"accuracy\"],\n",
    "                 optimizer='adam'\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/home/jasminekobayashi/anaconda3/envs/code-ref-notebook/lib/python3.11/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/jasminekobayashi/anaconda3/envs/code-ref-notebook/lib/python3.11/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/jasminekobayashi/anaconda3/envs/code-ref-notebook/lib/python3.11/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/jasminekobayashi/anaconda3/envs/code-ref-notebook/lib/python3.11/site-packages/keras/engine/training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/jasminekobayashi/anaconda3/envs/code-ref-notebook/lib/python3.11/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/home/jasminekobayashi/anaconda3/envs/code-ref-notebook/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/jasminekobayashi/anaconda3/envs/code-ref-notebook/lib/python3.11/site-packages/keras/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/jasminekobayashi/anaconda3/envs/code-ref-notebook/lib/python3.11/site-packages/keras/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/jasminekobayashi/anaconda3/envs/code-ref-notebook/lib/python3.11/site-packages/keras/losses.py\", line 358, in __init__  **\n        super().__init__(mean_squared_error, name=name, reduction=reduction)\n    File \"/home/jasminekobayashi/anaconda3/envs/code-ref-notebook/lib/python3.11/site-packages/keras/losses.py\", line 246, in __init__\n        super().__init__(reduction=reduction, name=name)\n    File \"/home/jasminekobayashi/anaconda3/envs/code-ref-notebook/lib/python3.11/site-packages/keras/losses.py\", line 83, in __init__\n        losses_utils.ReductionV2.validate(reduction)\n    File \"/home/jasminekobayashi/anaconda3/envs/code-ref-notebook/lib/python3.11/site-packages/keras/utils/losses_utils.py\", line 87, in validate\n        if key not in cls.all():\n\n    TypeError: Expected float32 passed to parameter 'y' of op 'Equal', got 'auto' of type 'str' instead. Error: Expected float32, but got auto of type 'str'.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/code-ref-notebook/lib/python3.11/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileyuo2_xrd.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/home/jasminekobayashi/anaconda3/envs/code-ref-notebook/lib/python3.11/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/jasminekobayashi/anaconda3/envs/code-ref-notebook/lib/python3.11/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/jasminekobayashi/anaconda3/envs/code-ref-notebook/lib/python3.11/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/jasminekobayashi/anaconda3/envs/code-ref-notebook/lib/python3.11/site-packages/keras/engine/training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/jasminekobayashi/anaconda3/envs/code-ref-notebook/lib/python3.11/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/home/jasminekobayashi/anaconda3/envs/code-ref-notebook/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/jasminekobayashi/anaconda3/envs/code-ref-notebook/lib/python3.11/site-packages/keras/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/jasminekobayashi/anaconda3/envs/code-ref-notebook/lib/python3.11/site-packages/keras/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/jasminekobayashi/anaconda3/envs/code-ref-notebook/lib/python3.11/site-packages/keras/losses.py\", line 358, in __init__  **\n        super().__init__(mean_squared_error, name=name, reduction=reduction)\n    File \"/home/jasminekobayashi/anaconda3/envs/code-ref-notebook/lib/python3.11/site-packages/keras/losses.py\", line 246, in __init__\n        super().__init__(reduction=reduction, name=name)\n    File \"/home/jasminekobayashi/anaconda3/envs/code-ref-notebook/lib/python3.11/site-packages/keras/losses.py\", line 83, in __init__\n        losses_utils.ReductionV2.validate(reduction)\n    File \"/home/jasminekobayashi/anaconda3/envs/code-ref-notebook/lib/python3.11/site-packages/keras/utils/losses_utils.py\", line 87, in validate\n        if key not in cls.all():\n\n    TypeError: Expected float32 passed to parameter 'y' of op 'Equal', got 'auto' of type 'str' instead. Error: Expected float32, but got auto of type 'str'.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Hist=RNN_Model.fit(X_train,y_train, epochs=5, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-ref-notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
