{"cells":[{"cell_type":"raw","metadata":{},"source":["---\n","title: \"HW6: Part A - Reasoning\"\n","author: Jasmine Kobayashi\n","format:\n","  html:\n","    code-fold: false\n","execute:\n","  output: true\n","toc: true\n","---"]},{"cell_type":"markdown","metadata":{"cell_id":"a064626745b049329a3f1caa4ef218fe","deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":["Answer these questions as you were asked during an interview: Don't talk too less, and don't talk too much!"]},{"cell_type":"markdown","metadata":{"cell_id":"cb5f196b74274ea5b9f83f890fb85a2a","deepnote_cell_type":"text-cell-h2","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":["# Question 1\n","\n","**Can Polynomial regression be compared with multiple regression? Why and why not?**"]},{"cell_type":"markdown","metadata":{"cell_id":"a95978aeb24645ef98759b694c054c64","deepnote_cell_type":"text-cell-p","formattedRanges":[{"fromCodePoint":70,"marks":{"italic":true},"toCodePoint":81,"type":"marks"}],"is_collapsed":false,"tags":[]},"source":["From what I understand, in some sense, yes, they can be compared. But technically, it seems like Polynomial Regression is a type of multiple (linear) regression, so it seems a bit redundant to compare them. "]},{"cell_type":"markdown","metadata":{"cell_id":"dd94ab34-a5a0-4435-827d-67151d7faebb","deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":["..."]},{"cell_type":"markdown","metadata":{"cell_id":"3b13b7cca09d455fada56c0affa74be9","deepnote_cell_type":"text-cell-h2","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":["# Question 2\n","\n","**Explain the difference between MAE and MSE, and provide scenarios in which one is better than the other.**"]},{"cell_type":"markdown","metadata":{"cell_id":"d89e1072ed8c4315aff202965a161c8a","deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":["MAE stands for Mean Average Error (I've also seen it as Mean Absolute Error), and MSE stands for Mean Squared Error. "]},{"cell_type":"markdown","metadata":{"cell_id":"f1a94c47-e7fa-446e-9228-a304ce84c5df","deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":["Both are essentially ways to measure the distance between two vectors (as a measurement of error, the two vectors would include a vector of actual values and a vector of predicted values). "]},{"cell_type":"markdown","metadata":{"cell_id":"ef91762f-6b8a-41c4-9ebe-ebd7e32ce0be","deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":["Since the mean squared error is a sum of square differences, MSE is associated with the well-known euclidean distance, which measures (shortest) distance using a direct straight \"line.\" Or more specifically, RMSE (root mean squared error, which is just the square root of MSE) can be thought of that way, and sometimes more popular because the units of RMSE are consistent with the original data. Because of the nature of squaring the errors, MSE can be known to be more sensitive to and amplify errors from outliers. So MSE may be a better metric to use if we don't want to amplify errors from potential outliers. "]},{"cell_type":"markdown","metadata":{"cell_id":"b06c4ee3-1999-4b71-83a6-e0f00b774a68","deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":["MAE is the sum of the absolute value of differences, thus can be considered to be associated with the L1 norm, or manhattan distance metric, which measures the distance between vectors along a path parallel to the axis of each dimension. (Manhattan distance got its name from being compared to finding the distance between two locations when walking along the city blocks of Manhattan as opposed to a direct line.) Since MAE doesn't square the error values, it's less sensitive to the error measurements from outliers and thus can be considered a potentially better alternative to MSE if there are possible outliers. "]},{"cell_type":"markdown","metadata":{"cell_id":"1c423a35a02e4698a328b068a8c3b527","deepnote_cell_type":"text-cell-h2","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":["# Question 3\n","\n","**What is $R^2$? Why pursuing $R^2$ may lead to overfitting?**"]},{"cell_type":"markdown","metadata":{"cell_id":"4c106ae165584571938ce4707d0c7b07","deepnote_cell_type":"markdown","tags":[]},"source":["$R^2$ is the squared value of the pearson correlation coefficient. It is often used to measure how well points fit a regression line (linear or otherwise). Pursuing $R^2$ has the potential to lead to overfitting because a $R^2$ score = 1 is associated with a \"perfect\" fit to a line. Thus too much focus on obtaining an $R^2$ score = 1 means obtaining a model perfectly fit to the data the model was trained on. (This is why $R^2$ is best considered in *addition* to other error/accuracy metrics like MAE and/or MSE.)"]}],"metadata":{"deepnote_execution_queue":[],"deepnote_notebook_id":"dd9a8af69c614d77b75c86d25f83a5df","language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
