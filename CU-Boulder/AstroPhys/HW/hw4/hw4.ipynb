{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"ASTR 5550: HW4\"\n",
    "author: \"Jasmine Kobayashi\"\n",
    "format:\n",
    "    html:\n",
    "        code-fold: false\n",
    "        toc: true\n",
    "    pdf:\n",
    "        code-fold: false\n",
    "        code-overflow: wrap\n",
    "        papersize: letter\n",
    "execute:\n",
    "    output: true\n",
    "    warning: false\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import os,sys\n",
    "\n",
    "# import helper script file\n",
    "#(change working directory to two folders up)\n",
    "src_dir = os.path.abspath(\"\")       \n",
    "hw_dir = os.path.dirname(src_dir)\n",
    "os.chdir(hw_dir)\n",
    "\n",
    "# import my own code\n",
    "import hw_helper_func2 as hf   # this is my own code I made (for probability/distribution fucntions, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(**JK note:** To view the code with the functions I made myself to (hopefully) help with all assignments [click here](https://rokkaan5.github.io/Code-Reference-Notebook/CU-Boulder/AstroPhys/HW/hw_helper_func2.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Combining Poisson Distributions\n",
    "Given two Poisson distributions:\n",
    "$$P(x,\\mu_A) = \\frac{\\mu_A^x}{x!}e^{-\\mu_A} \\text{ and } P(x,\\mu_B) = \\frac{\\mu_B^x}{x!}e^{-\\mu_B}$$\n",
    "Show that they combine to a Poisson distribution:\n",
    "$$P(x,\\mu_C) \\text{ where } \\mu_C = \\mu_A + \\mu_B$$\n",
    "\n",
    "**Hint:** For any given integer $x$, the one must sum all possibilities of $P(i,\\mu_A)P(x-i,\\mu_B)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Supernova Light Curve\n",
    "After a supernova reaches its maximum brightness, the light curve exponentially decays as do the radioactive materials. The decay time can tell us its type. Examine the light curve below.\n",
    "\n",
    "$$I = [0.921, 0.704, 0.623, 0.550, 0.426, 0.332, 0.258, 0.208, 0.143, 0.130, 0.137, 0.103, 0.058, 0.070, 0.042, 0.060, 0.022, 0.022, 0.011, 0.015]$$\n",
    "$$ \\sigma =  [0.026, 0.048, 0.026, 0.027, 0.068, 0.046, 0.034, 0.017, 0.020, 0.014, 0.015, 0.009, 0.019, 0.010, 0.012, 0.018, 0.007, 0.008, 0.005, 0.005]$$\n",
    "\n",
    "![](light_curve.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = [0.921, 0.704, 0.623, 0.550, 0.426, 0.332, 0.258, 0.208, 0.143, 0.130, \n",
    "     0.137, 0.103, 0.058, 0.070, 0.042, 0.060, 0.022, 0.022, 0.011, 0.015]\n",
    "sigma =  [0.026, 0.048, 0.026, 0.027, 0.068, 0.046, 0.034, 0.017, 0.020, 0.014, \n",
    "          0.015, 0.009, 0.019, 0.010, 0.012, 0.018, 0.007, 0.008, 0.005, 0.005]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part(a)\n",
    "Assuming that $\\sigma$ represents a 1-sigma Gaussian uncertainty, find the most likely parameters under the hypothesis that the intensity undergoes an exponential decay:\n",
    "$$I = I_0 e^{-t/\\tau}$$\n",
    "\n",
    "Here, $\\tau$ is the decay time. As one can see, $I_0$ should be nearly unity but, for this problem, do not fix $I_0 = 1$. Calculate the uncertainty in $\\tau$. Plot the observations and the fit. \n",
    "\n",
    "**Hint:** One way is to perform a linear fit to $ln(I)$. Be careful how you treat the uncertainty $\\sigma$; Taylor expand $ln(I\\pm \\sigma)$ to calculate the uncertainties of $ln(I)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b)\n",
    "Calculate $\\chi_\\nu^2$ and compare it to the expected PDF/CDF of $\\chi_\\nu^2$. Plot your results. Is the hypothesis justified? What is the probability for $\\chi_\\nu^2$ to be above the calculated value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extra-Solar Planet\n",
    "The Kepler mission used the transit method in which one examines a time series of a star's intensity for a negative excursion. Under this method, the parent distribution of a star's intensity can be well established. In this example, the star's intensity is measured at a 30 sec cadence and found to be $I_0 + 0.001 I_0$ (1-sigma) with a Gaussian parent distribution.\n",
    "\n",
    "Finding a transit often involves several steps. The first step is to identify intervals that may have a transiting planet. One way is to examine one hour (120-point) stretches (sliding every half an hour, 60 points) for a non-constant distribution.\n",
    "\n",
    "Read in the text file, `HW4_data_A`, from Canvas. It contains 120 points of intensity in units of $I_0$, one every 30 seconds. Create a corresponding time array going from 0 to 3570 seconds. Assume the uncertainty in time is negligible.\n",
    "\n",
    "![](exoplanet.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.998088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.999086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0  0.998088\n",
       "1  1.000580\n",
       "2  1.000070\n",
       "3  1.000850\n",
       "4  0.999086"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_fwf(\"hw4/HW4_data.txt\", sep=\" \",header=None,)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.998088, 1.00058 , 1.00007 , 1.00085 , 0.999086, 1.00013 ,\n",
       "       1.00114 , 0.999925, 1.00042 , 1.00063 , 1.00064 , 0.999636,\n",
       "       1.00013 , 0.999742, 1.00007 , 1.00132 , 0.999664, 0.999608,\n",
       "       0.999392, 0.999136, 1.00104 , 1.00014 , 0.99976 , 1.0011  ,\n",
       "       0.999397, 0.99961 , 0.998294, 0.998431, 0.998846, 0.99919 ,\n",
       "       0.998581, 1.00151 , 0.999167, 1.00092 , 1.00044 , 0.999879,\n",
       "       1.00071 , 0.998279, 1.00031 , 0.999818, 0.998973, 1.00086 ,\n",
       "       0.994858, 0.997352, 0.99644 , 0.994283, 0.994575, 0.995698,\n",
       "       0.994824, 0.994713, 0.995482, 0.994736, 0.996792, 0.995706,\n",
       "       0.994752, 0.993211, 0.994558, 0.993956, 0.995004, 0.993613,\n",
       "       0.993948, 0.995872, 0.996478, 0.994574, 0.995362, 0.99529 ,\n",
       "       0.996243, 0.993112, 1.00004 , 0.99792 , 0.999602, 1.00021 ,\n",
       "       1.00079 , 0.999311, 1.00047 , 0.999404, 0.999304, 1.00148 ,\n",
       "       1.00072 , 1.00014 , 1.00301 , 1.00228 , 0.998727, 1.00173 ,\n",
       "       0.998629, 1.00067 , 1.00009 , 0.999671, 1.00097 , 1.0004  ,\n",
       "       1.00117 , 1.00081 , 0.999229, 0.998998, 0.999459, 1.00084 ,\n",
       "       0.999723, 1.00293 , 1.00184 , 0.998846, 1.00044 , 1.00008 ,\n",
       "       0.999624, 0.999991, 1.00027 , 0.99914 , 1.00034 , 1.00097 ,\n",
       "       1.00092 , 0.998008, 1.00049 , 0.999557, 0.998361, 0.997969,\n",
       "       0.999991, 1.00061 , 0.998795, 1.00178 , 0.999991, 1.00003 ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.loadtxt('hw4/HW4_data.txt')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (a)\n",
    "Start by eliminating the possibility that the negative excursion is a random fluctuation. Plot the PDF of the expected $\\chi_\\nu^2$ under the hypothesis that the intensity is constant. Calculate $\\chi_\\nu^2$ and compare to show that this event is **not** consistent with a constant intensity. What is the mean of the intensity ($I_\\mu$) and the uncertainty of the mean ($\\sigma_{I\\mu}$)? Is $I_\\mu$ less than 1 by more than the $\\sigma_{I\\mu}$?\n",
    "\n",
    "**Hint:** $\\sigma$ of the parent distribution is known ($0.001 I_0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b)\n",
    "Now that the interval is identified as significant and negative, let's examine and fit the negative excursion. Keeping it simple, use a three-parameter ($I_0, t_{start},t_{end}$) fit:\n",
    "\n",
    "\\begin{equation}\n",
    "I = \n",
    "\\begin{cases}\n",
    "I_0 - \\Delta I & t_{start} \\le t \\le t_{end}\\\\\n",
    "I_0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Do a least-squares fit with a method of choose. My method is to guess $t_{start}$ and $t_{end}$ then calculate $\\chi_\\nu^2$ along with $\\Delta I$. Increment $t_{start}$ and $t_{end}$ and recalculate $\\Delta I$ until  $\\chi_\\nu^2$ is minimum. Plot the data (with error bars if you can) and overplot your fit. What are $I_0, t_{start},$ and $t_{end}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (c)\n",
    "Estimate the uncertainties of $I_0, t_{start},$ and $t_{end}$. Explain how you arrive at your values. \n",
    "\n",
    "**Hint:** The uncertainty of $\\Delta I$ is straight-forward. Recall that you can calculate $\\sigma_I$, but $\\partial t/ \\partial I$ can only be estimated. Can one have an uncertainty in time that is less than $\\delta t$ (30 seconds)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Kolmogorov-Smirnov Test\n",
    "Using a random number generator, create two distributions:\n",
    "$$f_1(x) = P(x,\\mu_1,n); \\mu_1 = 8, n = 100$$\n",
    "$$f_2(x) = P(x,\\mu_2,n); \\mu_2 = 5, n = 100$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (a)\n",
    "Calculated and plot the two CDFs for $n=100$. Compare the two distributions using the Kolmogorov-Smirnov Test with $\\alpha=0.1$. The more exact formula for the threshold is: \n",
    "$$D > \\sqrt{-\\frac{1}{2} \\ln \\left( \\frac{\\alpha}{2}\\right)} \\sqrt{\\frac{n+m}{nm}}; n,m \\text{ are number of points}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b)\n",
    "Repeat the test several (5 to 10) times recreating the distributions. Do $f_1$ and $f_2$ consistently pass or fail the test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (c)\n",
    "Repeat the test for higher $n$, say 1000 (for both $f_1$ and $f_2$) several times. Does the test at $n=1000$ reveal that the two distributions are not from the same parent? What does this exercise tell us about the Kolmogorov-Smirnov Test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
